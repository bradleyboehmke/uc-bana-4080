{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12 Lab: The Professional ML Workflow\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/labs/12_wk12_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Welcome to Week 12! This week marks a turning point in your data science journey. Up until now, you've been learning the fundamentals of building machine learning models‚Äîand you've done great work. But this week, you're leveling up from \"beginner data scientist\" to \"production-ready professional.\"\n",
    "\n",
    "Here's the uncomfortable truth: **we've been breaking a fundamental rule of production machine learning**. Every time you compared different model settings or tuned hyperparameters based on test set performance, you were \"peeking\" at the test set‚Äîmaking your performance estimates less trustworthy. This week, you'll learn the **proper workflow** that ensures honest, reliable model performance.\n",
    "\n",
    "In this lab, you'll practice three critical professional skills: **cross-validation** (comparing models without contaminating your test set), **hyperparameter tuning** (systematically finding optimal model configurations), and **feature engineering** (transforming raw data into powerful model inputs). By the end, you'll be able to build models the right way‚Äîwith techniques that work in production, not just in the classroom.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Implement k-fold cross-validation to compare models without touching the test set\n",
    "- Use GridSearchCV to systematically tune hyperparameters across multiple parameters\n",
    "- Apply feature engineering techniques including encoding, scaling, and creating new features\n",
    "- Build end-to-end pipelines that prevent data leakage and ensure reproducible workflows\n",
    "- Execute the complete 5-stage professional ML workflow from data preparation through final evaluation\n",
    "\n",
    "## üìö This Lab Reinforces\n",
    "- **Chapter 28: Cross-Validation** ‚Äî K-fold CV, test set contamination, the proper 5-stage workflow\n",
    "- **Chapter 29: Hyperparameter Tuning** ‚Äî Bias-variance tradeoff, GridSearchCV, systematic parameter search\n",
    "- **Chapter 30: Feature Engineering** ‚Äî Encoding strategies, feature scaling, pipelines, preventing data leakage\n",
    "\n",
    "## üïê Estimated Time & Structure\n",
    "**Total Time:** 75 minutes  \n",
    "**Mode:** Group work (2-4 students)\n",
    "\n",
    "- **[0‚Äì30 min]** Part A: Guided Reinforcement ‚Äî TA-led practice with cross-validation, GridSearchCV, and feature engineering\n",
    "- **[30‚Äì40 min]** Class Q&A ‚Äî Discussion and clarification of key concepts\n",
    "- **[40‚Äì72 min]** Part B: Independent Challenges ‚Äî 6 group challenges applying the complete professional workflow\n",
    "- **[72‚Äì75 min]** Wrap-Up & Reflection ‚Äî What you learned and next steps\n",
    "\n",
    "You are encouraged to work in small groups of **2‚Äì4 students** and complete the lab together.\n",
    "\n",
    "## üí° Why This Matters\n",
    "These are the skills that separate junior data scientists from professionals who build production-ready models. When you interview for data science positions, employers will ask about cross-validation, hyperparameter tuning, and feature engineering‚Äîthey want to know you can build models that perform reliably in the real world, not just on classroom exercises.\n",
    "\n",
    "More importantly, these techniques ensure your models are trustworthy. When you tell a business stakeholder \"this model will achieve 85% accuracy in production,\" you need to be confident that estimate is honest and reliable. The professional workflow you'll practice today is what makes that confidence possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We'll use the familiar Ames housing dataset for today's lab. You've worked with this data throughout the course, which means you can focus on learning the new workflow techniques rather than getting familiar with unfamiliar data.\n",
    "\n",
    "Make sure you have the required libraries installed and the dataset accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Ames housing data\n",
    "ames = pd.read_csv('../data/ames_clean.csv')\n",
    "\n",
    "# Quick preview\n",
    "print(f\"Dataset shape: {ames.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "ames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A ‚Äî Guided Reinforcement (30 minutes)\n",
    "\n",
    "In this section, your TA will lead you through the key techniques step-by-step. Follow along, run the code, and ask questions as we go. This guided practice will prepare you for the independent challenges in Part B.\n",
    "\n",
    "### Section 1: Cross-Validation Basics (10 minutes)\n",
    "\n",
    "Let's start by understanding **why** cross-validation matters and **how** to implement it properly.\n",
    "\n",
    "**The Problem:** In previous weeks, you evaluated models on the test set multiple times to make decisions. Each \"peek\" at the test set made your performance estimates less trustworthy.\n",
    "\n",
    "**The Solution:** Cross-validation lets you compare models and tune hyperparameters using only the training data, keeping your test set pristine for final evaluation.\n",
    "\n",
    "**üìã Step-by-step instructions:**\n",
    "1. Split data into train/test sets (80/20 split)\n",
    "2. Select a subset of features for simplicity\n",
    "3. Use `cross_val_score()` to evaluate a model with 5-fold CV\n",
    "4. Interpret the CV scores (mean and standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create features and target\n",
    "# We'll use a few numerical features to start\n",
    "features = ['GrLivArea', 'YearBuilt', 'TotalBsmtSF', 'GarageCars', 'FullBath']\n",
    "X = ames[features]\n",
    "y = ames['SalePrice']\n",
    "\n",
    "# Step 2: Split into train and test (test set is LOCKED)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(\"\\nüîí Test set is now LOCKED. We won't touch it until the very end.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use cross-validation to evaluate a decision tree\n",
    "# Notice: We ONLY use X_train and y_train here!\n",
    "dt = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    dt, X_train, y_train, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Convert to positive RMSE\n",
    "cv_rmse = -cv_scores\n",
    "\n",
    "print(\"Cross-Validation Results (5 folds):\")\n",
    "print(f\"RMSE per fold: {cv_rmse}\")\n",
    "print(f\"\\nMean RMSE: ${cv_rmse.mean():,.0f}\")\n",
    "print(f\"Std Dev: ${cv_rmse.std():,.0f}\")\n",
    "print(\"\\nüí° The mean tells us expected performance, std dev tells us consistency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Your Turn ‚Äî Compare Two Models with Cross-Validation\n",
    "\n",
    "Now you try! Use cross-validation to compare a shallow decision tree (`max_depth=5`) against a deeper tree (`max_depth=15`).\n",
    "\n",
    "**Tasks:**\n",
    "- Create two DecisionTreeRegressor models with different max_depth values\n",
    "- Evaluate both using 5-fold cross-validation on the training data\n",
    "- Compare the mean RMSE for both models\n",
    "- Decide which model performs better based on CV scores (not test scores!)\n",
    "\n",
    "üí° **Hint:** Use the same pattern as above, just change the `max_depth` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Check Your Understanding\n",
    "\n",
    "**Questions to consider:**\n",
    "- Which model had better (lower) mean RMSE?\n",
    "- Which model had more consistent scores across folds (lower std dev)?\n",
    "- Why is comparing models with CV better than repeatedly checking the test set?\n",
    "\n",
    "**Expected Result:** You should see that one model generalizes better than the other, and you made this decision without ever touching the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Hyperparameter Tuning with GridSearchCV (10 minutes)\n",
    "\n",
    "Now let's automate the hyperparameter search process. Instead of manually trying different values one by one, GridSearchCV will systematically test all combinations and use cross-validation to find the best configuration.\n",
    "\n",
    "**Why this matters:** Manual tuning is tedious, error-prone, and doesn't explore the full parameter space. GridSearchCV does this systematically and reproducibly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Example: Tuning a Random Forest\n",
    "\n",
    "Let's tune a Random Forest model by searching over multiple hyperparameters simultaneously.\n",
    "\n",
    "**Example:** We'll search over `n_estimators`, `max_depth`, and `min_samples_split` to find the optimal combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit on training data (this will try 2 √ó 3 √ó 2 = 12 configs, each with 5-fold CV = 60 models!)\n",
    "print(\"Starting grid search... this may take a minute\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Grid Search Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV RMSE: ${-grid_search.best_score_:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Practice Exercise ‚Äî Tune a Decision Tree\n",
    "\n",
    "**Business Scenario:** You're building a decision tree model for your real estate company's pricing tool. You need to find the optimal complexity settings.\n",
    "\n",
    "**Your Task:** Use GridSearchCV to tune a DecisionTreeRegressor across these parameters:\n",
    "- `max_depth`: [5, 10, 15, 20]\n",
    "- `min_samples_split`: [2, 5, 10]\n",
    "- `min_samples_leaf`: [1, 2, 4]\n",
    "\n",
    "**Step-by-step approach:**\n",
    "1. Define the parameter grid as a dictionary\n",
    "2. Create a DecisionTreeRegressor\n",
    "3. Create GridSearchCV with cv=5\n",
    "4. Fit on X_train and y_train\n",
    "5. Print the best parameters and best CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Feature Engineering Essentials (10 minutes)\n",
    "\n",
    "Raw data often isn't in the best format for machine learning. Feature engineering transforms your data to help models learn better patterns. Let's practice the core techniques: encoding categorical variables and scaling numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Example: Encoding and Scaling\n",
    "\n",
    "Let's prepare features using proper encoding and scaling techniques.\n",
    "\n",
    "**Example:** We'll encode a categorical variable (Neighborhood) and scale numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a categorical feature to our feature set\n",
    "print(\"Original Neighborhood (categorical):\")\n",
    "print(ames['Neighborhood'].head())\n",
    "print(f\"\\nUnique neighborhoods: {ames['Neighborhood'].nunique()}\")\n",
    "\n",
    "# Label encode the neighborhood (converts to integers 0-27)\n",
    "le = LabelEncoder()\n",
    "ames['Neighborhood_Encoded'] = le.fit_transform(ames['Neighborhood'])\n",
    "\n",
    "print(\"\\nEncoded Neighborhood (numerical):\")\n",
    "print(ames[['Neighborhood', 'Neighborhood_Encoded']].head())\n",
    "print(\"\\n‚úÖ Categorical variable converted to numerical format!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's scale our numerical features\n",
    "# First, create new feature set with encoded neighborhood\n",
    "features_enhanced = ['GrLivArea', 'YearBuilt', 'TotalBsmtSF', 'GarageCars', 'FullBath', 'Neighborhood_Encoded']\n",
    "X_enhanced = ames[features_enhanced]\n",
    "\n",
    "# Split again with enhanced features\n",
    "X_train_enh, X_test_enh, y_train_enh, y_test_enh = train_test_split(\n",
    "    X_enhanced, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Before scaling (first 3 rows):\")\n",
    "print(X_train_enh.head(3))\n",
    "\n",
    "# Scale the features (fit on training data only!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_enh)\n",
    "X_test_scaled = scaler.transform(X_test_enh)  # Use training stats to transform test\n",
    "\n",
    "# Convert back to DataFrame for viewing\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features_enhanced)\n",
    "\n",
    "print(\"\\nAfter scaling (first 3 rows):\")\n",
    "print(X_train_scaled_df.head(3))\n",
    "print(\"\\nüí° Notice: All features now have similar scales (mean‚âà0, std‚âà1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Your Turn ‚Äî Build a Pipeline\n",
    "\n",
    "Pipelines are the professional way to combine preprocessing and modeling. They ensure transformations are applied correctly and prevent data leakage.\n",
    "\n",
    "**Your Task:** Create a pipeline that:\n",
    "1. Scales features with StandardScaler\n",
    "2. Fits a LinearRegression model\n",
    "3. Evaluates performance using cross-validation\n",
    "\n",
    "**Tasks:**\n",
    "- Create a Pipeline with two steps: ('scaler', StandardScaler()) and ('model', LinearRegression())\n",
    "- Use cross_val_score to evaluate the pipeline with 5-fold CV\n",
    "- Print the mean RMSE\n",
    "\n",
    "üí° **Hint:** The pipeline syntax is: `Pipeline([('step1_name', transformer1), ('step2_name', model)])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Discussion/Q&A (5-10 minutes)\n",
    "\n",
    "Let's pause and discuss what we've learned so far. Your TA will facilitate a discussion around these key concepts.\n",
    "\n",
    "**Discussion prompts:**\n",
    "- Why is cross-validation more trustworthy than repeatedly checking the test set?\n",
    "- How does GridSearchCV save time and reduce errors compared to manual tuning?\n",
    "- What's the danger of fitting the scaler on the entire dataset before splitting?\n",
    "- When would you use label encoding vs. dummy encoding for categorical variables?\n",
    "\n",
    "**Common blockers and clarifications:**\n",
    "- **\"Why do we need so many folds in CV?\"** ‚Äî More folds = more reliable estimates but longer computation time. 5-fold is a good default balance.\n",
    "- **\"How do I know which hyperparameters to tune?\"** ‚Äî Read the documentation, start with complexity parameters (like max_depth, n_estimators), and use domain knowledge.\n",
    "- **\"What if GridSearchCV takes too long?\"** ‚Äî Reduce your parameter grid, use fewer folds, or try RandomizedSearchCV for very large grids.\n",
    "- **\"Why does the pipeline matter?\"** ‚Äî It prevents data leakage by ensuring the scaler fits on training data only, then transforms both train and test correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B ‚Äî Independent Challenges (35-40 minutes)\n",
    "\n",
    "Now it's your turn to apply the complete professional workflow! For the next several challenges...\n",
    "\n",
    "* You will not be given starter code to work with; rather, you need to start from a blank cell.\n",
    "* **DO NOT USE AI** to generate code for you. This is a group exercise, and you should be writing the code together.\n",
    "* Work with your group to write the code.\n",
    "* Feel free to ask questions or seek help from the instructor.\n",
    "* We'll stop and walk through each challenge together after each time block.\n",
    "\n",
    "**Timing for Part B:**\n",
    "- Challenge 1-2: 10 minutes\n",
    "- Challenge 3-4: 10 minutes  \n",
    "- Challenge 5-6: 12 minutes\n",
    "- Group discussion: 5-8 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 ‚Äî Compare Three Models with Cross-Validation\n",
    "\n",
    "**Question:** Your real estate company wants to choose between three modeling approaches for their pricing tool: Linear Regression, Decision Tree (max_depth=15), and Random Forest (n_estimators=100, max_depth=15). Which model should they use based on cross-validation performance?\n",
    "\n",
    "**Context:** Use the enhanced features we created (including Neighborhood_Encoded) and 5-fold cross-validation. Compare models based on mean RMSE. Remember: lower RMSE is better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: write code here to compare three models using cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 ‚Äî Tune a Random Forest Comprehensively\n",
    "\n",
    "**Question:** Find the optimal Random Forest configuration for predicting house prices. Search over these hyperparameters:\n",
    "- `n_estimators`: [50, 100, 200]\n",
    "- `max_depth`: [10, 15, 20, None]\n",
    "- `min_samples_split`: [2, 5]\n",
    "- `max_features`: ['sqrt', 'log2']\n",
    "\n",
    "What are the best hyperparameters, and what's the expected CV RMSE?\n",
    "\n",
    "**Hint:** This will search 3 √ó 4 √ó 2 √ó 2 = 48 configurations. With 5-fold CV, that's 240 models. It may take 2-3 minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: write code here to perform comprehensive hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 ‚Äî Engineer New Features\n",
    "\n",
    "**Question:** Create three new features that might improve prediction accuracy:\n",
    "1. **HouseAge**: Current year (2024) minus YearBuilt\n",
    "2. **TotalBathrooms**: FullBath + (0.5 √ó HalfBath)\n",
    "3. **QualityScore**: OverallQual √ó OverallCond (interaction of quality and condition)\n",
    "\n",
    "Add these to your feature set, split the data, and evaluate a Decision Tree (max_depth=15) using cross-validation. Does adding these features improve performance compared to Challenge 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: write code here to engineer new features and evaluate model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4 ‚Äî Build a Complete Pipeline\n",
    "\n",
    "**Question:** Create a pipeline that combines feature scaling and random forest modeling. The pipeline should:\n",
    "1. Scale features using StandardScaler\n",
    "2. Fit a RandomForestRegressor with your best hyperparameters from Challenge 2\n",
    "\n",
    "Evaluate this pipeline using 5-fold cross-validation. Compare the RMSE to the unscaled Random Forest from Challenge 2. Does scaling help tree-based models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: write code here to build and evaluate a pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5 ‚Äî The Complete Professional Workflow\n",
    "\n",
    "**Question:** Execute the full 5-stage professional ML workflow:\n",
    "\n",
    "**Stage 1:** Split data into train/test (80/20)\n",
    "\n",
    "**Stage 2:** Use cross-validation on the training set to:\n",
    "- Compare at least 2 different models\n",
    "- Tune hyperparameters for your best model\n",
    "- Decide on your final model choice\n",
    "\n",
    "**Stage 3:** Select your best model based on CV results\n",
    "\n",
    "**Stage 4:** Retrain your chosen model on the FULL training set (not just one fold)\n",
    "\n",
    "**Stage 5:** Evaluate on the test set ONCE and report final RMSE\n",
    "\n",
    "Print results at each stage so we can see your decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: write code here to execute the complete 5-stage workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6 ‚Äî Pipeline + GridSearchCV Integration\n",
    "\n",
    "**Question:** Combine everything you've learned! Create a pipeline that includes:\n",
    "1. StandardScaler for preprocessing\n",
    "2. RandomForestRegressor for modeling\n",
    "\n",
    "Then use GridSearchCV to tune the Random Forest hyperparameters WITHIN the pipeline. Search over:\n",
    "- `model__n_estimators`: [100, 200]\n",
    "- `model__max_depth`: [10, 15, 20]\n",
    "\n",
    "Note the double underscore syntax: `model__parameter_name` is how you specify parameters for a specific step in a pipeline.\n",
    "\n",
    "What are the best parameters? How does this compare to your previous results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: write code here to combine pipeline and GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ (Optional) Extension Activities\n",
    "\n",
    "If you finish early or want additional practice:\n",
    "\n",
    "### Extension 1: RandomizedSearchCV\n",
    "When the parameter space is too large for grid search, RandomizedSearchCV samples random combinations. Try using `RandomizedSearchCV` to search a much larger parameter space (e.g., n_estimators from 50-500, max_depth from 5-50) with only 20 iterations. Compare results to grid search.\n",
    "\n",
    "### Extension 2: Feature Importance Analysis\n",
    "After training your best Random Forest model, extract feature importances using `model.feature_importances_`. Create a bar plot showing which features matter most for predicting house prices. Do your engineered features rank highly?\n",
    "\n",
    "### Extension 3: Analyze Cross-Validation Variance\n",
    "Use `cross_validate()` with `return_train_score=True` to get both training and validation scores for each fold. Plot them to visualize if your model shows signs of overfitting (large gap between train and validation scores).\n",
    "\n",
    "### Extension 4: Brainstorm ‚Äî What else is interesting?\n",
    "Write **3 questions** you'd like to explore with this dataset in future weeks.\n",
    "\n",
    "Examples to spark ideas:\n",
    "- How would you handle missing data in features like GarageYrBlt or LotFrontage?\n",
    "- Could you create polynomial features (e.g., GrLivArea¬≤) to capture non-linear relationships?\n",
    "- What happens if you use dummy encoding instead of label encoding for Neighborhood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Lab Wrap-Up & Reflection\n",
    "\n",
    "### ‚úÖ What You Accomplished\n",
    "In this lab, you practiced:\n",
    "- **Cross-validation**: Comparing models using k-fold CV to get honest performance estimates without contaminating the test set\n",
    "- **Hyperparameter tuning**: Using GridSearchCV to systematically search parameter spaces and find optimal model configurations\n",
    "- **Feature engineering**: Encoding categorical variables, scaling numerical features, and creating new features based on domain knowledge\n",
    "- **Pipelines**: Building end-to-end workflows that prevent data leakage and ensure reproducibility\n",
    "- **Professional workflow**: Executing the complete 5-stage process from split through final evaluation\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "Take 2-3 minutes to consider:\n",
    "- What concept from today clicked for you?\n",
    "- What would you like more practice with?\n",
    "- How might you use today's skills in a real business context?\n",
    "\n",
    "### üîó Connection to Course Goals\n",
    "This week represents a major milestone in your data science journey. You've moved from learning individual modeling techniques to understanding the **complete professional workflow** that ensures your models are trustworthy, reproducible, and production-ready. These skills are what employers look for when hiring data scientists‚Äîthey want people who can build models that work reliably in the real world, not just on classroom exercises.\n",
    "\n",
    "The techniques you practiced today (cross-validation, hyperparameter tuning, feature engineering, pipelines) are foundational to every data science project you'll work on in your career. Master these, and you'll be well-prepared for advanced topics and real-world applications.\n",
    "\n",
    "### üìã Next Steps\n",
    "- **Homework:** Complete the Week 12 homework assignment applying these techniques to a new dataset\n",
    "- **Quiz:** Week 12 reading quiz covering Chapters 28-30 (due Sunday)\n",
    "- **Next Week:** We'll explore unsupervised learning techniques including clustering and dimensionality reduction\n",
    "- **Additional Practice:** Revisit previous homework assignments and apply the proper workflow‚Äîcompare your original results to what you get with cross-validation and hyperparameter tuning\n",
    "\n",
    "---\n",
    "**üíæ Save your work** and be ready to share your approach and findings. During our wrap-up discussion, be prepared to explain:\n",
    "- Which model performed best in your workflow and why\n",
    "- What hyperparameters had the biggest impact on performance\n",
    "- Whether your engineered features improved predictions\n",
    "- Any surprises or unexpected results you discovered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Troubleshooting & Common Issues\n",
    "\n",
    "**Issue 1:** \"GridSearchCV is taking forever to run!\"\n",
    "- **Solution:** Reduce the parameter grid (try fewer values per parameter), use fewer CV folds (try cv=3 instead of cv=5), set `n_jobs=-1` to use all CPU cores, or consider RandomizedSearchCV for large grids.\n",
    "\n",
    "**Issue 2:** \"I'm getting a 'ValueError: could not convert string to float' error\"\n",
    "- **Solution:** You likely have a categorical variable in your feature set that hasn't been encoded. Use LabelEncoder or get_dummies() to convert categorical variables to numerical format before modeling.\n",
    "\n",
    "**Issue 3:** \"My cross-validation scores vary a lot between folds\"\n",
    "- **Solution:** This is normal and indicates your model's performance is somewhat sensitive to the specific data it sees. A large standard deviation suggests either: (1) your dataset is small, (2) the model is overfitting, or (3) there are outliers. Try increasing the number of folds, simplifying your model, or investigating outliers.\n",
    "\n",
    "**Issue 4:** \"Pipeline syntax with GridSearchCV is confusing\"\n",
    "- **Solution:** Remember the double underscore syntax: `'step_name__parameter'`. For example, if your pipeline has a step named 'model', use `'model__max_depth'` in your param_grid. Print `pipeline.get_params()` to see all available parameters.\n",
    "\n",
    "**Issue 5:** \"My test set performance is much worse than CV performance\"\n",
    "- **Solution:** This suggests overfitting. Your model performed well during cross-validation but doesn't generalize to truly new data. Try: (1) simplifying your model (reduce max_depth, increase min_samples_split), (2) using more training data, or (3) adding regularization.\n",
    "\n",
    "**Issue 6:** \"StandardScaler is giving me weird results\"\n",
    "- **Solution:** Make sure you fit the scaler on training data only (`scaler.fit_transform(X_train)`), then transform test data using those fitted parameters (`scaler.transform(X_test)`). NEVER fit on test data‚Äîthat's data leakage! Using a pipeline helps prevent this mistake.\n",
    "\n",
    "**General Debugging Tips:**\n",
    "- **Start simple:** Get a basic version working before adding complexity\n",
    "- **Check shapes:** Use `.shape` to verify your arrays are the right size at each step\n",
    "- **Print intermediate results:** Don't just run everything at once‚Äîprint outputs to understand what's happening\n",
    "- **Use small grids first:** Test GridSearchCV with a small parameter grid (2-3 combinations) before running the full search\n",
    "- **Read error messages carefully:** They often tell you exactly what's wrong (e.g., \"expected 6 features but got 5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
