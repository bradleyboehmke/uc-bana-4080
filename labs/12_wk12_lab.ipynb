{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12 Lab: The Professional ML Workflow\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/labs/12_wk12_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Welcome to Week 12! This week marks a turning point in your data science journey. Up until now, you've been learning the fundamentals of building machine learning models‚Äîand you've done great work. But this week, you're leveling up from \"beginner data scientist\" to \"production-ready professional.\"\n",
    "\n",
    "Here's the uncomfortable truth: **we've been breaking a fundamental rule of production machine learning**. Every time you compared different model settings or tuned hyperparameters based on test set performance, you were \"peeking\" at the test set‚Äîmaking your performance estimates less trustworthy. This week, you'll learn the **proper workflow** that ensures honest, reliable model performance.\n",
    "\n",
    "In this lab, you'll practice three critical professional skills: **cross-validation** (comparing models without contaminating your test set), **hyperparameter tuning** (systematically finding optimal model configurations), and **feature engineering** (transforming raw data into powerful model inputs). By the end, you'll be able to build models the right way‚Äîwith techniques that work in production, not just in the classroom.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Implement k-fold cross-validation to compare models without touching the test set\n",
    "- Use GridSearchCV to systematically tune hyperparameters across multiple parameters\n",
    "- Apply feature engineering techniques including encoding, scaling, and creating new features\n",
    "- Build end-to-end pipelines that prevent data leakage and ensure reproducible workflows\n",
    "- Execute the complete 5-stage professional ML workflow from data preparation through final evaluation\n",
    "\n",
    "## üìö This Lab Reinforces\n",
    "- **Chapter 28: Cross-Validation** ‚Äî K-fold CV, test set contamination, the proper 5-stage workflow\n",
    "- **Chapter 29: Hyperparameter Tuning** ‚Äî Bias-variance tradeoff, GridSearchCV, systematic parameter search\n",
    "- **Chapter 30: Feature Engineering** ‚Äî Encoding strategies, feature scaling, pipelines, preventing data leakage\n",
    "\n",
    "## üïê Estimated Time & Structure\n",
    "**Total Time:** 75 minutes  \n",
    "**Mode:** Group work (2-4 students)\n",
    "\n",
    "- **[0‚Äì30 min]** Part A: Guided Reinforcement ‚Äî TA-led practice with cross-validation, GridSearchCV, and feature engineering\n",
    "- **[30‚Äì40 min]** Class Q&A ‚Äî Discussion and clarification of key concepts\n",
    "- **[40‚Äì72 min]** Part B: Independent Challenges ‚Äî 6 group challenges applying the complete professional workflow\n",
    "- **[72‚Äì75 min]** Wrap-Up & Reflection ‚Äî What you learned and next steps\n",
    "\n",
    "You are encouraged to work in small groups of **2‚Äì4 students** and complete the lab together.\n",
    "\n",
    "## üí° Why This Matters\n",
    "These are the skills that separate junior data scientists from professionals who build production-ready models. When you interview for data science positions, employers will ask about cross-validation, hyperparameter tuning, and feature engineering‚Äîthey want to know you can build models that perform reliably in the real world, not just on classroom exercises.\n",
    "\n",
    "More importantly, these techniques ensure your models are trustworthy. When you tell a business stakeholder \"this model will achieve 85% accuracy in production,\" you need to be confident that estimate is honest and reliable. The professional workflow you'll practice today is what makes that confidence possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We'll use the familiar Ames housing dataset for today's lab. You've worked with this data throughout the course, which means you can focus on learning the new workflow techniques rather than getting familiar with unfamiliar data.\n",
    "\n",
    "Make sure you have the required libraries installed and the dataset accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Hide warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Ames housing data\n",
    "# adjust path if running in Google Colab\n",
    "try:\n",
    "    # Try local path first\n",
    "    ames = pd.read_csv('../data/ames_clean.csv')\n",
    "except FileNotFoundError:\n",
    "    # If in Colab, load from GitHub\n",
    "    url = 'https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/main/data/ames_clean.csv'\n",
    "    ames = pd.read_csv(url)\n",
    "\n",
    "# Quick preview\n",
    "print(f\"Dataset shape: {ames.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "ames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A ‚Äî Guided Reinforcement (30 minutes)\n",
    "\n",
    "In this section, your TA will lead you through the key techniques step-by-step. Follow along, run the code, and ask questions as we go. This guided practice will prepare you for the independent challenges in Part B.\n",
    "\n",
    "### Section 1: Cross-Validation Basics (10 minutes)\n",
    "\n",
    "Let's start by understanding **why** cross-validation matters and **how** to implement it properly.\n",
    "\n",
    "**The Problem:** In previous weeks, you evaluated models on the test set multiple times to make decisions. Each \"peek\" at the test set made your performance estimates less trustworthy.\n",
    "\n",
    "**The Solution:** Cross-validation lets you compare models and tune hyperparameters using only the training data, keeping your test set pristine for final evaluation.\n",
    "\n",
    "**üìã Step-by-step instructions:**\n",
    "1. Split data into train/test sets (80/20 split)\n",
    "2. Select a subset of features for simplicity\n",
    "3. Use `cross_val_score()` to evaluate a model with 5-fold CV\n",
    "4. Interpret the CV scores (mean and standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create features and target\n",
    "# We'll use a few numerical features to start\n",
    "features = ['GrLivArea', 'YearBuilt', 'TotalBsmtSF', 'GarageCars', 'FullBath', 'OverallQual']\n",
    "X = ames[features]\n",
    "y = ames['SalePrice']\n",
    "\n",
    "# Step 2: Split into train and test (test set is LOCKED)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(\"\\nüîí Test set is now LOCKED. We won't touch it until the very end.\")\n",
    "print(\"\\nLet's check out our training data features:\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use cross-validation to evaluate a decision tree\n",
    "# Notice: We ONLY use X_train and y_train here!\n",
    "dt = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    dt, X_train, y_train, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Convert to positive RMSE\n",
    "cv_rmse = -cv_scores\n",
    "\n",
    "print(\"Cross-Validation Results (5 folds):\")\n",
    "print(f\"RMSE per fold: {cv_rmse}\")\n",
    "print(f\"\\nMean RMSE: ${cv_rmse.mean():,.0f}\")\n",
    "print(f\"Std Dev: ${cv_rmse.std():,.0f}\")\n",
    "print(\"\\nüí° The mean tells us expected performance, std dev tells us consistency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß† Your Turn ‚Äî Compare Two Models with Cross-Validation\n",
    "\n",
    "Now you try! Use cross-validation to compare a shallow decision tree (`max_depth=5`) against a deeper tree (`max_depth=15`).\n",
    "\n",
    "**Tasks:**\n",
    "- Use the code above, just change the `max_depth` parameter to test out different settings.\n",
    "- Evaluate these models using 5-fold cross-validation on the training data\n",
    "- Compare the mean RMSE for these models\n",
    "- Which model performs better based on CV scores (not test scores!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ Check Your Understanding\n",
    "\n",
    "**Questions to consider:**\n",
    "- Which model had better (lower) mean RMSE?\n",
    "- Which model had more consistent scores across folds (lower std dev)?\n",
    "- Why is comparing models with CV better than repeatedly checking the test set?\n",
    "\n",
    "**Expected Result:** You should see that one model generalizes better than the other, and you made this decision without ever touching the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Hyperparameter Tuning with GridSearchCV (10 minutes)\n",
    "\n",
    "Now let's automate the hyperparameter search process. Instead of manually trying different values one by one, GridSearchCV will systematically test all combinations and use cross-validation to find the best configuration.\n",
    "\n",
    "**Why this matters:** Manual tuning is tedious, error-prone, and doesn't explore the full parameter space. GridSearchCV does this systematically and reproducibly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Example: Tuning a Random Forest\n",
    "\n",
    "Let's tune a Random Forest model by searching over multiple hyperparameters simultaneously.\n",
    "\n",
    "**Example:** We'll search over `n_estimators`, `max_depth`, and `min_samples_split` to find the optimal combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit on training data (this will try 2 √ó 3 √ó 2 = 12 configs, each with 5-fold CV = 60 models!)\n",
    "print(\"Starting grid search... this may take a minute\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Grid Search Complete!\")\n",
    "\n",
    "# Check out the results\n",
    "grid_search_results = pd.DataFrame(grid_search.cv_results_)\n",
    "grid_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Best Hyperparameter Configuration\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV RMSE: ${-grid_search.best_score_:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Practice Exercise ‚Äî Tune a Decision Tree\n",
    "\n",
    "**Business Scenario:** You're building a decision tree model for your real estate company's pricing tool. You need to find the optimal complexity settings.\n",
    "\n",
    "**Your Task:** Fill in the blanks below to use GridSearchCV to tune a DecisionTreeRegressor across these parameters:\n",
    "- `max_depth`: [5, 10, 15, 20]\n",
    "- `min_samples_split`: [2, 5, 10]\n",
    "- `min_samples_leaf`: [1, 2, 4]\n",
    "\n",
    "**Step-by-step approach:**\n",
    "1. Define the parameter grid as a dictionary\n",
    "2. Create a DecisionTreeRegressor\n",
    "3. Create GridSearchCV with cv=5\n",
    "4. Fit on X_train and y_train\n",
    "5. Print the best parameters and best CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': _________,           \n",
    "    'min_samples_split': _________,       \n",
    "    'min_samples_leaf': _________         \n",
    "    }\n",
    "\n",
    "# Step 2: Create the DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Step 3: Create GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    dt,\n",
    "    ______,\n",
    "    cv=__,                                   \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 4: Fit on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Print best parameters and best CV score\n",
    "print(\"Best parameters:\", grid_search._______)\n",
    "print(\"Best CV RMSE: $\", -grid_search._______)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Feature Engineering Essentials (10 minutes)\n",
    "\n",
    "Raw data often isn't in the best format for machine learning. Feature engineering transforms your data to help models learn better patterns. Let's practice the core techniques: encoding categorical variables and scaling numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Numerical Features\n",
    "\n",
    "First, let's see how scaling may improve model performance. Let's first train a model without scaling, then apply StandardScaler and see if performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a KNN regression model using 5-fold cross-validation\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    knn, X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "cv_rmse_unscaled = -cv_scores\n",
    "\n",
    "print(\"KNN Regression Cross-Validation Results (5 folds):\")\n",
    "print(f\"Mean RMSE: ${cv_rmse_unscaled.mean():,.0f}\")\n",
    "print(f\"Std Dev: ${cv_rmse_unscaled.std():,.0f}\")\n",
    "print(f\"\\nüí° Represents baseline performance with raw feature values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Use training stats to transform test\n",
    "\n",
    "# Train a KNN regression model using 5-fold cross-validation on scaled features\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "cv_scores = cross_val_score(\n",
    "    knn, X_train_scaled, y_train,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "cv_rmse_scaled = -cv_scores\n",
    "\n",
    "print(\"KNN Regression with Scaled Features (5 folds):\")\n",
    "print(f\"Mean RMSE: ${cv_rmse_scaled.mean():,.0f}\")\n",
    "print(f\"Std Dev: ${cv_rmse_scaled.std():,.0f}\")\n",
    "print(\"\\nüí° Scaling numerical features can help some models like KNN perform better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ When scaling does matter**\n",
    "\n",
    "| Model Type                             | Does Scaling Change Performance? | Why                                         |\n",
    "| -------------------------------------- | -------------------------------- | ------------------------------------------- |\n",
    "| **LinearRegression (OLS)**             | ‚ùå No                             | Closed-form solution, not iterative         |\n",
    "| **Ridge / Lasso / ElasticNet**         | ‚úÖ Yes                            | Regularization depends on coefficient sizes |\n",
    "| **Logistic Regression (solver-based)** | ‚úÖ Yes                            | Gradient descent & regularization           |\n",
    "| **SVM / SVR**                          | ‚úÖ Yes                            | Distance-based kernel computations          |\n",
    "| **KNN / K-means / PCA / clustering**   | ‚úÖ Yes                            | Distance/variance sensitive                 |\n",
    "| **Neural networks**                    | ‚úÖ Yes                            | Gradient descent stability                  |\n",
    "| **Tree models (RF, XGBoost, etc.)**    | ‚ùå No                             | Trees are scale-invariant                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Variables\n",
    "\n",
    "Most machine learning algorithms require numerical inputs. When you have categorical variables (like neighborhood names or house styles), you need to convert them to numbers. Let's practice the two most common approaches:\n",
    "\n",
    "1. **One-Hot Encoding** ‚Äî Create binary (0/1) columns for each category\n",
    "2. **Label Encoding** ‚Äî Assign each category a unique integer\n",
    "\n",
    "**When to use which?**\n",
    "- **One-hot encoding:** Best for nominal categories (no inherent order) with tree-based models or linear models\n",
    "- **Label encoding:** Can work for ordinal categories (inherent order) or with tree-based models that can learn splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add some categorical features to our feature set\n",
    "# We'll use Neighborhood and BldgType as examples\n",
    "# Step 1: Create features and target\n",
    "# We'll use a few numerical features to start\n",
    "features = ['Neighborhood', 'BldgType', 'GrLivArea', 'YearBuilt', 'TotalBsmtSF', 'GarageCars', 'FullBath', 'OverallQual']\n",
    "X = ames[features]\n",
    "\n",
    "# Step 2: Split into train and test (test set is LOCKED)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# First, let's see what values these categorical variables have\n",
    "print(f\"Unique Neighborhoods: {X_train['Neighborhood'].nunique()}\")\n",
    "print(f\"Unique Building Types: {X_train['BldgType'].nunique()}\")\n",
    "\n",
    "print(\"\\nLet's check out our training data features:\")\n",
    "X_train.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Select categorical columns\n",
    "cat_cols = ['BldgType', 'Neighborhood']\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Fit and transform on training data\n",
    "X_cat_encoded = encoder.fit_transform(X_train[cat_cols])\n",
    "\n",
    "# Get feature names for encoded columns\n",
    "encoded_feature_names = encoder.get_feature_names_out(cat_cols)\n",
    "\n",
    "# Create DataFrame for encoded categorical features\n",
    "X_cat_encoded_df = pd.DataFrame(X_cat_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "\n",
    "# Concatenate with numerical features\n",
    "num_cols = [col for col in X_train.columns if col not in cat_cols]\n",
    "X_encoded = pd.concat([X_train[num_cols], X_cat_encoded_df], axis=1)\n",
    "\n",
    "print(f\"One-hot encoding turned our {X_train.shape[1]} original features into {X_encoded.shape[1]} features.\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative: Label Encoding for Categorical Variables\n",
    "\n",
    "While one-hot encoding is a common approach for handling categorical variables, another option is **label encoding**. This technique assigns each unique category an integer value. Label encoding can be especially useful when you have a categorical feature with many unique values‚Äîlike the `Neighborhood` column, which contains 25 unique neighborhoods.\n",
    "\n",
    "Let's see how to apply label encoding to the `Neighborhood` column in the next code chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform Neighborhood in training data\n",
    "X_train['Neighborhood_LE'] = le.fit_transform(X_train['Neighborhood'])\n",
    "\n",
    "# Transform Neighborhood in test data using the same encoder\n",
    "X_test['Neighborhood_LE'] = le.transform(X_test['Neighborhood'])\n",
    "\n",
    "print(X_train[['Neighborhood', 'Neighborhood_LE']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèóÔ∏è Putting Feature Engineering Steps Together with Pipelines\n",
    "\n",
    "When building machine learning models, it's critical to apply all preprocessing steps (like scaling and encoding) in a reproducible, leak-proof way. Scikit-learn's `Pipeline` lets you chain together feature engineering and modeling steps so that each transformation is fit only on the training data within each cross-validation fold. This minimizes data leakage and ensures your workflow is robust and production-ready.\n",
    "\n",
    "**Benefits of using a pipeline:**\n",
    "- All preprocessing steps are applied consistently and correctly\n",
    "- Prevents accidental data leakage by fitting transformers only on training data\n",
    "- Makes your workflow easier to reproduce and deploy\n",
    "- Allows you to tune preprocessing and model hyperparameters together\n",
    "\n",
    "Below is an example pipeline that:\n",
    "- Standardizes numeric features\n",
    "- One-hot encodes the `BldgType` variable\n",
    "- Label-encodes the `Neighborhood` variable\n",
    "- Trains a Decision Tree model using 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Define column types\n",
    "numeric_features = ['GrLivArea', 'YearBuilt', 'TotalBsmtSF', 'GarageCars', 'FullBath', 'OverallQual']\n",
    "onehot_features = ['BldgType', 'Neighborhood']          \n",
    "\n",
    "# Build preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'), onehot_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build full pipeline with DecisionTreeRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Set up hyperparameter grid for DecisionTreeRegressor\n",
    "param_grid = {\n",
    "    'model__max_depth': [5, 10, 15, 20],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV for pipeline\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit and evaluate\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_rmse = -grid_search.best_score_\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(f\"Best CV RMSE: ${cv_rmse:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Why pipelines are essential:**\n",
    "- ‚úÖ Prevent data leakage by ensuring transformations are fit only on training data\n",
    "- ‚úÖ Make your workflow reproducible and easier to deploy\n",
    "- ‚úÖ Allow you to tune preprocessing and model hyperparameters together\n",
    "- ‚úÖ Simplify your code by combining multiple steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Discussion/Q&A (10 minutes)\n",
    "\n",
    "Before moving to the independent challenges, let's discuss key concepts:\n",
    "\n",
    "**Discussion prompts:**\n",
    "- What's the difference between cross-validation and a simple train/test split?\n",
    "- Why shouldn't we look at the test set until the very end?\n",
    "- When would you use GridSearchCV vs. manually trying different hyperparameters?\n",
    "- What happens if you fit a scaler on all your data before splitting into train/test?\n",
    "- How do pipelines help prevent data leakage?\n",
    "\n",
    "**Common blockers and clarifications:**\n",
    "- **\"My GridSearchCV is taking forever!\"** ‚Äî Reduce the parameter grid size or use fewer CV folds during experimentation\n",
    "- **\"Do I need to scale features for Random Forest?\"** ‚Äî No! Tree-based models are scale-invariant\n",
    "- **\"When do I use one-hot vs. label encoding?\"** ‚Äî One-hot for nominal categories with no order; label encoding for ordinal or when working with tree models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B ‚Äî Independent Challenges (32 minutes)\n",
    "\n",
    "Now it's time to apply the complete professional ML workflow! These challenges require you to integrate everything you've learned.\n",
    "\n",
    "**Important rules for Part B:**\n",
    "* You will not be given starter code to work with; rather, you need to start from a blank cell.\n",
    "* **DO NOT USE AI** to generate code for you. This is a group exercise, and you should be writing the code together.\n",
    "* Work with your group to write the code.\n",
    "* Feel free to ask questions or seek help from the instructor.\n",
    "* We'll stop and walk through each challenge together after each time block.\n",
    "\n",
    "**The Professional ML Workflow (5 Stages):**\n",
    "1. **Data Preparation** ‚Äî Feature selection, encoding, handling missing values\n",
    "2. **Initial Train/Test Split** ‚Äî Lock away your test set (don't touch until Stage 5!)\n",
    "3. **Model Comparison with CV** ‚Äî Compare different model types using cross-validation\n",
    "4. **Hyperparameter Tuning** ‚Äî Optimize your chosen model with GridSearchCV\n",
    "5. **Final Test Evaluation** ‚Äî Evaluate your final model on the locked test set (ONCE!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 ‚Äî Compare Model Types (5 minutes)\n",
    "\n",
    "**Business Context:** Your real estate company wants to choose the best model architecture for predicting house prices. They're considering three options: Linear Regression, Decision Trees, and Random Forests.\n",
    "\n",
    "**Your Task:**\n",
    "1. Using the numerical features from Part A, create a train/test split (80/20)\n",
    "   - Numerical features: `['GrLivArea', 'YearBuilt', 'TotalBsmtSF', 'GarageCars', 'FullBath', 'OverallQual']`\n",
    "2. Use 5-fold cross-validation to compare these three models:\n",
    "   - Linear Regression\n",
    "   - DecisionTreeRegressor (max_depth=10)\n",
    "   - RandomForestRegressor (n_estimators=100, max_depth=10)\n",
    "3. Print the mean CV RMSE for each model\n",
    "4. Which model performs best? Why do you think that is?\n",
    "\n",
    "**Remember:** Use ONLY the training data for cross-validation! Make sure you always set your `random_state=42` whether you're splitting your data or when you initiate your model (i.e. `DecisionTreeRegressor(random_state=42)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: write code here to compare model types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 ‚Äî Systematic Hyperparameter Tuning (6 minutes)\n",
    "\n",
    "**Business Context:** Now that you've identified which model type works best, your manager wants you to find the optimal hyperparameter settings for that model.\n",
    "\n",
    "**Your Task:**\n",
    "1. Take the Random Forest model from Challenge 1\n",
    "2. Define a parameter grid that assesses the following values:\n",
    "    - `n_estimators`: [100, 200, 300]\n",
    "    - `max_depth`: [5, 10, 15, 20]\n",
    "    - `min_samples_split`: [2, 5, 10]\n",
    "    - `min_samples_leaf`: [1, 2, 5]\n",
    "3. Use GridSearchCV with 5-fold CV to find the best configuration\n",
    "4. Print the best parameters and best CV score\n",
    "5. Did tuning improve performance compared to the first random forest model you trained?  If so, by how much?\n",
    "\n",
    "**Tip:** Don't forget to set `random_state=42`!\n",
    "\n",
    "**‚ö†Ô∏è‚ö†Ô∏è Warning ‚ö†Ô∏è‚ö†Ô∏è**: This may take a couple minutes to run since we're are running 540 models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: write code here to tune hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 ‚Äî Build a Complete Pipeline (6 minutes)\n",
    "**Business Context:** Your model is performing well, but you think you can do better by adding more features and performing some feature engineering.  Let's do this but do it in a safe way by using a `Pipeline()` to ensure no data leakage and easy reproducibility.\n",
    "\n",
    "**Your Task:**\n",
    "1. Start from scratch with the original `ames` data and select the following features:\n",
    "   - Numeric: GrLivArea, YearBuilt, TotalBsmtSF, GarageCars, FullBath, OverallQual, YearRemodAdd, BedroomAbvGr, TotRmsAbvGrd\n",
    "   - Categorical: Neighborhood, HouseStyle\n",
    "2. Split the data into train/test sets (80/20 split) using these features.\n",
    "3. Build a pipeline that:\n",
    "   - Applies StandardScaler to numeric features\n",
    "   - Applies OneHotEncoder to categorical features (Neighborhood, HouseStyle)\n",
    "   - Uses RandomForestRegressor with the optimal hyperparameters from Challenge 2\n",
    "4. Use 5-fold cross-validation to evaluate the pipeline on the training data.\n",
    "5. Print the mean CV RMSE and compare it to your previous Random Forest model.  \n",
    "6. Does adding more features and applying feature engineering improve model performance? If so, by how much?\n",
    "\n",
    "**Tip:** Don't forget to set `random_state=42`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: write code here to build a pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4 ‚Äî Complete End-to-End Workflow (4 minutes)\n",
    "**Business Context:** Your manager wants you to execute the complete professional ML workflow from start to finish using the same feature set as the previous challenge.\n",
    "\n",
    "**Your Task:** Demonstrate the full end-to-end workflow:\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - Select features:  \n",
    "     - Numeric: GrLivArea, YearBuilt, TotalBsmtSF, GarageCars, FullBath, OverallQual, YearRemodAdd, BedroomAbvGr, TotRmsAbvGrd  \n",
    "     - Categorical: Neighborhood, HouseStyle\n",
    "   - Target variable: SalePrice\n",
    "\n",
    "2. **Initial Split:**\n",
    "   - Split data into train/test sets (80/20)\n",
    "   - Lock away the test set\n",
    "\n",
    "3. **Pipeline & Feature Engineering:**\n",
    "   - Build a pipeline that applies StandardScaler to numeric features and OneHotEncoder to categorical features\n",
    "\n",
    "4. **Hyperparameter Tuning:**\n",
    "   - Use GridSearchCV to tune a RandomForestRegressor with these settings:\n",
    "      - `n_estimators`: [100, 200, 300]\n",
    "      - `max_depth`: [5, 10, 15, 20]\n",
    "      - `min_samples_split`: [2, 5, 10]\n",
    "      - `min_samples_leaf`: [1, 2, 5]\n",
    "   - Use 5-fold cross-validation to select the best model\n",
    "\n",
    "5. **Final Evaluation:**\n",
    "   - Retrain the best model on the full training set\n",
    "   - Evaluate ONCE on the test set\n",
    "   - Report the final test RMSE\n",
    "\n",
    "**Success criteria:** Your code should demonstrate the complete workflow, use a pipeline for feature engineering, and report the final test RMSE with no data leakage!\n",
    "\n",
    "**‚ö†Ô∏è‚ö†Ô∏è Warning ‚ö†Ô∏è‚ö†Ô∏è**: This may take a couple minutes to run since we're are running 540 models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Turn: write code here to complete the end-to-end workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5 ‚Äî Model Interpretation: Permutation Importance (5 minutes)\n",
    "\n",
    "**Business Context:** Your manager wants to understand *why* your model makes its predictions. Which features are most influential for predicting house prices?\n",
    "\n",
    "**Your Task:**\n",
    "1. Take the best model you identified above.\n",
    "2. Use the permutation importance approach (`sklearn.inspection.permutation_importance`) on the test set to identify the most influential features.\n",
    "3. Create a bar chart that plots the permutation importance scores for all features.\n",
    "4. Identify and answer: Which feature is the most important for your model?\n",
    "\n",
    "**Tip:** Use the test set (`X_test`, `y_test`) for permutation importance. For plotting, feel free to use matplotlib, seaborn or any other library.\n",
    "\n",
    "**Success criteria:** You should be able to visualize the feature importances and clearly identify the most influential feature.## Challenge 5 ‚Äî Model Interpretation: Permutation Importance & PDP (5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Turn: write code here to compute feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6 ‚Äî Partial Dependence Plot (PDP) for Model Interpretation\n",
    "\n",
    "**Business Context:** Now that you've identified the most important feature using permutation importance, your manager wants to understand *how* this feature influences the model's predictions. A Partial Dependence Plot (PDP) helps visualize the relationship between the feature and the predicted sale price, holding all other features constant.\n",
    "\n",
    "**Your Task:**\n",
    "1. Use the best model and the most important feature identified in Challenge 5.\n",
    "2. Generate a Partial Dependence Plot (PDP) for this feature using the test set.\n",
    "3. Interpret the plot: How do changes in this feature affect the predicted house price? Is the relationship linear, monotonic, or more complex?\n",
    "\n",
    "**Success criteria:** Your code should generate a clear PDP for the most important feature and provide insights into how the model uses this feature to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Turn: write code here to compute the PDP plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Lab Wrap-Up & Reflection\n",
    "\n",
    "### üíæ Save Your Work\n",
    "You use your findings to complete this week's homework quiz!\n",
    "\n",
    "### ‚úÖ What You Accomplished\n",
    "In this lab, you practiced:\n",
    "- Implementing cross-validation to compare models without touching the test set\n",
    "- Using GridSearchCV to systematically tune hyperparameters\n",
    "- Engineering features through encoding categorical variables and creating new features\n",
    "- Building pipelines to prevent data leakage and ensure reproducible workflows\n",
    "- Executing the complete 5-stage professional ML workflow\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "Take 2-3 minutes to consider:\n",
    "- What concept from today clicked for you?\n",
    "- What would you like more practice with?\n",
    "- How does the professional workflow differ from what you were doing before?\n",
    "- Why is it critical to keep the test set locked until the very end?\n",
    "- How might you use pipelines in future projects?\n",
    "\n",
    "### üîó Connection to Course Goals\n",
    "This lab represents a critical milestone in your data science journey. You've moved from learning individual modeling techniques to understanding how professional data scientists build production-ready models. The workflow you practiced today is exactly what you'll use in real-world ML projects:\n",
    "\n",
    "1. **Cross-validation** ensures your model comparisons are trustworthy\n",
    "2. **Hyperparameter tuning** finds optimal configurations systematically\n",
    "3. **Feature engineering** extracts maximum predictive power from your data\n",
    "4. **Pipelines** prevent data leakage and make your work reproducible\n",
    "5. **Test set discipline** gives you honest performance estimates\n",
    "\n",
    "These aren't just classroom skills‚Äîthey're industry best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Troubleshooting & Common Issues\n",
    "\n",
    "**Issue 1: GridSearchCV is taking forever to run**\n",
    "- **Solution:** Reduce the size of your parameter grid during experimentation. Instead of `[1, 10, 100, 1000]`, try `[10, 100]` first. You can also reduce `cv` from 5 to 3 for faster prototyping.\n",
    "\n",
    "**Issue 2: \"ValueError: Input contains NaN\"**\n",
    "- **Solution:** Some features in the Ames dataset have missing values. Either drop rows with missing values using `.dropna()`, or impute missing values using `SimpleImputer` from sklearn.\n",
    "\n",
    "**Issue 3: One-hot encoding creates too many columns**\n",
    "- **Solution:** This is normal for features with many categories (like Neighborhood). You can: (1) use only high-frequency categories, (2) use label encoding for tree-based models instead, or (3) accept the high dimensionality.\n",
    "\n",
    "**Issue 4: \"Arrays must have the same shape\" when predicting on test set**\n",
    "- **Solution:** Make sure you apply the same transformations to both train and test sets. If you used `pd.get_dummies()` on the train set, you need to apply it to the test set too, ensuring the same columns exist.\n",
    "\n",
    "**Issue 5: Pipeline syntax is confusing**\n",
    "- **Solution:** Remember the double underscore notation: `'step_name__parameter_name'`. To tune a Ridge model in a pipeline, use `'model__alpha'` where 'model' is the name you gave that pipeline step.\n",
    "\n",
    "**Issue 6: CV scores vary dramatically between folds**\n",
    "- **Solution:** High variance in CV scores suggests your model is unstable or you have outliers. Try: (1) increasing training data, (2) simplifying your model, (3) checking for data quality issues, or (4) using stratified splits if appropriate.\n",
    "\n",
    "**General Debugging Tips:**\n",
    "- Print the shape of your data at each step to catch dimension mismatches early\n",
    "- Start simple (fewer features, simpler models) and add complexity gradually\n",
    "- Use `verbose=1` in GridSearchCV to see progress\n",
    "- Check for missing values with `df.isnull().sum()`\n",
    "- When encoding categorical variables, verify the columns match between train and test sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
