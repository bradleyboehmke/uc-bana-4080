{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13 Lab: Unsupervised Learning (Clustering and PCA)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/labs/13_wk13_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This lab introduces you to two fundamental unsupervised learning techniques: **K-Means clustering** and **Principal Component Analysis (PCA)**. Unlike supervised learning where we have labeled data, unsupervised learning discovers hidden patterns and structures in data without predefined outcomes.\n",
    "\n",
    "In Part A, you'll work through guided examples with customer segmentation data and housing data to understand how clustering groups similar observations together and how PCA reduces dimensionality. In Part B, you'll complete three homework challenges aligned with this week's online quiz, applying these techniques to real-world datasets.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Apply K-Means clustering to segment observations into meaningful groups\n",
    "- Use the elbow method and silhouette scores to determine optimal cluster numbers\n",
    "- Interpret cluster characteristics and understand what makes each group unique\n",
    "- Execute the complete PCA workflow: standardize, fit, transform, and interpret\n",
    "- Use scree plots to decide how many principal components to retain\n",
    "- Interpret component loadings to understand what each PC represents\n",
    "- Combine PCA with machine learning models to improve performance and reduce complexity\n",
    "\n",
    "## üìö This Lab Reinforces\n",
    "- **Chapter 31: Unsupervised Learning and Clustering**\n",
    "- **Chapter 32: Dimension Reduction with PCA**\n",
    "- **This week's homework quiz questions**\n",
    "\n",
    "## üïê Estimated Time & Structure\n",
    "**Total Time:** 65-70 minutes  \n",
    "**Mode:** Individual work recommended for Part B (homework)\n",
    "\n",
    "- **[0‚Äì30 min]** Part A: Guided Lab Practice (clustering and PCA walkthroughs with \"Your Turn\" exercises)\n",
    "- **[30‚Äì70 min]** Part B: Homework Challenges (complete the three challenges that directly correspond to quiz questions)\n",
    "\n",
    "You are encouraged to work in small groups of **2‚Äì4 students** during Part A to discuss concepts and approaches.\n",
    "\n",
    "## üí° Why This Matters\n",
    "Unsupervised learning is essential when you don't have labeled data or want to discover hidden patterns. Businesses use clustering to segment customers, identify fraud patterns, and group similar products. PCA helps data scientists visualize high-dimensional data, speed up machine learning models, and remove redundant features. These techniques are foundational for exploratory data analysis, feature engineering, and building production ML systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to import all necessary libraries and set random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Guided Lab Practice (~30 min)\n",
    "\n",
    "This section walks you through K-Means clustering and PCA with hands-on examples and \"Your Turn\" exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction to K-Means Clustering\n",
    "\n",
    "**K-Means clustering** is an algorithm that groups similar observations into k clusters. The algorithm:\n",
    "1. Randomly places k cluster centers\n",
    "2. Assigns each point to its nearest center\n",
    "3. Updates centers to the mean of assigned points\n",
    "4. Repeats steps 2-3 until convergence\n",
    "\n",
    "### Example: Customer Segmentation\n",
    "\n",
    "Imagine you're a retail analyst with customer data showing annual income and spending scores. You want to segment customers for targeted marketing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample customer data\n",
    "np.random.seed(42)\n",
    "n_customers = 200\n",
    "\n",
    "# Generate three natural customer segments\n",
    "# Segment 1: High income, high spending\n",
    "seg1_income = np.random.normal(80, 10, 50)\n",
    "seg1_spending = np.random.normal(75, 10, 50)\n",
    "\n",
    "# Segment 2: Low income, low spending\n",
    "seg2_income = np.random.normal(30, 8, 75)\n",
    "seg2_spending = np.random.normal(25, 8, 75)\n",
    "\n",
    "# Segment 3: Medium income, high spending\n",
    "seg3_income = np.random.normal(50, 10, 75)\n",
    "seg3_spending = np.random.normal(65, 10, 75)\n",
    "\n",
    "# Combine into dataframe\n",
    "customers = pd.DataFrame({\n",
    "    'Income': np.concatenate([seg1_income, seg2_income, seg3_income]),\n",
    "    'Spending_Score': np.concatenate([seg1_spending, seg2_spending, seg3_spending])\n",
    "})\n",
    "\n",
    "print(f\"Customer data shape: {customers.shape}\")\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the customer data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(customers['Income'], customers['Spending_Score'], alpha=0.6, s=60)\n",
    "plt.xlabel('Annual Income ($k)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.title('Customer Income vs Spending (Unlabeled Data)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Notice: We can visually see some natural groupings, but K-Means will find them algorithmically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Standardize the Data\n",
    "\n",
    "K-Means is sensitive to scale. We need to standardize features so they contribute equally to distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "customers_scaled = scaler.fit_transform(customers)\n",
    "\n",
    "print(f\"Original data shape: {customers.shape}\")\n",
    "print(f\"Scaled data shape: {customers_scaled.shape}\")\n",
    "print(f\"\\nScaled data mean: {customers_scaled.mean():.6f} (should be ~0)\")\n",
    "print(f\"Scaled data std: {customers_scaled.std():.6f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fit K-Means with k=3\n",
    "\n",
    "Let's assume we want to segment customers into 3 groups based on business requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit K-Means with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(customers_scaled)\n",
    "\n",
    "# Add cluster labels to original dataframe\n",
    "customers['Cluster'] = cluster_labels\n",
    "\n",
    "print(f\"K-Means fitted with {kmeans.n_clusters} clusters\")\n",
    "print(f\"\\nCluster sizes:\")\n",
    "print(customers['Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red', 'blue', 'green']\n",
    "for cluster in range(3):\n",
    "    cluster_data = customers[customers['Cluster'] == cluster]\n",
    "    plt.scatter(cluster_data['Income'], cluster_data['Spending_Score'], \n",
    "               c=colors[cluster], label=f'Cluster {cluster}', alpha=0.6, s=60)\n",
    "\n",
    "# Plot cluster centers (need to inverse transform)\n",
    "centers_original = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centers_original[:, 0], centers_original[:, 1], \n",
    "           c='black', marker='X', s=300, edgecolors='yellow', linewidths=2, label='Centers')\n",
    "\n",
    "plt.xlabel('Annual Income ($k)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.title('Customer Segmentation with K-Means (k=3)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate Clustering Quality\n",
    "\n",
    "Two common metrics:\n",
    "- **Inertia (WCSS)**: Sum of squared distances from points to their cluster centers (lower is better)\n",
    "- **Silhouette Score**: Measures how similar points are to their own cluster vs. other clusters (-1 to 1, higher is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate clustering metrics\n",
    "inertia = kmeans.inertia_\n",
    "silhouette = silhouette_score(customers_scaled, cluster_labels)\n",
    "\n",
    "print(f\"Inertia (WCSS): {inertia:.2f}\")\n",
    "print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(f\"   - Silhouette score of {silhouette:.2f} suggests {'good' if silhouette > 0.5 else 'moderate'} cluster separation\")\n",
    "print(\"   - Scores closer to 1.0 indicate well-separated, compact clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: The Elbow Method\n",
    "\n",
    "How do we choose k? The **elbow method** plots inertia for different k values and looks for an \"elbow\" where adding more clusters gives diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different k values\n",
    "k_range = range(2, 9)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "    kmeans_temp.fit(customers_scaled)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    silhouettes.append(silhouette_score(customers_scaled, kmeans_temp.labels_))\n",
    "\n",
    "# Plot elbow curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(k_range, inertias, marker='o', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia (WCSS)')\n",
    "ax1.set_title('Elbow Method: Inertia')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(k_range, silhouettes, marker='o', color='orange', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Scores')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Look for the 'elbow' where inertia starts decreasing more slowly.\")\n",
    "print(f\"   Best silhouette score: k={list(k_range)[np.argmax(silhouettes)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Your Turn ‚Äî Interpret the Clusters\n",
    "\n",
    "Now that you've clustered the customers, let's understand what makes each cluster unique.\n",
    "\n",
    "**Tasks:**\n",
    "- Calculate the mean income and spending score for each cluster\n",
    "- Give each cluster a descriptive business name (e.g., \"Budget Shoppers\", \"High Rollers\")\n",
    "- Consider: What marketing strategy would you recommend for each segment?\n",
    "\n",
    "üí° **Hint:** Use `customers.groupby('Cluster')[['Income', 'Spending_Score']].mean()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Introduction to Principal Component Analysis (PCA)\n",
    "\n",
    "**PCA** reduces dimensionality by creating new features (principal components) that are combinations of original features. These components:\n",
    "- Capture maximum variance in the data\n",
    "- Are uncorrelated with each other\n",
    "- Are ordered by importance (PC1 explains most variance, PC2 second-most, etc.)\n",
    "\n",
    "### Example: Reducing Housing Features\n",
    "\n",
    "Imagine you have many correlated housing features. PCA can reduce them to a few meaningful components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample housing data with correlated features\n",
    "np.random.seed(42)\n",
    "n_houses = 100\n",
    "\n",
    "# Base size variable\n",
    "size_factor = np.random.normal(1500, 300, n_houses)\n",
    "\n",
    "# Create correlated features based on size\n",
    "housing_data = pd.DataFrame({\n",
    "    'SqFt': size_factor + np.random.normal(0, 50, n_houses),\n",
    "    'Bedrooms': (size_factor / 300) + np.random.normal(0, 0.5, n_houses),\n",
    "    'Bathrooms': (size_factor / 400) + np.random.normal(0, 0.3, n_houses),\n",
    "    'Garage_Cars': (size_factor / 600) + np.random.normal(0, 0.3, n_houses)\n",
    "})\n",
    "\n",
    "print(f\"Housing data shape: {housing_data.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(housing_data.head())\n",
    "\n",
    "print(f\"\\nCorrelation matrix (notice high correlations):\")\n",
    "print(housing_data.corr().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Standardize Before PCA\n",
    "\n",
    "PCA is sensitive to scale, so we always standardize first (unless features are already on the same scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the housing features\n",
    "scaler_pca = StandardScaler()\n",
    "housing_scaled = scaler_pca.fit_transform(housing_data)\n",
    "\n",
    "print(f\"Scaled data shape: {housing_scaled.shape}\")\n",
    "print(f\"Mean: {housing_scaled.mean():.6f} (should be ~0)\")\n",
    "print(f\"Std: {housing_scaled.std():.6f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fit PCA\n",
    "\n",
    "Let's fit PCA with all components to see the full picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with all components\n",
    "pca_housing = PCA()\n",
    "pca_housing.fit(housing_scaled)\n",
    "\n",
    "print(f\"Number of components: {pca_housing.n_components_}\")\n",
    "print(f\"\\nVariance explained by each component:\")\n",
    "for i, var in enumerate(pca_housing.explained_variance_ratio_, 1):\n",
    "    print(f\"  PC{i}: {var*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nCumulative variance:\")\n",
    "cumsum = np.cumsum(pca_housing.explained_variance_ratio_)\n",
    "for i, var in enumerate(cumsum, 1):\n",
    "    print(f\"  First {i} PC(s): {var*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Interpret Components with Loadings\n",
    "\n",
    "**Loadings** show how original features contribute to each principal component. High absolute loadings indicate strong influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loadings dataframe\n",
    "loadings_df = pd.DataFrame(\n",
    "    pca_housing.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(pca_housing.n_components_)],\n",
    "    index=housing_data.columns\n",
    ")\n",
    "\n",
    "print(\"Component Loadings:\")\n",
    "print(loadings_df.round(3))\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   PC1: All loadings are similar and positive ‚Üí 'Overall house size'\")\n",
    "print(\"   PC2: Mixed signs ‚Üí Captures contrasts between features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Visualize with Scree Plot\n",
    "\n",
    "A **scree plot** helps decide how many components to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scree plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Individual variance\n",
    "ax1.bar(range(1, len(pca_housing.explained_variance_ratio_) + 1),\n",
    "        pca_housing.explained_variance_ratio_, alpha=0.7)\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Variance Explained')\n",
    "ax1.set_title('Scree Plot')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Cumulative variance\n",
    "cumulative = np.cumsum(pca_housing.explained_variance_ratio_)\n",
    "ax2.plot(range(1, len(cumulative) + 1), cumulative, marker='o', linewidth=2)\n",
    "ax2.axhline(y=0.90, color='r', linestyle='--', label='90% threshold')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Variance')\n",
    "ax2.set_title('Cumulative Variance Explained')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üí° PC1 alone captures {pca_housing.explained_variance_ratio_[0]*100:.1f}% of variance!\")\n",
    "print(f\"   This makes sense: these features are highly correlated (all related to size).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Transform the Data\n",
    "\n",
    "Finally, we transform our data into the principal component space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep just 2 components for demonstration\n",
    "pca_2 = PCA(n_components=2)\n",
    "housing_pca = pca_2.fit_transform(housing_scaled)\n",
    "\n",
    "print(f\"Original shape: {housing_scaled.shape} (100 houses √ó 4 features)\")\n",
    "print(f\"Transformed shape: {housing_pca.shape} (100 houses √ó 2 components)\")\n",
    "print(f\"\\nVariance retained: {pca_2.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "\n",
    "# Create dataframe with PC scores\n",
    "housing_pca_df = pd.DataFrame(housing_pca, columns=['PC1', 'PC2'])\n",
    "print(\"\\nFirst few rows in PC space:\")\n",
    "print(housing_pca_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Homework Challenges (~35-40 min)\n",
    "\n",
    "**IMPORTANT:** This section IS your homework for the week. You will complete these challenges and then answer questions in an online Canvas quiz based on your results.\n",
    "\n",
    "For reproducibility and to ensure your quiz answers match your analysis, you **MUST** use the exact parameters specified below (random_state, n_init, test_size, etc.).\n",
    "\n",
    "**DO NOT USE AI** to generate code for you. Write the code independently, building on what you learned in Part A. Feel free to ask the instructor for clarification or help if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: K-Means Clustering on Housing Data\n",
    "\n",
    "**Business Context:** You're a real estate analyst segmenting the Ames housing market to identify property types for targeted marketing. Your marketing team has requested 3 distinct segments.\n",
    "\n",
    "**Your Task:** Apply K-Means clustering (k=3) to the Ames housing dataset and analyze cluster quality.\n",
    "\n",
    "**Quiz Questions 1-4 will ask about your k=3 results. Record these values:**\n",
    "- Final silhouette score (round to 4 decimals)\n",
    "- Which cluster has the most homes?\n",
    "- Final inertia/WCSS value (round to 2 decimals)\n",
    "- Best k value by silhouette score from elbow analysis\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "**Step 1:** Load the Ames housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ames housing data\n",
    "ames = pd.read_csv('https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/main/data/ames_clean.csv')\n",
    "\n",
    "print(f\"Ames data shape: {ames.shape}\")\n",
    "ames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Select exactly these features for clustering and handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Select these exact features\n",
    "features = ['GrLivArea', 'TotalBsmtSF', 'GarageArea', 'YearBuilt', 'OverallQual', 'OverallCond']\n",
    "\n",
    "# Your code here: Handle missing values\n",
    "# Fill TotalBsmtSF and GarageArea missing values with 0\n",
    "# Create ames_cluster dataframe with selected features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Standardize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Standardize using StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Perform elbow analysis (k from 2 to 10)\n",
    "\n",
    "**CRITICAL:** Use `random_state=42` and `n_init=10` for all K-Means models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: \n",
    "# - Loop through k values from 2 to 10\n",
    "# - For each k, fit KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "# - Store inertia and silhouette scores\n",
    "# - Create elbow plots\n",
    "# - Identify best k by silhouette score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üìù RECORD THIS FOR QUIZ (Question 4):**\n",
    "\n",
    "Which k value had the highest silhouette score?\n",
    "\n",
    "**Step 5:** Fit final K-Means model with k=3\n",
    "\n",
    "**CRITICAL:** Use `random_state=42` and `n_init=10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# - Fit KMeans with k=3, random_state=42, n_init=10\n",
    "# - Calculate final inertia\n",
    "# - Calculate final silhouette score\n",
    "# - Print cluster sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üìù RECORD THESE FOR QUIZ:**\n",
    "\n",
    "- **Question 1:** Final silhouette score (round to 4 decimals)\n",
    "- **Question 2:** Which cluster (0, 1, or 2) contains the most homes?\n",
    "- **Question 3:** Final inertia/WCSS value (round to 2 decimals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge 2: PCA on Breast Cancer Data\n",
    "\n",
    "**Business Context:** You're a data scientist working with medical researchers who have 30 tumor measurements per patient. The dataset is high-dimensional and hard to visualize. You'll use PCA to reduce dimensionality while preserving diagnostic information.\n",
    "\n",
    "**Your Task:** Apply PCA to the breast cancer dataset and analyze variance explained.\n",
    "\n",
    "**Quiz Questions 5-9 will ask about your PCA results. Record:**\n",
    "- PC1 variance % (round to 2 decimals)\n",
    "- PC1+PC2 cumulative variance % (round to 2 decimals)\n",
    "- Number of components for 90% variance\n",
    "- Number of components for 95% variance\n",
    "- Top 5 features for PC1 by absolute loading\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "**Step 1:** Load the breast cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer = pd.read_csv('https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/main/data/breast_cancer.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = breast_cancer.drop('diagnosis', axis=1)\n",
    "y = breast_cancer['diagnosis']\n",
    "\n",
    "print(f\"Features: {X.shape[1]} features\")\n",
    "print(f\"Samples: {X.shape[0]} patients\")\n",
    "print(f\"Diagnoses: {(y=='M').sum()} Malignant, {(y=='B').sum()} Benign\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Standardize the features\n",
    "\n",
    "**CRITICAL:** Use `random_state=42` for all operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Standardize using StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Fit PCA with all components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# - Fit PCA() with all components\n",
    "# - Calculate explained variance ratio for each component\n",
    "# - Calculate cumulative variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üìù RECORD THESE FOR QUIZ:**\n",
    "\n",
    "- **Question 5:** PC1 variance % (round to 2 decimals, e.g., 44.27)\n",
    "- **Question 6:** PC1+PC2 cumulative variance % (round to 2 decimals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Determine components needed for 90% and 95% variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# - Find number of components for 90% variance threshold\n",
    "# - Find number of components for 95% variance threshold\n",
    "# Hint: Use np.argmax(cumulative_variance >= 0.90) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üìù RECORD THESE FOR QUIZ:**\n",
    "\n",
    "- **Question 7:** How many components needed for 90% variance?\n",
    "- **Question 8:** How many components needed for 95% variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Analyze PC1 loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# - Create loadings dataframe from pca.components_\n",
    "# - Find top 5 features for PC1 by absolute loading value\n",
    "# - Print feature names and their loadings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üìù RECORD THIS FOR QUIZ:**\n",
    "\n",
    "- **Question 9:** What are the top 5 features (by absolute loading) for PC1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** Create visualizations (scree plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Create scree plot showing variance explained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge 3: PCA + Logistic Regression\n",
    "\n",
    "**Business Context:** You're building a breast cancer diagnostic system. Your stakeholders want high accuracy but also model efficiency (fast predictions, easy deployment). You'll compare a baseline model using all 30 features against PCA-reduced models.\n",
    "\n",
    "**Your Task:** Compare logistic regression performance with and without PCA.\n",
    "\n",
    "**Quiz Questions 10-15 will ask about model performance. Record:**\n",
    "- Baseline test accuracy (all 30 features)\n",
    "- PCA model test accuracy (10 components, 95% variance)\n",
    "- Best configuration test accuracy\n",
    "- Overfitting gaps for baseline and PCA models\n",
    "- Dimensionality reduction percentage\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "**Step 1:** Train-test split\n",
    "\n",
    "**CRITICAL:** Use `test_size=0.2`, `random_state=42`, `stratify=y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# - Split X and y with test_size=0.2, random_state=42, stratify=y\n",
    "# - Standardize training and test data separately\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Baseline model (all 30 features)\n",
    "\n",
    "**CRITICAL:** Use `LogisticRegression(max_iter=10000, random_state=42)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# - Train logistic regression on standardized training data\n",
    "# - Calculate training accuracy\n",
    "# - Calculate test accuracy\n",
    "# - Calculate overfitting gap (train_acc - test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üìù RECORD THESE FOR QUIZ:**\n",
    "\n",
    "- **Question 10:** Baseline test accuracy (round to 4 decimals)\n",
    "- **Question 13:** Baseline overfitting gap (round to 4 decimals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** PCA model with 95% variance\n",
    "\n",
    "**CRITICAL:** Use `PCA(n_components=0.95, random_state=42)` and `LogisticRegression(max_iter=10000, random_state=42)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# - Apply PCA(n_components=0.95) to training data (fit_transform)\n",
    "# - Transform test data using the same PCA\n",
    "# - Train logistic regression on PCA-transformed training data\n",
    "# - Calculate training and test accuracies\n",
    "# - Calculate overfitting gap\n",
    "# - Calculate dimensionality reduction percentage: (1 - n_components/30) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üìù RECORD THESE FOR QUIZ:**\n",
    "\n",
    "- **Question 11:** PCA model test accuracy (round to 4 decimals)\n",
    "- **Question 14:** PCA model overfitting gap (round to 4 decimals)\n",
    "- **Question 15:** Dimensionality reduction percentage (should be 66.7%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Compare multiple PCA configurations\n",
    "\n",
    "Test with 2, 5, 10, 15, 20, 25 components to find the best configuration.\n",
    "\n",
    "**CRITICAL:** Use `random_state=42` for all PCA and LogisticRegression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# - Loop through n_components_list = [2, 5, 10, 15, 20, 25]\n",
    "# - For each, fit PCA, transform data, train logistic regression\n",
    "# - Record training and test accuracies\n",
    "# - Find configuration with best test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üìù RECORD THIS FOR QUIZ:**\n",
    "\n",
    "- **Question 12:** Which configuration (number of components) achieved the highest test accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Create comparison summary and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# - Create summary table comparing baseline vs PCA models\n",
    "# - Create plot showing accuracy vs number of components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Lab Wrap-Up & Reflection\n",
    "\n",
    "### ‚úÖ What You Accomplished\n",
    "In this lab, you practiced:\n",
    "- Applying K-Means clustering to segment data into meaningful groups\n",
    "- Using the elbow method and silhouette scores to evaluate cluster quality\n",
    "- Interpreting cluster characteristics for business insights\n",
    "- Executing the complete PCA workflow from standardization to transformation\n",
    "- Using scree plots to determine optimal components\n",
    "- Interpreting component loadings to understand what PCs represent\n",
    "- Comparing ML models with original vs. PCA-transformed features\n",
    "- Evaluating tradeoffs between dimensionality reduction and performance\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "Take 2-3 minutes to consider:\n",
    "- When would you choose clustering over PCA, and vice versa?\n",
    "- What surprised you about the PCA results? Did reducing dimensions improve or hurt model performance?\n",
    "- How might you use these techniques in a real data science project?\n",
    "\n",
    "### üîó Connection to Course Goals\n",
    "Unsupervised learning completes your machine learning toolkit. You now know supervised learning (prediction with labels), unsupervised learning (pattern discovery without labels), and how to combine these techniques effectively. These are foundational skills for any data science role.\n",
    "\n",
    "### üìã Next Steps\n",
    "- **Quiz:** Complete the Week 13 online quiz in Canvas using your recorded answers from Part B\n",
    "- **Next Week:** Final exam review and course wrap-up\n",
    "- **Additional Practice:** Try PCA on other high-dimensional datasets (images, text, genomics)\n",
    "\n",
    "---\n",
    "**üíæ Save your work** and make sure you've recorded all the values needed for the quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Troubleshooting & Common Issues\n",
    "\n",
    "**Issue 1:** \"My silhouette scores don't match the quiz\"\n",
    "- **Solution:** Make sure you're using exactly `random_state=42` and `n_init=10` for all KMeans models. Also verify you filled missing values correctly (TotalBsmtSF and GarageArea with 0).\n",
    "\n",
    "**Issue 2:** \"My PCA variance percentages are slightly different\"\n",
    "- **Solution:** Ensure you standardized the data before PCA. Also round to the exact number of decimals specified (2 decimals for variance %, 4 decimals for accuracy).\n",
    "\n",
    "**Issue 3:** \"My logistic regression accuracies don't match\"\n",
    "- **Solution:** Check three things: (1) `test_size=0.2, random_state=42, stratify=y` in train_test_split, (2) `max_iter=10000, random_state=42` in LogisticRegression, (3) `random_state=42` in PCA.\n",
    "\n",
    "**Issue 4:** \"What if my silhouette score for k=3 is slightly off?\"\n",
    "- **Solution:** The quiz accepts a small range (e.g., 0.2280-0.2300). If you're outside this range, double-check your random_state and n_init parameters.\n",
    "\n",
    "**General Debugging Tips:**\n",
    "- Print intermediate results to verify each step\n",
    "- Check data shapes after each transformation\n",
    "- Make sure you're fitting on training data and transforming both train and test\n",
    "- If stuck, review the guided examples in Part A for similar patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
