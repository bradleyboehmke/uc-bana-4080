{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 Lab: Decision Trees, Random Forests, and Feature Importance\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/labs/11_wk11_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Welcome to Week 11! This lab introduces you to tree-based machine learning methods‚Äîdecision trees and random forests‚Äîalong with techniques for understanding which features drive your model's predictions. You'll learn how these powerful algorithms automatically discover non-linear patterns and interactions in data, making them invaluable tools for real-world business applications.\n",
    "\n",
    "Today's lab has two distinct parts: First, your TA will walk through a complete analysis using the Boston housing dataset, demonstrating how to build, evaluate, and interpret tree-based models. Then, you'll apply these techniques independently to the Breast Cancer Wisconsin dataset, building on your classification work from Week 10.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Build and interpret decision tree models for regression and classification problems\n",
    "- Construct random forest models and tune key hyperparameters\n",
    "- Compare model performance to select the best approach for a given problem\n",
    "- Calculate and interpret feature importance using permutation importance\n",
    "- Create and interpret partial dependence plots to understand feature effects\n",
    "\n",
    "## üìö This Lab Reinforces\n",
    "- **Chapter 25: Decision Trees - Foundations and Interpretability**\n",
    "- **Chapter 26: Random Forests - Ensemble Power and Robustness**\n",
    "- **Chapter 27: Understanding Feature Importance**\n",
    "\n",
    "## üïê Estimated Time & Structure\n",
    "**Total Time:** 75 minutes  \n",
    "**Mode:** TA-led demonstration + group work (serves as homework)\n",
    "\n",
    "- **[0‚Äì30 min]** Part 1: TA walkthrough with Boston housing data\n",
    "- **[30‚Äì35 min]** Class Q&A and transition\n",
    "- **[35‚Äì70 min]** Part 2: Independent group challenges with Breast Cancer data\n",
    "- **[70‚Äì75 min]** Wrap-up and reflection\n",
    "\n",
    "You are encouraged to work in small groups of **2‚Äì4 students** for Part 2.\n",
    "\n",
    "## üí° Why This Matters\n",
    "Tree-based methods are among the most widely used machine learning algorithms in business. Unlike linear models that assume straight-line relationships, decision trees and random forests automatically discover complex patterns, threshold effects, and feature interactions. Banks use them for credit risk assessment, hospitals for readmission prediction, and retailers for customer churn modeling. Feature importance techniques help you understand *why* your model makes certain predictions‚Äîcritical for building stakeholder trust, debugging models, and making data-driven business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We'll work with two datasets today: the Boston housing dataset (for regression with the TA) and the Breast Cancer Wisconsin dataset (for your independent classification analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from ISLP import load_data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üå≥ Ready to explore tree-based methods!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 ‚Äî TA Walkthrough: Boston Housing Data (30 minutes)\n",
    "\n",
    "Your TA will demonstrate the complete workflow for tree-based modeling using the Boston housing dataset. This is a regression problem where we predict median home values based on neighborhood characteristics.\n",
    "\n",
    "**Key Steps in This Walkthrough:**\n",
    "1. Load and explore the Boston housing data\n",
    "2. Prepare data and create train/test split\n",
    "3. Build and evaluate a decision tree regression model\n",
    "4. Build and evaluate a random forest regression model\n",
    "5. Calculate and interpret feature importance\n",
    "6. Create partial dependence plots to understand feature effects\n",
    "\n",
    "**Follow along and take notes** ‚Äî you'll apply this same process to the Breast Cancer data in Part 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Explore Data\n",
    "\n",
    "**Context:** The Boston housing dataset contains information about housing in the Boston area. Each observation represents a neighborhood, with features like crime rate, property tax rate, and pupil-teacher ratio. Our goal is to predict the median home value (`medv`).\n",
    "\n",
    "**Reference:** See Chapter 25, \"Building Your First Decision Tree\" section for data preparation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boston housing data\n",
    "Boston = load_data('Boston')\n",
    "\n",
    "print(\"Boston Housing Dataset Overview:\")\n",
    "print(f\"Shape: {Boston.shape}\")\n",
    "print(f\"\\nFeatures: {Boston.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(Boston.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values: {Boston.isnull().sum().sum()}\")\n",
    "\n",
    "# Basic statistics on target variable\n",
    "print(f\"\\nTarget variable (medv) statistics:\")\n",
    "print(Boston['medv'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Data and Split\n",
    "\n",
    "**Key Concept:** We separate features (X) from the target variable (y), then split into training and test sets to evaluate model performance on unseen data.\n",
    "\n",
    "**Reference:** Chapter 25 reviews train/test splitting for tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X_boston = Boston.drop('medv', axis=1)\n",
    "y_boston = Boston['medv']\n",
    "\n",
    "# Create 70-30 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_boston, y_boston, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} observations\")\n",
    "print(f\"Test set: {len(X_test)} observations\")\n",
    "print(f\"\\nNumber of features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build Decision Tree Model\n",
    "\n",
    "**Key Concept:** Decision trees make predictions by learning a series of yes/no questions about features. Unlike linear models that assume straight-line relationships, trees automatically discover non-linear patterns and interactions.\n",
    "\n",
    "**Reference:** Chapter 25, \"Regression Trees\" section explains how trees minimize MSE to find optimal splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build decision tree with max_depth=5 to prevent overfitting\n",
    "dt_model = DecisionTreeRegressor(max_depth=5, random_state=RANDOM_STATE)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_dt = dt_model.predict(X_train)\n",
    "y_pred_test_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "train_rmse_dt = np.sqrt(mean_squared_error(y_train, y_pred_train_dt))\n",
    "test_rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_test_dt))\n",
    "train_r2_dt = r2_score(y_train, y_pred_train_dt)\n",
    "test_r2_dt = r2_score(y_test, y_pred_test_dt)\n",
    "\n",
    "print(\"Decision Tree Performance:\")\n",
    "print(f\"Training RMSE: ${train_rmse_dt:,.2f}\")\n",
    "print(f\"Test RMSE: ${test_rmse_dt:,.2f}\")\n",
    "print(f\"Training R¬≤: {train_r2_dt:.3f}\")\n",
    "print(f\"Test R¬≤: {test_r2_dt:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build Random Forest Model\n",
    "\n",
    "**Key Concept:** Random forests combine many decision trees (each trained on a different random sample of data) to make more accurate and stable predictions. While a single tree can be unstable, the wisdom of crowds from many diverse trees produces robust results.\n",
    "\n",
    "**Reference:** Chapter 26, \"How Random Forests Work\" section explains bootstrap sampling and feature randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build random forest with 100 trees\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_rf = rf_model.predict(X_train)\n",
    "y_pred_test_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_pred_train_rf))\n",
    "test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_test_rf))\n",
    "train_r2_rf = r2_score(y_train, y_pred_train_rf)\n",
    "test_r2_rf = r2_score(y_test, y_pred_test_rf)\n",
    "\n",
    "print(\"Random Forest Performance:\")\n",
    "print(f\"Training RMSE: ${train_rmse_rf:,.2f}\")\n",
    "print(f\"Test RMSE: ${test_rmse_rf:,.2f}\")\n",
    "print(f\"Training R¬≤: {train_r2_rf:.3f}\")\n",
    "print(f\"Test R¬≤: {test_r2_rf:.3f}\")\n",
    "\n",
    "# Compare models\n",
    "print(f\"\\nüìä Model Comparison:\")\n",
    "print(f\"Random Forest improves test R¬≤ by {test_r2_rf - test_r2_dt:.3f} over single tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Calculate and Interpret Feature Importance\n",
    "\n",
    "**Key Concept:** Feature importance tells us which variables have the strongest influence on predictions. For random forests, we'll use **permutation importance**, which measures how much performance drops when we shuffle each feature's values.\n",
    "\n",
    "**Why This Matters:** Understanding which features drive predictions helps us build stakeholder trust, validate that the model makes business sense, and identify which factors to focus on.\n",
    "\n",
    "**Reference:** Chapter 27, \"Permutation Importance\" section explains this model-agnostic approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importance on test set\n",
    "perm_importance = permutation_importance(\n",
    "    rf_model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "# Create DataFrame for easy viewing\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': perm_importance.importances_mean\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Permutation Method):\")\n",
    "print(importance_df)\n",
    "\n",
    "# Visualize top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = importance_df.head(10)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance (drop in R¬≤ when shuffled)')\n",
    "plt.title('Top 10 Most Important Features for House Price Prediction')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Interpretation: The top feature is '{importance_df.iloc[0]['feature']}' with importance {importance_df.iloc[0]['importance']:.3f}\")\n",
    "print(f\"This means shuffling this feature causes R¬≤ to drop by {importance_df.iloc[0]['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create Partial Dependence Plots\n",
    "\n",
    "**Key Concept:** Partial dependence plots (PDPs) show how predictions change as we vary one feature while holding others constant. They reveal the *shape* of the relationship between a feature and the target.\n",
    "\n",
    "**Why This Matters:** While feature importance tells us *which* features matter, PDPs tell us *how* they influence predictions. Do prices increase linearly with square footage? Is there a threshold effect?\n",
    "\n",
    "**Reference:** Chapter 27, \"Exploring Prediction Behavior Across Variables\" section covers PDPs in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PDP for the most important feature\n",
    "top_feature = importance_df.iloc[0]['feature']\n",
    "\n",
    "print(f\"Creating Partial Dependence Plot for: {top_feature}\")\n",
    "\n",
    "# Create PDP\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    rf_model,\n",
    "    X_train,\n",
    "    features=[top_feature],\n",
    "    ax=ax\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° How to read this plot:\")\n",
    "print(f\"- X-axis: Values of {top_feature}\")\n",
    "print(f\"- Y-axis: Predicted median home value (in $1000s)\")\n",
    "print(f\"- The line shows how predictions change as {top_feature} increases\")\n",
    "print(f\"- Steep slopes indicate strong effects; flat regions indicate weak effects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Part 1 Summary\n",
    "\n",
    "**What we demonstrated:**\n",
    "1. ‚úì Built a decision tree regression model\n",
    "2. ‚úì Built a random forest that outperformed the single tree\n",
    "3. ‚úì Identified the most important features using permutation importance\n",
    "4. ‚úì Visualized how the top feature influences predictions using PDPs\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Random forests typically outperform single decision trees\n",
    "- Feature importance helps identify which variables drive predictions\n",
    "- PDPs reveal *how* features influence the outcome\n",
    "\n",
    "**Next:** You'll apply this same workflow to the Breast Cancer dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Discussion/Q&A (5 minutes)\n",
    "\n",
    "Before moving to Part 2, let's address any questions about the workflow:\n",
    "\n",
    "**Discussion prompts:**\n",
    "- How do decision trees differ from linear regression in terms of the relationships they can capture?\n",
    "- Why did the random forest outperform the single decision tree?\n",
    "- What does feature importance tell us that model performance metrics don't?\n",
    "- How would you explain a partial dependence plot to a non-technical stakeholder?\n",
    "\n",
    "**Common questions:**\n",
    "- *\"How do I choose max_depth for a decision tree?\"* ‚Üí Start with smaller values (3-10) and increase if needed. Too deep = overfitting.\n",
    "- *\"How many trees should I use in a random forest?\"* ‚Üí Typically 100-500. More trees = better performance but diminishing returns.\n",
    "- *\"What's the difference between impurity-based and permutation importance?\"* ‚Üí Impurity is faster but biased toward high-cardinality features; permutation is more reliable but slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 ‚Äî Independent Group Challenges: Breast Cancer Classification (35 minutes)\n",
    "\n",
    "Now it's your turn! You'll apply the same tree-based modeling workflow to the **Breast Cancer Wisconsin dataset** that you worked with in Week 10. This time, instead of logistic regression, you'll use decision trees and random forests for classification.\n",
    "\n",
    "**Scenario:** You're a data scientist at a medical diagnostics company. Your team needs to build a model that predicts whether a breast tumor is malignant (cancerous) or benign (non-cancerous) based on cell nucleus measurements from medical images. Understanding which features drive these predictions is critical for clinical validation and physician trust.\n",
    "\n",
    "### üö® Important Guidelines for Part 2\n",
    "\n",
    "For the challenges below:\n",
    "- **NO starter code provided** ‚Äî you'll write everything from scratch\n",
    "- **DO NOT use AI tools** (ChatGPT, Copilot, etc.) to generate code\n",
    "- **Work in groups of 2-4 students** and collaborate\n",
    "- **Ask the instructor for help** if you get stuck\n",
    "- **Refer to the chapters** for syntax and examples\n",
    "- **This section serves as your homework** ‚Äî complete all challenges\n",
    "\n",
    "### üìä Data Preparation (PROVIDED)\n",
    "\n",
    "We'll load and prepare the Breast Cancer data for you. Your challenges begin with splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer Wisconsin dataset (PROVIDED)\n",
    "url = \"https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/refs/heads/main/data/breast_cancer.csv\"\n",
    "cancer_data = pd.read_csv(url)\n",
    "\n",
    "print(\"‚úÖ Breast Cancer Wisconsin dataset loaded\")\n",
    "print(f\"Shape: {cancer_data.shape}\")\n",
    "print(f\"\\nTarget variable distribution:\")\n",
    "print(cancer_data['diagnosis'].value_counts())\n",
    "print(f\"\\nBaseline: {(cancer_data['diagnosis']=='M').mean():.1%} malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target (PROVIDED)\n",
    "# Create binary target: 1=Malignant, 0=Benign\n",
    "y_cancer = (cancer_data['diagnosis'] == 'M').astype(int)\n",
    "\n",
    "# Select all numeric features (exclude diagnosis column)\n",
    "X_cancer = cancer_data.drop('diagnosis', axis=1)\n",
    "\n",
    "print(f\"‚úÖ Data prepared for modeling\")\n",
    "print(f\"Features: {X_cancer.shape[1]} columns\")\n",
    "print(f\"Target: {len(y_cancer)} observations\")\n",
    "print(f\"\\nFeature names:\")\n",
    "print(X_cancer.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 ‚Äî Split the Data into Training and Test Sets\n",
    "\n",
    "**Question:** Create a 70-30 train/test split of the breast cancer data. Use `RANDOM_STATE` for reproducibility and `stratify=y_cancer` to maintain the class balance in both sets.\n",
    "\n",
    "**Your Task:**\n",
    "- Split `X_cancer` and `y_cancer` into training and test sets\n",
    "- Use 70% for training, 30% for testing\n",
    "- Name your variables: `X_train_cancer`, `X_test_cancer`, `y_train_cancer`, `y_test_cancer`\n",
    "- Print the sizes of both sets and the malignant rate in each\n",
    "- **IMPORTANT**: make sure you use `random_state=RANDOM_STATE`\n",
    "\n",
    "**Hint:** Review Chapter 25, \"Regression Trees\" section for the train/test split syntax. Don't forget to use `stratify=y_cancer` to ensure both sets have similar proportions of malignant/benign cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Your turn: split the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 ‚Äî Build Decision Trees with Different Depths\n",
    "\n",
    "**Question:** Build decision tree classifiers with different `max_depth` values and compare their performance. This will help you understand the bias-variance tradeoff.\n",
    "\n",
    "**Your Task:**\n",
    "1. Test the following `max_depth` values: **3, 5, 10**\n",
    "2. For each depth:\n",
    "   - Create a `DecisionTreeClassifier` with that `max_depth` and `random_state=RANDOM_STATE`\n",
    "   - Fit it on the training data\n",
    "   - Calculate **training accuracy** and **test accuracy**\n",
    "3. Store the results and identify which depth gives the best test accuracy\n",
    "4. Create a plot showing how training and test accuracy change with depth\n",
    "\n",
    "**Hint:** Chapter 25, \"Tree Parameters and Overfitting\" section explains `max_depth`. You'll need to use `DecisionTreeClassifier` (not Regressor since this is classification). Use `.score()` method to get accuracy.\n",
    "\n",
    "**Expected pattern:** Training accuracy should increase with depth, but test accuracy may plateau or decrease (overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: test different tree depths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 ‚Äî Build Random Forests with Different Hyperparameters\n",
    "\n",
    "**Question:** Now build random forest classifiers and tune hyperparameters to find the best combination for predicting breast cancer.\n",
    "\n",
    "**Your Task:**\n",
    "Test the following hyperparameter combinations:\n",
    "\n",
    "| Model | n_estimators | max_depth | max_features |\n",
    "|-------|--------------|-----------|-------------|\n",
    "| RF1   | 100          | 10        | 'sqrt'      |\n",
    "| RF2   | 200          | 15        | 'sqrt'      |\n",
    "| RF3   | 300          | 20        | 10          |\n",
    "| RF4   | 500          | None      | 'sqrt'      |\n",
    "\n",
    "For each model:\n",
    "1. Create a `RandomForestClassifier` with the specified hyperparameters (use `random_state=RANDOM_STATE`)\n",
    "2. Fit on training data\n",
    "3. Calculate training accuracy, test accuracy, precision, recall, and F1-score on test set\n",
    "4. Store results for comparison\n",
    "\n",
    "**Hint:** Chapter 26, \"Key Hyperparameters\" section explains each parameter:\n",
    "- `n_estimators`: Number of trees in the forest\n",
    "- `max_depth`: Maximum depth of each tree (None = unlimited)\n",
    "- `max_features`: Number of features to consider at each split\n",
    "\n",
    "**Medical Context:** For cancer diagnosis, recall (catching all malignant cases) is often more important than precision (avoiding false alarms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: test different random forest configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4 ‚Äî Identify the Best Model\n",
    "\n",
    "**Question:** Compare all the models you've built (decision trees from Challenge 2 and random forests from Challenge 3) and identify which one performs best.\n",
    "\n",
    "**Your Task:**\n",
    "1. Create a summary table or visualization comparing:\n",
    "   - All decision tree depths tested (Challenge 2)\n",
    "   - All random forest configurations (Challenge 3)\n",
    "2. Compare them on test accuracy, precision, recall, and F1-score\n",
    "3. Identify the single best model overall\n",
    "4. Explain WHY this model is best (consider: which metrics matter most for cancer diagnosis?)\n",
    "\n",
    "**Discussion Questions:**\n",
    "- Did random forests outperform single decision trees? By how much?\n",
    "- Which random forest configuration worked best?\n",
    "- Given that missing a malignant tumor is more serious than a false alarm, which metric should we prioritize?\n",
    "- Would you recommend this model for clinical use? What would you want to improve?\n",
    "\n",
    "**Hint:** Chapter 26 discusses why random forests typically outperform single trees (ensemble averaging reduces overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: compare all models and identify the best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5 ‚Äî Identify Top 5 Most Important Features\n",
    "\n",
    "**Question:** Using your best-performing random forest model from Challenge 4, identify which features are most important for predicting breast cancer.\n",
    "\n",
    "**Your Task:**\n",
    "1. Use **permutation importance** (not impurity-based) on your best random forest model\n",
    "2. Calculate importance on the **test set** with `n_repeats=10`\n",
    "3. Identify the **top 5 most important features**\n",
    "4. Create a horizontal bar plot visualizing these top 5 features\n",
    "5. Interpret the results: Do these features make medical/biological sense?\n",
    "\n",
    "**Medical Context:** Understanding which cell characteristics drive cancer predictions helps clinicians validate the model and focus their diagnostic attention on the most informative measurements.\n",
    "\n",
    "**Hint:** Chapter 27, \"Permutation Importance\" section shows the complete workflow:\n",
    "```python\n",
    "from sklearn.inspection import permutation_importance\n",
    "perm_importance = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE\n",
    ")\n",
    "```\n",
    "\n",
    "**Expected features:** You should see features related to cell size, shape, and texture among the top predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: calculate permutation importance and identify top 5 features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6 ‚Äî Create and Interpret Partial Dependence Plot\n",
    "\n",
    "**Question:** Create a partial dependence plot (PDP) for the most important feature from Challenge 5 to understand HOW it influences cancer predictions.\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify the #1 most important feature from Challenge 5\n",
    "2. Create a partial dependence plot for this feature using your best random forest\n",
    "3. Interpret the plot by answering:\n",
    "   - As this feature increases, does malignancy probability increase or decrease?\n",
    "   - Is the relationship linear or non-linear?\n",
    "   - Are there any threshold effects (sudden changes)?\n",
    "   - What does this tell us about cancer diagnosis?\n",
    "\n",
    "**Clinical Interpretation:** If the most important feature is something like `radius_mean` (cell size), you might find that larger cells are associated with higher cancer probability. This aligns with medical knowledge that cancerous cells often grow larger than normal cells.\n",
    "\n",
    "**Hint:** Chapter 27, \"Exploring Prediction Behavior Across Variables\" section explains PDPs:\n",
    "```python\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    model, X_train, features=['feature_name']\n",
    ")\n",
    "```\n",
    "\n",
    "**Remember:** PDPs show the average predicted probability across all observations as we vary one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: create PDP for the most important feature and interpret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Lab Wrap-Up & Reflection\n",
    "\n",
    "### ‚úÖ What You Accomplished\n",
    "\n",
    "Congratulations! In this lab, you:\n",
    "- **Built decision tree models** and explored how max_depth affects the bias-variance tradeoff\n",
    "- **Constructed random forest models** and tuned multiple hyperparameters (n_estimators, max_depth, max_features)\n",
    "- **Compared model performance** across different configurations to select the best approach\n",
    "- **Calculated permutation importance** to identify which features drive cancer predictions\n",
    "- **Created partial dependence plots** to understand how features influence malignancy probability\n",
    "- **Applied tree-based methods** to a real medical classification problem\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "\n",
    "Take 2-3 minutes to consider:\n",
    "\n",
    "1. **Model Comparison**: Why did random forests outperform single decision trees? What advantage comes from combining many trees?\n",
    "\n",
    "2. **Feature Importance**: Which cell characteristics were most predictive of cancer? Do these align with medical knowledge about cancer cells?\n",
    "\n",
    "3. **Interpretability vs. Accuracy**: Decision trees are easier to visualize than random forests. Is the accuracy gain from random forests worth the loss of interpretability for medical diagnosis?\n",
    "\n",
    "4. **Business Impact**: How would you explain your model's predictions to a physician who doesn't have data science training? What role do PDPs play in building trust?\n",
    "\n",
    "5. **Actionable vs. Predictive**: Some features are important for prediction but not actionable (e.g., cell radius_mean). How does this distinction matter for clinical decision-making?\n",
    "\n",
    "### üîó Connection to Course Goals\n",
    "\n",
    "This lab bridges several key data science concepts:\n",
    "- **Supervised learning**: Classification with tree-based methods\n",
    "- **Model evaluation**: Comparing models using multiple metrics\n",
    "- **Interpretability**: Understanding what drives predictions\n",
    "- **Business application**: Medical diagnosis where false negatives have serious consequences\n",
    "\n",
    "Tree-based methods are among the most widely used algorithms in practice because they:\n",
    "- Handle non-linear relationships automatically\n",
    "- Require minimal data preprocessing\n",
    "- Provide feature importance insights\n",
    "- Work well out-of-the-box with minimal tuning\n",
    "\n",
    "### üìã Next Steps\n",
    "\n",
    "- **Homework**: This lab serves as your Week 11 homework. Make sure all challenges are complete and well-documented.\n",
    "- **Next Week**: We'll explore gradient boosting methods (XGBoost, LightGBM) which often outperform random forests by building trees sequentially rather than independently.\n",
    "- **Additional Practice**: Try applying these techniques to other datasets from previous weeks (Default, Ames housing, etc.)\n",
    "\n",
    "---\n",
    "**üíæ Save your work** and ensure all code cells run successfully from top to bottom!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Troubleshooting & Common Issues\n",
    "\n",
    "**Issue 1: \"ImportError: cannot import name 'permutation_importance'\"**\n",
    "- **Solution:** Make sure you're using scikit-learn version 0.22 or higher. Update with: `pip install --upgrade scikit-learn`\n",
    "\n",
    "**Issue 2: \"My decision tree has 100% training accuracy but poor test accuracy\"**\n",
    "- **Solution:** This is overfitting! Your tree is too deep. Try:\n",
    "  - Reducing `max_depth` to 5-10\n",
    "  - Increasing `min_samples_split` to 20-50\n",
    "  - Increasing `min_samples_leaf` to 10-20\n",
    "\n",
    "**Issue 3: \"Random forest takes a long time to train\"**\n",
    "- **Solution:** This is normal for large forests. Speed it up by:\n",
    "  - Reducing `n_estimators` (100 is usually sufficient)\n",
    "  - Adding `n_jobs=-1` to use all CPU cores\n",
    "  - Limiting `max_depth` to prevent very deep trees\n",
    "\n",
    "**Issue 4: \"Permutation importance shows negative values\"**\n",
    "- **Solution:** Small negative values are normal‚Äîthey mean shuffling that feature slightly improved performance by chance. These features are not important.\n",
    "\n",
    "**Issue 5: \"My PDP looks very jagged/noisy\"**\n",
    "- **Solution:** This happens in sparse data regions. Try:\n",
    "  - Increasing `grid_resolution` parameter\n",
    "  - Checking the data distribution (rug plot) to see where you have few observations\n",
    "  - Focus interpretation on regions with dense data\n",
    "\n",
    "**Issue 6: \"AttributeError: 'DecisionTreeClassifier' has no attribute 'feature_importances_'\"**\n",
    "- **Solution:** You must `.fit()` the model before accessing feature importance. Make sure you've trained the model first.\n",
    "\n",
    "**Issue 7: \"All my random forests perform similarly regardless of hyperparameters\"**\n",
    "- **Solution:** This could mean:\n",
    "  - The default parameters are already good for this dataset\n",
    "  - The dataset is relatively simple and most reasonable configurations work well\n",
    "  - Try more extreme parameter values to see bigger differences\n",
    "\n",
    "### General Debugging Tips:\n",
    "\n",
    "1. **Check shapes**: Always verify `X_train.shape`, `y_train.shape` match expectations\n",
    "2. **Print intermediate results**: Don't wait until the end to check if things work\n",
    "3. **Start simple**: Build one model successfully before trying multiple configurations\n",
    "4. **Read error messages carefully**: They often tell you exactly what's wrong\n",
    "5. **Use RANDOM_STATE consistently**: This ensures reproducible results for debugging\n",
    "6. **Refer to the chapters**: The code examples in Chapters 25-27 show complete working syntax\n",
    "\n",
    "### Getting Help:\n",
    "\n",
    "If you're stuck:\n",
    "1. Re-read the relevant chapter section mentioned in the challenge hint\n",
    "2. Check the Part 1 TA walkthrough for similar code patterns\n",
    "3. Ask your group members or neighboring groups\n",
    "4. Raise your hand and ask the instructor\n",
    "5. Post on Canvas discussion board with your specific error message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
