{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TA Guidance: Week 12 Lab - The Professional ML Workflow\n",
    "\n",
    "## üéØ Lab Overview and Teaching Philosophy\n",
    "\n",
    "**Critical Understanding:** This lab represents a **MAJOR MILESTONE** in the students' data science journey. Until now, they've been \"peeking\" at the test set repeatedly‚Äîa fundamental violation of professional ML practice. This week, they learn the **proper 5-stage workflow** used in production environments.\n",
    "\n",
    "### Learning Objectives\n",
    "- Students will implement k-fold cross-validation to compare models without contaminating the test set\n",
    "- Students will use GridSearchCV to systematically tune hyperparameters across multiple parameters\n",
    "- Students will apply feature engineering techniques (encoding, scaling, feature creation)\n",
    "- Students will build end-to-end pipelines that prevent data leakage\n",
    "- Students will execute the complete professional ML workflow from data preparation through final evaluation\n",
    "\n",
    "### Time Allocation & Teaching Strategy\n",
    "- **[0‚Äì30 min]** Part A: Guided Reinforcement ‚Äî TA-led practice with cross-validation, GridSearchCV, and feature engineering\n",
    "- **[30‚Äì40 min]** Class Q&A ‚Äî Discussion and clarification of key concepts\n",
    "- **[40‚Äì72 min]** Part B: Independent Challenges ‚Äî 6 group challenges applying the complete professional workflow\n",
    "- **[72‚Äì75 min]** Wrap-Up & Reflection ‚Äî What you learned and next steps\n",
    "\n",
    "### Content Alignment\n",
    "- Directly reinforces Tuesday's slides on cross-validation and hyperparameter tuning\n",
    "- Provides hands-on practice with concepts from Chapters 28-30\n",
    "- Uses familiar Ames housing data to focus on workflow rather than data exploration\n",
    "- Bridges beginner practices to professional production-ready techniques\n",
    "\n",
    "## üõ†Ô∏è Pre-Lab Setup Instructions\n",
    "\n",
    "**Technical Setup:**\n",
    "- Ensure all students can access Google Colab and load the lab notebook\n",
    "- Test that Ames dataset URL loads correctly (both local and GitHub paths)\n",
    "- Verify sklearn, pandas, numpy imports work properly\n",
    "- **WARNING**: Challenge 2 and 4 involve 540 model fits‚Äîwarn students this takes 2-3 minutes\n",
    "\n",
    "**Important Teaching Approach:**\n",
    "- **Part A**: Walk through guided examples step-by-step, ensure everyone follows along\n",
    "- **Part B**: Students work in groups of 2-4; circulate to provide help but let them struggle productively\n",
    "- **Group work encouraged**: This lab is designed for collaborative learning\n",
    "\n",
    "**Materials Needed:**\n",
    "- Lab notebook: `12_wk12_lab.ipynb`\n",
    "- This TA guidance notebook\n",
    "- Access to Ames housing dataset (`ames_clean.csv`)\n",
    "\n",
    "## üìö Key Concepts to Emphasize\n",
    "\n",
    "1. **Test Set Contamination**: Every \"peek\" at the test set makes performance estimates less trustworthy\n",
    "2. **Cross-Validation Philosophy**: Compare models and tune hyperparameters using ONLY training data\n",
    "3. **GridSearchCV Automation**: Systematic exploration of parameter space with built-in CV\n",
    "4. **Feature Engineering Impact**: Encoding and scaling can dramatically improve model performance\n",
    "5. **Pipeline Benefits**: Prevents data leakage and ensures reproducible workflows\n",
    "6. **The 5-Stage Workflow**: Data prep ‚Üí Split ‚Üí Compare models (CV) ‚Üí Tune (GridSearchCV) ‚Üí Final test evaluation\n",
    "7. **Random State Consistency**: Always use `random_state=42` for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A Teaching Guide: Guided Reinforcement (30 minutes)\n",
    "\n",
    "### Section 1: Cross-Validation Basics (10 minutes)\n",
    "\n",
    "**Teaching Approach:**\n",
    "- **Start with the problem**: \"How many of you have tuned models by checking test set performance multiple times?\" (Most will raise hands)\n",
    "- **Explain the issue**: Each peek makes your performance estimate less reliable‚Äîyou're overfitting to the test set\n",
    "- **Present the solution**: Cross-validation lets you compare models using only training data\n",
    "- **Emphasize the mindset shift**: \"The test set is LOCKED until the very end‚Äîwe pretend it doesn't exist\"\n",
    "\n",
    "### Key Teaching Points:\n",
    "\n",
    "**1. Why Cross-Validation Matters:**\n",
    "- Simple train/test split gives ONE performance estimate‚Äîcould be lucky or unlucky\n",
    "- Cross-validation gives MULTIPLE estimates‚Äîmore robust and reliable\n",
    "- Using CV, we can compare models without ever touching the test set\n",
    "\n",
    "**2. How K-Fold CV Works:**\n",
    "- Data is split into K folds (usually 5 or 10)\n",
    "- Train on K-1 folds, validate on the remaining fold\n",
    "- Repeat K times so each fold serves as validation once\n",
    "- Average the K performance scores\n",
    "\n",
    "**3. Interpreting CV Results:**\n",
    "- **Mean score**: Expected performance on unseen data\n",
    "- **Standard deviation**: Consistency across different data splits\n",
    "- Low std dev = stable model, high std dev = unstable model\n",
    "\n",
    "### Guided Example Walkthrough:\n",
    "\n",
    "**Cell 5 - Train/Test Split:**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "```\n",
    "**Teaching moment**: \"Once we run this cell, we LOCK the test set. We won't touch X_test or y_test until Challenge 4!\"\n",
    "\n",
    "**Cell 6 - First CV Example:**\n",
    "```python\n",
    "cv_scores = cross_val_score(\n",
    "    dt, X_train, y_train, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "```\n",
    "**Teaching points**:\n",
    "- `cv=5` means 5-fold cross-validation\n",
    "- `scoring='neg_root_mean_squared_error'` returns negative RMSE (convert to positive for interpretation)\n",
    "- Notice we ONLY use X_train and y_train‚Äîtest set untouched!\n",
    "\n",
    "**Expected Output Discussion:**\n",
    "```\n",
    "RMSE per fold: [34567, 35123, 33890, 34456, 35001]\n",
    "Mean RMSE: $34,607\n",
    "Std Dev: $456\n",
    "```\n",
    "- \"What does the mean tell us?\" ‚Üí Expected performance on new data\n",
    "- \"What does std dev tell us?\" ‚Üí How consistent the model is\n",
    "- \"Why are the scores different across folds?\" ‚Üí Different validation sets, natural variation\n",
    "\n",
    "### üß† \"Your Turn\" Exercise Guidance:\n",
    "\n",
    "**Give students 3-5 minutes** to modify `max_depth` and compare models.\n",
    "\n",
    "**Circulate and check:**\n",
    "- Are they changing only the `max_depth` parameter?\n",
    "- Are they comparing mean RMSE values?\n",
    "- Do they understand which model is \"better\" (lower RMSE)?\n",
    "\n",
    "**Expected behavior:**\n",
    "- `max_depth=5`: Higher RMSE (underfitting)\n",
    "- `max_depth=15`: Lower RMSE (better fit)\n",
    "- Students should observe the bias-variance tradeoff\n",
    "\n",
    "**Discussion prompts:**\n",
    "- \"Which model had lower mean RMSE?\"\n",
    "- \"Which model had more consistent scores (lower std dev)?\"\n",
    "- \"How is this better than checking the test set repeatedly?\"\n",
    "\n",
    "**Key insight to emphasize**: \"You just compared two models without EVER touching the test set. That's the power of cross-validation!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Hyperparameter Tuning with GridSearchCV (10 minutes)\n",
    "\n",
    "**Teaching Approach:**\n",
    "- **Start with the pain point**: \"Manually trying different hyperparameters one by one is tedious and error-prone\"\n",
    "- **Introduce automation**: \"GridSearchCV systematically tests all combinations and uses CV to find the best\"\n",
    "- **Emphasize scale**: \"We're going to train 60 models in one command‚Äîthis is the power of automation\"\n",
    "\n",
    "### Key Teaching Points:\n",
    "\n",
    "**1. What GridSearchCV Does:**\n",
    "- Takes a parameter grid (all combinations you want to test)\n",
    "- For each combination, performs k-fold CV\n",
    "- Returns the best parameters and best CV score\n",
    "- Automatically retrains on full training set with best parameters\n",
    "\n",
    "**2. Parameter Grid Structure:**\n",
    "```python\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],      # 2 values\n",
    "    'max_depth': [10, 15, 20],       # 3 values\n",
    "    'min_samples_split': [2, 5]      # 2 values\n",
    "}\n",
    "```\n",
    "**Teaching moment**: \"How many combinations will we test? 2 √ó 3 √ó 2 = 12 configurations. With 5-fold CV, that's 60 models!\"\n",
    "\n",
    "**3. Computational Cost Awareness:**\n",
    "- More parameters = exponentially more models\n",
    "- Start with coarse grid, then refine\n",
    "- Use `n_jobs=-1` to parallelize across CPU cores\n",
    "- Be strategic about parameter ranges\n",
    "\n",
    "### Guided Example Walkthrough:\n",
    "\n",
    "**Cell 11 - GridSearchCV Setup:**\n",
    "```python\n",
    "grid_search = GridSearchCV(\n",
    "    rf, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "```\n",
    "\n",
    "**Teaching points**:\n",
    "- `cv=5`: Each configuration tested with 5-fold CV\n",
    "- `scoring`: Same metric we want to optimize\n",
    "- `n_jobs=-1`: Speeds up computation by using all cores\n",
    "- `verbose=1`: Shows progress during fitting\n",
    "\n",
    "**Cell 11 - Fitting:**\n",
    "```python\n",
    "grid_search.fit(X_train, y_train)\n",
    "```\n",
    "**Teaching moment**: \"This one line trains 60 models! Watch the progress bar‚Äîyou'll see it working through all combinations.\"\n",
    "\n",
    "**Expected wait time**: 30-60 seconds depending on system\n",
    "\n",
    "**Cell 12 - Results:**\n",
    "```python\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV RMSE: ${-grid_search.best_score_:,.0f}\")\n",
    "```\n",
    "\n",
    "**Discussion prompts**:\n",
    "- \"What are the optimal hyperparameters GridSearchCV found?\"\n",
    "- \"How does the best CV RMSE compare to our earlier models?\"\n",
    "- \"Why is this better than manually trying different values?\"\n",
    "\n",
    "**Key insights to emphasize**:\n",
    "- Systematic exploration finds better configurations than manual search\n",
    "- Still haven't touched the test set!\n",
    "- `grid_search.best_estimator_` is already retrained on full training data\n",
    "\n",
    "### üß™ Practice Exercise Guidance:\n",
    "\n",
    "**Give students 5-7 minutes** to complete the DecisionTree GridSearchCV exercise.\n",
    "\n",
    "**Common issues to watch for:**\n",
    "1. **Syntax errors in param_grid**: Check dictionary structure and list syntax\n",
    "2. **Forgot `cv=5`**: Remind them this sets the number of folds\n",
    "3. **Wrong attribute names**: `best_params_` and `best_score_` (note the underscores!)\n",
    "4. **Forgot negative sign**: `best_score_` is negative, need `-grid_search.best_score_`\n",
    "\n",
    "**Solution (for reference):**\n",
    "```python\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20],           \n",
    "    'min_samples_split': [2, 5, 10],       \n",
    "    'min_samples_leaf': [1, 2, 4]         \n",
    "}\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    dt,\n",
    "    param_grid,\n",
    "    cv=5,                                   \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV RMSE: $\", -grid_search.best_score_)\n",
    "```\n",
    "\n",
    "**Expected result**: Best RMSE should be around $30,000-$35,000 depending on optimal params found.\n",
    "\n",
    "**Computational note**: 4 √ó 3 √ó 3 = 36 configs √ó 5 folds = 180 models‚Äîmay take 30-45 seconds.\n",
    "\n",
    "**After exercise discussion:**\n",
    "- \"How many total models did GridSearchCV train?\"\n",
    "- \"What were the optimal hyperparameters?\"\n",
    "- \"Did tuning improve performance significantly?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Feature Engineering Essentials (10 minutes)\n",
    "\n",
    "**Teaching Approach:**\n",
    "- **Start with motivation**: \"Raw data often isn't in the best format for ML‚Äîwe need to transform it\"\n",
    "- **Focus on practical techniques**: Scaling and encoding are the bread-and-butter of feature engineering\n",
    "- **Emphasize when techniques matter**: Not all models benefit from all transformations\n",
    "\n",
    "### Key Teaching Points:\n",
    "\n",
    "**1. When Scaling Matters:**\n",
    "- **Distance-based models** (KNN, SVM, Neural Networks): REQUIRE scaling\n",
    "- **Regularized models** (Ridge, Lasso, ElasticNet): REQUIRE scaling\n",
    "- **Tree-based models** (Decision Trees, Random Forest, XGBoost): DON'T need scaling\n",
    "- **Plain Linear Regression**: Doesn't need scaling (closed-form solution)\n",
    "\n",
    "**2. StandardScaler Mechanics:**\n",
    "- Transforms features to have mean=0, std=1\n",
    "- Formula: `(x - mean) / std`\n",
    "- **CRITICAL**: Fit on training data, transform both train and test\n",
    "- **Data leakage warning**: Never fit scaler on combined train+test data!\n",
    "\n",
    "**3. Encoding Categorical Variables:**\n",
    "- **One-Hot Encoding**: Creates binary columns for each category\n",
    "  - Best for nominal categories (no inherent order)\n",
    "  - Can create many columns with high-cardinality features\n",
    "- **Label Encoding**: Assigns integers to categories\n",
    "  - Best for ordinal categories (natural order)\n",
    "  - Works with tree-based models (they can learn splits)\n",
    "  - Can mislead linear models (implies order/magnitude)\n",
    "\n",
    "### Guided Example Walkthrough:\n",
    "\n",
    "**Cells 17-18 - Scaling Impact:**\n",
    "\n",
    "**Without scaling (Cell 17):**\n",
    "```python\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "cv_scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "```\n",
    "**Expected result**: RMSE around $60,000-$70,000 (poor performance)\n",
    "\n",
    "**Teaching moment**: \"Why is KNN performing so poorly? Because features are on different scales‚ÄîGrLivArea (1000-4000) dominates YearBuilt (1900-2010)!\"\n",
    "\n",
    "**With scaling (Cell 18):**\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "**CRITICAL TEACHING POINT**: \"Notice we FIT the scaler on training data, but TRANSFORM both train and test. This prevents data leakage!\"\n",
    "\n",
    "**Expected result**: RMSE around $35,000-$45,000 (dramatic improvement!)\n",
    "\n",
    "**Discussion prompts:**\n",
    "- \"Why did scaling help KNN but wouldn't help Random Forest?\"\n",
    "- \"What would happen if we fit the scaler on all the data before splitting?\"\n",
    "- \"When should we use scaling?\"\n",
    "\n",
    "**Cell 19 - When Scaling Matters Table:**\n",
    "Walk through this table carefully‚Äîstudents often misunderstand which models need scaling.\n",
    "\n",
    "**Key takeaways:**\n",
    "- **Always scale**: KNN, SVM, Neural Networks, Ridge/Lasso, PCA\n",
    "- **Doesn't matter**: Plain Linear Regression, Decision Trees, Random Forest, XGBoost, Gradient Boosting\n",
    "\n",
    "**Cells 21-22 - One-Hot Encoding:**\n",
    "\n",
    "**Setup (Cell 21):**\n",
    "```python\n",
    "features = ['Neighborhood', 'BldgType', 'GrLivArea', ...]\n",
    "```\n",
    "**Teaching moment**: \"We're adding categorical variables‚ÄîNeighborhood has 25 unique values, BldgType has 5.\"\n",
    "\n",
    "**Encoding (Cell 22):**\n",
    "```python\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_cat_encoded = encoder.fit_transform(X_train[cat_cols])\n",
    "```\n",
    "\n",
    "**Teaching points:**\n",
    "- `handle_unknown='ignore'`: Won't crash if test set has new categories\n",
    "- `sparse_output=False`: Returns dense array (easier to work with)\n",
    "- Creates 30 new columns (25 neighborhoods + 5 building types)\n",
    "\n",
    "**Expected output**: \"One-hot encoding turned our 8 original features into 36 features.\"\n",
    "\n",
    "**Discussion**: \"Why did we go from 8 to 36 features? Is this always necessary?\"\n",
    "\n",
    "**Cells 24 - Label Encoding Alternative:**\n",
    "\n",
    "```python\n",
    "le = LabelEncoder()\n",
    "X_train['Neighborhood_LE'] = le.fit_transform(X_train['Neighborhood'])\n",
    "```\n",
    "\n",
    "**Teaching moment**: \"Label encoding assigns each neighborhood a number 0-24. This works well for tree-based models but can mislead linear models.\"\n",
    "\n",
    "**When to use each:**\n",
    "- **One-hot**: Linear models, neural networks, when categories are nominal\n",
    "- **Label**: Tree-based models, when categories are ordinal, high cardinality features\n",
    "\n",
    "**Cells 26-27 - Pipelines:**\n",
    "\n",
    "**Why pipelines are critical:**\n",
    "1. **Prevent data leakage**: Transformations fit only on training folds during CV\n",
    "2. **Reproducibility**: Easy to apply same transformations to new data\n",
    "3. **Cleaner code**: All steps bundled together\n",
    "4. **Deployment-ready**: Can save and load entire pipeline\n",
    "\n",
    "**Pipeline structure:**\n",
    "```python\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'), onehot_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "```\n",
    "\n",
    "**Teaching points:**\n",
    "- `ColumnTransformer`: Applies different transformations to different columns\n",
    "- `Pipeline`: Chains preprocessing and modeling steps\n",
    "- Can pass entire pipeline to `GridSearchCV`\n",
    "- Use `model__parameter` syntax to tune model hyperparameters in pipeline\n",
    "\n",
    "**Expected output**: Best CV RMSE around $25,000-$30,000 (improvement from feature engineering!)\n",
    "\n",
    "**Key insight**: \"We just built a production-ready pipeline that prevents data leakage and is easy to deploy!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Discussion/Q&A Section (10 minutes)\n",
    "\n",
    "### Facilitation Strategy:\n",
    "\n",
    "**Open with reflection prompt**: \"Before we move to independent challenges, let's discuss some key concepts. I want to hear YOUR understanding.\"\n",
    "\n",
    "### Discussion Prompts (Select 3-4):\n",
    "\n",
    "**1. Cross-Validation vs. Train/Test Split**\n",
    "- Prompt: \"What's the difference between cross-validation and a simple train/test split?\"\n",
    "- Expected answers:\n",
    "  - CV gives multiple performance estimates, train/test gives one\n",
    "  - CV uses training data more efficiently\n",
    "  - CV provides measure of model stability (std dev)\n",
    "- **TA clarification**: \"Both approaches use a test set! CV just helps us make better decisions about model selection and tuning using ONLY the training portion.\"\n",
    "\n",
    "**2. Test Set Discipline**\n",
    "- Prompt: \"Why shouldn't we look at the test set until the very end?\"\n",
    "- Expected answers:\n",
    "  - Prevents overfitting to test set\n",
    "  - Keeps performance estimate honest\n",
    "  - Simulates real-world deployment (you don't have future data!)\n",
    "- **TA clarification**: \"Every time you peek at test set performance and make a decision based on it, you're indirectly optimizing for that specific test set. Your performance estimate becomes less trustworthy.\"\n",
    "\n",
    "**3. GridSearchCV vs. Manual Tuning**\n",
    "- Prompt: \"When would you use GridSearchCV vs. manually trying different hyperparameters?\"\n",
    "- Expected answers:\n",
    "  - GridSearchCV: When you have multiple parameters to tune, want systematic exploration\n",
    "  - Manual: When you have intuition about good values, doing quick experiments\n",
    "- **TA clarification**: \"In production ML, you almost always use GridSearchCV or RandomizedSearchCV. Manual tuning doesn't scale and introduces human bias.\"\n",
    "\n",
    "**4. Data Leakage from Scaling**\n",
    "- Prompt: \"What happens if you fit a scaler on all your data before splitting into train/test?\"\n",
    "- Expected answers (students often struggle here):\n",
    "  - Information from test set leaks into training data\n",
    "  - Performance estimates become overly optimistic\n",
    "  - Model won't perform as well on truly new data\n",
    "- **TA clarification**: \"The scaler computes mean and std from the data. If you fit on all data, your training set 'knows' information about the test set. Always fit transformers on training data only!\"\n",
    "\n",
    "**5. Pipeline Benefits**\n",
    "- Prompt: \"How do pipelines help prevent data leakage?\"\n",
    "- Expected answers:\n",
    "  - Automatically fit transformers only on training folds during CV\n",
    "  - Ensures consistent transformation order\n",
    "  - Makes it impossible to accidentally apply transformations incorrectly\n",
    "- **TA clarification**: \"When you pass a pipeline to GridSearchCV, the scaling/encoding is fit INSIDE each CV fold. This is the gold standard for preventing leakage.\"\n",
    "\n",
    "### Common Blockers and Clarifications:\n",
    "\n",
    "**1. \"My GridSearchCV is taking forever!\"**\n",
    "- **Root cause**: Too many parameter combinations (exponential growth)\n",
    "- **Solution**: \"Start with a coarse grid. If you have 5 values each for 4 parameters, that's 5^4 = 625 configurations! Try 2-3 values per parameter first.\"\n",
    "- **Teaching moment**: \"In practice, you often do a coarse grid search first, then a fine-grained search around the best region.\"\n",
    "\n",
    "**2. \"Do I need to scale features for Random Forest?\"**\n",
    "- **Answer**: \"No! Tree-based models are scale-invariant. They only care about feature order, not magnitude.\"\n",
    "- **Why students ask**: Confusion about when scaling matters\n",
    "- **Clarification**: Refer back to the table in Cell 19\n",
    "\n",
    "**3. \"When do I use one-hot vs. label encoding?\"**\n",
    "- **Rule of thumb**:\n",
    "  - Nominal categories (no order): One-hot encoding\n",
    "  - Ordinal categories (natural order): Label encoding or ordinal encoding\n",
    "  - High cardinality + tree models: Label encoding (avoids dimension explosion)\n",
    "  - Linear models: Almost always one-hot (label encoding implies order)\n",
    "- **Example**: \"Neighborhood is nominal (no inherent order), so one-hot is safer. But with 25 neighborhoods, label encoding works fine for tree models.\"\n",
    "\n",
    "**4. \"I'm getting different results than my partner!\"**\n",
    "- **Root cause**: Inconsistent random states\n",
    "- **Solution**: \"Always use `random_state=42` in ALL random operations: train_test_split, model initialization, GridSearchCV.\"\n",
    "- **Teaching moment**: \"Reproducibility is critical in professional ML. Different random states = different results = can't compare fairly.\"\n",
    "\n",
    "**5. \"What's the difference between .fit(), .transform(), and .fit_transform()?\"**\n",
    "- **Explanation**:\n",
    "  - `.fit()`: Learns parameters from data (e.g., mean/std for scaler)\n",
    "  - `.transform()`: Applies learned transformation\n",
    "  - `.fit_transform()`: Does both in one step (only use on training data!)\n",
    "- **Critical rule**: \"Fit on training data, transform on both train and test.\"\n",
    "\n",
    "### Transition to Part B:\n",
    "\n",
    "\"Great discussion! Now you'll apply these concepts in 6 independent challenges. Work in groups of 2-4. These challenges don't provide starter code‚Äîyou'll build everything from scratch using the patterns you learned in Part A. Don't worry, I'll be circulating to help. Let's get started!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B Teaching Strategy: Independent Challenges (32 minutes)\n",
    "\n",
    "### üö® Critical Teaching Philosophy for Part B\n",
    "\n",
    "**Group Work Approach:**\n",
    "- **Encourage collaboration**: \"Work in groups of 2-4. Discuss your approach before coding.\"\n",
    "- **Productive struggle**: Let groups struggle for 3-5 minutes before intervening\n",
    "- **Circulate actively**: Walk around, check progress, provide strategic hints\n",
    "- **Don't give complete solutions**: Guide thinking, don't write code for them\n",
    "\n",
    "**Time Management:**\n",
    "- **Each challenge has a time allocation** (5-6 minutes per challenge)\n",
    "- **Stop after each challenge**: Briefly discuss results as a class (1-2 minutes)\n",
    "- **Adjust pacing**: If groups are struggling, provide hints earlier; if succeeding, let them continue\n",
    "- **Priority challenges**: 1, 2, and 4 are most important‚Äîensure these are completed\n",
    "\n",
    "**Your Role:**\n",
    "1. **Monitor progress**: Who's stuck? Who's making good progress?\n",
    "2. **Provide strategic hints**: Guide approach, not specific code\n",
    "3. **Debug syntax issues**: Help with Python/sklearn syntax errors\n",
    "4. **Facilitate discussion**: After each challenge, share interesting observations\n",
    "5. **Keep energy high**: Encourage groups, celebrate successes\n",
    "\n",
    "### General Support Guidelines:\n",
    "\n",
    "**What YOU CAN Do:**\n",
    "- ‚úÖ Help with syntax errors: \"You need to pass `random_state=42` to train_test_split\"\n",
    "- ‚úÖ Clarify concepts: \"Cross-validation is for comparing models, not for final evaluation\"\n",
    "- ‚úÖ Provide general guidance: \"Follow the same pattern as the guided example in Part A\"\n",
    "- ‚úÖ Answer method questions: \"Use `cross_val_score(model, X_train, y_train, cv=5, scoring='...')`\"\n",
    "- ‚úÖ Suggest debugging approaches: \"Print the shape of your data at each step\"\n",
    "\n",
    "**What YOU SHOULD NOT Do:**\n",
    "- ‚ùå Write complete code solutions: Let students build code themselves\n",
    "- ‚ùå Tell them exactly what to type: Guide their thinking instead\n",
    "- ‚ùå Rush to help immediately: Allow productive struggle first\n",
    "- ‚ùå Do the work for them: They learn by doing, not watching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Compare Model Types (5 minutes)\n",
    "\n",
    "**Learning Goal**: Practice using cross-validation to compare different model architectures without touching the test set.\n",
    "\n",
    "**Time Allocation**: 5 minutes (3 min work, 2 min discussion)\n",
    "\n",
    "#### Expected Student Approach:\n",
    "1. Create train/test split with specified features\n",
    "2. Initialize three different model types\n",
    "3. Use `cross_val_score()` for each model with 5-fold CV\n",
    "4. Compare mean RMSE values\n",
    "\n",
    "#### Common Issues & Strategic Hints:\n",
    "\n",
    "**Issue 1: \"How do I create the train/test split?\"**\n",
    "- **Hint**: \"You did this in Part A, Cell 5. What was the pattern?\"\n",
    "- **If still stuck**: \"Use `train_test_split(X, y, test_size=0.2, random_state=42)`\"\n",
    "\n",
    "**Issue 2: \"How do I run cross-validation?\"**\n",
    "- **Hint**: \"Look at Cell 6 in Part A. What function did we use?\"\n",
    "- **If still stuck**: \"Use `cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')`\"\n",
    "\n",
    "**Issue 3: \"My RMSE values are negative!\"**\n",
    "- **Explanation**: \"Sklearn returns negative scores. Multiply by -1 or take the negative to convert to positive RMSE.\"\n",
    "- **Code**: `cv_rmse = -cv_scores`\n",
    "\n",
    "**Issue 4: \"Which model should I expect to perform best?\"**\n",
    "- **Don't tell them!** Let them discover: \"Run the code and see‚Äîwhat do YOUR results show?\"\n",
    "- **Expected result**: Random Forest should have lowest RMSE (~$30k), followed by DecisionTree (~$35k), then LinearRegression (~$40k)\n",
    "\n",
    "#### After 5 Minutes - Brief Class Discussion:\n",
    "\n",
    "**Questions to ask:**\n",
    "- \"Which model performed best? Show of hands: Linear Regression? Decision Tree? Random Forest?\"\n",
    "- \"Why do you think Random Forest outperformed the others?\"\n",
    "- \"Did anyone compare standard deviations? What did you notice?\"\n",
    "\n",
    "**Key teaching moments:**\n",
    "- Random Forest typically wins on tabular data (ensemble averaging)\n",
    "- Linear Regression assumes linear relationships‚Äîmay not fit housing data well\n",
    "- Decision Tree can overfit‚ÄîRandom Forest mitigates this\n",
    "- **Most important**: \"You compared three models without EVER touching the test set!\"\n",
    "\n",
    "#### Solution (FOR YOUR REFERENCE - Don't share unless necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1 Solution - FOR TA REFERENCE ONLY\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load data (assuming already done)\n",
    "# ames = pd.read_csv('../data/ames_clean.csv')\n",
    "\n",
    "# 1. Select features and create train/test split\n",
    "features = ['GrLivArea', 'YearBuilt', 'TotalBsmtSF', 'GarageCars', 'FullBath', 'OverallQual']\n",
    "X = ames[features]\n",
    "y = ames['SalePrice']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(\"üîí Test set is LOCKED\\n\")\n",
    "\n",
    "# 2. Compare three models using cross-validation\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train,\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error'\n",
    "    )\n",
    "    cv_rmse = -cv_scores\n",
    "    results[name] = cv_rmse\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean CV RMSE: ${cv_rmse.mean():,.0f}\")\n",
    "    print(f\"  Std Dev:      ${cv_rmse.std():,.0f}\\n\")\n",
    "\n",
    "# 3. Identify best model\n",
    "best_model = min(results.items(), key=lambda x: x[1].mean())\n",
    "print(f\"‚úÖ Best model: {best_model[0]} with RMSE ${best_model[1].mean():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Systematic Hyperparameter Tuning (6 minutes)\n",
    "\n",
    "**Learning Goal**: Use GridSearchCV to systematically tune hyperparameters using cross-validation.\n",
    "\n",
    "**Time Allocation**: 6 minutes (4 min work, 2 min discussion)\n",
    "\n",
    "**‚ö†Ô∏è WARNING**: This challenge trains 540 models (3√ó4√ó3√ó3√ó3 configs √ó 5 folds). Warn students it will take 2-3 minutes to run!\n",
    "\n",
    "#### Expected Student Approach:\n",
    "1. Define parameter grid with specified ranges\n",
    "2. Create GridSearchCV with RandomForestRegressor\n",
    "3. Fit on training data\n",
    "4. Print best parameters and best score\n",
    "5. Compare to Challenge 1 results\n",
    "\n",
    "#### Common Issues & Strategic Hints:\n",
    "\n",
    "**Issue 1: \"How do I create the parameter grid?\"**\n",
    "- **Hint**: \"Look at Cell 11 in Part A. It's a dictionary where keys are parameter names and values are lists to try.\"\n",
    "- **If still stuck**: Show syntax: `param_grid = {'n_estimators': [100, 200, 300], ...}`\n",
    "\n",
    "**Issue 2: \"It's taking forever to run!\"**\n",
    "- **Explanation**: \"You're training 540 models! This is normal‚Äîprofessional ML often takes time.\"\n",
    "- **Teaching moment**: \"This is why we use `n_jobs=-1` to parallelize. In production, you might run this overnight or on cloud GPUs.\"\n",
    "\n",
    "**Issue 3: \"How do I access the best parameters?\"**\n",
    "- **Hint**: \"GridSearchCV stores the best parameters in an attribute. Check the documentation or look at Cell 12.\"\n",
    "- **If still stuck**: \"`grid_search.best_params_` and `grid_search.best_score_`\"\n",
    "\n",
    "**Issue 4: \"How much did tuning improve performance?\"**\n",
    "- **Prompt**: \"Compare the best CV RMSE from this challenge to the Random Forest RMSE from Challenge 1.\"\n",
    "- **Expected**: Improvement of $2,000-$5,000 in RMSE\n",
    "- **Teaching moment**: \"Tuning doesn't always give huge improvements, but every bit counts in production!\"\n",
    "\n",
    "#### After 6 Minutes - Brief Class Discussion:\n",
    "\n",
    "**Questions to ask:**\n",
    "- \"What optimal hyperparameters did GridSearchCV find?\"\n",
    "- \"How much did tuning improve your RMSE compared to Challenge 1?\"\n",
    "- \"How many total models did GridSearchCV train?\"\n",
    "\n",
    "**Key teaching moments:**\n",
    "- Systematic tuning often finds better configurations than intuition\n",
    "- Computational cost grows exponentially with parameter grid size\n",
    "- GridSearchCV handles cross-validation automatically‚Äîless error-prone than manual tuning\n",
    "- Still haven't touched the test set!\n",
    "\n",
    "#### Solution (FOR YOUR REFERENCE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2 Solution - FOR TA REFERENCE ONLY\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 1. Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# 2. Create GridSearchCV\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    rf,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Fit (this will take 2-3 minutes!)\n",
    "print(\"‚ö†Ô∏è Training 540 models... this will take 2-3 minutes\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"‚úÖ Grid search complete!\\n\")\n",
    "\n",
    "# 4. Print results\n",
    "print(\"=\"*50)\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(\"=\"*50)\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(f\"\\nBest CV RMSE: ${best_cv_rmse:,.0f}\")\n",
    "\n",
    "# 5. Compare to Challenge 1\n",
    "# Assuming Challenge 1 Random Forest had RMSE ~$30,000\n",
    "challenge1_rmse = 30000  # Students should replace with their actual value\n",
    "improvement = challenge1_rmse - best_cv_rmse\n",
    "print(f\"\\nüí° Improvement over Challenge 1: ${improvement:,.0f}\")\n",
    "print(f\"   ({improvement/challenge1_rmse*100:.1f}% reduction in error)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Build a Complete Pipeline (6 minutes)\n",
    "\n",
    "**Learning Goal**: Construct a pipeline that handles feature engineering and modeling without data leakage.\n",
    "\n",
    "**Time Allocation**: 6 minutes (4 min work, 2 min discussion)\n",
    "\n",
    "#### Expected Student Approach:\n",
    "1. Start fresh with new feature set (numeric + categorical)\n",
    "2. Create new train/test split\n",
    "3. Build ColumnTransformer for preprocessing\n",
    "4. Build Pipeline combining preprocessing and model\n",
    "5. Evaluate with cross-validation\n",
    "6. Compare to Challenge 2 results\n",
    "\n",
    "#### Common Issues & Strategic Hints:\n",
    "\n",
    "**Issue 1: \"How do I build a ColumnTransformer?\"**\n",
    "- **Hint**: \"Look at Cell 27 in Part A. You need to specify which transformation applies to which columns.\"\n",
    "- **If still stuck**: Show structure:\n",
    "  ```python\n",
    "  preprocessor = ColumnTransformer(\n",
    "      transformers=[\n",
    "          ('num', StandardScaler(), numeric_features),\n",
    "          ('cat', OneHotEncoder(...), categorical_features)\n",
    "      ]\n",
    "  )\n",
    "  ```\n",
    "\n",
    "**Issue 2: \"Do I need to scale for Random Forest?\"**\n",
    "- **Answer**: \"No, but it doesn't hurt. The exercise asks you to do it for practice‚Äîin production, you'd skip scaling for tree models.\"\n",
    "- **Teaching moment**: \"Pipelines let you include transformations that might not be strictly necessary but ensure consistency.\"\n",
    "\n",
    "**Issue 3: \"How do I combine the preprocessor with the model?\"**\n",
    "- **Hint**: \"Use `Pipeline()` with two steps: preprocessing and modeling.\"\n",
    "- **If still stuck**: `pipeline = Pipeline([('preprocessor', preprocessor), ('model', model)])`\n",
    "\n",
    "**Issue 4: \"How do I use the optimal hyperparameters from Challenge 2?\"**\n",
    "- **Hint**: \"When creating your RandomForestRegressor, pass the best parameters as keyword arguments.\"\n",
    "- **Example**: `RandomForestRegressor(n_estimators=200, max_depth=15, ...)`\n",
    "\n",
    "**Issue 5: \"Did adding features improve performance?\"**\n",
    "- **Prompt**: \"Compare your CV RMSE to Challenge 2. Did it go down?\"\n",
    "- **Expected**: Should improve by $2,000-$5,000 due to additional informative features\n",
    "- **If worse**: \"Check for errors‚Äîadding good features should help, not hurt!\"\n",
    "\n",
    "#### After 6 Minutes - Brief Class Discussion:\n",
    "\n",
    "**Questions to ask:**\n",
    "- \"Did adding more features and encoding categoricals improve performance?\"\n",
    "- \"What was your CV RMSE? How does it compare to Challenge 2?\"\n",
    "- \"Why are pipelines better than manually applying transformations?\"\n",
    "\n",
    "**Key teaching moments:**\n",
    "- More informative features ‚Üí better performance (if features are relevant)\n",
    "- Categorical variables often contain valuable information\n",
    "- Pipelines prevent data leakage by fitting transformers inside CV folds\n",
    "- This approach is production-ready and deployment-friendly\n",
    "\n",
    "#### Solution (FOR YOUR REFERENCE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3 Solution - FOR TA REFERENCE ONLY\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "# 1. Start fresh with expanded feature set\n",
    "numeric_features = ['GrLivArea', 'YearBuilt', 'TotalBsmtSF', 'GarageCars', \n",
    "                   'FullBath', 'OverallQual', 'YearRemodAdd', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "categorical_features = ['Neighborhood', 'HouseStyle']\n",
    "all_features = numeric_features + categorical_features\n",
    "\n",
    "X = ames[all_features]\n",
    "y = ames['SalePrice']\n",
    "\n",
    "# 2. New train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Features: {len(all_features)} ({len(numeric_features)} numeric, {len(categorical_features)} categorical)\")\n",
    "print(f\"Training: {len(X_train)}, Test: {len(X_test)}\\n\")\n",
    "\n",
    "# 3. Build preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Build pipeline with optimal hyperparameters from Challenge 2\n",
    "# (Students should use their actual best parameters)\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(\n",
    "        n_estimators=200,      # Replace with actual best params\n",
    "        max_depth=15,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 5. Evaluate with cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    pipeline, X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "cv_rmse = -cv_scores\n",
    "\n",
    "print(\"Pipeline Cross-Validation Results:\")\n",
    "print(f\"  Mean CV RMSE: ${cv_rmse.mean():,.0f}\")\n",
    "print(f\"  Std Dev:      ${cv_rmse.std():,.0f}\")\n",
    "\n",
    "# 6. Compare to Challenge 2\n",
    "challenge2_rmse = 28000  # Students should replace with their actual value\n",
    "improvement = challenge2_rmse - cv_rmse.mean()\n",
    "print(f\"\\nüí° Improvement over Challenge 2: ${improvement:,.0f}\")\n",
    "if improvement > 0:\n",
    "    print(f\"   Adding more features and encoding categoricals helped!\")\n",
    "else:\n",
    "    print(f\"   Performance is similar‚Äîfeatures may not add much information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Complete End-to-End Workflow (4 minutes)\n",
    "\n",
    "**Learning Goal**: Execute the full 5-stage professional ML workflow from data preparation through final test evaluation.\n",
    "\n",
    "**Time Allocation**: 4 minutes work (most code is from Challenge 3), then 2-3 min discussion\n",
    "\n",
    "**‚ö†Ô∏è WARNING**: GridSearchCV with 540 models will take 2-3 minutes!\n",
    "\n",
    "#### Expected Student Approach:\n",
    "This challenge combines all previous challenges:\n",
    "1. **Data Preparation**: Select features (from Challenge 3)\n",
    "2. **Split**: Create train/test split\n",
    "3. **Pipeline**: Build preprocessing pipeline (from Challenge 3)\n",
    "4. **Tune**: Use GridSearchCV with pipeline (from Challenge 2)\n",
    "5. **Final Evaluation**: Evaluate ONCE on test set\n",
    "\n",
    "#### Teaching Strategy:\n",
    "\n",
    "**Key message before they start:**\n",
    "\"This challenge brings together everything you've learned. Most of the code you've already written in Challenges 2 and 3‚Äîyou're just combining them and adding the final test evaluation. The new part is Stage 5: evaluating on the test set for the FIRST and ONLY time.\"\n",
    "\n",
    "#### Common Issues & Strategic Hints:\n",
    "\n",
    "**Issue 1: \"This seems like a lot of work!\"**\n",
    "- **Encouragement**: \"You've already done most of it! Combine your Challenge 3 pipeline with Challenge 2's GridSearchCV.\"\n",
    "- **Hint**: \"Copy your code from Challenges 2 and 3, then add the final test evaluation.\"\n",
    "\n",
    "**Issue 2: \"How do I tune a pipeline's hyperparameters?\"**\n",
    "- **Hint**: \"Use the double underscore syntax: `'model__n_estimators'`, `'model__max_depth'`, etc.\"\n",
    "- **Example**: \n",
    "  ```python\n",
    "  param_grid = {\n",
    "      'model__n_estimators': [100, 200, 300],\n",
    "      'model__max_depth': [5, 10, 15, 20],\n",
    "      ...\n",
    "  }\n",
    "  ```\n",
    "\n",
    "**Issue 3: \"Do I retrain the model before testing?\"**\n",
    "- **Answer**: \"No! GridSearchCV automatically retrains the best model on the full training set. Just use `grid_search.best_estimator_` to predict on test set.\"\n",
    "- **Alternative**: Use `grid_search.score(X_test, y_test)` directly\n",
    "\n",
    "**Issue 4: \"Should my test RMSE match my CV RMSE?\"**\n",
    "- **Answer**: \"Not exactly, but they should be close. CV gives you an ESTIMATE of test performance.\"\n",
    "- **If very different**: \"If test RMSE is much worse than CV RMSE, you might have data leakage or a lucky/unlucky split.\"\n",
    "- **Teaching moment**: \"This is why we use CV‚Äîto get a more reliable estimate before committing to the test set.\"\n",
    "\n",
    "#### After Challenge 4 - Important Class Discussion (3-5 minutes):\n",
    "\n",
    "**This is a CRITICAL teaching moment‚Äîdon't rush it!**\n",
    "\n",
    "**Questions to ask:**\n",
    "1. \"What was your final test RMSE?\"\n",
    "2. \"How close was it to your best CV RMSE from GridSearchCV?\"\n",
    "3. \"Why did we wait until NOW to touch the test set?\"\n",
    "4. \"What would have happened if we had repeatedly checked test performance while tuning?\"\n",
    "\n",
    "**Key teaching moments:**\n",
    "- **Professional workflow**: \"This is exactly how professional data scientists build production models.\"\n",
    "- **Test set discipline**: \"We made ALL our decisions‚Äîmodel type, hyperparameters, features‚Äîusing only CV. The test set was locked.\"\n",
    "- **Honest evaluation**: \"Because we didn't peek at the test set, our final RMSE is an honest estimate of how this model will perform on new data.\"\n",
    "- **One-time test**: \"In production, you report this test RMSE to stakeholders. You can't tune further and re-test‚Äîthat would be cheating!\"\n",
    "\n",
    "**Connection to real-world:**\n",
    "\"In production, you'd deploy this model to predict prices on houses you haven't seen yet. The test set simulates that future data. By keeping it locked, you ensure your performance estimate is trustworthy.\"\n",
    "\n",
    "#### Solution (FOR YOUR REFERENCE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 4 Solution - FOR TA REFERENCE ONLY\n",
    "# This combines Challenges 2 and 3\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROFESSIONAL ML WORKFLOW: 5 STAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# STAGE 1: Data Preparation\n",
    "print(\"\\n[Stage 1] Data Preparation\")\n",
    "numeric_features = ['GrLivArea', 'YearBuilt', 'TotalBsmtSF', 'GarageCars', \n",
    "                   'FullBath', 'OverallQual', 'YearRemodAdd', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "categorical_features = ['Neighborhood', 'HouseStyle']\n",
    "all_features = numeric_features + categorical_features\n",
    "\n",
    "X = ames[all_features]\n",
    "y = ames['SalePrice']\n",
    "print(f\"  ‚úÖ Selected {len(all_features)} features\")\n",
    "\n",
    "# STAGE 2: Initial Split (LOCK THE TEST SET)\n",
    "print(\"\\n[Stage 2] Train/Test Split\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"  ‚úÖ Training: {len(X_train)}, Test: {len(X_test)}\")\n",
    "print(\"  üîí TEST SET IS NOW LOCKED\")\n",
    "\n",
    "# Build preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "print(\"  ‚úÖ Pipeline created (preprocessing + model)\")\n",
    "\n",
    "# STAGE 3: Model Comparison with CV\n",
    "# (Skipped in this challenge since we already chose Random Forest)\n",
    "\n",
    "# STAGE 4: Hyperparameter Tuning with GridSearchCV\n",
    "print(\"\\n[Stage 4] Hyperparameter Tuning with GridSearchCV\")\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [5, 10, 15, 20],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"  ‚ö†Ô∏è  Training 540 models (this takes 2-3 minutes)...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"  ‚úÖ GridSearchCV complete\")\n",
    "\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print(f\"\\n  Best hyperparameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "print(f\"  Best CV RMSE: ${best_cv_rmse:,.0f}\")\n",
    "\n",
    "# STAGE 5: Final Test Evaluation (FIRST AND ONLY TIME)\n",
    "print(\"\\n[Stage 5] Final Test Evaluation\")\n",
    "print(\"  üîì UNLOCKING TEST SET FOR FINAL EVALUATION\")\n",
    "\n",
    "y_pred_test = grid_search.best_estimator_.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"\\n  Final Test RMSE: ${test_rmse:,.0f}\")\n",
    "print(f\"  Best CV RMSE:    ${best_cv_rmse:,.0f}\")\n",
    "print(f\"  Difference:      ${abs(test_rmse - best_cv_rmse):,.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WORKFLOW COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ We made ALL decisions using only training data (via CV)\")\n",
    "print(\"‚úÖ Test set was evaluated ONCE at the very end\")\n",
    "print(\"‚úÖ Our test RMSE is an honest estimate of real-world performance\")\n",
    "print(\"‚úÖ This model is ready for production deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5: Model Interpretation - Permutation Importance (5 minutes)\n",
    "\n",
    "**Learning Goal**: Understand which features drive model predictions using permutation importance.\n",
    "\n",
    "**Time Allocation**: 5 minutes (3 min work, 2 min discussion)\n",
    "\n",
    "#### Expected Student Approach:\n",
    "1. Use the best model from Challenge 4\n",
    "2. Apply `permutation_importance` from sklearn.inspection\n",
    "3. Create a bar chart of feature importances\n",
    "4. Identify the most important feature\n",
    "\n",
    "#### Common Issues & Strategic Hints:\n",
    "\n",
    "**Issue 1: \"How do I import permutation_importance?\"**\n",
    "- **Hint**: \"It's in `sklearn.inspection`. Check the sklearn documentation or search for 'permutation importance sklearn'.\"\n",
    "- **If still stuck**: `from sklearn.inspection import permutation_importance`\n",
    "\n",
    "**Issue 2: \"What data should I use for permutation importance?\"**\n",
    "- **Answer**: \"Use the test set (X_test, y_test). We want to understand feature importance on held-out data.\"\n",
    "- **Teaching moment**: \"We already evaluated on the test set once in Challenge 4, so it's okay to use it again for interpretation (we're not making decisions that affect the model).\"\n",
    "\n",
    "**Issue 3: \"How do I get feature names from the pipeline?\"**\n",
    "- **Challenge**: Pipeline transforms features (one-hot encoding), so feature names change\n",
    "- **Hint**: \"The preprocessor step has a `get_feature_names_out()` method.\"\n",
    "- **If stuck**: Provide code:\n",
    "  ```python\n",
    "  feature_names = grid_search.best_estimator_.named_steps['preprocessor'].get_feature_names_out()\n",
    "  ```\n",
    "\n",
    "**Issue 4: \"My bar chart is too crowded!\"**\n",
    "- **Solution**: \"Show only the top 10-15 features, or rotate x-axis labels.\"\n",
    "- **Code hints**:\n",
    "  - Top N features: Sort by importance and slice\n",
    "  - Rotate labels: `plt.xticks(rotation=90)`\n",
    "  - Horizontal bar chart: `plt.barh()` works better for many features\n",
    "\n",
    "**Issue 5: \"What feature should I expect to be most important?\"**\n",
    "- **Don't tell them!** Let them discover: \"Run the code and see what YOUR data shows.\"\n",
    "- **Expected**: Usually `GrLivArea` or `OverallQual`, possibly `Neighborhood` features\n",
    "- **Teaching moment**: \"Does this make business sense? Larger homes typically cost more!\"\n",
    "\n",
    "#### After 5 Minutes - Brief Class Discussion:\n",
    "\n",
    "**Questions to ask:**\n",
    "- \"What was the most important feature for predicting house prices?\"\n",
    "- \"Were you surprised by any features that ranked high or low?\"\n",
    "- \"How does this help you trust or explain the model to stakeholders?\"\n",
    "\n",
    "**Key teaching moments:**\n",
    "- Model interpretation builds trust with stakeholders\n",
    "- Permutation importance shows feature contribution without making assumptions about model internals\n",
    "- Important features often align with domain knowledge (GrLivArea, OverallQual make intuitive sense)\n",
    "- Some encoded features (neighborhood dummies) might rank high‚Äîneighborhood location matters!\n",
    "\n",
    "#### Solution (FOR YOUR REFERENCE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 5 Solution - FOR TA REFERENCE ONLY\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Get the best model from Challenge 4\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# 2. Compute permutation importance on test set\n",
    "perm_importance = permutation_importance(\n",
    "    best_model, X_test, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "# 3. Get feature names (accounting for one-hot encoding)\n",
    "feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "# 4. Create DataFrame for easier sorting and plotting\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': perm_importance.importances_mean\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# 5. Plot top 15 features\n",
    "top_n = 15\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(top_n), importance_df['importance'].head(top_n))\n",
    "plt.yticks(range(top_n), importance_df['feature'].head(top_n))\n",
    "plt.xlabel('Permutation Importance (RMSE increase when shuffled)')\n",
    "plt.title(f'Top {top_n} Most Important Features for House Price Prediction')\n",
    "plt.gca().invert_yaxis()  # Highest importance at top\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Identify most important feature\n",
    "most_important = importance_df.iloc[0]\n",
    "print(f\"\\nüèÜ Most Important Feature: {most_important['feature']}\")\n",
    "print(f\"   Importance Score: {most_important['importance']:.2f}\")\n",
    "print(f\"\\nüí° Interpretation: Shuffling this feature increases RMSE by ~${most_important['importance']:,.0f}\")\n",
    "print(\"   This means the model heavily relies on this feature for accurate predictions.\")\n",
    "\n",
    "# Show top 5 for discussion\n",
    "print(\"\\nTop 5 Features:\")\n",
    "for i, row in importance_df.head(5).iterrows():\n",
    "    print(f\"  {row['feature']:30s}: {row['importance']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6: Partial Dependence Plot (PDP) (5 minutes)\n",
    "\n",
    "**Learning Goal**: Visualize how the most important feature affects predictions using Partial Dependence Plots.\n",
    "\n",
    "**Time Allocation**: 5 minutes (3 min work, 2 min discussion)\n",
    "\n",
    "#### Expected Student Approach:\n",
    "1. Use the most important feature identified in Challenge 5\n",
    "2. Use `PartialDependenceDisplay` from sklearn.inspection\n",
    "3. Generate PDP for that feature\n",
    "4. Interpret the relationship between feature values and predictions\n",
    "\n",
    "#### Common Issues & Strategic Hints:\n",
    "\n",
    "**Issue 1: \"How do I create a partial dependence plot?\"**\n",
    "- **Hint**: \"Look up `PartialDependenceDisplay.from_estimator()` in sklearn documentation.\"\n",
    "- **If still stuck**: Show import: `from sklearn.inspection import PartialDependenceDisplay`\n",
    "\n",
    "**Issue 2: \"What feature should I plot?\"**\n",
    "- **Answer**: \"Use the most important feature from Challenge 5.\"\n",
    "- **Note**: \"If it's a one-hot encoded categorical feature, you might want to choose a continuous feature instead for a clearer interpretation.\"\n",
    "- **Suggestion**: \"Try GrLivArea or OverallQual if your top feature is categorical.\"\n",
    "\n",
    "**Issue 3: \"How do I specify which feature to plot in the pipeline?\"**\n",
    "- **Challenge**: After preprocessing, feature indices change\n",
    "- **Hint**: \"You can pass the feature name or index. If using a name, make sure it matches the preprocessed feature name.\"\n",
    "- **Alternative**: \"Use the original feature name if plotting a numeric feature that wasn't one-hot encoded.\"\n",
    "\n",
    "**Issue 4: \"How do I interpret the PDP?\"**\n",
    "- **Prompt questions**:\n",
    "  - \"Is the relationship linear or non-linear?\"\n",
    "  - \"As the feature value increases, what happens to the predicted price?\"\n",
    "  - \"Are there any plateaus or sharp changes?\"\n",
    "- **Expected for GrLivArea**: Generally increasing relationship (bigger homes = higher prices)\n",
    "- **Expected for OverallQual**: Strong positive relationship (better quality = higher prices)\n",
    "\n",
    "#### After 5 Minutes - Brief Class Discussion:\n",
    "\n",
    "**Questions to ask:**\n",
    "- \"What feature did you create a PDP for?\"\n",
    "- \"How does that feature affect the predicted sale price?\"\n",
    "- \"Is the relationship linear, or more complex?\"\n",
    "- \"Does this align with your intuition about real estate?\"\n",
    "\n",
    "**Key teaching moments:**\n",
    "- PDPs show the marginal effect of a feature on predictions\n",
    "- Helps explain \"how\" the model uses features, not just \"which\" features matter\n",
    "- Can reveal non-linear relationships that might not be obvious\n",
    "- Critical for explaining models to non-technical stakeholders\n",
    "- \"Interpretability builds trust in ML systems\"\n",
    "\n",
    "**Real-world connection:**\n",
    "\"In production ML, you'll often need to explain model decisions to business stakeholders who aren't data scientists. PDPs and feature importance are your primary tools for making models interpretable and trustworthy.\"\n",
    "\n",
    "#### Solution (FOR YOUR REFERENCE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 6 Solution - FOR TA REFERENCE ONLY\n",
    "\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Use best model from Challenge 4\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# 2. Choose feature to plot\n",
    "# Option 1: Use the most important feature from Challenge 5\n",
    "# Option 2: Choose a continuous feature for clearer interpretation\n",
    "# We'll demonstrate with GrLivArea (usually top feature and continuous)\n",
    "\n",
    "# Get the feature index after preprocessing\n",
    "feature_to_plot = 'GrLivArea'  # or 'OverallQual', or top feature from Challenge 5\n",
    "\n",
    "# Find feature index in original feature list\n",
    "feature_idx = all_features.index(feature_to_plot)\n",
    "\n",
    "# 3. Create Partial Dependence Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    best_model,\n",
    "    X_test,\n",
    "    [feature_idx],  # Feature index\n",
    "    feature_names=all_features,  # Original feature names\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "plt.suptitle(f'Partial Dependence Plot: {feature_to_plot}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Interpretation\n",
    "print(f\"\\nüìä Partial Dependence Plot Interpretation for '{feature_to_plot}':\\n\")\n",
    "\n",
    "if feature_to_plot == 'GrLivArea':\n",
    "    print(\"What the plot shows:\")\n",
    "    print(\"  ‚Ä¢ X-axis: Above-grade living area (square feet)\")\n",
    "    print(\"  ‚Ä¢ Y-axis: Predicted sale price (marginal effect)\")\n",
    "    print(\"\\nTypical pattern:\")\n",
    "    print(\"  ‚Ä¢ Generally INCREASING relationship\")\n",
    "    print(\"  ‚Ä¢ As living area increases, predicted price increases\")\n",
    "    print(\"  ‚Ä¢ May show non-linearity (e.g., larger homes don't always increase price proportionally)\")\n",
    "    print(\"  ‚Ä¢ Possible plateau at very high square footage (fewer data points, less confident)\")\n",
    "\n",
    "elif feature_to_plot == 'OverallQual':\n",
    "    print(\"What the plot shows:\")\n",
    "    print(\"  ‚Ä¢ X-axis: Overall quality rating (1-10)\")\n",
    "    print(\"  ‚Ä¢ Y-axis: Predicted sale price (marginal effect)\")\n",
    "    print(\"\\nTypical pattern:\")\n",
    "    print(\"  ‚Ä¢ Strong POSITIVE relationship\")\n",
    "    print(\"  ‚Ä¢ Higher quality homes command significantly higher prices\")\n",
    "    print(\"  ‚Ä¢ May be relatively linear or show accelerating returns at high quality levels\")\n",
    "\n",
    "else:\n",
    "    print(\"  Look for:\")\n",
    "    print(\"  ‚Ä¢ Direction: Does the predicted price increase or decrease?\")\n",
    "    print(\"  ‚Ä¢ Linearity: Is the relationship straight or curved?\")\n",
    "    print(\"  ‚Ä¢ Magnitude: How much does price change across the feature range?\")\n",
    "    print(\"  ‚Ä¢ Business sense: Does this align with real-world intuition?\")\n",
    "\n",
    "print(\"\\nüí° Why PDPs Matter:\")\n",
    "print(\"  ‚Ä¢ Shows HOW the model uses each feature to make predictions\")\n",
    "print(\"  ‚Ä¢ Helps identify non-linear relationships\")\n",
    "print(\"  ‚Ä¢ Makes black-box models more interpretable\")\n",
    "print(\"  ‚Ä¢ Builds stakeholder trust in model decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Lab Wrap-Up & Reflection (3-5 minutes)\n",
    "\n",
    "### Facilitation Strategy:\n",
    "\n",
    "**Gather the class together** for final reflection and next steps.\n",
    "\n",
    "### Key Messages to Deliver:\n",
    "\n",
    "**1. Celebrate Progress:**\n",
    "\"You just executed the professional ML workflow used in production data science! This is a huge milestone in your journey from beginner to professional.\"\n",
    "\n",
    "**2. Emphasize the Paradigm Shift:**\n",
    "\"Before today: You built models and checked test performance repeatedly.  \n",
    "After today: You know the proper workflow that keeps test set locked until the end.  \n",
    "This is the difference between classroom exercises and production ML.\"\n",
    "\n",
    "**3. Recap the 5-Stage Workflow:**\n",
    "1. **Data Preparation**: Feature selection, handling missing values\n",
    "2. **Train/Test Split**: LOCK the test set\n",
    "3. **Model Comparison**: Use CV to compare different model types\n",
    "4. **Hyperparameter Tuning**: Use GridSearchCV to optimize\n",
    "5. **Final Evaluation**: Test ONCE on held-out data\n",
    "\n",
    "\"This workflow isn't just for school‚Äîit's exactly what you'll do in industry.\"\n",
    "\n",
    "**4. Key Takeaways:**\n",
    "- ‚úÖ **Cross-validation** lets you compare models without touching the test set\n",
    "- ‚úÖ **GridSearchCV** systematically finds optimal hyperparameters\n",
    "- ‚úÖ **Feature engineering** (encoding, scaling) often improves performance\n",
    "- ‚úÖ **Pipelines** prevent data leakage and ensure reproducibility\n",
    "- ‚úÖ **Test set discipline** gives honest performance estimates\n",
    "- ‚úÖ **Model interpretation** (feature importance, PDPs) builds stakeholder trust\n",
    "\n",
    "### Save Your Work:\n",
    "\"Make sure you save your notebook! You'll use your findings for this week's homework quiz.\"\n",
    "\n",
    "### Reflection Prompts (Optional, if time allows):\n",
    "Ask 1-2 of these questions:\n",
    "- \"What concept from today clicked for you?\"\n",
    "- \"What would you like more practice with?\"\n",
    "- \"How will you approach ML projects differently after today?\"\n",
    "- \"What surprised you most about the professional workflow?\"\n",
    "\n",
    "### Preview Next Week:\n",
    "\"Next week, we'll build on this foundation by exploring advanced ensemble methods and learning how to handle even more complex ML scenarios. The workflow you learned today will be your foundation for everything that follows.\"\n",
    "\n",
    "### Final Encouragement:\n",
    "\"You've leveled up from beginner data scientist to professional practitioner. This is a skill that will serve you throughout your career. Great work today!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Common Issues & Troubleshooting Guide\n",
    "\n",
    "### Technical Issues:\n",
    "\n",
    "**1. \"GridSearchCV is taking forever to run\"**\n",
    "- **Root cause**: Too many parameter combinations\n",
    "- **Solution**: Reduce parameter grid size during experimentation\n",
    "- **Teaching moment**: \"In production, you balance thoroughness with computational cost. Start coarse, then refine.\"\n",
    "- **Quick fix**: Reduce cv from 5 to 3 for faster prototyping\n",
    "\n",
    "**2. \"ValueError: Input contains NaN\"**\n",
    "- **Root cause**: Missing values in selected features\n",
    "- **Solution**: \n",
    "  - Check for missing values: `X_train.isnull().sum()`\n",
    "  - Either drop problematic features or use `SimpleImputer`\n",
    "- **Teaching moment**: \"Real-world data is messy. Always check for missing values!\"\n",
    "\n",
    "**3. \"One-hot encoding creates too many columns\"**\n",
    "- **Root cause**: High cardinality categorical feature (e.g., Neighborhood with 25 values)\n",
    "- **This is normal!** One-hot encoding increases dimensionality\n",
    "- **Alternatives**:\n",
    "  - Use label encoding for tree-based models\n",
    "  - Use only high-frequency categories\n",
    "  - Accept the dimensionality (modern ML can handle it)\n",
    "\n",
    "**4. \"ColumnNotFoundError when applying transformations\"**\n",
    "- **Root cause**: Column name mismatch between specified features and actual data\n",
    "- **Solution**: Verify feature names with `print(X_train.columns.tolist())`\n",
    "- **Common mistake**: Typo in feature name or column doesn't exist in dataset\n",
    "\n",
    "**5. \"My train and test transforms don't match\"**\n",
    "- **Root cause**: Fit scaler/encoder on test data instead of training data\n",
    "- **Solution**: Always `fit_transform` on train, `transform` on test\n",
    "- **Teaching moment**: \"This is data leakage! Always fit on training data only.\"\n",
    "\n",
    "### Conceptual Issues:\n",
    "\n",
    "**1. \"CV scores vary dramatically between folds\"**\n",
    "- **Explanation**: High variance suggests:\n",
    "  - Model is unstable\n",
    "  - Data has outliers\n",
    "  - Small dataset size\n",
    "- **Solutions**:\n",
    "  - Try simpler model (reduce complexity)\n",
    "  - Check for outliers in data\n",
    "  - Use more CV folds (cv=10)\n",
    "  - Ensure consistent random_state\n",
    "\n",
    "**2. \"Test RMSE is much worse than CV RMSE\"**\n",
    "- **Possible causes**:\n",
    "  - Overfitting (model too complex)\n",
    "  - Data leakage during preprocessing\n",
    "  - Unlucky train/test split\n",
    "  - Test set has different distribution\n",
    "- **What to check**:\n",
    "  - Are transformations fit only on training data?\n",
    "  - Is model complexity reasonable?\n",
    "  - Are train/test distributions similar?\n",
    "\n",
    "**3. \"Pipeline syntax is confusing\"**\n",
    "- **Reminder**: Double underscore notation for nested parameters\n",
    "- **Pattern**: `'step_name__parameter_name'`\n",
    "- **Example**: To tune Ridge alpha in a pipeline:\n",
    "  - Pipeline step named 'model'\n",
    "  - Parameter: 'alpha'\n",
    "  - Grid parameter: `'model__alpha': [0.1, 1.0, 10.0]`\n",
    "\n",
    "**4. \"When do I scale vs. not scale?\"**\n",
    "- **Quick reference**:\n",
    "  - **MUST scale**: KNN, SVM, Neural Nets, Ridge/Lasso, PCA\n",
    "  - **DON'T scale**: Decision Trees, Random Forest, XGBoost\n",
    "  - **Doesn't matter**: Plain Linear Regression\n",
    "- **Rule of thumb**: When in doubt, scale for safety (won't hurt tree models)\n",
    "\n",
    "### Debugging Tips:\n",
    "\n",
    "**1. Check data shapes at each step**\n",
    "```python\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "```\n",
    "\n",
    "**2. Verify random states are consistent**\n",
    "```python\n",
    "# ALL of these should use random_state=42\n",
    "train_test_split(..., random_state=42)\n",
    "DecisionTreeRegressor(random_state=42)\n",
    "RandomForestRegressor(random_state=42)\n",
    "GridSearchCV(..., random_state=42)  # if available\n",
    "```\n",
    "\n",
    "**3. Check for missing values**\n",
    "```python\n",
    "print(X_train.isnull().sum())\n",
    "```\n",
    "\n",
    "**4. Verify feature names match**\n",
    "```python\n",
    "print(\"Expected features:\", features)\n",
    "print(\"Actual columns:\", X_train.columns.tolist())\n",
    "```\n",
    "\n",
    "**5. Use verbose mode to track progress**\n",
    "```python\n",
    "GridSearchCV(..., verbose=1)  # Shows progress\n",
    "```\n",
    "\n",
    "### Time Management Issues:\n",
    "\n",
    "**If running behind schedule:**\n",
    "- **Priority**: Ensure Challenges 1, 2, and 4 are completed (core workflow)\n",
    "- **Optional**: Challenges 5 and 6 (interpretation) can be homework if needed\n",
    "- **Skip**: Detailed discussion if necessary‚Äîfocus on hands-on practice\n",
    "\n",
    "**If ahead of schedule:**\n",
    "- Encourage deeper exploration of parameter grids\n",
    "- Discuss additional feature engineering ideas\n",
    "- Compare additional model types (Gradient Boosting, SVM, etc.)\n",
    "- Explore partial dependence plots for multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Post-Lab Checklist\n",
    "\n",
    "### For TAs - After Lab is Complete:\n",
    "\n",
    "**Student Understanding:**\n",
    "- [ ] Students understand why test set should be locked until final evaluation\n",
    "- [ ] Students can implement cross-validation to compare models\n",
    "- [ ] Students can use GridSearchCV to tune hyperparameters\n",
    "- [ ] Students understand when to scale features and when to encode categoricals\n",
    "- [ ] Students can build pipelines to prevent data leakage\n",
    "- [ ] Students completed the 5-stage professional ML workflow\n",
    "\n",
    "**Technical Completion:**\n",
    "- [ ] All students successfully ran cross-validation (Challenge 1)\n",
    "- [ ] Most students completed GridSearchCV tuning (Challenge 2)\n",
    "- [ ] Students built at least one working pipeline (Challenge 3)\n",
    "- [ ] Students completed the end-to-end workflow (Challenge 4)\n",
    "- [ ] Students saved their notebooks with results\n",
    "\n",
    "**Homework Preparation:**\n",
    "- [ ] Students know to save their numerical results for homework quiz\n",
    "- [ ] Reminded students to use `random_state=42` for reproducibility\n",
    "- [ ] Clarified any confusion about homework requirements\n",
    "\n",
    "### For Students - What You Should Have:\n",
    "\n",
    "**Completed Challenges:**\n",
    "- [ ] Challenge 1: Compared 3 model types using cross-validation\n",
    "- [ ] Challenge 2: Tuned Random Forest with GridSearchCV\n",
    "- [ ] Challenge 3: Built a preprocessing pipeline\n",
    "- [ ] Challenge 4: Executed complete end-to-end workflow with final test evaluation\n",
    "- [ ] Challenge 5: Computed feature importance\n",
    "- [ ] Challenge 6: Created partial dependence plot\n",
    "\n",
    "**Understanding Checkpoints:**\n",
    "- [ ] Can explain why we don't touch the test set until the end\n",
    "- [ ] Understand the difference between cross-validation and train/test split\n",
    "- [ ] Know when to use GridSearchCV vs. manual tuning\n",
    "- [ ] Can identify when scaling is necessary vs. unnecessary\n",
    "- [ ] Understand how pipelines prevent data leakage\n",
    "- [ ] Can interpret feature importance and partial dependence plots\n",
    "\n",
    "**Saved Artifacts:**\n",
    "- [ ] Jupyter notebook with all code and outputs\n",
    "- [ ] Numerical results (CV RMSEs, test RMSE, best parameters)\n",
    "- [ ] Feature importance visualizations\n",
    "- [ ] Partial dependence plot\n",
    "\n",
    "### Notes for Next Time:\n",
    "\n",
    "**What worked well:**\n",
    "- (TA fills in after lab)\n",
    "\n",
    "**What needs improvement:**\n",
    "- (TA fills in after lab)\n",
    "\n",
    "**Concepts that need reinforcement:**\n",
    "- (TA fills in after lab)\n",
    "\n",
    "**Student questions that came up frequently:**\n",
    "- (TA fills in after lab)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Lab Success Metrics\n",
    "\n",
    "**Lab is successful if:**\n",
    "- Students can execute the 5-stage professional ML workflow independently\n",
    "- Students understand test set discipline and can articulate why it matters\n",
    "- Students can use cross-validation and GridSearchCV effectively\n",
    "- Students recognize when feature engineering is needed and can implement it\n",
    "- Students can interpret model results and explain them in business terms\n",
    "- Students feel confident applying these techniques to their own ML projects\n",
    "\n",
    "**This lab represents a critical milestone**: Students are now equipped to build production-ready ML models using industry best practices!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
