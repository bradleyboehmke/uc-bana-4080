{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 30: Feature Engineering\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/30_feature_engineering.ipynb)\n",
    "\n",
    "This notebook accompanies Chapter 30 of the BANA 4080 textbook. It provides interactive examples of feature engineering techniques including encoding, scaling, feature creation, handling missing data, and building scikit-learn pipelines.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By working through this notebook, you will be able to:\n",
    "\n",
    "- Apply different encoding strategies for categorical variables (dummy/one-hot, label, and ordinal encoding)\n",
    "- Scale and normalize numerical features using StandardScaler and MinMaxScaler\n",
    "- Create new features using polynomial terms, interaction terms, and domain knowledge\n",
    "- Handle missing data strategically through imputation or deletion, including missingness indicators\n",
    "- Build end-to-end feature engineering pipelines with scikit-learn to prevent data leakage\n",
    "- Recognize when different techniques are appropriate based on your data, model, and goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by importing the libraries we'll need and loading the Ames housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Scikit-learn pipeline tools\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Ames Housing Data\n",
    "\n",
    "We'll use the Ames housing dataset throughout this notebook. This dataset contains information about house sales in Ames, Iowa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - adjust path if running in Google Colab\n",
    "try:\n",
    "    # Try local path first\n",
    "    ames = pd.read_csv('../data/ames_clean.csv')\n",
    "except FileNotFoundError:\n",
    "    # If in Colab, load from GitHub\n",
    "    url = 'https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/main/data/ames_clean.csv'\n",
    "    ames = pd.read_csv(url)\n",
    "\n",
    "print(f\"Dataset shape: {ames.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "ames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly explore the dataset structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Number of rows: {len(ames):,}\")\n",
    "print(f\"Number of columns: {len(ames.columns)}\")\n",
    "print(f\"\\nColumn types:\")\n",
    "print(ames.dtypes.value_counts())\n",
    "print(f\"\\nTarget variable (SalePrice) summary:\")\n",
    "print(ames['SalePrice'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Encoding Categorical Variables\n",
    "\n",
    "Machine learning algorithms work with numbers, not categories. We need to convert categorical variables into numerical format.\n",
    "\n",
    "### 1.1 Dummy/One-Hot Encoding\n",
    "\n",
    "Creates separate binary columns for each category. Best for nominal variables (no inherent order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the BldgType (building type) variable\n",
    "print(\"Building types in the dataset:\")\n",
    "print(ames['BldgType'].value_counts())\n",
    "print(f\"\\nNumber of unique building types: {ames['BldgType'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for building type\n",
    "bldg_dummies = pd.get_dummies(ames['BldgType'], prefix='BldgType')\n",
    "\n",
    "print(\"Dummy encoded building types (first 5 rows):\")\n",
    "print(bldg_dummies.head())\n",
    "print(f\"\\nNumber of columns created: {len(bldg_dummies.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid the dummy variable trap by dropping the first category\n",
    "bldg_dummies_safe = pd.get_dummies(ames['BldgType'], \n",
    "                                   prefix='BldgType',\n",
    "                                   drop_first=True)\n",
    "\n",
    "print(f\"Original columns: {len(bldg_dummies.columns)}\")\n",
    "print(f\"After dropping first: {len(bldg_dummies_safe.columns)}\")\n",
    "print(f\"\\nColumns kept: {list(bldg_dummies_safe.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **Key Insight:** For linear regression, use `drop_first=True` to avoid multicollinearity (the dummy variable trap)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Label Encoding\n",
    "\n",
    "Assigns a unique integer to each category, creating just one column. More compact but can be misleading for linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many neighborhoods we have\n",
    "print(f\"Number of unique neighborhoods: {ames['Neighborhood'].nunique()}\")\n",
    "print(f\"\\nSample neighborhoods:\")\n",
    "print(ames['Neighborhood'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding\n",
    "le = LabelEncoder()\n",
    "ames_encoded = ames.copy()\n",
    "ames_encoded['Neighborhood_Encoded'] = le.fit_transform(ames['Neighborhood'])\n",
    "\n",
    "# Show the mapping for a few examples\n",
    "print(\"Label encoding results (first 10 rows):\")\n",
    "print(ames_encoded[['Neighborhood', 'Neighborhood_Encoded']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ **Warning:** Never use label encoding for non-ordinal categorical variables with linear models! The model will incorrectly treat the numeric codes as having magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Ordinal Encoding\n",
    "\n",
    "For categorical variables with a natural order, create custom mappings that preserve the ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the exterior quality variable\n",
    "print(\"Exterior quality categories:\")\n",
    "print(ames['ExterQual'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom ordinal mapping that preserves the quality order\n",
    "# Po = Poor, Fa = Fair, TA = Typical/Average, Gd = Good, Ex = Excellent\n",
    "quality_map = {\n",
    "    'Po': 1,  # Poor\n",
    "    'Fa': 2,  # Fair\n",
    "    'TA': 3,  # Typical/Average\n",
    "    'Gd': 4,  # Good\n",
    "    'Ex': 5   # Excellent\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "ames_encoded['ExterQual_Encoded'] = ames['ExterQual'].map(quality_map)\n",
    "\n",
    "# Show the results\n",
    "print(\"Ordinal encoding results (first 10 rows):\")\n",
    "print(ames_encoded[['ExterQual', 'ExterQual_Encoded']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the encoding preserves order by checking mean prices\n",
    "print(\"Mean sale price by exterior quality:\")\n",
    "quality_prices = ames_encoded.groupby('ExterQual_Encoded')['SalePrice'].mean().sort_index()\n",
    "print(quality_prices)\n",
    "print(\"\\nâœ“ Higher quality ratings correspond to higher prices!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Try It Yourself: Encoding\n",
    "\n",
    "Try encoding the `KitchenQual` variable (kitchen quality) using ordinal encoding. Create an appropriate quality mapping and verify it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a quality_map for KitchenQual\n",
    "# TODO: Apply the mapping\n",
    "# TODO: Verify by checking mean sale prices\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Scaling and Normalization\n",
    "\n",
    "Many ML algorithms are sensitive to the scale of features. Scaling puts all features on a level playing field.\n",
    "\n",
    "### 2.1 StandardScaler (Z-score Normalization)\n",
    "\n",
    "Transforms features to have mean=0 and standard deviation=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with different scales\n",
    "data = pd.DataFrame({\n",
    "    'HouseSize': [1200, 1800, 950, 2400, 1600],\n",
    "    'Bedrooms': [2, 3, 2, 4, 3]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(f\"\\nHouseSize range: {data['HouseSize'].min()} to {data['HouseSize'].max()}\")\n",
    "print(f\"Bedrooms range: {data['Bedrooms'].min()} to {data['Bedrooms'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "scaled_df = pd.DataFrame(\n",
    "    scaled_data,\n",
    "    columns=['HouseSize_Scaled', 'Bedrooms_Scaled']\n",
    ")\n",
    "\n",
    "print(\"After StandardScaler:\")\n",
    "print(scaled_df)\n",
    "print(f\"\\nMean: {scaled_df.mean().values}\")\n",
    "print(f\"Std: {scaled_df.std().values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MinMaxScaler\n",
    "\n",
    "Transforms features to a fixed range, typically [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler\n",
    "minmax = MinMaxScaler()\n",
    "minmax_scaled = minmax.fit_transform(data)\n",
    "\n",
    "minmax_df = pd.DataFrame(\n",
    "    minmax_scaled,\n",
    "    columns=['HouseSize_MinMax', 'Bedrooms_MinMax']\n",
    ")\n",
    "\n",
    "print(\"After MinMaxScaler:\")\n",
    "print(minmax_df)\n",
    "print(f\"\\nMin: {minmax_df.min().values}\")\n",
    "print(f\"Max: {minmax_df.max().values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualizing the Effect of Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization comparing original, StandardScaler, and MinMaxScaler\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original data\n",
    "axes[0].scatter(data['HouseSize'], data['Bedrooms'], s=100, alpha=0.6)\n",
    "axes[0].set_xlabel('HouseSize')\n",
    "axes[0].set_ylabel('Bedrooms')\n",
    "axes[0].set_title('Original Data\\n(Different Scales)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# StandardScaler\n",
    "axes[1].scatter(scaled_df['HouseSize_Scaled'], scaled_df['Bedrooms_Scaled'], \n",
    "               s=100, alpha=0.6, color='orange')\n",
    "axes[1].set_xlabel('HouseSize (Standardized)')\n",
    "axes[1].set_ylabel('Bedrooms (Standardized)')\n",
    "axes[1].set_title('StandardScaler\\n(Mean=0, Std=1)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# MinMaxScaler\n",
    "axes[2].scatter(minmax_df['HouseSize_MinMax'], minmax_df['Bedrooms_MinMax'], \n",
    "               s=100, alpha=0.6, color='green')\n",
    "axes[2].set_xlabel('HouseSize (Min-Max)')\n",
    "axes[2].set_ylabel('Bedrooms (Min-Max)')\n",
    "axes[2].set_title('MinMaxScaler\\n(Range [0,1])')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Try It Yourself: Scaling\n",
    "\n",
    "Apply StandardScaler to three Ames features: `GrLivArea`, `YearBuilt`, and `GarageArea`. Compare their ranges before and after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select the three features\n",
    "# TODO: Apply StandardScaler\n",
    "# TODO: Compare the ranges and verify meanâ‰ˆ0, stdâ‰ˆ1\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Creating New Features\n",
    "\n",
    "Some of the most powerful features are ones you create yourself by combining or transforming existing features.\n",
    "\n",
    "### 3.1 Domain-Specific Features\n",
    "\n",
    "Use domain knowledge to create meaningful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create domain-specific features for the Ames dataset\n",
    "ames_features = ames.copy()\n",
    "current_year = pd.Timestamp.now().year\n",
    "\n",
    "# Feature 1: House age (more intuitive than year built)\n",
    "ames_features['Age'] = current_year - ames_features['YearBuilt']\n",
    "\n",
    "# Feature 2: Was the house renovated?\n",
    "ames_features['Was_Renovated'] = (ames_features['YearRemodAdd'] > ames_features['YearBuilt']).astype(int)\n",
    "\n",
    "# Feature 3: Years since renovation\n",
    "ames_features['Years_Since_Reno'] = current_year - ames_features['YearRemodAdd']\n",
    "\n",
    "# Feature 4: Total bathrooms\n",
    "ames_features['Total_Baths'] = (ames_features['FullBath'] + \n",
    "                                0.5 * ames_features['HalfBath'] + \n",
    "                                ames_features['BsmtFullBath'] + \n",
    "                                0.5 * ames_features['BsmtHalfBath'])\n",
    "\n",
    "# Feature 5: Square feet per bathroom\n",
    "ames_features['Sqft_Per_Bath'] = ames_features['GrLivArea'] / (ames_features['Total_Baths'] + 0.1)\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(ames_features[['Age', 'Was_Renovated', 'Years_Since_Reno', 'Total_Baths', 'Sqft_Per_Bath']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation of new features with SalePrice\n",
    "new_features = ['Age', 'Was_Renovated', 'Years_Since_Reno', 'Total_Baths', 'Sqft_Per_Bath']\n",
    "correlations = ames_features[new_features + ['SalePrice']].corr()['SalePrice'].drop('SalePrice').sort_values(ascending=False)\n",
    "\n",
    "print(\"Correlation with SalePrice:\")\n",
    "print(correlations)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "correlations.plot(kind='barh', color='skyblue')\n",
    "plt.xlabel('Correlation with SalePrice')\n",
    "plt.title('New Feature Correlations')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Polynomial Features\n",
    "\n",
    "Capture non-linear relationships by creating powers of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with simple data\n",
    "simple_data = pd.DataFrame({\n",
    "    'x1': [1, 2, 3],\n",
    "    'x2': [4, 5, 6]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(simple_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features (degree=2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(simple_data)\n",
    "\n",
    "poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out())\n",
    "print(\"After polynomial transformation (degree=2):\")\n",
    "print(poly_df)\n",
    "print(f\"\\nOriginal features: {simple_data.shape[1]}\")\n",
    "print(f\"Polynomial features: {poly_df.shape[1]}\")\n",
    "print(f\"\\nFeatures created: {list(poly_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualizing Polynomial Relationships\n",
    "\n",
    "Let's see how polynomial features help capture non-linear patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data with a non-linear relationship (diminishing returns)\n",
    "np.random.seed(42)\n",
    "X = np.linspace(500, 3500, 50).reshape(-1, 1)  # Square footage\n",
    "# Price follows a curve with diminishing returns + some noise\n",
    "y = 50000 + 100*X + -0.015*(X**2) + np.random.normal(0, 10000, X.shape)\n",
    "\n",
    "# Fit three models: linear, 2nd degree polynomial, 3rd degree polynomial\n",
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "# Linear model\n",
    "models['Linear'] = LinearRegression()\n",
    "models['Linear'].fit(X, y)\n",
    "predictions['Linear'] = models['Linear'].predict(X)\n",
    "\n",
    "# 2nd degree polynomial\n",
    "poly2 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly2 = poly2.fit_transform(X)\n",
    "models['2nd Degree'] = LinearRegression()\n",
    "models['2nd Degree'].fit(X_poly2, y)\n",
    "predictions['2nd Degree'] = models['2nd Degree'].predict(X_poly2)\n",
    "\n",
    "# 3rd degree polynomial\n",
    "poly3 = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly3 = poly3.fit_transform(X)\n",
    "models['3rd Degree'] = LinearRegression()\n",
    "models['3rd Degree'].fit(X_poly3, y)\n",
    "predictions['3rd Degree'] = models['3rd Degree'].predict(X_poly3)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(X, y, alpha=0.5, s=30, label='Actual Data', color='gray')\n",
    "ax.plot(X, predictions['Linear'], label='Linear (no polynomial)',\n",
    "        linewidth=2, linestyle='--', color='red')\n",
    "ax.plot(X, predictions['2nd Degree'], label='2nd Degree Polynomial',\n",
    "        linewidth=2, color='blue')\n",
    "ax.plot(X, predictions['3rd Degree'], label='3rd Degree Polynomial',\n",
    "        linewidth=2, linestyle=':', color='green')\n",
    "\n",
    "ax.set_xlabel('Square Footage', fontsize=12)\n",
    "ax.set_ylabel('House Price ($)', fontsize=12)\n",
    "ax.set_title('How Polynomial Features Capture Non-Linear Relationships', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the 2nd degree polynomial captures the curve,\")\n",
    "print(\"while the linear model misses the diminishing returns pattern.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Interaction Terms\n",
    "\n",
    "Capture how features affect each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real estate example: size Ã— quality interaction\n",
    "houses = pd.DataFrame({\n",
    "    'Size_Sqft': [1200, 1200, 2400, 2400],\n",
    "    'Neighborhood_Quality': [1, 5, 1, 5]  # 1=poor, 5=excellent\n",
    "})\n",
    "\n",
    "# Create interaction\n",
    "houses['Size_x_Quality'] = houses['Size_Sqft'] * houses['Neighborhood_Quality']\n",
    "\n",
    "print(\"Interaction term example:\")\n",
    "print(houses)\n",
    "print(\"\\nðŸ’¡ The interaction captures that an extra sq ft in a good neighborhood\")\n",
    "print(\"   is worth more than in a poor neighborhood.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Try It Yourself: Feature Creation\n",
    "\n",
    "Create a new feature that combines `GrLivArea` and `OverallQual` as an interaction term. Check if it correlates more strongly with `SalePrice` than the individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create interaction feature: GrLivArea Ã— OverallQual\n",
    "# TODO: Calculate correlations with SalePrice\n",
    "# TODO: Compare to individual feature correlations\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Handling Missing Data\n",
    "\n",
    "Real-world data often has missing values. We need strategies to handle them.\n",
    "\n",
    "### 4.1 Identifying Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in Ames dataset\n",
    "missing_counts = ames.isnull().sum()\n",
    "missing_percent = (missing_counts / len(ames)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Percent': missing_percent\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "# Show features with missing values\n",
    "missing_features = missing_df[missing_df['Missing_Count'] > 0]\n",
    "print(f\"Features with missing values: {len(missing_features)}\")\n",
    "print(\"\\nTop 10 features by missing count:\")\n",
    "print(missing_features.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "if len(missing_features) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_missing = missing_features.head(15)\n",
    "    plt.barh(range(len(top_missing)), top_missing['Percent'])\n",
    "    plt.yticks(range(len(top_missing)), top_missing.index)\n",
    "    plt.xlabel('Percentage Missing (%)')\n",
    "    plt.title('Features with Most Missing Values')\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Imputation Strategies\n",
    "\n",
    "#### Mean/Median Imputation (Numerical Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with missing values\n",
    "data_missing = pd.DataFrame({\n",
    "    'Age': [25, 30, np.nan, 45, np.nan, 35],\n",
    "    'Income': [50000, 60000, 55000, np.nan, 70000, 65000]\n",
    "})\n",
    "\n",
    "print(\"Original data with missing values:\")\n",
    "print(data_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with median (more robust to outliers)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputed_data = imputer.fit_transform(data_missing)\n",
    "\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=['Age', 'Income'])\n",
    "print(\"After median imputation:\")\n",
    "print(imputed_df)\n",
    "print(f\"\\nMedian Age used for imputation: {imputer.statistics_[0]}\")\n",
    "print(f\"Median Income used for imputation: {imputer.statistics_[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode Imputation (Categorical Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical data with missing values\n",
    "cat_data = pd.DataFrame({\n",
    "    'Color': ['Red', 'Blue', np.nan, 'Red', 'Blue', np.nan, 'Red']\n",
    "})\n",
    "\n",
    "print(\"Original categorical data:\")\n",
    "print(cat_data)\n",
    "print(f\"\\nMissing values: {cat_data['Color'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with most frequent value (mode)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "cat_data['Color_Imputed'] = imputer.fit_transform(cat_data[['Color']]).ravel()\n",
    "\n",
    "print(\"After mode imputation:\")\n",
    "print(cat_data)\n",
    "print(f\"\\nMost frequent value used: {imputer.statistics_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constant Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with a constant value\n",
    "imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "cat_data['Color_Constant'] = imputer.fit_transform(cat_data[['Color']]).ravel()\n",
    "\n",
    "print(\"After constant imputation with 'Unknown':\")\n",
    "print(cat_data[['Color', 'Color_Constant']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Missingness Indicators\n",
    "\n",
    "Sometimes the fact that a value is missing is itself informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh data for this example\n",
    "indicator_data = pd.DataFrame({\n",
    "    'Age': [25, 30, np.nan, 45, np.nan, 35],\n",
    "    'Income': [50000, 60000, 55000, np.nan, 70000, 65000]\n",
    "})\n",
    "\n",
    "# Create indicator for missingness BEFORE imputing\n",
    "indicator_data['Age_Was_Missing'] = indicator_data['Age'].isna().astype(int)\n",
    "indicator_data['Income_Was_Missing'] = indicator_data['Income'].isna().astype(int)\n",
    "\n",
    "# Then impute the original columns\n",
    "indicator_data['Age'] = indicator_data['Age'].fillna(indicator_data['Age'].median())\n",
    "indicator_data['Income'] = indicator_data['Income'].fillna(indicator_data['Income'].median())\n",
    "\n",
    "print(\"Data with missingness indicators:\")\n",
    "print(indicator_data)\n",
    "print(\"\\nðŸ’¡ Now the model has both the imputed value AND information about whether it was missing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Try It Yourself: Missing Data\n",
    "\n",
    "For the Ames dataset, pick a feature with missing values (like `LotFrontage`). Create a missingness indicator, impute with median, and check if houses with missing values have different sale prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pick a feature with missing values\n",
    "# TODO: Create missingness indicator\n",
    "# TODO: Impute with median\n",
    "# TODO: Compare mean SalePrice for missing vs non-missing\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Building Scikit-Learn Pipelines\n",
    "\n",
    "Pipelines chain transformations together and prevent data leakage.\n",
    "\n",
    "### 5.1 Simple Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple pipeline: scaling â†’ model\n",
    "simple_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Prepare simple data\n",
    "X_simple = pd.DataFrame({\n",
    "    'GrLivArea': [1200, 1500, 1800, 2100],\n",
    "    'YearBuilt': [1990, 2000, 1985, 2015]\n",
    "})\n",
    "y_simple = pd.Series([200000, 250000, 240000, 350000])\n",
    "\n",
    "# Fit the entire pipeline\n",
    "simple_pipeline.fit(X_simple, y_simple)\n",
    "\n",
    "# Make predictions (scaling happens automatically!)\n",
    "predictions = simple_pipeline.predict(X_simple)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"\\nâœ“ Pipeline automatically scales data before prediction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 End-to-End Pipeline with Mixed Features\n",
    "\n",
    "Real datasets have both numerical and categorical features requiring different preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature types for Ames\n",
    "numeric_features = ['GrLivArea', 'YearBuilt', 'TotalBsmtSF']\n",
    "categorical_features = ['Neighborhood', 'BldgType']\n",
    "\n",
    "# Create preprocessing for numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create preprocessing for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "print(\"âœ“ Preprocessor created with separate pipelines for numeric and categorical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full pipeline with model\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "# Prepare data\n",
    "X = ames[numeric_features + categorical_features]\n",
    "y = ames['SalePrice']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]:,}\")\n",
    "print(f\"Test set size: {X_test.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the entire pipeline\n",
    "print(\"Fitting pipeline...\")\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_score = full_pipeline.score(X_train, y_train)\n",
    "test_score = full_pipeline.score(X_test, y_test)\n",
    "\n",
    "# Get predictions for additional metrics\n",
    "y_train_pred = full_pipeline.predict(X_train)\n",
    "y_test_pred = full_pipeline.predict(X_test)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PIPELINE RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training RÂ² Score: {train_score:.3f}\")\n",
    "print(f\"Test RÂ² Score: {test_score:.3f}\")\n",
    "print(f\"\\nTraining RMSE: ${train_rmse:,.0f}\")\n",
    "print(f\"Test RMSE: ${test_rmse:,.0f}\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nâœ“ Pipeline handles all preprocessing automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualizing Pipeline Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "ax1.scatter(y_train, y_train_pred, alpha=0.5, s=20)\n",
    "ax1.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('Actual Price ($)', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Price ($)', fontsize=12)\n",
    "ax1.set_title(f'Training Set\\nRÂ² = {train_score:.3f}, RMSE = ${train_rmse:,.0f}', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "ax2.scatter(y_test, y_test_pred, alpha=0.5, s=20, color='orange')\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "ax2.set_xlabel('Actual Price ($)', fontsize=12)\n",
    "ax2.set_ylabel('Predicted Price ($)', fontsize=12)\n",
    "ax2.set_title(f'Test Set\\nRÂ² = {test_score:.3f}, RMSE = ${test_rmse:,.0f}', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Try It Yourself: Build Your Own Pipeline\n",
    "\n",
    "Extend the pipeline above by:\n",
    "1. Adding more numerical features (like `OverallQual`, `GarageCars`)\n",
    "2. Adding more categorical features (like `MSZoning`)\n",
    "3. Try different models (LinearRegression, Ridge with different alpha values)\n",
    "4. Compare the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define new feature lists\n",
    "# TODO: Create preprocessor with ColumnTransformer\n",
    "# TODO: Build pipeline with your choice of model\n",
    "# TODO: Fit and evaluate\n",
    "# TODO: Compare to the baseline pipeline above\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "In this notebook, we explored:\n",
    "\n",
    "1. **Encoding Categorical Variables**\n",
    "   - Dummy/one-hot encoding for nominal variables\n",
    "   - Label encoding for high-cardinality features (use with tree-based models)\n",
    "   - Ordinal encoding for variables with natural order\n",
    "\n",
    "2. **Scaling and Normalization**\n",
    "   - StandardScaler (mean=0, std=1) for most use cases\n",
    "   - MinMaxScaler (range [0,1]) for bounded ranges\n",
    "   - When scaling matters (distance-based algorithms) vs doesn't (tree-based)\n",
    "\n",
    "3. **Creating New Features**\n",
    "   - Domain-specific features using expert knowledge\n",
    "   - Polynomial features for capturing non-linearity\n",
    "   - Interaction terms for feature dependencies\n",
    "\n",
    "4. **Handling Missing Data**\n",
    "   - Imputation strategies (mean/median, mode, constant)\n",
    "   - Missingness indicators to preserve signal\n",
    "   - When to drop vs impute\n",
    "\n",
    "5. **Scikit-Learn Pipelines**\n",
    "   - Chain transformations to prevent data leakage\n",
    "   - ColumnTransformer for mixed feature types\n",
    "   - End-to-end reproducible workflows\n",
    "\n",
    "### Critical Principles\n",
    "\n",
    "âœ… **Feature engineering often matters more than model selection**  \n",
    "âœ… **Always use pipelines to prevent data leakage**  \n",
    "âœ… **Fit transformers on training data only**  \n",
    "âœ… **Domain knowledge creates the most powerful features**  \n",
    "âœ… **Test different approaches and let results guide decisions**  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Practice with different datasets\n",
    "- Experiment with feature combinations\n",
    "- Explore advanced techniques (target encoding, feature selection, dimensionality reduction)\n",
    "- Build end-to-end ML projects with proper pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "**Books:**\n",
    "- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/) by Alice Zheng and Amanda Casari\n",
    "- [Feature Engineering and Selection](http://www.feat.engineering/) by Max Kuhn and Kjell Johnson\n",
    "\n",
    "**Online Resources:**\n",
    "- [Kaggle's Feature Engineering Course](https://www.kaggle.com/learn/feature-engineering)\n",
    "- [Scikit-learn User Guide: Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- [Scikit-learn User Guide: Pipelines](https://scikit-learn.org/stable/modules/compose.html)\n",
    "\n",
    "**Practice:**\n",
    "- [Kaggle Competitions](https://www.kaggle.com/competitions) - Real-world datasets\n",
    "- [Kaggle Notebooks](https://www.kaggle.com/code) - Learn from others' feature engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
