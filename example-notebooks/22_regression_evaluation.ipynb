{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/22_regression_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Regression Models\n",
    "\n",
    "This notebook contains code examples from the **Evaluating Regression Models** chapter (Chapter 22) of the BANA 4080 textbook. Follow along to practice model evaluation techniques using scikit-learn and Python.\n",
    "\n",
    "## ðŸ“š Chapter Overview\n",
    "\n",
    "Building a regression model is only half the battleâ€”the real question is how good is your model? This chapter teaches you to measure model performance using various metrics, understand when each metric is most appropriate for business decision-making, and evaluate whether your models are ready for real-world deployment.\n",
    "\n",
    "## ðŸŽ¯ What You'll Practice\n",
    "\n",
    "- Calculate and interpret error metrics (SSE, RÂ², MSE, RMSE, MAE, MAPE)\n",
    "- Apply train/test splits to evaluate model generalization\n",
    "- Diagnose overfitting vs underfitting using performance comparisons\n",
    "- Connect evaluation metrics to business decision contexts\n",
    "\n",
    "## ðŸ’¡ How to Use This Notebook\n",
    "\n",
    "1. **Read the chapter first** - This notebook supplements the textbook, not replaces it\n",
    "2. **Run cells sequentially** - Code builds on previous examples\n",
    "3. **Experiment freely** - Modify code to test your understanding\n",
    "4. **Practice variations** - Try different approaches to reinforce learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create our advertising dataset used throughout the chapter\n",
    "data = pd.DataFrame({\n",
    "    \"ad_spend\": [400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300],\n",
    "    \"weekly_sales\": [4200, 4400, 4100, 4800, 5600, 5200, 4900, 5500, 5300, 5900, 5700, 6300, 6900, 6200, 5800, 6600, 7100, 6800, 7300, 7800]\n",
    "})\n",
    "\n",
    "print(\"Advertising dataset shape:\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Sum of Squared Errors (SSE)\n",
    "\n",
    "Linear regression finds the \"best-fit line\" by minimizing the Sum of Squared Errors. Let's build a model and calculate SSE manually to understand this fundamental concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple regression model\n",
    "X = data[['ad_spend']]\n",
    "y = data['weekly_sales']\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(f\"Intercept: {model.intercept_:.2f}\")\n",
    "print(f\"Ad Spend Coefficient: {model.coef_[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SSE manually\n",
    "residuals = y - predictions\n",
    "squared_residuals = residuals ** 2\n",
    "sse_manual = np.sum(squared_residuals)\n",
    "\n",
    "print(f\"Sum of Squared Errors: {sse_manual:,.0f}\")\n",
    "print(f\"Number of data points: {len(y)}\")\n",
    "print(f\"Average squared error per point: {sse_manual/len(y):,.0f}\")\n",
    "\n",
    "# Show calculation breakdown for first 5 points\n",
    "print(f\"\\nBreaking down the first 5 predictions:\")\n",
    "print(f\"{'Point':<8} {'Actual':<8} {'Predicted':<10} {'Error':<8} {'Squared Error':<12}\")\n",
    "print(f\"{'-'*50}\")\n",
    "for i in range(5):\n",
    "    actual = y.iloc[i]\n",
    "    predicted = predictions[i]\n",
    "    error = actual - predicted\n",
    "    squared_error = error ** 2\n",
    "    print(f\"{i+1:<8} ${actual:<7.0f} ${predicted:<9.0f} {error:<+7.0f} {squared_error:<11.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸƒâ€â™‚ï¸ Try It Yourself\n",
    "\n",
    "Calculate the residuals for data points 10-15 and find their contribution to the total SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Squared: Measuring Goodness of Fit\n",
    "\n",
    "RÂ² converts error into an interpretable percentage that represents the proportion of variation explained by your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RÂ² manually and compare with sklearn\n",
    "y_mean = np.mean(y)\n",
    "tss = np.sum((y - y_mean) ** 2)  # Total Sum of Squares\n",
    "sse = np.sum((y - predictions) ** 2)  # Sum of Squared Errors\n",
    "r_squared_manual = 1 - (sse / tss)\n",
    "\n",
    "# Compare with sklearn's calculation\n",
    "r_squared_sklearn = r2_score(y, predictions)\n",
    "\n",
    "print(f\"Manual RÂ² calculation: {r_squared_manual:.4f}\")\n",
    "print(f\"Sklearn RÂ² calculation: {r_squared_sklearn:.4f}\")\n",
    "print(f\"Model RÂ² (from .score() method): {model.score(X, y):.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation: {r_squared_manual:.1%} of the variation in weekly sales\")\n",
    "print(f\"is explained by advertising spend in our model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸƒâ€â™‚ï¸ Try It Yourself\n",
    "\n",
    "Create a \"null model\" that always predicts the mean of y. Calculate its RÂ² value. What should it be and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Metrics for Business Decisions\n",
    "\n",
    "Different error metrics emphasize different aspects of prediction accuracy. Let's calculate and compare MSE, RMSE, MAE, and MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all major error metrics\n",
    "mse = mean_squared_error(y, predictions)\n",
    "rmse = root_mean_squared_error(y, predictions)\n",
    "mae = mean_absolute_error(y, predictions)\n",
    "mape = mean_absolute_percentage_error(y, predictions) * 100  # Convert to percentage\n",
    "\n",
    "print(\"Error Metrics Summary:\")\n",
    "print(f\"{'Metric':<20} {'Value':<15} {'Interpretation'}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'RÂ²':<20} {r_squared_sklearn:<15.3f} {'Proportion of variation explained'}\")\n",
    "print(f\"{'MSE':<20} {mse:<15,.0f} {'Average squared error'}\")\n",
    "print(f\"{'RMSE':<20} ${rmse:<14,.0f} {'Typical prediction error (same units)'}\")\n",
    "print(f\"{'MAE':<20} ${mae:<14,.0f} {'Average absolute error'}\")\n",
    "print(f\"{'MAPE':<20} {mape:<14.1f}% {'Average percentage error'}\")\n",
    "\n",
    "print(f\"\\nBusiness Interpretation:\")\n",
    "print(f\"On average, our predictions are off by about ${rmse:,.0f} when predicting weekly sales.\")\n",
    "print(f\"This represents roughly {mape:.1f}% error relative to actual sales values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate difference between MAE and RMSE with an outlier\n",
    "y_with_outlier = y.copy()\n",
    "predictions_with_outlier = predictions.copy()\n",
    "y_with_outlier.iloc[0] = 10000  # Simulate one very bad prediction\n",
    "\n",
    "mae_outlier = mean_absolute_error(y_with_outlier, predictions_with_outlier)\n",
    "rmse_outlier = root_mean_squared_error(y_with_outlier, predictions_with_outlier)\n",
    "\n",
    "print(\"Impact of Outliers on Error Metrics:\")\n",
    "print(f\"{'Metric':<20} {'Original':<15} {'With Outlier':<15} {'Change'}\")\n",
    "print(f\"{'-'*65}\")\n",
    "print(f\"{'MAE':<20} ${mae:<14,.0f} ${mae_outlier:<14,.0f} {((mae_outlier/mae - 1)*100):+.0f}%\")\n",
    "print(f\"{'RMSE':<20} ${rmse:<14,.0f} ${rmse_outlier:<14,.0f} {((rmse_outlier/rmse - 1)*100):+.0f}%\")\n",
    "print(f\"\\nRMSE is much more sensitive to outliers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸƒâ€â™‚ï¸ Try It Yourself\n",
    "\n",
    "Which error metric would be most appropriate for:\n",
    "1. A financial risk model where large losses could bankrupt the company\n",
    "2. A customer service staffing model where errors have roughly linear costs\n",
    "3. A retail forecasting model comparing across different product categories\n",
    "\n",
    "Justify your choices below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your analysis here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splits: Evaluating Generalization\n",
    "\n",
    "The most critical aspect of model evaluation is testing performance on unseen data. Let's implement proper train/test evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=30\n",
    ")\n",
    "\n",
    "print(f\"Total data points: {len(X)}\")\n",
    "print(f\"Training set: {len(X_train)} points ({len(X_train)/len(X):.1%})\")\n",
    "print(f\"Test set: {len(X_test)} points ({len(X_test)/len(X):.1%})\")\n",
    "\n",
    "# Train model on training data only\n",
    "model_train = LinearRegression()\n",
    "model_train.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on both training and test sets\n",
    "train_predictions = model_train.predict(X_train)\n",
    "test_predictions = model_train.predict(X_test)\n",
    "\n",
    "# Calculate metrics for both sets\n",
    "print(f\"\\n{'Metric':<20} {'Training Set':<15} {'Test Set':<15}\")\n",
    "print(f\"{'-'*50}\")\n",
    "print(f\"{'RÂ²':<20} {r2_score(y_train, train_predictions):<15.3f} {r2_score(y_test, test_predictions):<15.3f}\")\n",
    "print(f\"{'RMSE':<20} {root_mean_squared_error(y_train, train_predictions):<15.0f} {root_mean_squared_error(y_test, test_predictions):<15.0f}\")\n",
    "print(f\"{'MAE':<20} {mean_absolute_error(y_train, train_predictions):<15.0f} {mean_absolute_error(y_test, test_predictions):<15.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸƒâ€â™‚ï¸ Try It Yourself\n",
    "\n",
    "Try different random_state values (e.g., 42, 123, 999) and observe how train/test performance varies. What does this tell you about the reliability of your evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting vs Underfitting Demonstration\n",
    "\n",
    "Let's create synthetic data to clearly demonstrate the difference between underfitting, good fit, and overfitting using polynomial models of different complexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic non-linear data for demonstration\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for high-degree polynomial demonstration\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "np.random.seed(42)\n",
    "X_demo = np.linspace(0, 10, 50).reshape(-1, 1)\n",
    "y_true = 2 * X_demo.ravel() + 0.5 * X_demo.ravel()**2 + np.random.normal(0, 8, 50)\n",
    "\n",
    "# Split the demonstration data\n",
    "X_demo_train, X_demo_test, y_demo_train, y_demo_test = train_test_split(\n",
    "    X_demo, y_true, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Create three models with different complexity\n",
    "models = {\n",
    "    'Linear (Underfit)': LinearRegression(),\n",
    "    'Polynomial-2 (Good)': Pipeline([('poly', PolynomialFeatures(degree=2)), ('linear', LinearRegression())]),\n",
    "    'Polynomial-15 (Overfit)': Pipeline([('poly', PolynomialFeatures(degree=15)), ('linear', LinearRegression())])\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_demo_train, y_demo_train)\n",
    "    train_rmse = root_mean_squared_error(y_demo_train, model.predict(X_demo_train))\n",
    "    test_rmse = root_mean_squared_error(y_demo_test, model.predict(X_demo_test))\n",
    "    results[name] = {'train_rmse': train_rmse, 'test_rmse': test_rmse}\n",
    "\n",
    "# Display results\n",
    "print(\"RMSE Comparison:\")\n",
    "print(f\"{'Model':<25} {'Train RMSE':<12} {'Test RMSE':<12} {'Interpretation'}\")\n",
    "print(f\"{'-'*70}\")\n",
    "for name, metrics in results.items():\n",
    "    if 'Underfit' in name:\n",
    "        interpretation = 'Poor on both'\n",
    "    elif 'Good' in name:\n",
    "        interpretation = 'Good on both'\n",
    "    else:\n",
    "        interpretation = 'Great on train, poor on test'\n",
    "    print(f\"{name:<25} {metrics['train_rmse']:<12.1f} {metrics['test_rmse']:<12.1f} {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸƒâ€â™‚ï¸ Try It Yourself\n",
    "\n",
    "Create a polynomial model with degree 5 and evaluate its performance. Does it underfit, overfit, or show good generalization? Explain your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Application: Advertising Dataset Evaluation\n",
    "\n",
    "Let's apply comprehensive evaluation to the Advertising dataset from Chapter 21, building multiple models and comparing their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full Advertising dataset\n",
    "# You can use this GitHub URL or load locally if you have the file\n",
    "advertising_url = \"https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/main/data/Advertising.csv\"\n",
    "advertising = pd.read_csv(advertising_url)\n",
    "\n",
    "print(\"Advertising dataset shape:\", advertising.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(advertising.head())\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(advertising.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate three different models\n",
    "models_to_compare = {\n",
    "    'TV Only': ['TV'],\n",
    "    'TV + Radio': ['TV', 'radio'],\n",
    "    'All Channels': ['TV', 'radio', 'newspaper']\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name, features in models_to_compare.items():\n",
    "    # Prepare data\n",
    "    X = advertising[features]\n",
    "    y = advertising['sales']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    train_rmse = root_mean_squared_error(y_train, train_pred)\n",
    "    test_rmse = root_mean_squared_error(y_test, test_pred)\n",
    "    train_mae = mean_absolute_error(y_train, train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, test_pred)\n",
    "    \n",
    "    model_results[model_name] = {\n",
    "        'train_r2': train_r2, 'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse, 'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae, 'test_mae': test_mae,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "# Display comparison\n",
    "print(\"Model Comparison - RÂ² Performance:\")\n",
    "print(f\"{'Model':<15} {'Train RÂ²':<12} {'Test RÂ²':<12} {'Difference':<12}\")\n",
    "print(f\"{'-'*55}\")\n",
    "for name, results in model_results.items():\n",
    "    diff = results['train_r2'] - results['test_r2']\n",
    "    print(f\"{name:<15} {results['train_r2']:<12.3f} {results['test_r2']:<12.3f} {diff:<12.3f}\")\n",
    "\n",
    "print(\"\\nModel Comparison - RMSE Performance:\")\n",
    "print(f\"{'Model':<15} {'Train RMSE':<12} {'Test RMSE':<12} {'Generalization':<15}\")\n",
    "print(f\"{'-'*65}\")\n",
    "for name, results in model_results.items():\n",
    "    if results['test_rmse'] <= results['train_rmse'] * 1.1:  # Within 10%\n",
    "        generalization = \"Good\"\n",
    "    elif results['test_rmse'] <= results['train_rmse'] * 1.2:  # Within 20%\n",
    "        generalization = \"Fair\"\n",
    "    else:\n",
    "        generalization = \"Poor\"\n",
    "    print(f\"{name:<15} {results['train_rmse']:<12.2f} {results['test_rmse']:<12.2f} {generalization:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸƒâ€â™‚ï¸ Try It Yourself\n",
    "\n",
    "Based on the model comparison above:\n",
    "1. Which model would you deploy for business use? Why?\n",
    "2. Calculate MAPE for your chosen model and interpret it in business terms\n",
    "3. If a marketing manager has a $50,000 advertising budget, how would you use your model to guide allocation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your analysis here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Practice Challenges\n",
    "\n",
    "Test your understanding with these additional exercises that combine multiple concepts from the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Multiple Random Splits Analysis\n",
    "\n",
    "Instead of relying on a single train/test split, evaluate model stability by using multiple different random splits. Test the same model with 5 different random_state values and analyze the variation in performance metrics. What does this tell you about the reliability of your evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Business Metric Design\n",
    "\n",
    "Create a custom business metric that combines prediction accuracy with cost considerations. For example, if overestimating sales costs $10 per unit in lost opportunity, while underestimating costs $50 per unit in excess inventory, design a metric that reflects these asymmetric costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Evaluation Across Time\n",
    "\n",
    "If the advertising dataset represented time-series data (e.g., weeks 1-200), how would you modify your evaluation approach? Implement a time-aware train/test split and discuss the implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Chapter Summary\n",
    "\n",
    "In this notebook, you practiced:\n",
    "\n",
    "- âœ… Calculating SSE manually and understanding why regression minimizes squared errors\n",
    "- âœ… Computing and interpreting RÂ² as a measure of explained variation\n",
    "- âœ… Applying different error metrics (MSE, RMSE, MAE, MAPE) and understanding their business contexts\n",
    "- âœ… Implementing train/test splits to evaluate model generalization\n",
    "- âœ… Diagnosing overfitting vs underfitting using train/test performance comparisons\n",
    "\n",
    "## ðŸ”— Connections to Other Chapters\n",
    "\n",
    "- **Previous chapters**: Chapter 21 (regression modeling) provided the foundation models that this chapter teaches you to evaluate\n",
    "- **Upcoming chapters**: These evaluation techniques will apply to every machine learning algorithm you'll learn, from decision trees to neural networks\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "- [Scikit-learn Model Evaluation Documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [Cross-validation techniques](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Metrics for regression problems](https://scikit-learn.org/stable/modules/classes.html#regression-metrics)\n",
    "\n",
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "1. **Review the chapter** to reinforce theoretical concepts\n",
    "2. **Complete the end-of-chapter exercises** in the textbook\n",
    "3. **Practice with your own datasets** to build confidence\n",
    "4. **Apply these evaluation techniques** to future modeling projects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
