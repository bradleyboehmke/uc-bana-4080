{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/22_regression_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Regression Models\n",
    "\n",
    "This notebook contains code examples from the **Evaluating Regression Models** chapter (Chapter 22) of the BANA 4080 textbook. Follow along to practice model evaluation techniques using scikit-learn and Python.\n",
    "\n",
    "## 📚 Chapter Overview\n",
    "\n",
    "Building a regression model is only half the battle—the real question is how good is your model? This chapter teaches you to measure model performance using various metrics, understand when each metric is most appropriate for business decision-making, and evaluate whether your models are ready for real-world deployment.\n",
    "\n",
    "## 🎯 What You'll Practice\n",
    "\n",
    "- Calculate and interpret error metrics (SSE, R², MSE, RMSE, MAE, MAPE)\n",
    "- Apply train/test splits to evaluate model generalization\n",
    "- Diagnose overfitting vs underfitting using performance comparisons\n",
    "- Connect evaluation metrics to business decision contexts\n",
    "\n",
    "## 💡 How to Use This Notebook\n",
    "\n",
    "1. **Read the chapter first** - This notebook supplements the textbook, not replaces it\n",
    "2. **Run cells sequentially** - Code builds on previous examples\n",
    "3. **Experiment freely** - Modify code to test your understanding\n",
    "4. **Practice variations** - Try different approaches to reinforce learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create our advertising dataset used throughout the chapter\n",
    "data = pd.DataFrame({\n",
    "    \"ad_spend\": [400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300],\n",
    "    \"weekly_sales\": [4200, 4400, 4100, 4800, 5600, 5200, 4900, 5500, 5300, 5900, 5700, 6300, 6900, 6200, 5800, 6600, 7100, 6800, 7300, 7800]\n",
    "})\n",
    "\n",
    "print(\"Advertising dataset shape:\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Sum of Squared Errors (SSE)\n",
    "\n",
    "Linear regression finds the \"best-fit line\" by minimizing the Sum of Squared Errors. Let's build a model and calculate SSE manually to understand this fundamental concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple regression model\n",
    "X = data[['ad_spend']]\n",
    "y = data['weekly_sales']\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(f\"Intercept: {model.intercept_:.2f}\")\n",
    "print(f\"Ad Spend Coefficient: {model.coef_[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SSE manually\n",
    "residuals = y - predictions\n",
    "squared_residuals = residuals ** 2\n",
    "sse_manual = np.sum(squared_residuals)\n",
    "\n",
    "print(f\"Sum of Squared Errors: {sse_manual:,.0f}\")\n",
    "print(f\"Number of data points: {len(y)}\")\n",
    "print(f\"Average squared error per point: {sse_manual/len(y):,.0f}\")\n",
    "\n",
    "# Show calculation breakdown for first 5 points\n",
    "print(f\"\\nBreaking down the first 5 predictions:\")\n",
    "print(f\"{'Point':<8} {'Actual':<8} {'Predicted':<10} {'Error':<8} {'Squared Error':<12}\")\n",
    "print(f\"{'-'*50}\")\n",
    "for i in range(5):\n",
    "    actual = y.iloc[i]\n",
    "    predicted = predictions[i]\n",
    "    error = actual - predicted\n",
    "    squared_error = error ** 2\n",
    "    print(f\"{i+1:<8} ${actual:<7.0f} ${predicted:<9.0f} {error:<+7.0f} {squared_error:<11.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏃‍♂️ Try It Yourself\n",
    "\n",
    "Calculate the residuals for data points 10-15 and find their contribution to the total SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Squared: Measuring Goodness of Fit\n",
    "\n",
    "R² converts error into an interpretable percentage that represents the proportion of variation explained by your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R² manually and compare with sklearn\n",
    "y_mean = np.mean(y)\n",
    "tss = np.sum((y - y_mean) ** 2)  # Total Sum of Squares\n",
    "sse = np.sum((y - predictions) ** 2)  # Sum of Squared Errors\n",
    "r_squared_manual = 1 - (sse / tss)\n",
    "\n",
    "# Compare with sklearn's calculation\n",
    "r_squared_sklearn = r2_score(y, predictions)\n",
    "\n",
    "print(f\"Manual R² calculation: {r_squared_manual:.4f}\")\n",
    "print(f\"Sklearn R² calculation: {r_squared_sklearn:.4f}\")\n",
    "print(f\"Model R² (from .score() method): {model.score(X, y):.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation: {r_squared_manual:.1%} of the variation in weekly sales\")\n",
    "print(f\"is explained by advertising spend in our model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏃‍♂️ Try It Yourself\n",
    "\n",
    "Create a \"null model\" that always predicts the mean of y. Calculate its R² value. What should it be and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Metrics for Business Decisions\n",
    "\n",
    "Different error metrics emphasize different aspects of prediction accuracy. Let's calculate and compare MSE, RMSE, MAE, and MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all major error metrics\n",
    "mse = mean_squared_error(y, predictions)\n",
    "rmse = root_mean_squared_error(y, predictions)\n",
    "mae = mean_absolute_error(y, predictions)\n",
    "mape = mean_absolute_percentage_error(y, predictions) * 100  # Convert to percentage\n",
    "\n",
    "print(\"Error Metrics Summary:\")\n",
    "print(f\"{'Metric':<20} {'Value':<15} {'Interpretation'}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'R²':<20} {r_squared_sklearn:<15.3f} {'Proportion of variation explained'}\")\n",
    "print(f\"{'MSE':<20} {mse:<15,.0f} {'Average squared error'}\")\n",
    "print(f\"{'RMSE':<20} ${rmse:<14,.0f} {'Typical prediction error (same units)'}\")\n",
    "print(f\"{'MAE':<20} ${mae:<14,.0f} {'Average absolute error'}\")\n",
    "print(f\"{'MAPE':<20} {mape:<14.1f}% {'Average percentage error'}\")\n",
    "\n",
    "print(f\"\\nBusiness Interpretation:\")\n",
    "print(f\"On average, our predictions are off by about ${rmse:,.0f} when predicting weekly sales.\")\n",
    "print(f\"This represents roughly {mape:.1f}% error relative to actual sales values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate difference between MAE and RMSE with an outlier\n",
    "y_with_outlier = y.copy()\n",
    "predictions_with_outlier = predictions.copy()\n",
    "y_with_outlier.iloc[0] = 10000  # Simulate one very bad prediction\n",
    "\n",
    "mae_outlier = mean_absolute_error(y_with_outlier, predictions_with_outlier)\n",
    "rmse_outlier = root_mean_squared_error(y_with_outlier, predictions_with_outlier)\n",
    "\n",
    "print(\"Impact of Outliers on Error Metrics:\")\n",
    "print(f\"{'Metric':<20} {'Original':<15} {'With Outlier':<15} {'Change'}\")\n",
    "print(f\"{'-'*65}\")\n",
    "print(f\"{'MAE':<20} ${mae:<14,.0f} ${mae_outlier:<14,.0f} {((mae_outlier/mae - 1)*100):+.0f}%\")\n",
    "print(f\"{'RMSE':<20} ${rmse:<14,.0f} ${rmse_outlier:<14,.0f} {((rmse_outlier/rmse - 1)*100):+.0f}%\")\n",
    "print(f\"\\nRMSE is much more sensitive to outliers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏃‍♂️ Try It Yourself\n",
    "\n",
    "Which error metric would be most appropriate for:\n",
    "1. A financial risk model where large losses could bankrupt the company\n",
    "2. A customer service staffing model where errors have roughly linear costs\n",
    "3. A retail forecasting model comparing across different product categories\n",
    "\n",
    "Justify your choices below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your analysis here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splits: Evaluating Generalization\n",
    "\n",
    "The most critical aspect of model evaluation is testing performance on unseen data. Let's implement proper train/test evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=30\n",
    ")\n",
    "\n",
    "print(f\"Total data points: {len(X)}\")\n",
    "print(f\"Training set: {len(X_train)} points ({len(X_train)/len(X):.1%})\")\n",
    "print(f\"Test set: {len(X_test)} points ({len(X_test)/len(X):.1%})\")\n",
    "\n",
    "# Train model on training data only\n",
    "model_train = LinearRegression()\n",
    "model_train.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on both training and test sets\n",
    "train_predictions = model_train.predict(X_train)\n",
    "test_predictions = model_train.predict(X_test)\n",
    "\n",
    "# Calculate metrics for both sets\n",
    "print(f\"\\n{'Metric':<20} {'Training Set':<15} {'Test Set':<15}\")\n",
    "print(f\"{'-'*50}\")\n",
    "print(f\"{'R²':<20} {r2_score(y_train, train_predictions):<15.3f} {r2_score(y_test, test_predictions):<15.3f}\")\n",
    "print(f\"{'RMSE':<20} {root_mean_squared_error(y_train, train_predictions):<15.0f} {root_mean_squared_error(y_test, test_predictions):<15.0f}\")\n",
    "print(f\"{'MAE':<20} {mean_absolute_error(y_train, train_predictions):<15.0f} {mean_absolute_error(y_test, test_predictions):<15.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏃‍♂️ Try It Yourself\n",
    "\n",
    "Try different random_state values (e.g., 42, 123, 999) and observe how train/test performance varies. What does this tell you about the reliability of your evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting vs Underfitting Demonstration\n",
    "\n",
    "Let's create synthetic data to clearly demonstrate the difference between underfitting, good fit, and overfitting using polynomial models of different complexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic non-linear data for demonstration\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for high-degree polynomial demonstration\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "np.random.seed(42)\n",
    "X_demo = np.linspace(0, 10, 50).reshape(-1, 1)\n",
    "y_true = 2 * X_demo.ravel() + 0.5 * X_demo.ravel()**2 + np.random.normal(0, 8, 50)\n",
    "\n",
    "# Split the demonstration data\n",
    "X_demo_train, X_demo_test, y_demo_train, y_demo_test = train_test_split(\n",
    "    X_demo, y_true, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Create three models with different complexity\n",
    "models = {\n",
    "    'Linear (Underfit)': LinearRegression(),\n",
    "    'Polynomial-2 (Good)': Pipeline([('poly', PolynomialFeatures(degree=2)), ('linear', LinearRegression())]),\n",
    "    'Polynomial-15 (Overfit)': Pipeline([('poly', PolynomialFeatures(degree=15)), ('linear', LinearRegression())])\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_demo_train, y_demo_train)\n",
    "    train_rmse = root_mean_squared_error(y_demo_train, model.predict(X_demo_train))\n",
    "    test_rmse = root_mean_squared_error(y_demo_test, model.predict(X_demo_test))\n",
    "    results[name] = {'train_rmse': train_rmse, 'test_rmse': test_rmse}\n",
    "\n",
    "# Display results\n",
    "print(\"RMSE Comparison:\")\n",
    "print(f\"{'Model':<25} {'Train RMSE':<12} {'Test RMSE':<12} {'Interpretation'}\")\n",
    "print(f\"{'-'*70}\")\n",
    "for name, metrics in results.items():\n",
    "    if 'Underfit' in name:\n",
    "        interpretation = 'Poor on both'\n",
    "    elif 'Good' in name:\n",
    "        interpretation = 'Good on both'\n",
    "    else:\n",
    "        interpretation = 'Great on train, poor on test'\n",
    "    print(f\"{name:<25} {metrics['train_rmse']:<12.1f} {metrics['test_rmse']:<12.1f} {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏃‍♂️ Try It Yourself\n",
    "\n",
    "Create a polynomial model with degree 5 and evaluate its performance. Does it underfit, overfit, or show good generalization? Explain your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Application: Advertising Dataset Evaluation\n",
    "\n",
    "Let's apply comprehensive evaluation to the Advertising dataset from Chapter 21, building multiple models and comparing their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full Advertising dataset\n",
    "# You can use this GitHub URL or load locally if you have the file\n",
    "advertising_url = \"https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/main/data/Advertising.csv\"\n",
    "advertising = pd.read_csv(advertising_url)\n",
    "\n",
    "print(\"Advertising dataset shape:\", advertising.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(advertising.head())\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(advertising.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate three different models\n",
    "models_to_compare = {\n",
    "    'TV Only': ['TV'],\n",
    "    'TV + Radio': ['TV', 'radio'],\n",
    "    'All Channels': ['TV', 'radio', 'newspaper']\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name, features in models_to_compare.items():\n",
    "    # Prepare data\n",
    "    X = advertising[features]\n",
    "    y = advertising['sales']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    train_rmse = root_mean_squared_error(y_train, train_pred)\n",
    "    test_rmse = root_mean_squared_error(y_test, test_pred)\n",
    "    train_mae = mean_absolute_error(y_train, train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, test_pred)\n",
    "    \n",
    "    model_results[model_name] = {\n",
    "        'train_r2': train_r2, 'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse, 'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae, 'test_mae': test_mae,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "# Display comparison\n",
    "print(\"Model Comparison - R² Performance:\")\n",
    "print(f\"{'Model':<15} {'Train R²':<12} {'Test R²':<12} {'Difference':<12}\")\n",
    "print(f\"{'-'*55}\")\n",
    "for name, results in model_results.items():\n",
    "    diff = results['train_r2'] - results['test_r2']\n",
    "    print(f\"{name:<15} {results['train_r2']:<12.3f} {results['test_r2']:<12.3f} {diff:<12.3f}\")\n",
    "\n",
    "print(\"\\nModel Comparison - RMSE Performance:\")\n",
    "print(f\"{'Model':<15} {'Train RMSE':<12} {'Test RMSE':<12} {'Generalization':<15}\")\n",
    "print(f\"{'-'*65}\")\n",
    "for name, results in model_results.items():\n",
    "    if results['test_rmse'] <= results['train_rmse'] * 1.1:  # Within 10%\n",
    "        generalization = \"Good\"\n",
    "    elif results['test_rmse'] <= results['train_rmse'] * 1.2:  # Within 20%\n",
    "        generalization = \"Fair\"\n",
    "    else:\n",
    "        generalization = \"Poor\"\n",
    "    print(f\"{name:<15} {results['train_rmse']:<12.2f} {results['test_rmse']:<12.2f} {generalization:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏃‍♂️ Try It Yourself\n",
    "\n",
    "Based on the model comparison above:\n",
    "1. Which model would you deploy for business use? Why?\n",
    "2. Calculate MAPE for your chosen model and interpret it in business terms\n",
    "3. If a marketing manager has a $50,000 advertising budget, how would you use your model to guide allocation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your analysis here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Practice Challenges\n",
    "\n",
    "Test your understanding with these additional exercises that combine multiple concepts from the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Multiple Random Splits Analysis\n",
    "\n",
    "Instead of relying on a single train/test split, evaluate model stability by using multiple different random splits. Test the same model with 5 different random_state values and analyze the variation in performance metrics. What does this tell you about the reliability of your evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Business Metric Design\n",
    "\n",
    "Create a custom business metric that combines prediction accuracy with cost considerations. For example, if overestimating sales costs $10 per unit in lost opportunity, while underestimating costs $50 per unit in excess inventory, design a metric that reflects these asymmetric costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Evaluation Across Time\n",
    "\n",
    "If the advertising dataset represented time-series data (e.g., weeks 1-200), how would you modify your evaluation approach? Implement a time-aware train/test split and discuss the implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Chapter Summary\n",
    "\n",
    "In this notebook, you practiced:\n",
    "\n",
    "- ✅ Calculating SSE manually and understanding why regression minimizes squared errors\n",
    "- ✅ Computing and interpreting R² as a measure of explained variation\n",
    "- ✅ Applying different error metrics (MSE, RMSE, MAE, MAPE) and understanding their business contexts\n",
    "- ✅ Implementing train/test splits to evaluate model generalization\n",
    "- ✅ Diagnosing overfitting vs underfitting using train/test performance comparisons\n",
    "\n",
    "## 🔗 Connections to Other Chapters\n",
    "\n",
    "- **Previous chapters**: Chapter 21 (regression modeling) provided the foundation models that this chapter teaches you to evaluate\n",
    "- **Upcoming chapters**: These evaluation techniques will apply to every machine learning algorithm you'll learn, from decision trees to neural networks\n",
    "\n",
    "## 📚 Additional Resources\n",
    "\n",
    "- [Scikit-learn Model Evaluation Documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [Cross-validation techniques](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Metrics for regression problems](https://scikit-learn.org/stable/modules/classes.html#regression-metrics)\n",
    "\n",
    "## 🎯 Next Steps\n",
    "\n",
    "1. **Review the chapter** to reinforce theoretical concepts\n",
    "2. **Complete the end-of-chapter exercises** in the textbook\n",
    "3. **Practice with your own datasets** to build confidence\n",
    "4. **Apply these evaluation techniques** to future modeling projects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
