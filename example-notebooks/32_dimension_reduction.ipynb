{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 32: Dimension Reduction with PCA\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/32_dimension_reduction.ipynb)\n",
    "\n",
    "This notebook contains all the executable code examples from Chapter 32 of the BANA 4080 textbook. You can run each code cell and experiment with the examples to deepen your understanding of Principal Component Analysis (PCA) and dimension reduction.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By working through this notebook, you will be able to:\n",
    "\n",
    "- Understand the curse of dimensionality and why dimension reduction is important\n",
    "- Explain the difference between feature selection and feature extraction\n",
    "- Understand how PCA finds directions of maximum variance\n",
    "- Apply PCA using scikit-learn's `PCA` class\n",
    "- Interpret principal components, loadings, and explained variance\n",
    "- Use scree plots and the elbow method to select the number of components\n",
    "- Transform data to principal component space\n",
    "- Integrate PCA into machine learning pipelines\n",
    "- Compare model performance with and without PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries we'll need throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding PCA with a Simple Example\n",
    "\n",
    "We'll start with a small dataset of student study habits to understand how PCA works step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Student Study Habits Dataset\n",
    "\n",
    "This simple dataset has 6 students and 4 features that are likely correlated (students who study more tend to do more practice problems, attend more classes, and score higher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple student study habits dataset\n",
    "data = {\n",
    "    'Student': ['Alice', 'Bob', 'Carol', 'David', 'Emma', 'Frank'],\n",
    "    'Hours_Studied': [10, 5, 8, 12, 6, 9],\n",
    "    'Practice_Problems': [50, 20, 40, 60, 25, 45],\n",
    "    'Attendance_Pct': [95, 70, 85, 98, 75, 90],\n",
    "    'Quiz_Score': [88, 65, 78, 92, 68, 82]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Student Study Habits Dataset:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Standardize the Features\n",
    "\n",
    "**Why standardize?** PCA is sensitive to feature scales. Features with larger values will dominate the principal components. Standardization (z-score normalization) ensures all features contribute equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for PCA (exclude student names)\n",
    "X = df[['Hours_Studied', 'Practice_Problems', 'Attendance_Pct', 'Quiz_Score']].values\n",
    "feature_names = ['Hours_Studied', 'Practice_Problems', 'Attendance_Pct', 'Quiz_Score']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Show Standardized data\n",
    "print(\"Standardized features (mean=0, std=1 for each column):\")\n",
    "pd.DataFrame(\n",
    "    np.round(X_scaled, 2),\n",
    "    columns=feature_names,\n",
    "    index=df['Student']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compute the Covariance Matrix\n",
    "\n",
    "The covariance matrix shows how features vary together. High covariance indicates redundancy - this is what PCA exploits to reduce dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute covariance matrix (what PCA does behind the scenes)\n",
    "cov_matrix = np.cov(X_scaled.T)\n",
    "\n",
    "print(\"Covariance Matrix (4x4 for our 4 features):\")\n",
    "print(np.round(cov_matrix, 2))\n",
    "\n",
    "# Create a labeled version for easier interpretation\n",
    "cov_df = pd.DataFrame(cov_matrix,\n",
    "                      columns=feature_names,\n",
    "                      index=feature_names)\n",
    "print(\"\\nLabeled Covariance Matrix:\")\n",
    "cov_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 3-4: Fit PCA and Extract Components\n",
    "\n",
    "Now PCA will:\n",
    "- Find eigenvectors (directions of maximum variance) = **principal components**\n",
    "- Find eigenvalues (amount of variance in each direction) = **explained variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA on our student data (keep all 4 components initially)\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "print(\"PCA fitted successfully!\")\n",
    "print(\"PCA has computed eigenvectors and eigenvalues internally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Principal Component Loadings\n",
    "\n",
    "**Loadings** show how much each original feature contributes to each principal component. Think of them as the \"recipe\" for each PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract eigenvectors (feature weights/loadings)\n",
    "components_df = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=feature_names,\n",
    "    index=['PC1', 'PC2', 'PC3', 'PC4']\n",
    ")\n",
    "\n",
    "print(\"Principal Component Loadings:\")\n",
    "print(\"(How much each feature contributes to each PC)\\n\")\n",
    "components_df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Eigenvalues (Explained Variance)\n",
    "\n",
    "**Eigenvalues** tell us how much variance each PC captures. Larger eigenvalues = more important components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw Eigenvalues (Explained Variance):\")\n",
    "print(pca.explained_variance_)\n",
    "\n",
    "print(\"\\nNicely formatted Eigenvalues:\")\n",
    "for i, eigenvalue in enumerate(pca.explained_variance_, 1):\n",
    "    print(f\"PC{i}: {eigenvalue:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Explained Ratios\n",
    "\n",
    "It's more useful to look at the **proportion** of total variance explained by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at variance explained by each component\n",
    "print(\"Variance Explained by Each PC:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_, 1):\n",
    "    print(f\"  PC{i}: {var*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nCumulative Variance Explained:\")\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "for i, var in enumerate(cumsum, 1):\n",
    "    print(f\"  First {i} PC(s): {var*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Number of Components\n",
    "\n",
    "You can choose components based on:\n",
    "1. **Variance threshold**: Keep components that explain X% of variance\n",
    "2. **Fixed number**: Keep a specific number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Keep enough components to explain 90% of variance\n",
    "pca_90 = PCA(n_components=0.90)\n",
    "pca_90.fit(X_scaled)\n",
    "print(f\"To explain 90% of variance, keep {pca_90.n_components_} components\")\n",
    "print(f\"Actual variance explained: {pca_90.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Method 2: Keep exactly 2 components for visualization\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2.fit(X_scaled)\n",
    "print(f\"\\nKeeping 2 components explains {pca_2.explained_variance_ratio_.sum()*100:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Transform Data to Principal Component Space\n",
    "\n",
    "Now we'll **transform** our original 4-feature data into 2-component data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the original data into principal component space (using 2 components)\n",
    "X_pca = pca_2.transform(X_scaled)\n",
    "\n",
    "print(\"Dimensionality Reduction:\")\n",
    "print(f\"  Original Data Shape:    {X_scaled.shape} (6 students × 4 features)\")\n",
    "print(f\"  Transformed Data Shape: {X_pca.shape} (6 students × 2 components)\")\n",
    "\n",
    "# Show the transformed data for our students\n",
    "pca_df = pd.DataFrame(X_pca,\n",
    "                      columns=['PC1', 'PC2'],\n",
    "                      index=df['Student'])\n",
    "print(\"\\nStudent Data in PC Space:\")\n",
    "print(pca_df.round(2))\n",
    "\n",
    "# Compare: Alice vs Bob\n",
    "print(f\"\\nInterpretation Example:\")\n",
    "print(f\"  Alice's PC1 score: {X_pca[0, 0]:+.2f} (overall academic performance)\")\n",
    "print(f\"  Bob's PC1 score:   {X_pca[1, 0]:+.2f} (overall academic performance)\")\n",
    "print(\"  → Alice scores much higher on PC1, reflecting stronger overall performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Real-World Application - Breast Cancer Classification\n",
    "\n",
    "Now let's apply PCA to a real dataset with 30 features! This is where PCA really shines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Breast Cancer Dataset\n",
    "\n",
    "This dataset has measurements from tumor cell nuclei images. **30 features** - perfect for dimension reduction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "print(\"Breast Cancer Dataset:\")\n",
    "print(f\"  Samples: {X.shape[0]}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Classes: {len(np.unique(y))} (malignant=0, benign=1)\")\n",
    "\n",
    "print(\"\\nFeature names:\")\n",
    "for i, name in enumerate(data.feature_names, 1):\n",
    "    print(f\"  {i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data and Standardize\n",
    "\n",
    "Always split **before** standardizing to avoid data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set:     {X_test.shape[0]} samples\")\n",
    "\n",
    "# Standardize the features (fit on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nOriginal training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Number of features: {X_train_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit PCA with All Components\n",
    "\n",
    "First, let's fit PCA with all 30 components to see the full variance breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with all components to see the full picture\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "print(f\"Number of components: {pca.n_components_}\")\n",
    "print(f\"\\nFirst 10 components explain:\")\n",
    "for i in range(10):\n",
    "    print(f\"  PC{i+1:2d}: {pca.explained_variance_ratio_[i]*100:5.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize with Scree Plot\n",
    "\n",
    "A **scree plot** shows the variance explained by each component. Look for the \"elbow\" where adding more components gives diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scree plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Individual variance explained\n",
    "ax1.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "         pca.explained_variance_ratio_, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Principal Component', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Variance Explained', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Scree Plot: Variance per Component', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Cumulative variance explained\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "ax2.plot(range(1, len(cumsum) + 1), cumsum, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.axhline(y=0.95, color='green', linestyle='--', linewidth=2, label='95% threshold')\n",
    "ax2.axhline(y=0.90, color='orange', linestyle='--', linewidth=2, label='90% threshold')\n",
    "ax2.set_xlabel('Number of Components', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Cumulative Variance Explained', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "n_components_90 = np.argmax(cumsum >= 0.90) + 1\n",
    "\n",
    "print(f\"Components needed for 95% variance: {n_components_95}\")\n",
    "print(f\"Components needed for 90% variance: {n_components_90}\")\n",
    "print(f\"\\nDimensionality reduction: 30 → {n_components_95} features ({(1-n_components_95/30)*100:.0f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refit PCA with Optimal Number of Components\n",
    "\n",
    "Based on the scree plot, let's use 7 components (which captures ~95% of variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit with 7 components\n",
    "pca_7 = PCA(n_components=7)\n",
    "pca_7.fit(X_train_scaled)\n",
    "\n",
    "# Show variance explained by each component\n",
    "print(\"Variance explained by each component:\")\n",
    "total = 0\n",
    "for i, var in enumerate(pca_7.explained_variance_ratio_, 1):\n",
    "    total += var\n",
    "    print(f\"  PC{i}: {var*100:5.1f}%  (cumulative: {total*100:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal variance explained by 7 components: {pca_7.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Principal Component Loadings\n",
    "\n",
    "Let's examine what each principal component represents by looking at which original features contribute most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loadings dataframe\n",
    "loadings_df = pd.DataFrame(\n",
    "    pca_7.components_.T,\n",
    "    columns=[f'PC{i}' for i in range(1, 8)],\n",
    "    index=data.feature_names\n",
    ")\n",
    "\n",
    "# Display top contributing features for each PC\n",
    "print(\"Top 3 features (by absolute loading) for each PC:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for pc in loadings_df.columns:\n",
    "    top_features = loadings_df[pc].abs().nlargest(3)\n",
    "    print(f\"\\n{pc}:\")\n",
    "    for feature, _ in top_features.items():\n",
    "        actual_loading = loadings_df.loc[feature, pc]\n",
    "        print(f\"  {feature:30s}: {actual_loading:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Data\n",
    "\n",
    "Now let's transform both training and test data from 30 features to 7 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform both training and test data\n",
    "X_train_pca = pca_7.transform(X_train_scaled)\n",
    "X_test_pca = pca_7.transform(X_test_scaled)\n",
    "\n",
    "print(\"Dimensionality Reduction Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original training shape:    {X_train_scaled.shape} (30 features)\")\n",
    "print(f\"Transformed training shape: {X_train_pca.shape} (7 components)\")\n",
    "print(f\"\\nOriginal test shape:        {X_test_scaled.shape} (30 features)\")\n",
    "print(f\"Transformed test shape:     {X_test_pca.shape} (7 components)\")\n",
    "print(f\"\\nDimensionality reduced from 30 to 7 features ({(1-7/30)*100:.0f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Compare Models\n",
    "\n",
    "Let's compare classification performance using:\n",
    "1. All 30 original features\n",
    "2. Only 7 PCA components\n",
    "\n",
    "This shows the **practical benefit** of PCA: similar accuracy with fewer features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with all 30 original features\n",
    "print(\"Training model with 30 original features...\")\n",
    "model_original = LogisticRegression(max_iter=5000, random_state=42)\n",
    "model_original.fit(X_train_scaled, y_train)\n",
    "y_pred_original = model_original.predict(X_test_scaled)\n",
    "acc_original = accuracy_score(y_test, y_pred_original)\n",
    "\n",
    "# Model with 7 PCA components\n",
    "print(\"Training model with 7 PCA components...\")\n",
    "model_pca = LogisticRegression(max_iter=5000, random_state=42)\n",
    "model_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = model_pca.predict(X_test_pca)\n",
    "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy with 30 original features: {acc_original:.3f}\")\n",
    "print(f\"Accuracy with 7 PCA components:     {acc_pca:.3f}\")\n",
    "print(f\"Accuracy difference:                {abs(acc_original - acc_pca):.3f}\")\n",
    "print(f\"\\nFeatures used: 30 → 7 ({(1-7/30)*100:.0f}% reduction)\")\n",
    "print(f\"Variance retained: {pca_7.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "\n",
    "if acc_pca >= acc_original - 0.02:  # Within 2% of original\n",
    "    print(\"\\n✓ PCA achieves comparable accuracy with far fewer features!\")\n",
    "else:\n",
    "    print(\"\\n⚠ PCA reduced accuracy significantly - may need more components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Visualizing High-Dimensional Data in 2D\n",
    "\n",
    "One powerful use of PCA is visualizing high-dimensional data by projecting it to 2 or 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with just 2 components for visualization\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_train_2d = pca_2d.fit_transform(X_train_scaled)\n",
    "\n",
    "# Create scatter plot colored by class\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot malignant (class 0) in red\n",
    "malignant = y_train == 0\n",
    "plt.scatter(X_train_2d[malignant, 0], X_train_2d[malignant, 1],\n",
    "           c='red', label='Malignant', alpha=0.6, s=60, edgecolors='black', linewidths=0.5)\n",
    "\n",
    "# Plot benign (class 1) in blue\n",
    "benign = y_train == 1\n",
    "plt.scatter(X_train_2d[benign, 0], X_train_2d[benign, 1],\n",
    "           c='blue', label='Benign', alpha=0.6, s=60, edgecolors='black', linewidths=0.5)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variance)', \n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variance)', \n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.title('Breast Cancer Data Projected to 2D with PCA', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"2D visualization captures {pca_2d.explained_variance_ratio_.sum()*100:.1f}% of variance\")\n",
    "print(\"Notice how the two classes (malignant vs benign) show some separation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "Congratulations! You've mastered Principal Component Analysis. Here's what you learned:\n",
    "\n",
    "### Core Concepts\n",
    "1. **Dimension Reduction**: Compress many features into fewer components while retaining information\n",
    "2. **PCA Algorithm**: Finds directions of maximum variance (eigenvectors) and their importance (eigenvalues)\n",
    "3. **Principal Components**: Linear combinations of original features that capture variance\n",
    "4. **Loadings**: Weights showing how original features contribute to each PC\n",
    "\n",
    "### Practical Skills\n",
    "1. **Standardization**: Always standardize before PCA\n",
    "2. **Scree Plots**: Visualize variance explained to choose number of components\n",
    "3. **Elbow Method**: Look for the \"elbow\" where variance gains diminish\n",
    "4. **Interpretation**: Understand what each PC represents by examining loadings\n",
    "5. **Transformation**: Convert data from original features to PC space\n",
    "\n",
    "### When to Use PCA\n",
    "✓ **High-dimensional data** (many features)  \n",
    "✓ **Correlated features** (redundancy to exploit)  \n",
    "✓ **Visualization** (project to 2D or 3D)  \n",
    "✓ **Speed up training** (fewer features = faster models)  \n",
    "✓ **Noise reduction** (minor components often capture noise)  \n",
    "\n",
    "### Tradeoffs\n",
    "⚠ **Loss of interpretability**: PCs are combinations, not original features  \n",
    "⚠ **Linear assumptions**: PCA assumes linear relationships  \n",
    "⚠ **Variance ≠ Importance**: High variance doesn't always mean predictive power  \n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you've mastered PCA, try:\n",
    "1. Experimenting with different numbers of components\n",
    "2. Comparing PCA performance on different datasets\n",
    "3. Using PCA for visualization of your own high-dimensional data\n",
    "4. Exploring other dimension reduction techniques (t-SNE, UMAP)\n",
    "5. Integrating PCA into your machine learning pipelines\n",
    "\n",
    "Happy dimension reducing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
