{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/24_classification_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Classification Models\n",
    "\n",
    "This notebook contains code examples from the **Evaluating Classification Models** chapter (Chapter 24) of the BANA 4080 textbook. Follow along to practice advanced classification evaluation metrics using pandas, scikit-learn, and Python.\n",
    "\n",
    "## üìö Chapter Overview\n",
    "\n",
    "This chapter teaches you to evaluate classification models using metrics that align with business reality. While accuracy seems intuitive, it can be deeply misleading in real business scenarios, especially with imbalanced datasets like credit default prediction.\n",
    "\n",
    "## üéØ What You'll Practice\n",
    "\n",
    "- Identify the \"accuracy trap\" and understand why 97.3% accuracy can be misleading with imbalanced data\n",
    "- Construct and interpret confusion matrices to understand exactly how your model makes errors\n",
    "- Calculate precision, recall, and F1-score and explain their business implications for different scenarios\n",
    "- Use ROC curves and AUC to evaluate model ranking quality for risk-based pricing strategies\n",
    "- Design business-aligned evaluation frameworks that select appropriate metrics based on specific costs\n",
    "\n",
    "## üí° How to Use This Notebook\n",
    "\n",
    "1. **Read the chapter first** - This notebook supplements the textbook, not replaces it\n",
    "2. **Run cells sequentially** - Code builds on previous examples\n",
    "3. **Experiment freely** - Modify code to test your understanding\n",
    "4. **Practice variations** - Try different approaches to reinforce learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from ISLP import load_data\n",
    "import warnings\n",
    "\n",
    "# Suppress numerical warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# Load the Default dataset from Chapter 23\n",
    "Default = load_data('Default')\n",
    "print(\"Default dataset shape:\", Default.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "Default.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Accuracy Trap: When High Accuracy Misleads\n",
    "\n",
    "Let's start by exploring why accuracy can be deeply misleading in business scenarios, especially with imbalanced datasets like credit default prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the class imbalance in our Default dataset\n",
    "print(\"Default distribution:\")\n",
    "print(Default['default'].value_counts())\n",
    "print(f\"\\nDefault rate: {Default['default'].value_counts(normalize=True)['Yes']:.1%}\")\n",
    "\n",
    "# What would happen if we always predicted \"No Default\"?\n",
    "naive_accuracy = (Default['default'] == 'No').mean()\n",
    "print(f\"\\nAccuracy of always predicting 'No Default': {naive_accuracy:.1%}\")\n",
    "print(f\"This 'lazy' model provides ZERO business value but achieves high accuracy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate fraud detection scenario to demonstrate the accuracy paradox\n",
    "np.random.seed(42)\n",
    "n_transactions = 100000\n",
    "fraud_rate = 0.01\n",
    "\n",
    "# True labels: 1% fraud, 99% legitimate\n",
    "y_true = np.random.binomial(1, fraud_rate, n_transactions)\n",
    "\n",
    "# Model A: \"Lazy\" model that always predicts \"legitimate\" \n",
    "y_pred_lazy = np.zeros(n_transactions)  # Always predicts 0 (legitimate)\n",
    "\n",
    "# Model B: \"Smart\" model that catches some fraud but makes some mistakes\n",
    "y_pred_smart = y_true.copy()\n",
    "# Miss 20% of fraud (false negatives)\n",
    "fraud_indices = np.where(y_true == 1)[0]\n",
    "missed_fraud = np.random.choice(fraud_indices, int(0.2 * len(fraud_indices)), replace=False)\n",
    "y_pred_smart[missed_fraud] = 0\n",
    "\n",
    "# Flag 2% of legitimate transactions as fraud (false positives)  \n",
    "legit_indices = np.where(y_true == 0)[0]\n",
    "false_flags = np.random.choice(legit_indices, int(0.02 * len(legit_indices)), replace=False)\n",
    "y_pred_smart[false_flags] = 1\n",
    "\n",
    "# Calculate accuracies\n",
    "accuracy_lazy = accuracy_score(y_true, y_pred_lazy)\n",
    "accuracy_smart = accuracy_score(y_true, y_pred_smart)\n",
    "\n",
    "print(\"Fraud Detection Model Comparison:\")\n",
    "print(f\"Dataset: {n_transactions:,} transactions, {fraud_rate:.1%} fraud rate\")\n",
    "print(f\"\\nModel A (Lazy): {accuracy_lazy:.1%} accuracy\")\n",
    "print(f\"Model B (Smart): {accuracy_smart:.1%} accuracy\")\n",
    "print(f\"\\nWhich model would you choose for your business?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "\n",
    "Create your own accuracy paradox example. Try different fraud rates (0.5%, 2%, 5%) and see how the \"lazy model\" accuracy changes. What happens to the accuracy as fraud becomes rarer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Try different fraud rates and calculate lazy model accuracy\n",
    "fraud_rates = [0.005, 0.02, 0.05]\n",
    "for rate in fraud_rates:\n",
    "    # Create a small dataset to test\n",
    "    y_test = np.random.binomial(1, rate, 10000)\n",
    "    lazy_pred = np.zeros(len(y_test))  # Always predict 0\n",
    "    lazy_acc = accuracy_score(y_test, lazy_pred)\n",
    "    print(f\"Fraud rate: {rate:.1%}, Lazy model accuracy: {lazy_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classification Model (Following Chapter 23)\n",
    "\n",
    "Let's build the same logistic regression model from Chapter 23 to evaluate its performance using proper metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Default dataset from chapter 23 with identical preparation\n",
    "Default_encoded = pd.get_dummies(Default, columns=['student'], drop_first=True)\n",
    "Default_encoded['default_binary'] = (Default_encoded['default'] == 'Yes').astype(int)\n",
    "\n",
    "# Use the same feature matrix and target as chapter 23\n",
    "X = Default_encoded[['balance', 'income', 'student_Yes']]\n",
    "y = Default_encoded['default_binary']\n",
    "\n",
    "# Split the data using the same approach as chapter 23 for consistency\n",
    "X_simple = Default_encoded[['balance']]\n",
    "X_simple_train, X_simple_test, X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_simple, X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the same logistic regression model from chapter 23\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Dataset context (matching chapter 23 results):\")\n",
    "print(f\"Total test examples: {len(y_test):,}\")\n",
    "print(f\"Actual default cases: {y_test.sum():,} ({y_test.mean():.1%})\")\n",
    "print(f\"Actual non-default cases: {len(y_test) - y_test.sum():,} ({1-y_test.mean():.1%})\")\n",
    "print(f\"\\nBasic accuracy: {accuracy_score(y_test, y_pred):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Confusion Matrix: Foundation for Understanding Model Performance\n",
    "\n",
    "The confusion matrix shows exactly how your model makes mistakes, providing the foundation for all other classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix for Default Prediction Model:\")\n",
    "print(cm)\n",
    "\n",
    "# Extract the four core values from our confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "total = tn + fp + fn + tp\n",
    "\n",
    "print(\"\\nConfusion Matrix Components for Default Prediction:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"True Negatives (TN):  {tn:,} - Correctly identified non-default customers\")\n",
    "print(f\"False Positives (FP): {fp:,} - Safe customers incorrectly flagged as high risk\")\n",
    "print(f\"False Negatives (FN): {fn:,} - Risky customers that were missed\")\n",
    "print(f\"True Positives (TP):  {tp:,} - Correctly identified default customers\")\n",
    "print(f\"Total customers:      {total:,}\")\n",
    "\n",
    "# Manually calculate basic accuracy\n",
    "accuracy = (tp + tn) / total\n",
    "print(f\"\\nAccuracy = (TP + TN) / Total = ({tp} + {tn}) / {total} = {accuracy:.3f} or {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the confusion matrix with business context\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Predicted: No Default', 'Predicted: Default'],\n",
    "            yticklabels=['Actual: No Default', 'Actual: Default'])\n",
    "plt.title('Confusion Matrix: Default Prediction Model')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBusiness Impact Analysis:\")\n",
    "print(f\"‚Ä¢ False Positives ({fp} customers): Good customers denied credit ‚Üí Lost revenue, customer churn\")\n",
    "print(f\"‚Ä¢ False Negatives ({fn} customers): Bad customers approved for credit ‚Üí Direct financial losses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "\n",
    "Calculate the business impact of this confusion matrix. Assume each false positive costs $50 (unhappy customer) and each false negative costs $500 (missed default). What's the total cost of model errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "fp_cost = 50   # Cost of incorrectly flagging good customer\n",
    "fn_cost = 500  # Cost of missing default\n",
    "\n",
    "total_fp_cost = fp * fp_cost\n",
    "total_fn_cost = fn * fn_cost\n",
    "total_cost = total_fp_cost + total_fn_cost\n",
    "\n",
    "print(f\"Business costs:\")\n",
    "print(f\"‚Ä¢ False positives: {fp:,} √ó ${fp_cost} = ${total_fp_cost:,}\")\n",
    "print(f\"‚Ä¢ False negatives: {fn:,} √ó ${fn_cost} = ${total_fn_cost:,}\")\n",
    "print(f\"‚Ä¢ Total error cost: ${total_cost:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential Classification Metrics: Precision, Recall, and F1-Score\n",
    "\n",
    "While accuracy treats all errors equally, business decisions require understanding the specific types of errors your model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision manually and verify with sklearn\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "# Verify with sklearn's precision_score function\n",
    "sklearn_precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(\"PRECISION ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Precision = TP / (TP + FP) = {tp} / ({tp} + {fp}) = {precision:.3f} or {precision:.1%}\")\n",
    "print(f\"\\nBusiness Interpretation:\")\n",
    "print(f\"‚Ä¢ When our model flags a customer as 'high default risk', it's correct {precision:.1%} of the time\")\n",
    "print(f\"‚Ä¢ Out of {tp + fp} customers flagged as high risk, {tp} actually defaulted\")\n",
    "print(f\"‚Ä¢ {fp} customers were incorrectly flagged (false alarms)\")\n",
    "\n",
    "print(f\"\\nSklearn verification:\")\n",
    "print(f\"‚Ä¢ Manual calculation: {precision:.3f}\")\n",
    "print(f\"‚Ä¢ sklearn precision_score: {sklearn_precision:.3f}\")\n",
    "print(f\"‚Ä¢ Results match: {'‚úì' if abs(precision - sklearn_precision) < 0.001 else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall manually and verify with sklearn\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "# Verify with sklearn's recall_score function\n",
    "sklearn_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"RECALL ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Recall = TP / (TP + FN) = {tp} / ({tp} + {fn}) = {recall:.3f} or {recall:.1%}\")\n",
    "print(f\"\\nBusiness Interpretation:\")\n",
    "print(f\"‚Ä¢ Our model catches {recall:.1%} of all customers who actually default\")\n",
    "print(f\"‚Ä¢ Out of {tp + fn} customers who actually defaulted, we caught {tp}\")\n",
    "print(f\"‚Ä¢ We missed {fn} customers who defaulted (this could be costly!)\")\n",
    "\n",
    "print(f\"\\nSklearn verification:\")\n",
    "print(f\"‚Ä¢ Manual calculation: {recall:.3f}\")\n",
    "print(f\"‚Ä¢ sklearn recall_score: {sklearn_recall:.3f}\")\n",
    "print(f\"‚Ä¢ Results match: {'‚úì' if abs(recall - sklearn_recall) < 0.001 else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model evaluation\n",
    "print(\"DEFAULT PREDICTION MODEL EVALUATION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Precision: {precision:.1%} - When we flag someone as high risk, we're right {precision:.1%} of the time\")\n",
    "print(f\"Recall: {recall:.1%} - We catch {recall:.1%} of all customers who actually default\")\n",
    "\n",
    "# Business cost implications\n",
    "print(f\"\\nBusiness Impact:\")\n",
    "print(f\"‚Ä¢ High precision ({precision:.1%}) = Few false alarms = Happy customers\")\n",
    "print(f\"‚Ä¢ Low recall ({recall:.1%}) = Miss many defaults = Financial losses\")\n",
    "print(f\"\\nThis suggests our model is conservative - it makes fewer false accusations,\")\n",
    "print(f\"but it misses many customers who will actually default.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the precision-recall trade-off\n",
    "print(\"PRECISION-RECALL TRADE-OFF DEMONSTRATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'Business Impact'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Make predictions at this threshold\n",
    "    y_pred_thresh = (y_pred_proba > threshold).astype(int)\n",
    "    \n",
    "    if y_pred_thresh.sum() > 0:  # Avoid division by zero\n",
    "        prec = precision_score(y_test, y_pred_thresh)\n",
    "        rec = recall_score(y_test, y_pred_thresh)\n",
    "        \n",
    "        # Interpret the business impact\n",
    "        if threshold <= 0.3:\n",
    "            impact = \"Flag many as risky - catch more defaults but annoy customers\"\n",
    "        elif threshold >= 0.7:\n",
    "            impact = \"Flag few as risky - happy customers but miss defaults\"\n",
    "        else:\n",
    "            impact = \"Balanced approach\"\n",
    "            \n",
    "        print(f\"{threshold:<12.1f} {prec:<12.3f} {rec:<12.3f} {impact}\")\n",
    "    else:\n",
    "        print(f\"{threshold:<12.1f} {'N/A':<12} {'0.000':<12} No customers flagged as risky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1-score manually and verify with sklearn\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Compare with sklearn\n",
    "sklearn_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"F1-SCORE CALCULATION\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"F1-Score = 2 √ó (Precision √ó Recall) / (Precision + Recall)\")\n",
    "print(f\"F1-Score = 2 √ó ({precision:.3f} √ó {recall:.3f}) / ({precision:.3f} + {recall:.3f})\")\n",
    "print(f\"F1-Score = {f1:.3f} or {f1:.1%}\")\n",
    "\n",
    "print(f\"\\nSklearn verification: F1-Score = {sklearn_f1:.3f}\")\n",
    "print(f\"Results match: {'‚úì' if abs(f1 - sklearn_f1) < 0.001 else '‚úó'}\")\n",
    "\n",
    "print(f\"\\nBusiness Interpretation:\")\n",
    "print(f\"The F1-score of {f1:.1%} reflects the challenge of predicting rare events.\")\n",
    "print(f\"While our model has good precision (few false alarms), it suffers from\")\n",
    "print(f\"poor recall (missing many actual defaults).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "\n",
    "Find the threshold that maximizes F1-score for our model. What business trade-offs does this represent? Would you recommend this threshold for a conservative bank vs. an aggressive lender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Find optimal threshold for F1-score\n",
    "thresholds_fine = np.arange(0.05, 0.95, 0.05)\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds_fine:\n",
    "    y_pred_thresh = (y_pred_proba > threshold).astype(int)\n",
    "    if y_pred_thresh.sum() > 0:  # Avoid division by zero\n",
    "        f1_thresh = f1_score(y_test, y_pred_thresh)\n",
    "        f1_scores.append((threshold, f1_thresh))\n",
    "\n",
    "# Find best threshold\n",
    "best_threshold, best_f1 = max(f1_scores, key=lambda x: x[1])\n",
    "print(f\"Optimal threshold for F1-score: {best_threshold:.2f}\")\n",
    "print(f\"Best F1-score: {best_f1:.3f}\")\n",
    "\n",
    "# Analyze what this means\n",
    "y_pred_optimal = (y_pred_proba > best_threshold).astype(int)\n",
    "prec_opt = precision_score(y_test, y_pred_optimal)\n",
    "rec_opt = recall_score(y_test, y_pred_optimal)\n",
    "\n",
    "print(f\"\\nAt optimal threshold:\")\n",
    "print(f\"‚Ä¢ Precision: {prec_opt:.1%}\")\n",
    "print(f\"‚Ä¢ Recall: {rec_opt:.1%}\")\n",
    "print(f\"\\nThis represents a more balanced approach between catching defaults and avoiding false alarms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves and AUC: Evaluating Ranking Quality\n",
    "\n",
    "ROC curves evaluate how well your model distinguishes between classes across all possible thresholds - crucial for risk ranking and pricing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve and AUC score\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Our model's AUC: {auc_score:.3f}\")\n",
    "print(f\"\\nSimple interpretation: {auc_score:.1%} chance our model correctly ranks\")\n",
    "print(f\"a defaulting customer as higher risk than a non-defaulting customer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ROC curve visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=3, label=f'Our Model (AUC = {auc_score:.3f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Guessing (AUC = 0.5)', alpha=0.7)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('ROC Curve: How Well Our Model Ranks Customers by Risk')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight the ideal points\n",
    "plt.annotate('Perfect Model\\n(0, 1)', xy=(0, 1), xytext=(0.02, 0.9),\n",
    "            arrowprops=dict(arrowstyle='->', color='green'), fontsize=10)\n",
    "plt.annotate('Our Model\\nPerformance', xy=(0.18, 0.9), xytext=(0.25, 0.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# AUC interpretation guide\n",
    "print(\"\\nAUC Score Interpretation Guide:\")\n",
    "print(\"‚Ä¢ 0.9 - 1.0: Outstanding (Deploy with confidence)\")\n",
    "print(\"‚Ä¢ 0.8 - 0.9: Excellent (Strong business value)\")\n",
    "print(\"‚Ä¢ 0.7 - 0.8: Good (Useful with monitoring)\")\n",
    "print(\"‚Ä¢ 0.6 - 0.7: Fair (Limited value)\")\n",
    "print(\"‚Ä¢ 0.5 - 0.6: Poor (Barely better than random)\")\n",
    "print(f\"\\nOur model AUC of {auc_score:.3f} is {('Excellent' if auc_score >= 0.9 else 'Good' if auc_score >= 0.8 else 'Fair')}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create risk tiers using our model's probability predictions\n",
    "risk_buckets = pd.qcut(y_pred_proba, q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "risk_analysis = pd.DataFrame({\n",
    "    'Risk_Bucket': risk_buckets,\n",
    "    'Actual_Default': y_test\n",
    "}).groupby('Risk_Bucket', observed=True)['Actual_Default'].mean()\n",
    "\n",
    "print(\"Default Rates by Risk Tier:\")\n",
    "for bucket, default_rate in risk_analysis.items():\n",
    "    print(f\"{bucket:>10}: {default_rate:.4%} default rate\")\n",
    "\n",
    "print(f\"\\nOur model creates excellent risk separation!\")\n",
    "print(f\"This AUC of {auc_score:.3f} enables confident risk-based pricing decisions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "\n",
    "Create a risk-based pricing strategy using the model's probability scores. Suggest appropriate interest rates for each risk bucket based on the default rates you calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Create pricing strategy based on risk\n",
    "base_rate = 5.0  # Base interest rate in %\n",
    "\n",
    "print(\"Risk-Based Pricing Strategy:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "for bucket, default_rate in risk_analysis.items():\n",
    "    # Add risk premium based on default rate\n",
    "    risk_premium = default_rate * 100 * 2  # 2x the default rate as premium\n",
    "    interest_rate = base_rate + risk_premium\n",
    "    \n",
    "    print(f\"{bucket:>10}: {interest_rate:.1f}% interest rate ({default_rate:.1%} default risk)\")\n",
    "\n",
    "print(f\"\\nThis pricing strategy reflects actual risk levels while remaining competitive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Metric for Your Business Context\n",
    "\n",
    "The most sophisticated aspect of classification evaluation is aligning your choice of metrics with your specific business context and cost structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business-aligned metric selection framework\n",
    "print(\"BUSINESS-ALIGNED METRIC SELECTION GUIDE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n‚Ä¢ Use PRECISION when: False positives cost more than false negatives\")\n",
    "print(\"  Examples: Credit card fraud detection, Email spam filtering\")\n",
    "print(\"\\n‚Ä¢ Use RECALL when: False negatives cost more than false positives\")\n",
    "print(\"  Examples: Disease screening, Safety system alerts\")\n",
    "print(\"\\n‚Ä¢ Use F1-SCORE when: Both error types matter equally\")\n",
    "print(\"  Examples: Marketing campaigns, Quality control\")\n",
    "print(\"\\n‚Ä¢ Use ROC-AUC when: You need ranking quality across all thresholds\")\n",
    "print(\"  Examples: Insurance pricing, Risk assessment\")\n",
    "print(\"\\n‚Ä¢ Avoid ACCURACY when: You have imbalanced classes\")\n",
    "print(\"  Like our 3% default rate - accuracy can be misleading!\")\n",
    "\n",
    "# Summary of our model's performance\n",
    "print(f\"\\nOUR DEFAULT MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"‚Ä¢ Precision: {precision:.1%} (Good - few false alarms)\")\n",
    "print(f\"‚Ä¢ Recall: {recall:.1%} (Poor - misses many defaults)\")\n",
    "print(f\"‚Ä¢ F1-Score: {f1:.1%} (Low - reflects precision-recall imbalance)\")\n",
    "print(f\"‚Ä¢ AUC: {auc_score:.1%} (Excellent - great for risk ranking)\")\n",
    "\n",
    "print(f\"\\nBUSINESS RECOMMENDATION:\")\n",
    "print(f\"This model is excellent for risk-based pricing (high AUC) but may\")\n",
    "print(f\"need threshold adjustment for binary default decisions (low recall).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Practice Challenges\n",
    "\n",
    "Test your understanding with these additional exercises that combine multiple concepts from the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Custom Business Metric\n",
    "\n",
    "Create a custom evaluation metric that calculates the total business cost for our Default model. Use the costs: False Positive = $50, False Negative = $500. Which threshold minimizes total cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "def calculate_business_cost(y_true, y_pred, fp_cost=50, fn_cost=500):\n",
    "    \"\"\"\n",
    "    Calculate total business cost based on false positives and false negatives\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    total_cost = (fp * fp_cost) + (fn * fn_cost)\n",
    "    return total_cost, fp, fn\n",
    "\n",
    "# Test different thresholds to find minimum cost\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "costs = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba > thresh).astype(int)\n",
    "    cost, fp_count, fn_count = calculate_business_cost(y_test, y_pred_thresh)\n",
    "    costs.append((thresh, cost, fp_count, fn_count))\n",
    "\n",
    "# Find optimal threshold\n",
    "best_threshold, min_cost, best_fp, best_fn = min(costs, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Optimal threshold for minimum business cost: {best_threshold:.2f}\")\n",
    "print(f\"Minimum total cost: ${min_cost:,}\")\n",
    "print(f\"At this threshold: {best_fp} false positives, {best_fn} false negatives\")\n",
    "\n",
    "# Compare with default threshold\n",
    "default_cost, default_fp, default_fn = calculate_business_cost(y_test, y_pred)\n",
    "print(f\"\\nComparison with default threshold (0.5):\")\n",
    "print(f\"Default cost: ${default_cost:,}\")\n",
    "print(f\"Cost savings: ${default_cost - min_cost:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Model Comparison\n",
    "\n",
    "Compare our logistic regression model with a simple rule-based model (predict default if balance > $1500). Which performs better on different metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# Create simple rule-based model\n",
    "balance_threshold = 1500\n",
    "X_test_balance = X_test['balance']\n",
    "y_pred_rule = (X_test_balance > balance_threshold).astype(int)\n",
    "\n",
    "# Calculate metrics for both models\n",
    "models = {\n",
    "    'Logistic Regression': y_pred,\n",
    "    'Rule-Based (Balance > $1500)': y_pred_rule\n",
    "}\n",
    "\n",
    "print(\"MODEL COMPARISON:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Model':<25} {'Precision':<12} {'Recall':<10} {'F1-Score':<10} {'Accuracy':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, predictions in models.items():\n",
    "    prec = precision_score(y_test, predictions)\n",
    "    rec = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(f\"{name:<25} {prec:<12.3f} {rec:<10.3f} {f1:<10.3f} {acc:<10.3f}\")\n",
    "\n",
    "print(f\"\\nBusiness Insights:\")\n",
    "print(f\"‚Ä¢ Logistic regression is more sophisticated and generally performs better\")\n",
    "print(f\"‚Ä¢ Rule-based model is simpler to explain but may miss nuanced patterns\")\n",
    "print(f\"‚Ä¢ Choice depends on business needs: interpretability vs. performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Threshold Strategy for Different Business Goals\n",
    "\n",
    "Design optimal thresholds for three different business strategies: (1) Conservative bank (minimize defaults), (2) Growth-focused bank (maximize approvals), (3) Balanced approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# Define different business strategies\n",
    "strategies = {\n",
    "    'Conservative (Minimize Defaults)': 'recall',  # Catch more defaults\n",
    "    'Growth-Focused (Maximize Approvals)': 'precision',  # Fewer false alarms\n",
    "    'Balanced Approach': 'f1'  # Balance both\n",
    "}\n",
    "\n",
    "print(\"BUSINESS STRATEGY OPTIMIZATION:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for strategy_name, metric in strategies.items():\n",
    "    best_score = 0\n",
    "    best_thresh = 0.5\n",
    "    \n",
    "    for thresh in np.arange(0.1, 0.9, 0.05):\n",
    "        y_pred_thresh = (y_pred_proba > thresh).astype(int)\n",
    "        \n",
    "        if y_pred_thresh.sum() > 0:  # Avoid division by zero\n",
    "            if metric == 'recall':\n",
    "                score = recall_score(y_test, y_pred_thresh)\n",
    "            elif metric == 'precision':\n",
    "                score = precision_score(y_test, y_pred_thresh)\n",
    "            else:  # f1\n",
    "                score = f1_score(y_test, y_pred_thresh)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_thresh = thresh\n",
    "    \n",
    "    # Calculate metrics at optimal threshold\n",
    "    y_pred_optimal = (y_pred_proba > best_thresh).astype(int)\n",
    "    prec = precision_score(y_test, y_pred_optimal)\n",
    "    rec = recall_score(y_test, y_pred_optimal)\n",
    "    f1 = f1_score(y_test, y_pred_optimal)\n",
    "    \n",
    "    print(f\"\\n{strategy_name}:\")\n",
    "    print(f\"  Optimal threshold: {best_thresh:.2f}\")\n",
    "    print(f\"  Precision: {prec:.1%}, Recall: {rec:.1%}, F1: {f1:.1%}\")\n",
    "    \n",
    "    # Business interpretation\n",
    "    approval_rate = (y_pred_proba < best_thresh).mean()\n",
    "    print(f\"  Approval rate: {approval_rate:.1%} of applicants\")\n",
    "\n",
    "print(f\"\\nConclusion: Different business goals require different thresholds!\")\n",
    "print(f\"The 'optimal' threshold depends entirely on your business priorities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Chapter Summary\n",
    "\n",
    "In this notebook, you practiced:\n",
    "\n",
    "- ‚úÖ Understanding why accuracy can be misleading with imbalanced data (3% default rate)\n",
    "- ‚úÖ Constructing and interpreting confusion matrices for business decision-making\n",
    "- ‚úÖ Calculating precision, recall, and F1-score and understanding their business implications\n",
    "- ‚úÖ Using ROC curves and AUC to evaluate model ranking quality for risk-based pricing\n",
    "- ‚úÖ Aligning evaluation metrics with specific business costs and objectives\n",
    "- ‚úÖ Analyzing precision-recall trade-offs and threshold optimization for different business strategies\n",
    "\n",
    "## üîó Connections to Other Chapters\n",
    "\n",
    "- **Previous chapters**: Built on logistic regression from Chapter 23, using the same Default dataset and model\n",
    "- **Upcoming chapters**: These evaluation principles apply to all classification algorithms you'll learn (decision trees, random forests, neural networks, etc.)\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- [Scikit-learn Classification Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n",
    "- [Understanding ROC Curves](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "- [Precision and Recall Trade-offs](https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/)\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "1. **Review the chapter** to reinforce concepts about business-aligned evaluation\n",
    "2. **Complete the end-of-chapter exercises** in the textbook using different business scenarios\n",
    "3. **Practice with your own classification problems** to build intuition\n",
    "4. **Apply these evaluation principles** to future classification algorithms in the course"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}