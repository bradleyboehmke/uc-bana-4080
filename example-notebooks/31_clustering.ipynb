{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 31: Unsupervised Learning and Clustering\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/31_clustering.ipynb)\n",
    "\n",
    "This notebook contains all the executable code examples from Chapter 31 of the BANA 4080 textbook. You can run each code cell and experiment with the examples to deepen your understanding of clustering and unsupervised learning.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By working through this notebook, you will be able to:\n",
    "\n",
    "- Understand the difference between supervised and unsupervised learning\n",
    "- Apply the K-Means clustering algorithm using scikit-learn\n",
    "- Engineer behavioral features from transactional data\n",
    "- Use the elbow method and silhouette scores to select optimal number of clusters\n",
    "- Apply proper feature scaling for clustering\n",
    "- Interpret cluster profiles and translate them into business insights\n",
    "- Explore alternative clustering methods (hierarchical, DBSCAN)\n",
    "- Complete a real-world customer segmentation case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries we'll need throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Introduction to Clustering with Synthetic Data\n",
    "\n",
    "We'll start with a simple 2D example to visualize how K-Means clustering works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Customer Data\n",
    "\n",
    "Let's create three distinct groups of customers based on age and income to see how K-Means discovers these natural groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate three distinct customer groups\n",
    "# Group 1: Young, lower income (students/entry-level)\n",
    "group1_age = np.random.normal(25, 3, 50)\n",
    "group1_income = np.random.normal(35000, 5000, 50)\n",
    "\n",
    "# Group 2: Middle-aged, moderate income (professionals)\n",
    "group2_age = np.random.normal(40, 4, 50)\n",
    "group2_income = np.random.normal(65000, 8000, 50)\n",
    "\n",
    "# Group 3: Older, higher income (executives/established)\n",
    "group3_age = np.random.normal(55, 5, 50)\n",
    "group3_income = np.random.normal(95000, 12000, 50)\n",
    "\n",
    "# Combine into single dataset\n",
    "age = np.concatenate([group1_age, group2_age, group3_age])\n",
    "income = np.concatenate([group1_income, group2_income, group3_income])\n",
    "\n",
    "# Create DataFrame\n",
    "customer_data = pd.DataFrame({\n",
    "    'age': age,\n",
    "    'income': income\n",
    "})\n",
    "\n",
    "print(f\"Created {len(customer_data)} customers\")\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply K-Means Clustering\n",
    "\n",
    "Now let's use K-Means to discover the three customer segments and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit K-Means with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "customer_data['cluster'] = kmeans.fit_predict(customer_data[['age', 'income']])\n",
    "\n",
    "# Get cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Original data (no clusters visible)\n",
    "axes[0].scatter(customer_data['age'], customer_data['income'],\n",
    "                alpha=0.6, s=50, color='gray')\n",
    "axes[0].set_xlabel('Age (years)', fontsize=12)\n",
    "axes[0].set_ylabel('Income ($)', fontsize=12)\n",
    "axes[0].set_title('Before Clustering: Unlabeled Customer Data', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: After clustering with centroids\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "for i in range(3):\n",
    "    cluster_data = customer_data[customer_data['cluster'] == i]\n",
    "    axes[1].scatter(cluster_data['age'], cluster_data['income'],\n",
    "                   alpha=0.6, s=50, color=colors[i], label=f'Cluster {i+1}')\n",
    "\n",
    "# Plot centroids\n",
    "axes[1].scatter(centers[:, 0], centers[:, 1],\n",
    "               marker='X', s=300, c='black', edgecolors='white', linewidths=2,\n",
    "               label='Centroids', zorder=5)\n",
    "\n",
    "axes[1].set_xlabel('Age (years)', fontsize=12)\n",
    "axes[1].set_ylabel('Income ($)', fontsize=12)\n",
    "axes[1].set_title('After K-Means: Discovered Customer Segments', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print cluster summaries\n",
    "print(\"\\nCluster Summaries:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(3):\n",
    "    cluster_data = customer_data[customer_data['cluster'] == i]\n",
    "    print(f\"\\nCluster {i+1}:\")\n",
    "    print(f\"  Size: {len(cluster_data)} customers\")\n",
    "    print(f\"  Average age: {cluster_data['age'].mean():.1f} years\")\n",
    "    print(f\"  Average income: ${cluster_data['income'].mean():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: K-Means Implementation with Scikit-Learn\n",
    "\n",
    "Let's explore the complete K-Means workflow including feature scaling and accessing model results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample Customer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample customer data with multiple features\n",
    "np.random.seed(42)\n",
    "n_customers = 150\n",
    "\n",
    "customer_data_multi = pd.DataFrame({\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'age': np.random.randint(20, 70, n_customers),\n",
    "    'annual_income': np.random.randint(20000, 120000, n_customers),\n",
    "    'purchase_frequency': np.random.randint(1, 50, n_customers)\n",
    "})\n",
    "\n",
    "print(\"Sample customer data:\")\n",
    "customer_data_multi.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete K-Means Workflow with Feature Scaling\n",
    "\n",
    "Feature scaling is **critical** for K-Means because it uses distance-based calculations. Without scaling, features with larger values will dominate the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare your data (features only, no target variable)\n",
    "X = customer_data_multi[['age', 'annual_income', 'purchase_frequency']]\n",
    "\n",
    "# Step 2: Scale your features (IMPORTANT!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Create and fit the K-Means model\n",
    "kmeans = KMeans(\n",
    "    n_clusters=3,        # Number of clusters\n",
    "    random_state=42,     # For reproducibility\n",
    "    n_init=10            # Number of different initializations (default=10)\n",
    ")\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Step 4: Get cluster assignments\n",
    "customer_data_multi['cluster'] = kmeans.predict(X_scaled)\n",
    "\n",
    "print(\"Clustering complete!\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "print(customer_data_multi['cluster'].value_counts().sort_index())\n",
    "print(\"\\nFirst 10 customers with cluster assignments:\")\n",
    "customer_data_multi.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing K-Means Results\n",
    "\n",
    "The K-Means model stores useful information that we can access after fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster centers (centroids)\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(\"Cluster centroids shape:\", centroids.shape)  # (n_clusters, n_features)\n",
    "print(\"\\nCentroid values (scaled):\")\n",
    "print(centroids)\n",
    "\n",
    "# Get cluster labels for training data\n",
    "labels = kmeans.labels_\n",
    "print(\"\\nFirst 10 cluster assignments:\", labels[:10])\n",
    "\n",
    "# Get WCSS (inertia)\n",
    "wcss = kmeans.inertia_\n",
    "print(f\"\\nWithin-Cluster Sum of Squares: {wcss:.2f}\")\n",
    "\n",
    "# Predict cluster for new data\n",
    "new_customer = pd.DataFrame([[35, 60000, 12]], \n",
    "                           columns=['age', 'annual_income', 'purchase_frequency'])\n",
    "new_customer_scaled = scaler.transform(new_customer)\n",
    "predicted_cluster = kmeans.predict(new_customer_scaled)\n",
    "print(f\"\\nNew customer (age=35, income=$60k, frequency=12) assigned to cluster: {predicted_cluster[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Choosing the Optimal Number of Clusters\n",
    "\n",
    "One of the biggest challenges in clustering is determining how many clusters (k) to use. We'll explore two popular methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method\n",
    "\n",
    "The elbow method plots WCSS (Within-Cluster Sum of Squares) for different values of k. We look for the \"elbow\" where the rate of decrease sharply changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WCSS for different values of k\n",
    "wcss = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_temp.fit(customer_data[['age', 'income']])\n",
    "    wcss.append(kmeans_temp.inertia_)  # inertia_ is scikit-learn's name for WCSS\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(k_range, wcss, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)', fontsize=12)\n",
    "plt.title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_range)\n",
    "\n",
    "# Highlight the elbow at k=3\n",
    "plt.axvline(x=3, color='red', linestyle='--', linewidth=2, label='Elbow at k=3')\n",
    "plt.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"WCSS values:\")\n",
    "for k, wcss_val in zip(k_range, wcss):\n",
    "    print(f\"  k={k}: WCSS = {wcss_val:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Analysis\n",
    "\n",
    "Silhouette scores measure how similar each point is to its own cluster compared to other clusters. Scores range from -1 to +1:\n",
    "- **+1**: Point is well-matched to its cluster\n",
    "- **0**: Point is on the border between clusters\n",
    "- **-1**: Point might be assigned to the wrong cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate silhouette scores for k=2 through k=10\n",
    "silhouette_scores = []\n",
    "k_range_sil = range(2, 11)  # Need at least 2 clusters for silhouette\n",
    "\n",
    "for k in k_range_sil:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans_temp.fit_predict(customer_data[['age', 'income']])\n",
    "    silhouette_avg = silhouette_score(customer_data[['age', 'income']], cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(k_range_sil, silhouette_scores, marker='o', linewidth=2, markersize=8, color='green')\n",
    "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Average Silhouette Score', fontsize=12)\n",
    "plt.title('Silhouette Analysis for Optimal k', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_range_sil)\n",
    "\n",
    "# Highlight the maximum\n",
    "max_k = k_range_sil[silhouette_scores.index(max(silhouette_scores))]\n",
    "plt.axvline(x=max_k, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Maximum at k={max_k}')\n",
    "plt.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Silhouette scores:\")\n",
    "for k, score in zip(k_range_sil, silhouette_scores):\n",
    "    print(f\"  k={k}: Silhouette = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Alternative Clustering Methods\n",
    "\n",
    "K-Means works well for spherical, evenly-sized clusters. But what about other data structures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering builds a tree-like structure (dendrogram) showing how observations group together at different levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset of customers for clarity\n",
    "sample_customers = customer_data.sample(30, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "sample_scaled = scaler.fit_transform(sample_customers[['age', 'income']])\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(sample_scaled, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linkage_matrix,\n",
    "           labels=sample_customers.index.tolist(),\n",
    "           leaf_font_size=8)\n",
    "plt.xlabel('Customer Index', fontsize=12)\n",
    "plt.ylabel('Distance (Ward Linkage)', fontsize=12)\n",
    "plt.title('Hierarchical Clustering Dendrogram', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=6, color='red', linestyle='--', linewidth=2, label='Cut at k=3')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Reading the dendrogram:\")\n",
    "print(\"- Each leaf (bottom) represents one customer\")\n",
    "print(\"- Branches merge at heights indicating dissimilarity\")\n",
    "print(\"- Cutting the tree horizontally (red line) gives k=3 clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN: Density-Based Clustering\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) can find clusters of arbitrary shapes and identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate crescent-shaped data where K-Means would struggle\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_moons)\n",
    "\n",
    "# Plot DBSCAN results\n",
    "plt.figure(figsize=(10, 6))\n",
    "unique_labels = set(dbscan_labels)\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#FFA500']\n",
    "\n",
    "for label in unique_labels:\n",
    "    if label == -1:\n",
    "        # Outliers\n",
    "        color = 'yellow'\n",
    "        marker = 'x'\n",
    "        label_text = 'Outliers'\n",
    "    else:\n",
    "        color = colors[label % len(colors)]\n",
    "        marker = 'o'\n",
    "        label_text = f'Cluster {label+1}'\n",
    "    \n",
    "    mask = dbscan_labels == label\n",
    "    plt.scatter(X_moons[mask, 0], X_moons[mask, 1],\n",
    "               c=color, marker=marker, alpha=0.6, s=60,\n",
    "               edgecolors='black', linewidths=0.5,\n",
    "               label=label_text)\n",
    "\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('DBSCAN: Handles Non-Spherical Clusters', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"DBSCAN found {len(set(dbscan_labels) - {-1})} clusters\")\n",
    "print(f\"Number of outliers: {sum(dbscan_labels == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Real-World Case Study - Complete Journey Customer Segmentation\n",
    "\n",
    "Now let's apply everything we've learned to a real grocery store dataset. We'll segment customers based on their shopping behavior and demographics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Complete Journey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package if running in Colab\n",
    "try:\n",
    "    from completejourney_py import get_data\n",
    "except ImportError:\n",
    "    print(\"Installing completejourney_py package...\")\n",
    "    !pip install completejourney-py\n",
    "    from completejourney_py import get_data\n",
    "\n",
    "# Load the Complete Journey datasets\n",
    "print(\"Loading Complete Journey data...\")\n",
    "data = get_data()\n",
    "transactions = data['transactions']\n",
    "demographics = data[\"demographics\"]\n",
    "\n",
    "print(f\"\\nTransactions: {len(transactions):,} rows\")\n",
    "print(f\"Households: {demographics['household_id'].nunique():,} unique households\")\n",
    "\n",
    "print(\"\\nTransaction data sample:\")\n",
    "print(transactions.head())\n",
    "\n",
    "print(\"\\nDemographic data sample:\")\n",
    "print(demographics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering from Transactions\n",
    "\n",
    "We'll transform raw transaction records into behavioral features that describe each customer's shopping patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create behavioral features from transactions\n",
    "\n",
    "# Convert transaction_timestamp to datetime\n",
    "transactions['transaction_timestamp'] = pd.to_datetime(transactions['transaction_timestamp'], format='mixed')\n",
    "\n",
    "# Find the last date in the dataset for recency calculations\n",
    "max_date = transactions['transaction_timestamp'].max()\n",
    "\n",
    "# Aggregate transaction data by household\n",
    "behavioral_features = transactions.groupby('household_id').agg({\n",
    "    # Spending metrics\n",
    "    'sales_value': ['sum', 'mean'],  # Total spending and average transaction value\n",
    "    'basket_id': 'nunique',  # Number of unique shopping trips\n",
    "    'product_id': 'nunique',  # Number of unique products purchased\n",
    "    \n",
    "    # Discount sensitivity\n",
    "    'retail_disc': 'sum',  # Total retail discounts used\n",
    "    'coupon_disc': 'sum',  # Total coupon discounts used\n",
    "    \n",
    "    # Temporal patterns\n",
    "    'transaction_timestamp': ['min', 'max']  # First and last purchase dates\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "behavioral_features.columns = ['household_id', 'total_spending', 'avg_basket_value',\n",
    "                                'num_trips', 'num_unique_products',\n",
    "                                'total_retail_disc', 'total_coupon_disc',\n",
    "                                'first_purchase', 'last_purchase']\n",
    "\n",
    "# Create additional engineered features\n",
    "behavioral_features['days_active'] = (behavioral_features['last_purchase'] - \n",
    "                                      behavioral_features['first_purchase']).dt.days + 1\n",
    "behavioral_features['recency_days'] = (max_date - behavioral_features['last_purchase']).dt.days\n",
    "behavioral_features['avg_days_between_trips'] = (behavioral_features['days_active'] / \n",
    "                                                  behavioral_features['num_trips'])\n",
    "\n",
    "# Calculate discount usage rates\n",
    "behavioral_features['total_discount'] = (behavioral_features['total_retail_disc'] + \n",
    "                                         behavioral_features['total_coupon_disc'])\n",
    "behavioral_features['discount_rate'] = (behavioral_features['total_discount'] / \n",
    "                                        behavioral_features['total_spending'])\n",
    "behavioral_features['coupon_usage_rate'] = (behavioral_features['total_coupon_disc'] / \n",
    "                                            behavioral_features['total_spending'])\n",
    "\n",
    "# Drop temporary date columns\n",
    "behavioral_features = behavioral_features.drop(['first_purchase', 'last_purchase'], axis=1)\n",
    "\n",
    "print(\"Behavioral features created!\")\n",
    "print(f\"\\nFeatures per household: {len(behavioral_features.columns)-1}\")\n",
    "print(\"\\nSample behavioral features:\")\n",
    "behavioral_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with Demographics and Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Merge behavioral features with demographics\n",
    "customer_data = behavioral_features.merge(demographics, on='household_id', how='inner')\n",
    "\n",
    "print(f\"Merged data: {len(customer_data)} households\")\n",
    "\n",
    "# Step 3: Encode demographic features\n",
    "# Map column names (handling different possible column names)\n",
    "col_mapping = {}\n",
    "for col in customer_data.columns:\n",
    "    lower_col = col.lower()\n",
    "    if 'age' in lower_col and 'age_encoded' not in lower_col:\n",
    "        col_mapping['age'] = col\n",
    "    elif 'income' in lower_col and 'income_encoded' not in lower_col:\n",
    "        col_mapping['income'] = col\n",
    "    elif 'household_size' in lower_col or 'hh_size' in lower_col:\n",
    "        col_mapping['household_size'] = col\n",
    "    elif 'marital' in lower_col:\n",
    "        col_mapping['marital_status'] = col\n",
    "    elif 'homeowner' in lower_col or 'home_owner' in lower_col:\n",
    "        col_mapping['homeowner'] = col\n",
    "    elif 'kid' in lower_col or 'child' in lower_col:\n",
    "        col_mapping['kids'] = col\n",
    "\n",
    "# Convert age brackets to ordinal numbers\n",
    "age_map = {\n",
    "    '19-24': 1, '25-34': 2, '35-44': 3, '45-54': 4,\n",
    "    '55-64': 5, '65+': 6\n",
    "}\n",
    "customer_data['age_encoded'] = customer_data[col_mapping.get('age', 'age')].map(age_map)\n",
    "\n",
    "# Convert income brackets to ordinal numbers\n",
    "income_map = {\n",
    "    'Under 15K': 1, '15-24K': 2, '25-34K': 3, '35-49K': 4,\n",
    "    '50-74K': 5, '75-99K': 6, '100-124K': 7, '125-149K': 8,\n",
    "    '150-174K': 9, '175-199K': 10, '200-249K': 11, '250K+': 12\n",
    "}\n",
    "customer_data['income_encoded'] = customer_data[col_mapping.get('income', 'income')].map(income_map)\n",
    "\n",
    "# Extract household size\n",
    "hh_size_col = col_mapping.get('household_size', 'household_size')\n",
    "if customer_data[hh_size_col].dtype == 'object':\n",
    "    customer_data['household_size_num'] = customer_data[hh_size_col].str.extract(r'(\\d+)').astype(float)\n",
    "else:\n",
    "    customer_data['household_size_num'] = customer_data[hh_size_col]\n",
    "\n",
    "# Extract number of kids\n",
    "if 'kids' in col_mapping:\n",
    "    kids_col = col_mapping['kids']\n",
    "    if customer_data[kids_col].dtype == 'object':\n",
    "        customer_data['num_kids'] = customer_data[kids_col].replace('None/Unknown', '0')\n",
    "        customer_data['num_kids'] = customer_data['num_kids'].str.extract(r'(\\d+)').fillna(0).astype(int)\n",
    "    else:\n",
    "        customer_data['num_kids'] = customer_data[kids_col].fillna(0).astype(int)\n",
    "else:\n",
    "    customer_data['num_kids'] = 0\n",
    "\n",
    "# Create binary features\n",
    "marital_col = col_mapping.get('marital_status', 'marital_status')\n",
    "customer_data['is_married'] = (customer_data[marital_col] == 'Married').astype(int)\n",
    "\n",
    "homeowner_col = col_mapping.get('homeowner', 'homeowner')\n",
    "customer_data['is_homeowner'] = (customer_data[homeowner_col] == 'Homeowner').astype(int)\n",
    "\n",
    "# Handle missing values\n",
    "customer_data_clean = customer_data.dropna(subset=['age_encoded', 'income_encoded'])\n",
    "\n",
    "print(f\"\\nCleaned data: {len(customer_data_clean)} households\")\n",
    "print(\"\\nEncoded features sample:\")\n",
    "customer_data_clean[['household_id', 'age_encoded', 'income_encoded', \n",
    "                     'household_size_num', 'num_kids', 'is_married', 'is_homeowner']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Select features for clustering\n",
    "cluster_features = [\n",
    "    # Behavioral features\n",
    "    'total_spending',\n",
    "    'avg_basket_value',\n",
    "    'num_trips',\n",
    "    'num_unique_products',\n",
    "    'avg_days_between_trips',\n",
    "    'recency_days',\n",
    "    'discount_rate',\n",
    "    'coupon_usage_rate',\n",
    "    \n",
    "    # Demographic features\n",
    "    'age_encoded',\n",
    "    'income_encoded',\n",
    "    'household_size_num',\n",
    "    'num_kids',\n",
    "    'is_married',\n",
    "    'is_homeowner'\n",
    "]\n",
    "\n",
    "X_cluster = customer_data_clean[cluster_features]\n",
    "\n",
    "print(f\"Clustering features: {len(cluster_features)}\")\n",
    "print(\"\\nFeature names:\")\n",
    "for i, feature in enumerate(cluster_features, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "print(\"\\nData shape:\", X_cluster.shape)\n",
    "X_cluster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Optimal Number of Clusters\n",
    "\n",
    "Let's use both the elbow method and silhouette analysis to choose k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features first (CRITICAL!)\n",
    "scaler = StandardScaler()\n",
    "X_cluster_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "print(\"Testing different values of k...\")\n",
    "\n",
    "# Elbow method\n",
    "wcss_values = []\n",
    "k_range = range(2, 21)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "    kmeans_temp.fit(X_cluster_scaled)\n",
    "    wcss_values.append(kmeans_temp.inertia_)\n",
    "\n",
    "# Silhouette scores\n",
    "sil_scores = []\n",
    "for k in k_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "    labels_temp = kmeans_temp.fit_predict(X_cluster_scaled)\n",
    "    sil_score = silhouette_score(X_cluster_scaled, labels_temp)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "# Plot both methods\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(k_range, wcss_values, marker='o', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[0].set_ylabel('WCSS', fontsize=12)\n",
    "axes[0].set_title('Elbow Method', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(k_range)\n",
    "\n",
    "# Silhouette plot\n",
    "axes[1].plot(k_range, sil_scores, marker='o', linewidth=2,\n",
    "             markersize=8, color='green')\n",
    "axes[1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Analysis', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks(k_range)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResults for different k values:\")\n",
    "print(\"k  | WCSS       | Silhouette\")\n",
    "print(\"-\" * 35)\n",
    "for k, wcss_val, sil_val in zip(k_range, wcss_values, sil_scores):\n",
    "    print(f\"{k:2d} | {wcss_val:10,.0f} | {sil_val:6.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Final K-Means Model\n",
    "\n",
    "Based on the elbow and silhouette analysis, let's choose k=4 for our segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit final K-Means model with k=4\n",
    "optimal_k = 4\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)\n",
    "customer_data_clean['cluster'] = kmeans_final.fit_predict(X_cluster_scaled)\n",
    "\n",
    "print(f\"K-Means clustering complete with k={optimal_k}\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "cluster_counts = customer_data_clean['cluster'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    pct = (count / len(customer_data_clean)) * 100\n",
    "    print(f\"  Cluster {cluster_id}: {count:4d} households ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Cluster Profiles\n",
    "\n",
    "Now for the most important part: understanding what each cluster represents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster profiles using original (unscaled) features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLUSTER PROFILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Behavioral characteristics by cluster\n",
    "behavioral_profiles = customer_data_clean.groupby('cluster').agg({\n",
    "    'total_spending': 'mean',\n",
    "    'avg_basket_value': 'mean',\n",
    "    'num_trips': 'mean',\n",
    "    'num_unique_products': 'mean',\n",
    "    'avg_days_between_trips': 'mean',\n",
    "    'recency_days': 'mean',\n",
    "    'discount_rate': 'mean',\n",
    "    'coupon_usage_rate': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "behavioral_profiles['count'] = customer_data_clean['cluster'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nBehavioral Characteristics:\")\n",
    "print(behavioral_profiles)\n",
    "\n",
    "# Demographic characteristics by cluster\n",
    "demo_agg_dict = {\n",
    "    'household_size_num': 'mean',\n",
    "    'num_kids': 'mean',\n",
    "    'is_married': lambda x: f\"{x.mean():.1%}\",\n",
    "    'is_homeowner': lambda x: f\"{x.mean():.1%}\"\n",
    "}\n",
    "\n",
    "age_col = col_mapping.get('age', None)\n",
    "income_col = col_mapping.get('income', None)\n",
    "\n",
    "if age_col and age_col in customer_data_clean.columns:\n",
    "    demo_agg_dict[age_col] = lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Mixed'\n",
    "if income_col and income_col in customer_data_clean.columns:\n",
    "    demo_agg_dict[income_col] = lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Mixed'\n",
    "\n",
    "demographic_profiles = customer_data_clean.groupby('cluster').agg(demo_agg_dict).round(1)\n",
    "\n",
    "print(\"\\nDemographic Characteristics:\")\n",
    "print(demographic_profiles)\n",
    "\n",
    "# Add more detailed analysis for each cluster\n",
    "print(\"\\n\\nDetailed Segment Descriptions:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = customer_data_clean[customer_data_clean['cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} (n={len(cluster_data)}):\")\n",
    "    print(f\"  Total spending: ${cluster_data['total_spending'].mean():,.0f}\")\n",
    "    print(f\"  Avg basket value: ${cluster_data['avg_basket_value'].mean():.2f}\")\n",
    "    print(f\"  Shopping trips: {cluster_data['num_trips'].mean():.0f}\")\n",
    "    print(f\"  Discount rate: {cluster_data['discount_rate'].mean():.1%}\")\n",
    "    print(f\"  Coupon usage: {cluster_data['coupon_usage_rate'].mean():.1%}\")\n",
    "    \n",
    "    if age_col and age_col in cluster_data.columns:\n",
    "        print(f\"  Dominant age: {cluster_data[age_col].mode()[0] if len(cluster_data[age_col].mode()) > 0 else 'Mixed'}\")\n",
    "    if income_col and income_col in cluster_data.columns:\n",
    "        print(f\"  Dominant income: {cluster_data[income_col].mode()[0] if len(cluster_data[income_col].mode()) > 0 else 'Mixed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Customer Segments\n",
    "\n",
    "Let's create visualizations to help interpret the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D visualization showing behavioral patterns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Total Spending vs Number of Trips\n",
    "colors_segments = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "for i in range(optimal_k):\n",
    "    cluster_data = customer_data_clean[customer_data_clean['cluster'] == i]\n",
    "    axes[0].scatter(cluster_data['num_trips'], cluster_data['total_spending'],\n",
    "                   alpha=0.6, s=50, color=colors_segments[i],\n",
    "                   label=f'Cluster {i}', edgecolors='black', linewidths=0.3)\n",
    "\n",
    "axes[0].set_xlabel('Number of Shopping Trips', fontsize=12)\n",
    "axes[0].set_ylabel('Total Spending ($)', fontsize=12)\n",
    "axes[0].set_title('Customer Segments: Spending vs. Frequency', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='best', fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Discount Rate vs Coupon Usage\n",
    "for i in range(optimal_k):\n",
    "    cluster_data = customer_data_clean[customer_data_clean['cluster'] == i]\n",
    "    axes[1].scatter(cluster_data['discount_rate'], cluster_data['coupon_usage_rate'],\n",
    "                   alpha=0.6, s=50, color=colors_segments[i],\n",
    "                   label=f'Cluster {i}', edgecolors='black', linewidths=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Discount Rate', fontsize=12)\n",
    "axes[1].set_ylabel('Coupon Usage Rate', fontsize=12)\n",
    "axes[1].set_title('Customer Segments: Discount Sensitivity', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='best', fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "Congratulations! You've completed the clustering chapter examples. Here's what you learned:\n",
    "\n",
    "### Core Concepts\n",
    "1. **Unsupervised Learning**: Discovering patterns without labeled outcomes\n",
    "2. **K-Means Algorithm**: Iteratively assigns points to nearest centroids\n",
    "3. **Distance Metrics**: How to measure similarity between observations\n",
    "\n",
    "### Practical Skills\n",
    "1. **Feature Scaling**: Critical for distance-based algorithms\n",
    "2. **Choosing k**: Elbow method and silhouette analysis\n",
    "3. **Feature Engineering**: Creating behavioral features from transactions\n",
    "4. **Cluster Interpretation**: Translating statistics into business insights\n",
    "\n",
    "### Alternative Methods\n",
    "1. **Hierarchical Clustering**: When you want to see nested groupings\n",
    "2. **DBSCAN**: When clusters have irregular shapes or contain outliers\n",
    "\n",
    "### Real-World Application\n",
    "1. **Complete Journey Case Study**: End-to-end customer segmentation\n",
    "2. **Profile Analysis**: Understanding what makes each segment unique\n",
    "3. **Visualization**: Communicating findings effectively\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you've mastered clustering, try:\n",
    "1. Experimenting with different values of k\n",
    "2. Adding or removing features to see how clusters change\n",
    "3. Trying hierarchical clustering or DBSCAN on the Complete Journey data\n",
    "4. Applying these techniques to your own datasets\n",
    "\n",
    "Happy clustering!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
