{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Cross-validation: Reliable Model Evaluation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/28_cross_validation.ipynb)\n",
    "\n",
    "This companion notebook provides hands-on exercises for the **Cross-validation** chapter. You'll learn why repeatedly evaluating on the test set leads to contamination, how k-fold cross-validation solves this problem, and how to implement the proper 5-stage workflow for honest model evaluation.\n",
    "\n",
    "**What you'll practice**\n",
    "- Understand test set contamination through examples\n",
    "- Visualize how k-fold cross-validation works\n",
    "- Implement cross-validation using `cross_val_score()` and `cross_validate()`\n",
    "- Compare multiple models using CV instead of test set\n",
    "- Apply the proper 5-stage workflow: split, CV comparison, select, train, evaluate\n",
    "- Choose appropriate k values and scoring metrics\n",
    "\n",
    "**How to use**\n",
    "- Run from top to bottom. When you see **üèÉ‚Äç‚ôÇÔ∏è Try It Yourself**, add your code beneath the prompt.\n",
    "- In Colab: `Runtime ‚Üí Restart and run all` to test from a clean environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "Install and import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Colab/a fresh env, uncomment to install\n",
    "# !pip -q install scikit-learn pandas numpy matplotlib ISLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6g7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ISLP import load_data\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "## 1) The Peeking Problem: Demonstrating Test Set Contamination\n",
    "\n",
    "Let's see exactly what happens when you repeatedly evaluate models on the test set to select the \"best\" one. This simulates a realistic scenario where you're trying different hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6g7h8i9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic customer churn dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    n_redundant=2,\n",
    "    n_classes=2,\n",
    "    weights=[0.8, 0.2],  # 20% churn rate\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g7h8i9j0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE WRONG WAY: Peeking at test set 20 times to select hyperparameters\n",
    "best_test_score = 0\n",
    "best_config = None\n",
    "peek_count = 0\n",
    "\n",
    "for n_estimators in [50, 100, 200, 300]:\n",
    "    for max_depth in [5, 10, 15, 20, None]:\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42\n",
    "        )\n",
    "        rf.fit(X_train, y_train)\n",
    "        test_score = rf.score(X_test, y_test)  # ‚ö†Ô∏è Peeking!\n",
    "        peek_count += 1\n",
    "        \n",
    "        if test_score > best_test_score:\n",
    "            best_test_score = test_score\n",
    "            best_config = (n_estimators, max_depth)\n",
    "\n",
    "print(f\"‚ö†Ô∏è WARNING: Peeked at test set {peek_count} times!\")\n",
    "print(f\"Best config found: n_estimators={best_config[0]}, max_depth={best_config[1]}\")\n",
    "print(f\"Test accuracy: {best_test_score:.3f}\")\n",
    "print(\"\\n‚ùå This test score is CONTAMINATED - we can't trust it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h8i9j0k1",
   "metadata": {},
   "source": [
    "**What went wrong?**\n",
    "\n",
    "We selected the model specifically because it performed best on this test set. Some of that performance is genuine, but some is just luck‚Äîthe model happened to work well with these specific test observations. When deployed to production, performance will likely drop.\n",
    "\n",
    "This is **test set contamination** in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "- Create a new random train/test split (use `random_state=99`) and re-run the same hyperparameter search\n",
    "- Does the same configuration win? \n",
    "- What does this tell you about the stability of selecting models based on test set performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0k1l2m3",
   "metadata": {},
   "source": [
    "## 2) The Solution: Cross-Validation\n",
    "\n",
    "Cross-validation solves the contamination problem by creating validation estimates entirely within the training set. Let's see how k-fold CV works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4",
   "metadata": {},
   "source": [
    "### Visualizing 5-Fold Cross-Validation\n",
    "\n",
    "In 5-fold CV:\n",
    "- Training set is split into 5 equal folds\n",
    "- Each fold takes a turn being the validation set\n",
    "- Model is trained on the other 4 folds\n",
    "- We get 5 performance scores, then average them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l2m3n4o5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Visualize 5-fold CV\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "train_color = '#4CAF50'  # Green\n",
    "val_color = '#FFC107'     # Amber\n",
    "\n",
    "k = 5\n",
    "fold_labels = ['Fold 1', 'Fold 2', 'Fold 3', 'Fold 4', 'Fold 5']\n",
    "\n",
    "# Create visualization for each iteration\n",
    "for iteration in range(k):\n",
    "    y_pos = k - iteration - 1\n",
    "    \n",
    "    for fold in range(k):\n",
    "        if fold == iteration:\n",
    "            color = val_color\n",
    "            label = 'Validate'\n",
    "        else:\n",
    "            color = train_color\n",
    "            label = 'Train'\n",
    "        \n",
    "        rect = mpatches.Rectangle((fold, y_pos), 1, 0.8,\n",
    "                                   facecolor=color,\n",
    "                                   edgecolor='black',\n",
    "                                   linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        ax.text(fold + 0.5, y_pos + 0.4, label,\n",
    "                ha='center', va='center',\n",
    "                fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Add iteration label\n",
    "    ax.text(-0.5, y_pos + 0.4, f'Iteration {iteration + 1}',\n",
    "            ha='right', va='center', fontsize=11, fontweight='bold')\n",
    "    ax.text(5.5, y_pos + 0.4, f'‚Üí Score {iteration + 1}',\n",
    "            ha='left', va='center', fontsize=10, style='italic')\n",
    "\n",
    "# Add fold labels at top\n",
    "for fold in range(k):\n",
    "    ax.text(fold + 0.5, k + 0.2, fold_labels[fold],\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add final averaging step\n",
    "ax.text(2.5, -0.8, 'Final CV Score = Average of 5 scores',\n",
    "        ha='center', va='top', fontsize=12, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', \n",
    "                  edgecolor='black', linewidth=2))\n",
    "\n",
    "ax.set_xlim(-1, 6.5)\n",
    "ax.set_ylim(-1.2, k + 0.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.title('5-Fold Cross-Validation: Each Fold Takes a Turn as Validation Set',\n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3n4o5p6",
   "metadata": {},
   "source": [
    "**Key insight**: Every observation in the training set is used for:\n",
    "- **Training** in 4 out of 5 iterations (80%)\n",
    "- **Validation** in 1 out of 5 iterations (20%)\n",
    "\n",
    "This maximizes data efficiency while providing honest performance estimates!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n4o5p6q7",
   "metadata": {},
   "source": [
    "## 3) Implementing Cross-Validation with scikit-learn\n",
    "\n",
    "Let's use the Default dataset to practice implementing CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o5p6q7r8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Default dataset\n",
    "Default = load_data('Default')\n",
    "\n",
    "# Prepare features and target\n",
    "X = pd.get_dummies(Default[['balance', 'income', 'student']], drop_first=True)\n",
    "y = (Default['default'] == 'Yes').astype(int)\n",
    "\n",
    "# Create train/test split (lock away test set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} observations\")\n",
    "print(f\"Test set: {len(X_test)} observations (üîí locked away)\")\n",
    "print(f\"Default rate: {y_train.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q7r8s9",
   "metadata": {},
   "source": [
    "### Basic Cross-Validation with `cross_val_score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7r8s9t0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation on TRAINING SET ONLY\n",
    "cv_scores = cross_val_score(\n",
    "    estimator=model,\n",
    "    X=X_train,             # Training features only!\n",
    "    y=y_train,             # Training labels only!\n",
    "    cv=5,                  # Number of folds\n",
    "    scoring='accuracy'     # Metric to compute\n",
    ")\n",
    "\n",
    "print(f\"Individual fold scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.3f}\")\n",
    "print(f\"Std deviation: {cv_scores.std():.3f}\")\n",
    "print(f\"\\n‚úì Test set was never touched!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r8s9t0u1",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- Mean: Our best estimate of model performance on new data\n",
    "- Std dev: How variable performance is across different data splits\n",
    "- Low std dev = stable, consistent model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s9t0u1v2",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "- Try different k values: `cv=3` and `cv=10`\n",
    "- How does the mean CV score change?\n",
    "- How does the standard deviation change?\n",
    "- Which k value gives the most stable estimates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t0u1v2w3",
   "metadata": {},
   "source": [
    "### Using Different Scoring Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different metrics for classification\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "print(\"Cross-Validation with Different Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for metric in metrics:\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=metric)\n",
    "    print(f\"{metric:12s}: {scores.mean():.3f} (¬±{scores.std():.3f})\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2w3x4y5",
   "metadata": {},
   "source": [
    "### Advanced: Multiple Metrics with `cross_validate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w3x4y5z6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate multiple metrics simultaneously\n",
    "cv_results = cross_validate(\n",
    "    model, X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],\n",
    "    return_train_score=True  # Also get training scores\n",
    ")\n",
    "\n",
    "print(\"Detailed Cross-Validation Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"CV Accuracy:  {cv_results['test_accuracy'].mean():.3f}\")\n",
    "print(f\"CV Precision: {cv_results['test_precision'].mean():.3f}\")\n",
    "print(f\"CV Recall:    {cv_results['test_recall'].mean():.3f}\")\n",
    "print(f\"CV F1:        {cv_results['test_f1'].mean():.3f}\")\n",
    "print(f\"CV ROC AUC:   {cv_results['test_roc_auc'].mean():.3f}\")\n",
    "print(\"\\n--- Checking for overfitting ---\")\n",
    "print(f\"Train Accuracy: {cv_results['train_accuracy'].mean():.3f}\")\n",
    "print(f\"CV Accuracy:    {cv_results['test_accuracy'].mean():.3f}\")\n",
    "print(f\"Gap:            {cv_results['train_accuracy'].mean() - cv_results['test_accuracy'].mean():.3f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x4y5z6a7",
   "metadata": {},
   "source": [
    "**Detecting overfitting**: If train scores are much higher than CV scores, your model is overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y5z6a7b8",
   "metadata": {},
   "source": [
    "## 4) Comparing Models: The RIGHT Way\n",
    "\n",
    "Now let's compare multiple models using CV instead of peeking at the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data with non-linear patterns\n",
    "X_comp, y_comp = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=3,\n",
    "    flip_y=0.1,\n",
    "    class_sep=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add polynomial interactions\n",
    "X_poly = np.column_stack([\n",
    "    X_comp,\n",
    "    X_comp[:, 0] * X_comp[:, 1],\n",
    "    X_comp[:, 2] ** 2,\n",
    "    X_comp[:, 3] * X_comp[:, 4]\n",
    "])\n",
    "\n",
    "# Train/test split (lock away test set)\n",
    "X_train_comp, X_test_comp, y_train_comp, y_test_comp = train_test_split(\n",
    "    X_poly, y_comp, test_size=0.2, random_state=42, stratify=y_comp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train_comp)} observations\")\n",
    "print(f\"Test set: {len(X_test_comp)} observations (üîí locked away)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree (depth=5)': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Decision Tree (depth=10)': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'Random Forest (n=50)': RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    'Random Forest (n=100)': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Random Forest (n=200)': RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate each with 5-fold CV (NEVER touching test set)\n",
    "print(\"Model Comparison Using Cross-Validation:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<30s} {'Mean ROC AUC':<15s} {'Std Dev'}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cv_results_all = {}\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_comp, y_train_comp,\n",
    "        cv=5,\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    \n",
    "    cv_results_all[name] = {\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std(),\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:<30s} {cv_scores.mean():.4f}          (¬±{cv_scores.std():.4f})\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úì All comparisons done WITHOUT touching test set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "- Add a Random Forest with `max_depth=5` to the comparison\n",
    "- Does limiting depth help or hurt performance?\n",
    "- Which model would you select based on CV scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## 5) The Complete Proper Workflow (5 Stages)\n",
    "\n",
    "Let's put everything together: the professional workflow you'll use for every ML project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2g3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset for demonstration\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(\"Starting Complete 5-Stage Workflow...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2g3h4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STAGE 1: Initial Setup ==========\n",
    "print(\"STAGE 1: Initial Setup - Split data and lock away test set\")\n",
    "print(\"=\" * 60)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(\"üîí Test set locked away\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2g3h4i5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STAGE 2: Model Development ==========\n",
    "print(\"STAGE 2: Model Development - Compare models using CV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "models_workflow = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=10000),\n",
    "    'Decision Tree (depth=5)': DecisionTreeClassifier(max_depth=5),\n",
    "    'Decision Tree (depth=10)': DecisionTreeClassifier(max_depth=10),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "cv_results_workflow = {}\n",
    "for name, model in models_workflow.items():\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    cv_results_workflow[name] = {\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std(),\n",
    "        'model': model\n",
    "    }\n",
    "    print(f\"{name:30s} {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g3h4i5j6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STAGE 3: Select Best ==========\n",
    "print(\"STAGE 3: Select Best - Choose model with highest CV score\")\n",
    "print(\"=\" * 60)\n",
    "best_name = max(cv_results_workflow, key=lambda k: cv_results_workflow[k]['mean'])\n",
    "best_model = cv_results_workflow[best_name]['model']\n",
    "best_cv_score = cv_results_workflow[best_name]['mean']\n",
    "print(f\"Selected model: {best_name}\")\n",
    "print(f\"CV score: {best_cv_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h4i5j6k7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STAGE 4: Train Final Model ==========\n",
    "print(\"STAGE 4: Train Final Model - Retrain on all training data\")\n",
    "print(\"=\" * 60)\n",
    "best_model.fit(X_train, y_train)\n",
    "print(f\"Trained {best_name} on {len(X_train)} training samples\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i5j6k7l8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STAGE 5: Final Evaluation ==========\n",
    "print(\"STAGE 5: Final Evaluation - Test set evaluation (ONLY ONCE)\")\n",
    "print(\"=\" * 60)\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(f\"Cross-validation score (training): {best_cv_score:.4f}\")\n",
    "print(f\"Test score (held-out data):        {test_score:.4f}\")\n",
    "print(f\"\\nDifference: {abs(test_score - best_cv_score):.4f}\")\n",
    "\n",
    "if abs(test_score - best_cv_score) < 0.02:\n",
    "    print(\"‚úì CV and test scores are close - good sign!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Larger gap between CV and test - check for issues\")\n",
    "\n",
    "print(\"\\nüîì Test set has now been used - project complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j6k7l8m9",
   "metadata": {},
   "source": [
    "### Key Takeaways from the 5-Stage Workflow:\n",
    "\n",
    "1. **Test set touches = 1** (only in Stage 5)\n",
    "2. All model comparison happens in Stage 2 using CV\n",
    "3. Selection (Stage 3) is based on CV scores, NOT test scores\n",
    "4. Final model (Stage 4) uses all training data\n",
    "5. Test evaluation (Stage 5) gives honest production estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o1p2q3r4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Summary: The Proper Cross-Validation Workflow\n",
    "\n",
    "**The Golden Rule**: Test set touches = EXACTLY ONE\n",
    "\n",
    "**The 5-Stage Workflow:**\n",
    "1. Split data ‚Üí Lock away test set\n",
    "2. Compare models ‚Üí Use CV on training set ONLY\n",
    "3. Select best ‚Üí Based on CV scores\n",
    "4. Train final model ‚Üí On all training data\n",
    "5. Evaluate once ‚Üí On test set\n",
    "\n",
    "**Key Functions:**\n",
    "- `cross_val_score()` - Single metric evaluation\n",
    "- `cross_validate()` - Multiple metrics + training scores\n",
    "\n",
    "**Best Practices:**\n",
    "- Use k=5 as default (k=10 for small datasets)\n",
    "- Use stratified CV for classification (automatic in scikit-learn)\n",
    "- Choose scoring metric appropriate for your problem\n",
    "- Check train vs CV scores to detect overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2q3r4s5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ End-of-Chapter Exercises\n",
    "\n",
    "These exercises ask you to revisit your previous work using the proper cross-validation workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3r4s5t6",
   "metadata": {},
   "source": [
    "### Exercise 1: Fixing the Baseball Salary Predictions\n",
    "\n",
    "Revisit your Chapter 26 Exercise 1 work on the Hitters dataset using proper cross-validation.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "**Part A: Identify the problem**\n",
    "1. How many times did you evaluate on the test set in your original work?\n",
    "2. Why is this problematic?\n",
    "\n",
    "**Part B: Implement proper workflow**\n",
    "1. Load Hitters data and create train/test split\n",
    "2. Compare 4+ model configurations using 5-fold CV on training set:\n",
    "   - Decision tree with different depths\n",
    "   - Random Forest with different parameters\n",
    "3. Select best model based on CV scores\n",
    "4. Train final model on full training set\n",
    "5. Evaluate on test set ONCE\n",
    "\n",
    "**Part C: Analysis**\n",
    "1. Is the CV score close to the test score?\n",
    "2. Which estimate would you trust for production?\n",
    "3. Try k=3 and k=10. Does the same model win?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r4s5t6u7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code for Exercise 1 here\n",
    "Hitters = load_data('Hitters')\n",
    "# ... continue implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5t6u7v8",
   "metadata": {},
   "source": [
    "### Exercise 2: Proper Default Risk Assessment\n",
    "\n",
    "Apply the complete 5-stage workflow to the Default dataset.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "**Part A: Clean slate with proper workflow**\n",
    "1. Load Default dataset\n",
    "2. Create train/test split (80/20, stratified)\n",
    "3. Define 5 different models to compare\n",
    "4. Use 5-fold stratified CV to evaluate all models\n",
    "5. Report accuracy, precision, recall, F1, and ROC AUC\n",
    "\n",
    "**Part B: Selection and evaluation**\n",
    "1. Select best model (consider: which metric matters most for credit default?)\n",
    "2. Train final model on full training set\n",
    "3. Evaluate on test set once\n",
    "4. Create confusion matrix\n",
    "5. Is test performance consistent with CV?\n",
    "\n",
    "**Part C: Business communication**\n",
    "Write a memo explaining:\n",
    "- Which model you selected and why\n",
    "- Expected production accuracy (and why it's trustworthy)\n",
    "- Why this approach is better than repeatedly using test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t6u7v8w9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code for Exercise 2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u7v8w9x0",
   "metadata": {},
   "source": [
    "### Exercise 3: The Ames Housing Challenge\n",
    "\n",
    "Apply the complete workflow to the Ames housing dataset (regression).\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "**Part A: Setup**\n",
    "1. Load Ames data from URL: `https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/main/data/ames_clean.csv`\n",
    "2. Select 8+ features\n",
    "3. Handle missing values\n",
    "4. Create train/test split (80/20)\n",
    "\n",
    "**Part B: Model comparison**\n",
    "1. Compare 5+ approaches (linear regression, decision trees, random forests with different parameters)\n",
    "2. Use 5-fold CV with R¬≤ and RMSE metrics\n",
    "3. Create visualization comparing models\n",
    "\n",
    "**Part C: Final evaluation**\n",
    "1. Select best model based on CV\n",
    "2. Train on full training set\n",
    "3. Evaluate on test set ONCE\n",
    "4. Compare CV estimate to test performance\n",
    "5. Create predicted vs actual scatter plot\n",
    "\n",
    "**Part D: Critical analysis**\n",
    "1. Try the WRONG way: select based on test set performance\n",
    "2. Compare test scores from wrong vs right approach\n",
    "3. Which would you trust for production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v8w9x0y1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code for Exercise 3 here\n",
    "# ames_url = \"https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/main/data/ames_clean.csv\"\n",
    "# ames = pd.read_csv(ames_url)\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
