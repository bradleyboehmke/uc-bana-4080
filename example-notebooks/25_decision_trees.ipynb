{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3241ae",
   "metadata": {},
   "source": [
    "# Decision Trees: Foundations and Interpretability\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/25_decision_trees.ipynb)\n",
    "\n",
    "This notebook contains hands-on code examples and exercises from the **Decision Trees: Foundations and Interpretability** chapter. Use it to follow along, experiment with variations, and complete the practice activities.\n",
    "\n",
    "**What you'll practice:** building and interpreting CART decision trees for both classification and regression, controlling tree complexity, and translating tree paths into business rules.\n",
    "\n",
    "**Learning Objectives**\n",
    "- Explain how decision trees split data using Gini impurity (classification) and SSE (regression)\n",
    "- Build classification and regression trees with scikit-learn\n",
    "- Control complexity with `max_depth`, `min_samples_split`, and `min_samples_leaf`\n",
    "- Interpret tree structure and extract business rules from paths\n",
    "- Evaluate train/test performance and identify overfitting\n",
    "\n",
    "**How to use this notebook**\n",
    "- Run cells from top to bottom. When you see **üèÉ‚Äç‚ôÇÔ∏è Try It Yourself**, write your code below the prompt.\n",
    "- If you‚Äôre in Colab, use `Runtime ‚Üí Restart & run all` to test from a clean environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7a71e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install and import required libraries. (Skip the install cells locally if you already have these packages.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb636b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab or a fresh environment, uncomment to install\n",
    "# !pip -q install scikit-learn pandas numpy matplotlib ISLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5feb30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    "                             mean_absolute_error, r2_score, mean_squared_error)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7bc12",
   "metadata": {},
   "source": [
    "## 1) Why Decision Trees?\n",
    "\n",
    "Decision trees automatically discover non-linear patterns and context-dependent rules by asking a series of yes/no questions. Below, we start with a tiny single-variable example to see how a tree creates thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee964d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Single-variable example: Income ‚Üí Loan approval (simulated) ---\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "income = np.random.uniform(30_000, 120_000, n)\n",
    "approval_prob = np.where(income < 50_000, 0.2,\n",
    "                         np.where(income < 80_000, 0.7, 0.9))\n",
    "approval_prob += np.random.normal(0, 0.1, n)\n",
    "approved = (approval_prob > 0.5).astype(int)\n",
    "\n",
    "df_simple = pd.DataFrame({'income': income, 'approved': approved})\n",
    "X = df_simple[['income']]\n",
    "y = df_simple['approved']\n",
    "\n",
    "clf_simple = DecisionTreeClassifier(max_depth=2, min_samples_split=20, random_state=42)\n",
    "clf_simple.fit(X, y)\n",
    "\n",
    "print(f\"Single-variable tree accuracy: {clf_simple.score(X, y):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "tree.plot_tree(clf_simple, feature_names=['Income'], class_names=['Denied', 'Approved'],\n",
    "               filled=True, rounded=True, fontsize=10)\n",
    "plt.title(\"Simple Decision Tree: Approval by Income Only\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6852343b",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "- Change `max_depth` and `min_samples_split` above and re-run. How do thresholds and colors (purity) change?\n",
    "- Add noise: try `approval_prob += np.random.normal(0, 0.2, n)` and observe the impact on splits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264d9e2",
   "metadata": {},
   "source": [
    "## 2) Interactions Emerge Naturally (Two Variables)\n",
    "\n",
    "Trees pick the split (feature + threshold) that best reduces impurity at each step, automatically capturing interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e82d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Two-variable example: Income + Credit Score ---\n",
    "np.random.seed(123)\n",
    "n = 300\n",
    "income = np.random.uniform(30_000, 120_000, n)\n",
    "credit = np.random.uniform(500, 800, n)\n",
    "\n",
    "approval_prob = (0.3 * (income > 60_000) +\n",
    "                 0.4 * (credit > 650) +\n",
    "                 0.2 * ((income > 60_000) & (credit > 650)) +\n",
    "                 np.random.normal(0, 0.1, n))\n",
    "approved = (approval_prob > 0.5).astype(int)\n",
    "\n",
    "df_two = pd.DataFrame({'income': income, 'credit_score': credit, 'approved': approved})\n",
    "X2 = df_two[['income', 'credit_score']]\n",
    "y2 = df_two['approved']\n",
    "\n",
    "clf_two = DecisionTreeClassifier(max_depth=3, min_samples_split=30, random_state=42)\n",
    "clf_two.fit(X2, y2)\n",
    "\n",
    "print(f\"Two-variable tree accuracy (train): {clf_two.score(X2, y2):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "tree.plot_tree(clf_two, feature_names=['Income', 'Credit Score'],\n",
    "               class_names=['Denied', 'Approved'], filled=True, rounded=True, fontsize=9)\n",
    "plt.title(\"Decision Tree: Approval by Income & Credit Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57738f54",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "- Force the tree to start with income by limiting depth or raising `min_samples_split`. Does it still prefer credit score first?\n",
    "- Increase `max_depth` to 4‚Äì5. Do you notice overfitting (very pure leaves)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf04aab",
   "metadata": {},
   "source": [
    "## 3) Classification Tree in Practice: Heart Disease\n",
    "\n",
    "We‚Äôll predict heart disease (1/0) from clinical features. This dataset is included in the course repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c318e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load heart disease data\n",
    "heart = pd.read_csv(\"https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/main/data/heart.csv\")\n",
    "heart.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1a4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode any non-numeric columns (LabelEncoder is adequate for trees)\n",
    "heart_enc = heart.copy()\n",
    "categorical_cols = heart_enc.select_dtypes(include=['object']).columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    heart_enc[col] = le.fit_transform(heart_enc[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features/target\n",
    "Xh = heart_enc.drop('disease', axis=1)\n",
    "yh = heart_enc['disease']\n",
    "\n",
    "Xh_train, Xh_test, yh_train, yh_test = train_test_split(\n",
    "    Xh, yh, test_size=0.3, random_state=42, stratify=yh\n",
    ")\n",
    "\n",
    "# Default tree\n",
    "heart_tree = DecisionTreeClassifier(random_state=42)\n",
    "heart_tree.fit(Xh_train, yh_train)\n",
    "\n",
    "yh_pred = heart_tree.predict(Xh_test)\n",
    "print(\"Classification Report (Default Tree):\")\n",
    "print(classification_report(yh_test, yh_pred, target_names=['No Disease', 'Disease']))\n",
    "\n",
    "print(\"\\nComplexity:\")\n",
    "print(f\"Depth: {heart_tree.get_depth()}\")\n",
    "print(f\"Leaves: {heart_tree.get_n_leaves()}\")\n",
    "print(f\"Train acc: {heart_tree.score(Xh_train, yh_train):.3f} | Test acc: {heart_tree.score(Xh_test, yh_test):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Visualize the full tree ‚Äî can be large!\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(heart_tree, feature_names=Xh.columns, class_names=['No Disease', 'Disease'],\n",
    "          filled=True, rounded=True, fontsize=6)\n",
    "plt.title(\"Heart Disease Tree (Default Parameters)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f98947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrain complexity to improve generalization\n",
    "heart_tree_tuned = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42\n",
    ").fit(Xh_train, yh_train)\n",
    "\n",
    "print(\"Model Comparison\")\n",
    "print(\"Default Tree:  Train {:.3f} | Test {:.3f}\".format(heart_tree.score(Xh_train, yh_train),\n",
    "                                                         heart_tree.score(Xh_test, yh_test)))\n",
    "print(\"Tuned   Tree:  Train {:.3f} | Test {:.3f}\".format(heart_tree_tuned.score(Xh_train, yh_train),\n",
    "                                                         heart_tree_tuned.score(Xh_test, yh_test)))\n",
    "print(f\"Depth (tuned): {heart_tree_tuned.get_depth()} | Leaves: {heart_tree_tuned.get_n_leaves()}\")\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plot_tree(heart_tree_tuned, feature_names=Xh.columns, class_names=['No Disease', 'Disease'],\n",
    "          filled=True, rounded=True, fontsize=8)\n",
    "plt.title(\"Heart Disease Tree (Tuned Parameters)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64310ed2",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "- Change `max_depth` and `min_samples_leaf` for the tuned model. Where is the bias‚Äìvariance sweet spot?\n",
    "- Use `classification_report` to compare precision/recall across settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edad98c",
   "metadata": {},
   "source": [
    "## 4) Regression Tree in Practice: Ames Housing\n",
    "\n",
    "We‚Äôll predict sale price from a subset of intuitive home features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372230ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ames = pd.read_csv(\"https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/main/data/ames_clean.csv\")\n",
    "features = ['GrLivArea', 'OverallQual', 'TotalBsmtSF', 'GarageArea', 'YearBuilt', 'LotArea', 'FullBath', 'BedroomAbvGr']\n",
    "df_house = ames[features + ['SalePrice']].dropna().copy()\n",
    "df_house.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63421fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr = df_house.drop('SalePrice', axis=1)\n",
    "yr = df_house['SalePrice']\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.3, random_state=42)\n",
    "\n",
    "reg_tree = DecisionTreeRegressor(random_state=42).fit(Xr_train, yr_train)\n",
    "pred_train = reg_tree.predict(Xr_train)\n",
    "pred_test = reg_tree.predict(Xr_test)\n",
    "\n",
    "def eval_reg(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred) ** 0.5\n",
    "    return r2, mae, rmse\n",
    "\n",
    "r2_t, mae_t, rmse_t = eval_reg(yr_test, pred_test)\n",
    "print(\"Ames Price Prediction (Default Tree)\")\n",
    "print(f\"Test R^2: {r2_t:.3f} | MAE: ${mae_t:,.0f} | RMSE: ${rmse_t:,.0f}\")\n",
    "print(f\"Depth: {reg_tree.get_depth()} | Leaves: {reg_tree.get_n_leaves()}\")\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plot_tree(reg_tree, feature_names=Xr.columns, filled=True, rounded=True, fontsize=7, max_depth=3)\n",
    "plt.title(\"Ames Regression Tree (Top 3 Levels)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb58ea1",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "- Add `min_samples_leaf=20` or set `max_depth=6` on a new model and compare metrics.\n",
    "- Which features tend to appear near the root? Why does that make sense economically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893fd74",
   "metadata": {},
   "source": [
    "## 5) Practical Controls for Overfitting\n",
    "\n",
    "The most common levers:\n",
    "- `max_depth`: limits the number of splits (path length)\n",
    "- `min_samples_split`: minimum samples to split a node\n",
    "- `min_samples_leaf`: minimum samples per leaf\n",
    "\n",
    "Use validation (or cross-validation) to pick values that generalize well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94225cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sweep example (illustrative; keep small to run fast in class)\n",
    "depths = [3, 4, 5, 6, 8, 10]\n",
    "results = []\n",
    "for d in depths:\n",
    "    m = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    m.fit(Xh_train, yh_train)\n",
    "    results.append((d, m.score(Xh_train, yh_train), m.score(Xh_test, yh_test)))\n",
    "\n",
    "pd.DataFrame(results, columns=['max_depth', 'train_acc', 'test_acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cab577",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Exercise 1: Baseball Salary Prediction (Regression)\n",
    "\n",
    "**Goal:** Build a regression tree to predict `Salary` using the Hitters dataset from ISLP.\n",
    "\n",
    "**Steps** (start code provided):\n",
    "1. Load and clean data (drop NAs in `Salary`).\n",
    "2. Select features `['Years','Hits','RBI','Walks','PutOuts']`.\n",
    "3. Build a default tree, then a constrained tree (`max_depth=4`).\n",
    "4. Evaluate Train/Test R¬≤, MAE, RMSE.\n",
    "5. Visualize the constrained tree and extract 2‚Äì3 if-then rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33691b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code\n",
    "from ISLP import load_data\n",
    "\n",
    "Hitters = load_data('Hitters')\n",
    "Hitters_clean = Hitters.dropna(subset=['Salary']).copy()\n",
    "\n",
    "features = ['Years', 'Hits', 'RBI', 'Walks', 'PutOuts']\n",
    "Xb = Hitters_clean[features]\n",
    "yb = Hitters_clean['Salary']\n",
    "\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(Xb, yb, test_size=0.3, random_state=42)\n",
    "\n",
    "# TODO: build default tree\n",
    "# default_reg = DecisionTreeRegressor(random_state=42).fit(Xb_train, yb_train)\n",
    "\n",
    "# TODO: evaluate metrics with helper\n",
    "def eval_reg(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred) ** 0.5\n",
    "    return r2, mae, rmse\n",
    "\n",
    "# TODO: build constrained tree (e.g., max_depth=4), visualize with plot_tree\n",
    "# constrained_reg = DecisionTreeRegressor(max_depth=4, random_state=42).fit(Xb_train, yb_train)\n",
    "# plt.figure(figsize=(14, 8)); plot_tree(constrained_reg, feature_names=features, filled=True, rounded=True, fontsize=9); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94a254",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Exercise 2: Credit Default Classification\n",
    "\n",
    "**Goal:** Predict default using balance, income, and student status.\n",
    "\n",
    "**Steps** (start code provided):\n",
    "1. Create two models: (A) default params, (B) constrained (`max_depth=3`, `min_samples_split=50`, `min_samples_leaf=25`).\n",
    "2. Compare train vs. test accuracy, classification report, and complexity.\n",
    "3. Discuss class imbalance (~3% default): focus on precision/recall for the positive class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code\n",
    "Default = load_data('Default')\n",
    "Default_enc = pd.get_dummies(Default, columns=['student'], drop_first=True)\n",
    "Default_enc['default_binary'] = (Default_enc['default'] == 'Yes').astype(int)\n",
    "\n",
    "Xd = Default_enc[['balance', 'income', 'student_Yes']]\n",
    "yd = Default_enc['default_binary']\n",
    "\n",
    "Xd_train, Xd_test, yd_train, yd_test = train_test_split(Xd, yd, test_size=0.3, random_state=42, stratify=yd)\n",
    "\n",
    "# TODO: Model A (default) and Model B (constrained)\n",
    "# clfA = DecisionTreeClassifier(random_state=42).fit(Xd_train, yd_train)\n",
    "# clfB = DecisionTreeClassifier(max_depth=3, min_samples_split=50, min_samples_leaf=25, random_state=42).fit(Xd_train, yd_train)\n",
    "\n",
    "# TODO: Compare performance\n",
    "# for name, model in [('A', clfA), ('B', clfB)]: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a9c64",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚≠ê Optional Challenge: Stock Market Direction (Weekly)\n",
    "\n",
    "**Goal:** Predict market direction (Up/Down) using lagged returns.\n",
    "\n",
    "- Split chronologically (first 80% train, last 20% test).\n",
    "- Compare to a naive \"always Up\" rule.\n",
    "- Interpret which lags matter (if any) and discuss why this problem is hard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code\n",
    "Weekly = load_data('Weekly').copy()\n",
    "Weekly['Direction_binary'] = (Weekly['Direction'] == 'Up').astype(int)\n",
    "lag_features = ['Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5']\n",
    "\n",
    "Xw = Weekly[lag_features]\n",
    "yw = Weekly['Direction_binary']\n",
    "\n",
    "# Chronological split: first 80% train, last 20% test\n",
    "split_idx = int(0.8 * len(Weekly))\n",
    "Xw_train, Xw_test = Xw.iloc[:split_idx], Xw.iloc[split_idx:]\n",
    "yw_train, yw_test = yw.iloc[:split_idx], yw.iloc[split_idx:]\n",
    "\n",
    "# TODO: Fit a tree and evaluate vs. a naive baseline that predicts all Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a30ae4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- CART selects (feature, threshold) splits that minimize impurity (classification) or SSE (regression).\n",
    "- Trees easily model thresholds and interactions, but can overfit without constraints.\n",
    "- Use `max_depth`, `min_samples_split`, and `min_samples_leaf` to control complexity.\n",
    "- Translate paths to if‚Äìthen business rules for stakeholder communication.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
