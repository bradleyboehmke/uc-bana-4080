{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b540ff1c",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning: Finding Optimal Model Configurations\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/29_hyperparameter_tuning.ipynb)\n",
    "\n",
    "This companion notebook provides hands-on exercises for the **Hyperparameter Tuning** chapter. You'll explore the bias-variance tradeoff, use K-Nearest Neighbors to see how hyperparameters affect model performance, and systematically tune decision trees and random forests using grid search.\n",
    "\n",
    "**What you'll practice**\n",
    "- Understand the bias-variance tradeoff through visualization\n",
    "- Tune K-Nearest Neighbors (KNN) regression to find optimal K\n",
    "- Use GridSearchCV to systematically search hyperparameter spaces\n",
    "- Tune decision trees and random forests with multiple hyperparameters\n",
    "- Compare grid search vs. random search\n",
    "- Apply the complete 5-stage workflow: split, tune, compare, train, evaluate\n",
    "\n",
    "**How to use**\n",
    "- Run from top to bottom. When you see **üèÉ‚Äç‚ôÇÔ∏è Try It Yourself**, add your code beneath the prompt.\n",
    "- In Colab: `Runtime ‚Üí Restart and run all` to test from a clean environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c85fc",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "Install and import the required packages. In local environments where these libraries are already installed, you can skip the install cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc82f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Colab/a fresh env, uncomment to install\n",
    "# !pip -q install scikit-learn pandas numpy matplotlib ISLP optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ISLP import load_data\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53f0f9",
   "metadata": {},
   "source": [
    "## 1) The Bias-Variance Tradeoff: Visualizing Underfitting vs. Overfitting\n",
    "\n",
    "We'll create synthetic data (sine wave with noise) and fit three models with different complexity levels to illustrate:\n",
    "- **High bias (underfitting)**: Model too simple to capture the pattern\n",
    "- **Balanced (good fit)**: Model captures the signal without memorizing noise\n",
    "- **High variance (overfitting)**: Model memorizes training data including noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea706970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data: sine wave with noise\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100)\n",
    "y_true = np.sin(X) + 0.5 * X  # True underlying pattern\n",
    "y = y_true + np.random.normal(0, 0.4, 100)  # Add noise\n",
    "\n",
    "# Reshape for sklearn\n",
    "X_reshaped = X.reshape(-1, 1)\n",
    "X_plot = np.linspace(0, 10, 300).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437dfa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Three models with different complexity\n",
    "# 1. High bias (underfitting): Linear regression\n",
    "bias_model = LinearRegression()\n",
    "bias_model.fit(X_reshaped, y)\n",
    "y_bias = bias_model.predict(X_plot)\n",
    "\n",
    "# 2. Balanced (good fit): Random forest with limited depth\n",
    "good_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "good_model.fit(X_reshaped, y)\n",
    "y_good = good_model.predict(X_plot)\n",
    "\n",
    "# 3. High variance (overfitting): Deep decision tree\n",
    "variance_model = DecisionTreeRegressor(max_depth=None, min_samples_split=2, random_state=42)\n",
    "variance_model.fit(X_reshaped, y)\n",
    "y_variance = variance_model.predict(X_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd894638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias-variance tradeoff\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Left: High bias (underfitting)\n",
    "axes[0].scatter(X, y, alpha=0.6, s=30, edgecolors='black', label='Training data')\n",
    "axes[0].plot(X_plot, y_bias, 'r-', linewidth=2.5, label='Linear model')\n",
    "axes[0].plot(X, y_true, 'g--', linewidth=1.5, alpha=0.7, label='True pattern')\n",
    "axes[0].set_title('High Bias (Underfitting)\\nToo Simple', fontsize=11, fontweight='bold')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Middle: Balanced (good fit)\n",
    "axes[1].scatter(X, y, alpha=0.6, s=30, edgecolors='black', label='Training data')\n",
    "axes[1].plot(X_plot, y_good, 'b-', linewidth=2.5, label='Random forest')\n",
    "axes[1].plot(X, y_true, 'g--', linewidth=1.5, alpha=0.7, label='True pattern')\n",
    "axes[1].set_title('Balanced (Good Fit)\\nJust Right', fontsize=11, fontweight='bold')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: High variance (overfitting)\n",
    "axes[2].scatter(X, y, alpha=0.6, s=30, edgecolors='black', label='Training data')\n",
    "axes[2].plot(X_plot, y_variance, 'orange', linewidth=2.5, label='Deep tree')\n",
    "axes[2].plot(X, y_true, 'g--', linewidth=1.5, alpha=0.7, label='True pattern')\n",
    "axes[2].set_title('High Variance (Overfitting)\\nToo Complex', fontsize=11, fontweight='bold')\n",
    "axes[2].set_xlabel('X')\n",
    "axes[2].set_ylabel('y')\n",
    "axes[2].legend(fontsize=9)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877efde8",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "- What happens if you change `max_depth=5` to `max_depth=2` for the random forest? Does it become more biased or more variant?\n",
    "- Try `max_depth=15` for the decision tree. Does overfitting improve or worsen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a0f259",
   "metadata": {},
   "source": [
    "## 2) K-Nearest Neighbors: A Case Study in Bias-Variance\n",
    "\n",
    "KNN has a single intuitive hyperparameter: **K** (number of neighbors). Let's see how different K values affect performance on our sine wave data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different K values\n",
    "k_values = [1, 2, 5, 10, 25, 50, 75]\n",
    "\n",
    "print(\"K-Nearest Neighbors Regressor: Effect of K on Performance\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    \n",
    "    # Use cross-validation to evaluate (negative MSE)\n",
    "    cv_scores = cross_val_score(knn, X_reshaped, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Also check training score to see overfitting\n",
    "    knn.fit(X_reshaped, y)\n",
    "    train_score = knn.score(X_reshaped, y)\n",
    "    \n",
    "    print(f\"K={k:2d}  |  Train R¬≤: {train_score:.3f}  |  CV MSE: {-cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4567fbf2",
   "metadata": {},
   "source": [
    "**Key observations:**\n",
    "- **K=1**: Very high training R¬≤ but higher CV error ‚Üí Overfitting\n",
    "- **Larger K**: Training R¬≤ decreases, CV error may improve then worsen\n",
    "- **Optimal K**: Around K=5-7 where CV error is minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different K values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "k_examples = [75, 5, 1]\n",
    "titles = ['High Bias (K=75)', 'Balanced (K=5)', 'High Variance (K=1)']\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "for idx, (k, title, color) in enumerate(zip(k_examples, titles, colors)):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_reshaped, y)\n",
    "    y_pred = knn.predict(X_plot)\n",
    "    \n",
    "    axes[idx].scatter(X, y, alpha=0.4, s=20, color='gray', label='Training data')\n",
    "    axes[idx].plot(X_plot, y_pred, color=color, linewidth=2.5, label=f'KNN (K={k})')\n",
    "    axes[idx].set_xlabel('X', fontsize=11)\n",
    "    axes[idx].set_ylabel('y', fontsize=11)\n",
    "    axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend(fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015e195",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "- Plot training R¬≤ vs. CV MSE for all K values. Where do you see the biggest gap (overfitting)?\n",
    "- Try K values from 1 to 100 in steps of 5. What K gives the best CV score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5001bf",
   "metadata": {},
   "source": [
    "## 3) Grid Search: Automating Hyperparameter Tuning\n",
    "\n",
    "Instead of manually trying different K values, we can use **GridSearchCV** to systematically search and find the optimal hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c99cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reshaped, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6625f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 10, 15, 20, 30, 50]\n",
    "}\n",
    "\n",
    "# Step 2: Create GridSearchCV object\n",
    "knn = KNeighborsRegressor()\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                               # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',   # Metric to optimize\n",
    "    return_train_score=True,            # Also return training scores\n",
    "    verbose=1                           # Show progress\n",
    ")\n",
    "\n",
    "# Step 3: Fit grid search (tries all combinations)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: View results\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score (MSE): {grid_search.best_score_:.3f}\")\n",
    "print(f\"\\nBest model (already retrained on all training data):\")\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54aba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for easier viewing\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns\n",
    "results_summary = results_df[[\n",
    "    'param_n_neighbors',\n",
    "    'mean_train_score',\n",
    "    'mean_test_score',\n",
    "    'std_test_score'\n",
    "]].copy()\n",
    "\n",
    "results_summary.columns = ['K', 'Train Score', 'CV Score', 'CV Std']\n",
    "results_summary = results_summary.sort_values('K')\n",
    "\n",
    "print(\"\\nGrid Search Results Summary:\")\n",
    "print(results_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on test set (Stage 5: final evaluation)\n",
    "y_pred_test = grid_search.best_estimator_.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nFinal Test Performance:\")\n",
    "print(f\"Test MSE: {test_mse:.3f}\")\n",
    "print(f\"Test R¬≤: {test_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b0ea4d",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "- Add more hyperparameters to the grid: `weights`: ['uniform', 'distance'] and `p`: [1, 2]\n",
    "- How many total model fits does GridSearchCV perform with these additional parameters?\n",
    "- Does adding these parameters improve CV performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec55fa",
   "metadata": {},
   "source": [
    "## 4) Tuning Decision Trees\n",
    "\n",
    "Decision trees have multiple hyperparameters. Let's tune them on the Default dataset (classification task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0765d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare Default dataset\n",
    "Default = load_data('Default')\n",
    "X = pd.get_dummies(Default[['balance', 'income', 'student']], drop_first=True)\n",
    "y = (Default['default'] == 'Yes').astype(int)\n",
    "\n",
    "# Train/test split (stratified for imbalanced data)\n",
    "X_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train_dt)} samples\")\n",
    "print(f\"Test set: {len(X_test_dt)} samples\")\n",
    "print(f\"\\nClass distribution (training):\")\n",
    "print(y_train_dt.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for decision tree\n",
    "param_grid_tree = {\n",
    "    'max_depth': [3, 5, 7, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 10, 20, 50],\n",
    "    'min_samples_leaf': [1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "grid_search_tree = GridSearchCV(\n",
    "    estimator=tree,\n",
    "    param_grid=param_grid_tree,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',  # Better metric for imbalanced data\n",
    "    verbose=1,\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "print(\"Searching for best decision tree hyperparameters...\")\n",
    "print(f\"Total combinations to try: {len(param_grid_tree['max_depth']) * len(param_grid_tree['min_samples_split']) * len(param_grid_tree['min_samples_leaf'])}\")\n",
    "grid_search_tree.fit(X_train_dt, y_train_dt)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search_tree.best_params_}\")\n",
    "print(f\"Best CV ROC AUC: {grid_search_tree.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8153c6f",
   "metadata": {},
   "source": [
    "**Note:** This grid searches 7 √ó 4 √ó 4 = 112 hyperparameter combinations, each evaluated with 5-fold CV = 560 model fits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "best_tree = grid_search_tree.best_estimator_\n",
    "y_pred_proba_tree = best_tree.predict_proba(X_test_dt)[:, 1]\n",
    "tree_test_auc = roc_auc_score(y_test_dt, y_pred_proba_tree)\n",
    "\n",
    "print(f\"\\nDecision Tree Final Performance:\")\n",
    "print(f\"  Best params: {grid_search_tree.best_params_}\")\n",
    "print(f\"  CV ROC AUC:  {grid_search_tree.best_score_:.4f}\")\n",
    "print(f\"  Test ROC AUC: {tree_test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37421464",
   "metadata": {},
   "source": [
    "## 5) Tuning Random Forests\n",
    "\n",
    "Random forests have additional hyperparameters like `n_estimators` and `max_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for random forest (smaller to save computation)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_split': [2, 10, 20]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "print(\"Searching for best random forest hyperparameters...\")\n",
    "print(f\"Total combinations to try: {len(param_grid_rf['n_estimators']) * len(param_grid_rf['max_depth']) * len(param_grid_rf['max_features']) * len(param_grid_rf['min_samples_split'])}\")\n",
    "grid_search_rf.fit(X_train_dt, y_train_dt)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search_rf.best_params_}\")\n",
    "print(f\"Best CV ROC AUC: {grid_search_rf.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e82781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_pred_proba_rf = best_rf.predict_proba(X_test_dt)[:, 1]\n",
    "rf_test_auc = roc_auc_score(y_test_dt, y_pred_proba_rf)\n",
    "\n",
    "print(f\"\\nRandom Forest Final Performance:\")\n",
    "print(f\"  Best params: {grid_search_rf.best_params_}\")\n",
    "print(f\"  CV ROC AUC:  {grid_search_rf.best_score_:.4f}\")\n",
    "print(f\"  Test ROC AUC: {rf_test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c990cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare decision tree vs. random forest\n",
    "print(\"\\nFinal Model Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Decision Tree:\")\n",
    "print(f\"  Best params: {grid_search_tree.best_params_}\")\n",
    "print(f\"  CV ROC AUC:  {grid_search_tree.best_score_:.4f}\")\n",
    "print(f\"  Test ROC AUC: {tree_test_auc:.4f}\")\n",
    "print()\n",
    "print(f\"Random Forest:\")\n",
    "print(f\"  Best params: {grid_search_rf.best_params_}\")\n",
    "print(f\"  CV ROC AUC:  {grid_search_rf.best_score_:.4f}\")\n",
    "print(f\"  Test ROC AUC: {rf_test_auc:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f560ff6",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "- Are the CV and test scores close? What does this indicate about overfitting?\n",
    "- Which model performs better? By how much?\n",
    "- Try adding `min_samples_leaf` to the random forest parameter grid. Does it improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9e903",
   "metadata": {},
   "source": [
    "## 6) Beyond Grid Search: Random Search\n",
    "\n",
    "When you have many hyperparameters or a large search space, random search can be more efficient than grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a02786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distributions (instead of fixed values)\n",
    "param_distributions_rf = {\n",
    "    'n_estimators': randint(50, 300),           # Random integers from 50-300\n",
    "    'max_depth': [5, 10, 15, 20, None],         # Can still use lists\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_split': randint(2, 50),        # Random integers from 2-50\n",
    "    'min_samples_leaf': randint(1, 20)          # Random integers from 1-20\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_distributions_rf,\n",
    "    n_iter=50,  # Number of random samples to try\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Random search: trying 50 random hyperparameter combinations...\")\n",
    "random_search.fit(X_train_dt, y_train_dt)\n",
    "\n",
    "print(f\"\\nBest parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV ROC AUC: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate random search best model on test set\n",
    "best_rf_random = random_search.best_estimator_\n",
    "y_pred_proba_random = best_rf_random.predict_proba(X_test_dt)[:, 1]\n",
    "rf_random_test_auc = roc_auc_score(y_test_dt, y_pred_proba_random)\n",
    "\n",
    "print(f\"\\nRandom Search vs. Grid Search Comparison:\")\n",
    "print(f\"Grid Search:   CV AUC = {grid_search_rf.best_score_:.4f}, Test AUC = {rf_test_auc:.4f}\")\n",
    "print(f\"Random Search: CV AUC = {random_search.best_score_:.4f}, Test AUC = {rf_random_test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c5d6e",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "- Increase `n_iter` to 100. Does random search find better hyperparameters?\n",
    "- Time both grid search and random search. Which is faster?\n",
    "- When would you prefer random search over grid search?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d6e7f",
   "metadata": {},
   "source": [
    "## 7) (Optional) Bayesian Optimization with Optuna\n",
    "\n",
    "For even more efficient hyperparameter tuning, you can use Bayesian optimization libraries like Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e7f8g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install optuna\n",
    "# !pip install optuna\n",
    "\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)  # Reduce output\n",
    "\n",
    "def objective(trial):\n",
    "    # Optuna suggests hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 50),\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate\n",
    "    rf = RandomForestClassifier(**params, random_state=42)\n",
    "    score = cross_val_score(rf, X_train_dt, y_train_dt, cv=5, scoring='roc_auc').mean()\n",
    "    return score\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest params: {study.best_params}\")\n",
    "print(f\"Best CV ROC AUC: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f8g9h",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Summary: The Complete Workflow\n",
    "\n",
    "1. **Train/test split** ‚Üí Lock away test set\n",
    "2. **Define models and grids** ‚Üí Set up GridSearchCV for each candidate model\n",
    "3. **Grid search with CV** ‚Üí Find best hyperparameters using training set only\n",
    "4. **Compare models** ‚Üí Use CV scores to select best model type and configuration\n",
    "5. **Final evaluation** ‚Üí Test set evaluation EXACTLY ONCE\n",
    "\n",
    "**Key takeaways:**\n",
    "- Hyperparameters control the bias-variance tradeoff\n",
    "- Grid search systematically finds optimal hyperparameters using cross-validation\n",
    "- Random search is more efficient for large hyperparameter spaces\n",
    "- Bayesian optimization (Optuna) can be even more efficient for expensive models\n",
    "- Always evaluate on test set only once, after all tuning is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8g9h0i",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ End-of-Chapter Exercises\n",
    "\n",
    "These exercises give you hands-on practice with hyperparameter tuning using grid search and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8g9h0i1j",
   "metadata": {},
   "source": [
    "### Exercise 1: Tuning KNN for Regression\n",
    "\n",
    "Use the [Ames housing dataset](https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/refs/heads/main/data/ames_clean.csv) to predict `SalePrice` using K-Nearest Neighbors regression.\n",
    "\n",
    "**Your tasks:**\n",
    "1. Load the Ames housing data and select at least 5 numerical features\n",
    "2. Create a train/test split (80/20)\n",
    "3. Use `GridSearchCV` to tune these KNN hyperparameters:\n",
    "   - `n_neighbors`: [3, 5, 7, 10, 15, 20, 30, 50]\n",
    "   - `weights`: ['uniform', 'distance']\n",
    "   - `p`: [1, 2] (1 = Manhattan distance, 2 = Euclidean distance)\n",
    "4. Use 5-fold CV and optimize for R¬≤ score\n",
    "5. Report the best hyperparameters and CV score\n",
    "6. Evaluate the best model on the test set\n",
    "7. Create a visualization showing how `n_neighbors` affects performance\n",
    "\n",
    "**Reflection questions:**\n",
    "- How does the `weights` parameter affect performance?\n",
    "- What does the `p` parameter control? Which distance metric worked better?\n",
    "- Is there a large gap between CV and test performance? What does this indicate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9h0i1j2k",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code for Exercise 1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0i1j2k3l",
   "metadata": {},
   "source": [
    "### Exercise 2: Decision Tree Depth Analysis\n",
    "\n",
    "Systematically analyze how `max_depth` affects decision tree performance on the Default dataset.\n",
    "\n",
    "**Your tasks:**\n",
    "1. Load the Default dataset and prepare features\n",
    "2. Create train/test split (80/20, stratified)\n",
    "3. For each `max_depth` in [1, 2, 3, 5, 7, 10, 15, 20, None]:\n",
    "   - Train a decision tree\n",
    "   - Compute training accuracy and 5-fold CV accuracy\n",
    "   - Store results\n",
    "4. Create a line plot showing training vs. CV accuracy across depths\n",
    "5. Identify the depth where overfitting begins (gap between train and CV widens)\n",
    "6. Use `GridSearchCV` to tune multiple hyperparameters simultaneously:\n",
    "   - `max_depth`: [3, 5, 7, 10, 15, None]\n",
    "   - `min_samples_split`: [2, 10, 20, 50]\n",
    "   - `min_samples_leaf`: [1, 5, 10, 20]\n",
    "7. Compare the best tuned tree to your depth-only analysis\n",
    "\n",
    "**Reflection questions:**\n",
    "- At what depth does overfitting become apparent?\n",
    "- Did tuning multiple hyperparameters improve performance over just tuning depth?\n",
    "- Which hyperparameter had the largest impact on performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1j2k3l4m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code for Exercise 2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2k3l4m5n",
   "metadata": {},
   "source": [
    "### Exercise 3: Random Forest Comprehensive Tuning\n",
    "\n",
    "Perform comprehensive hyperparameter tuning for a random forest classifier on the Default dataset.\n",
    "\n",
    "**Part A: Grid Search**\n",
    "1. Define a parameter grid with:\n",
    "   - `n_estimators`: [50, 100, 200, 300]\n",
    "   - `max_depth`: [5, 10, 15, 20, None]\n",
    "   - `max_features`: ['sqrt', 'log2']\n",
    "   - `min_samples_split`: [2, 10, 20]\n",
    "2. Use `GridSearchCV` with 5-fold CV and ROC AUC scoring\n",
    "3. Report best parameters and CV score\n",
    "4. Evaluate on test set\n",
    "\n",
    "**Part B: Random Search**\n",
    "1. Define parameter distributions:\n",
    "   - `n_estimators`: integers from 50 to 500\n",
    "   - `max_depth`: [3, 5, 7, 10, 15, 20, None]\n",
    "   - `max_features`: ['sqrt', 'log2']\n",
    "   - `min_samples_split`: integers from 2 to 100\n",
    "   - `min_samples_leaf`: integers from 1 to 50\n",
    "2. Use `RandomizedSearchCV` with `n_iter=100`\n",
    "3. Compare results to grid search\n",
    "\n",
    "**Part C: Analysis**\n",
    "1. Create a bar chart comparing:\n",
    "   - Default random forest (no tuning)\n",
    "   - Grid search tuned\n",
    "   - Random search tuned\n",
    "2. Show both CV and test scores\n",
    "3. Report computation time for each approach\n",
    "\n",
    "**Reflection questions:**\n",
    "- Did random search find better hyperparameters than grid search?\n",
    "- Was random search faster? By how much?\n",
    "- How much improvement did tuning provide over defaults?\n",
    "- Would you recommend random search or grid search for this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3l4m5n6o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code for Exercise 3 here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
