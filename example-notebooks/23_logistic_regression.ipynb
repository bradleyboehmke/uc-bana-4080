{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/23_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Logistic Regression for Classification\n",
    "\n",
    "This notebook contains code examples from the **Introduction to Logistic Regression for Classification** chapter (Chapter 23) of the BANA 4080 textbook. Follow along to practice building classification models using logistic regression with pandas, scikit-learn, and Python.\n",
    "\n",
    "## üìö Chapter Overview\n",
    "\n",
    "This chapter introduces logistic regression, the foundational algorithm for classification problems in business. You'll learn how to predict categories (like Yes/No, Default/No Default) instead of continuous numbers, and understand how to interpret probability-based predictions for business decision-making.\n",
    "\n",
    "## üéØ What You'll Practice\n",
    "\n",
    "- Understand why linear regression fails for classification and how logistic regression solves this\n",
    "- Build and interpret simple and multiple logistic regression models using scikit-learn\n",
    "- Work with probabilities, odds, and log-odds in business contexts\n",
    "- Make probability-based predictions and apply the 0.5 classification threshold\n",
    "- Use proper train/test splits to evaluate classification model performance\n",
    "- Recognize practical considerations like class imbalance in real datasets\n",
    "\n",
    "## üí° How to Use This Notebook\n",
    "\n",
    "1. **Read the chapter first** - This notebook supplements the textbook, not replaces it\n",
    "2. **Run cells sequentially** - Code builds on previous examples\n",
    "3. **Experiment freely** - Modify code to test your understanding\n",
    "4. **Practice variations** - Try different approaches to reinforce learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ISLP import load_data\n",
    "\n",
    "# Suppress numerical warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# Load the Default dataset used in this chapter\n",
    "Default = load_data('Default')\n",
    "print(\"Default dataset shape:\", Default.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "Default.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Linear Regression Fails for Classification\n",
    "\n",
    "Let's first explore why we can't use linear regression for classification problems by creating some sample credit default data and seeing what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample credit default data to demonstrate the problem\n",
    "np.random.seed(42)\n",
    "balances = np.linspace(0, 3000, 100)\n",
    "# Higher balances increase default probability\n",
    "probabilities = 1 / (1 + np.exp(-(balances - 1500) / 300))\n",
    "defaults = np.random.binomial(1, probabilities)\n",
    "\n",
    "default_data = pd.DataFrame({\n",
    "    'balance': balances,\n",
    "    'default': defaults\n",
    "})\n",
    "\n",
    "print(\"Sample of credit default data:\")\n",
    "print(default_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try linear regression on classification data\n",
    "X = default_data[['balance']]\n",
    "y = default_data['default']\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X, y)\n",
    "linear_predictions = linear_model.predict(X)\n",
    "\n",
    "# Visualize the problem\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(default_data['balance'], default_data['default'], alpha=0.6, label='Actual data')\n",
    "plt.plot(default_data['balance'], linear_predictions, color='red', linewidth=2, label='Linear regression')\n",
    "plt.xlabel('Credit Card Balance ($)')\n",
    "plt.ylabel('Default (0=No, 1=Yes)')\n",
    "plt.title('Why Linear Regression Fails for Classification')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight the problems\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.axhline(y=1, color='black', linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Linear regression predictions range from {linear_predictions.min():.2f} to {linear_predictions.max():.2f}\")\n",
    "print(\"But we need probabilities between 0 and 1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "\n",
    "Examine the linear regression results above. What specific problems do you see with using linear regression for classification? List at least 2 issues with the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your observations here\n",
    "# Problem 1:\n",
    "# Problem 2:\n",
    "# Problem 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Logistic Function\n",
    "\n",
    "The logistic function solves these problems by transforming any real number into a value between 0 and 1, creating the characteristic S-shaped curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the logistic function\n",
    "z_values = np.linspace(-6, 6, 100)\n",
    "probabilities = 1 / (1 + np.exp(-z_values))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z_values, probabilities, linewidth=3, color='blue')\n",
    "plt.xlabel('z (linear combination: Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ...)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('The Logistic Function: Transforming Linear Predictions to Probabilities')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight key points\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='50% probability threshold')\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.axhline(y=1, color='black', linestyle='-', alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate('Approaches 0\\n(Very Low Probability)', xy=(-4, 0.02), xytext=(-5, 0.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'), fontsize=10)\n",
    "plt.annotate('Approaches 1\\n(Very High Probability)', xy=(4, 0.98), xytext=(3, 0.8),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'), fontsize=10)\n",
    "plt.annotate('50% Decision\\nBoundary', xy=(0, 0.5), xytext=(1, 0.6),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding Probability vs. Odds vs. Log-odds\n",
    "probabilities = [0.1, 0.2, 0.5, 0.8, 0.9]\n",
    "odds = [p/(1-p) for p in probabilities]\n",
    "log_odds = [np.log(o) for o in odds]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Probability': probabilities,\n",
    "    'Odds': odds,\n",
    "    'Log-odds': log_odds,\n",
    "    'Business_Interpretation': [\n",
    "        'Very unlikely event (10% chance)',\n",
    "        'Unlikely event (20% chance)',\n",
    "        'Neutral/uncertain (50-50 chance)',\n",
    "        'Likely event (80% chance)',\n",
    "        'Very likely event (90% chance)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Understanding Probability vs. Odds vs. Log-odds:\")\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "\n",
    "Calculate the odds and log-odds for a probability of 0.75 (75% chance). What does this mean in business terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "p = 0.75\n",
    "# Calculate odds: \n",
    "# Calculate log-odds:\n",
    "# Business interpretation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Your First Logistic Regression Model\n",
    "\n",
    "Now let's apply logistic regression to the Default dataset to predict customer default based on their credit card balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the Default dataset\n",
    "print(\"Default distribution:\")\n",
    "print(Default['default'].value_counts())\n",
    "print(f\"\\nDefault rate: {Default['default'].value_counts(normalize=True)['Yes']:.1%}\")\n",
    "\n",
    "# Summary statistics by default status\n",
    "print(\"\\nSummary by default status:\")\n",
    "print(Default.groupby('default', observed=False)[['balance', 'income']].mean().round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for modeling\n",
    "# Convert categorical variables to numeric\n",
    "Default_encoded = pd.get_dummies(Default, columns=['student'], drop_first=True)\n",
    "Default_encoded['default_binary'] = (Default_encoded['default'] == 'Yes').astype(int)\n",
    "\n",
    "print(\"Encoded dataset:\")\n",
    "print(Default_encoded.head())\n",
    "\n",
    "# Define features and target\n",
    "X = Default_encoded[['balance', 'income', 'student_Yes']]\n",
    "y = Default_encoded['default_binary']\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")\n",
    "print(f\"Default rate in our target: {y.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple logistic regression with balance only\n",
    "X_simple = Default_encoded[['balance']]\n",
    "\n",
    "# Fit the logistic regression model\n",
    "log_reg_simple = LogisticRegression(random_state=42)\n",
    "log_reg_simple.fit(X_simple, y)\n",
    "\n",
    "print(\"Simple logistic regression model fitted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model components\n",
    "intercept = log_reg_simple.intercept_[0]\n",
    "balance_coef = log_reg_simple.coef_[0][0]\n",
    "\n",
    "print(\"Simple Logistic Regression Results:\")\n",
    "print(f\"Intercept: {intercept:.4f}\")\n",
    "print(f\"Balance coefficient: {balance_coef:.6f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nModel interpretation:\")\n",
    "print(f\"Log-odds equation: log-odds = {intercept:.4f} + {balance_coef:.6f} √ó balance\")\n",
    "print(f\"\\nFor each $1 increase in balance, log-odds increase by {balance_coef:.6f}\")\n",
    "print(f\"For each $1,000 increase in balance, log-odds increase by {balance_coef*1000:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "\n",
    "Create a visualization showing the S-shaped logistic regression curve fitted to the Default data. Compare this to the linear regression line we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use predict_proba to get probability predictions\n",
    "# Create a range of balance values for smooth curve\n",
    "# Plot both the actual data points and the fitted curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions with Logistic Regression\n",
    "\n",
    "One of the key advantages of logistic regression is that it provides both probability estimates and binary classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for specific balance amounts to show the progression\n",
    "example_balances = pd.DataFrame({'balance': [500, 1000, 1500, 2000, 2500, 3000]})\n",
    "\n",
    "# Get probability predictions - returns probabilities for both classes\n",
    "probabilities = log_reg_simple.predict_proba(example_balances)\n",
    "print(\"predict_proba() output (columns: [No Default, Default]):\")\n",
    "print(probabilities.round(4))\n",
    "\n",
    "# Get binary classifications - returns 0 or 1 based on 50% threshold\n",
    "classifications = log_reg_simple.predict(example_balances)\n",
    "print(f\"\\npredict() output (0=No Default, 1=Default):\")\n",
    "print(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize this information into a clear table\n",
    "prob_default = probabilities[:, 1]  # Extract just the default probabilities\n",
    "\n",
    "prediction_results = pd.DataFrame({\n",
    "    'Balance': example_balances['balance'],\n",
    "    'Probability_of_Default': prob_default,\n",
    "    'Predicted_Class': classifications,\n",
    "    'Business_Interpretation': [\n",
    "        'Very low risk - safe customer',\n",
    "        'Low risk - monitor balance growth',\n",
    "        'Moderate risk - consider credit limit review',\n",
    "        'High risk - proactive intervention recommended',\n",
    "        'Very high risk - immediate attention required',\n",
    "        'Extremely high risk - consider account restrictions'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Prediction Examples:\")\n",
    "print(prediction_results.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "\n",
    "Manually calculate the probability of default for a customer with a $2,000 balance using the logistic regression equation. Compare your result to scikit-learn's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Manual calculation: probability = 1 / (1 + exp(-(intercept + coefficient * balance)))\n",
    "balance_test = 2000\n",
    "\n",
    "# Manual calculation:\n",
    "\n",
    "# Scikit-learn prediction:\n",
    "\n",
    "# Compare results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Predictor Logistic Regression\n",
    "\n",
    "Now let's build a more comprehensive model using all available features: balance, income, and student status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the features we'll use in our multiple regression model\n",
    "print(\"Features in our model:\")\n",
    "print(X.columns.tolist())\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Sample of feature data:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple logistic regression with all features\n",
    "log_reg_multiple = LogisticRegression(random_state=42)\n",
    "log_reg_multiple.fit(X, y)\n",
    "\n",
    "# Extract coefficients\n",
    "intercept_multi = log_reg_multiple.intercept_[0]\n",
    "coefficients = log_reg_multiple.coef_[0]\n",
    "\n",
    "print(\"Multiple Logistic Regression Results:\")\n",
    "print(f\"Intercept: {intercept_multi:.6f}\")\n",
    "print(f\"Balance coefficient: {coefficients[0]:.6f}\")\n",
    "print(f\"Income coefficient: {coefficients[1]:.6f}\")\n",
    "print(f\"Student coefficient: {coefficients[2]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the coefficients in business context\n",
    "print(\"Business Interpretation of Coefficients:\")\n",
    "print(\"\\n‚Ä¢ Balance coefficient ({:.6f}):\".format(coefficients[0]))\n",
    "print(\"  Higher balances increase default risk - customers with more debt struggle with payments\")\n",
    "\n",
    "print(\"\\n‚Ä¢ Income coefficient ({:.6f}):\".format(coefficients[1]))\n",
    "print(\"  Income has negligible effect once balance is accounted for\")\n",
    "\n",
    "print(\"\\n‚Ä¢ Student coefficient ({:.6f}):\".format(coefficients[2]))\n",
    "print(\"  Being a student decreases default risk, possibly due to family support or careful spending\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "\n",
    "Create a model using only income as a predictor. How do the coefficients compare to the multiple regression model? What does this tell you about the relationship between income and default?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Build income-only model\n",
    "# Compare coefficients\n",
    "# Interpret the differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with Train/Test Split\n",
    "\n",
    "To properly evaluate our models, we need to use a train/test split to assess performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_simple_train, X_simple_test, X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_simple, X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} observations\")\n",
    "print(f\"Test set size: {len(X_test)} observations\")\n",
    "print(f\"Training default rate: {y_train.mean():.1%}\")\n",
    "print(f\"Test default rate: {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain both models on training data only\n",
    "log_reg_simple_new = LogisticRegression(random_state=42)\n",
    "log_reg_multiple_new = LogisticRegression(random_state=42)\n",
    "\n",
    "log_reg_simple_new.fit(X_simple_train, y_train)\n",
    "log_reg_multiple_new.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "pred_simple_test = log_reg_simple_new.predict(X_simple_test)\n",
    "pred_multiple_test = log_reg_multiple_new.predict(X_test)\n",
    "\n",
    "# Calculate test accuracy\n",
    "accuracy_simple_test = accuracy_score(y_test, pred_simple_test)\n",
    "accuracy_multiple_test = accuracy_score(y_test, pred_multiple_test)\n",
    "\n",
    "print(f\"Model Performance on Test Data:\")\n",
    "print(f\"Simple model (balance only): {accuracy_simple_test:.1%} accuracy\")\n",
    "print(f\"Multiple model (all features): {accuracy_multiple_test:.1%} accuracy\")\n",
    "\n",
    "print(f\"\\nBoth models achieve the same accuracy!\")\n",
    "print(f\"This suggests accuracy alone may not tell the full story...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare individual predictions\n",
    "sample_customers = X_test.head(10)\n",
    "prob_simple = log_reg_simple_new.predict_proba(sample_customers[['balance']])[:, 1]\n",
    "prob_multiple = log_reg_multiple_new.predict_proba(sample_customers)[:, 1]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Balance': sample_customers['balance'].values,\n",
    "    'Income': sample_customers['income'].values,\n",
    "    'Student': sample_customers['student_Yes'].values,\n",
    "    'Actual_Default': y_test.head(10).values,\n",
    "    'Simple_Model_Prob': prob_simple,\n",
    "    'Multiple_Model_Prob': prob_multiple,\n",
    "    'Probability_Difference': prob_multiple - prob_simple\n",
    "})\n",
    "\n",
    "print(\"Sample Predictions Comparison:\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è Try It Yourself\n",
    "\n",
    "Examine the probability differences between the two models. For which types of customers do the models give the most different predictions? What might this tell us about when additional features matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your analysis here\n",
    "# Look at cases with largest probability differences\n",
    "# What patterns do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Class Imbalance\n",
    "\n",
    "Our Default dataset has a severe class imbalance issue - only about 3% of customers default. This can make accuracy misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class imbalance in our dataset\n",
    "class_counts = Default['default'].value_counts()\n",
    "minority_percentage = class_counts.min() / class_counts.sum()\n",
    "\n",
    "print(f\"Class balance analysis:\")\n",
    "print(class_counts)\n",
    "print(f\"\\nMinority class (defaults) represents {minority_percentage:.1%} of data\")\n",
    "\n",
    "# What would happen if we always predicted \"No Default\"?\n",
    "naive_accuracy = (Default['default'] == 'No').mean()\n",
    "print(f\"\\nAccuracy of always predicting 'No Default': {naive_accuracy:.1%}\")\n",
    "print(f\"Our model accuracy: {accuracy_simple_test:.1%}\")\n",
    "print(f\"\\nOur model is only slightly better than always guessing 'No'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what our model actually predicts\n",
    "test_predictions = log_reg_simple_new.predict(X_simple_test)\n",
    "prediction_counts = pd.Series(test_predictions).value_counts()\n",
    "\n",
    "print(\"What our simple model predicts on test data:\")\n",
    "print(f\"Predicted No Default (0): {prediction_counts.get(0, 0)} customers\")\n",
    "print(f\"Predicted Default (1): {prediction_counts.get(1, 0)} customers\")\n",
    "\n",
    "print(f\"\\nOur model predicts 'Default' for {prediction_counts.get(1, 0) / len(test_predictions):.1%} of customers\")\n",
    "print(f\"Actual default rate in test set: {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Practice Challenges\n",
    "\n",
    "Test your understanding with these additional exercises that combine multiple concepts from the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Custom Threshold Analysis\n",
    "\n",
    "Instead of using the default 0.5 threshold, experiment with different probability thresholds (0.1, 0.3, 0.7) for classification. How does this affect the predictions? What threshold would you recommend for a conservative bank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# Get probability predictions\n",
    "# Apply different thresholds\n",
    "# Compare results and business implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Feature Engineering\n",
    "\n",
    "Create a new feature called `balance_to_income_ratio` and add it to your model. Does this improve the model's ability to distinguish between different customer types? What does the coefficient tell you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# Create the new feature\n",
    "# Add it to the model\n",
    "# Interpret the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Business Scenario Analysis\n",
    "\n",
    "Imagine you're advising a credit card company. Using your logistic regression model, analyze these three customer profiles and provide business recommendations:\n",
    "- Customer A: $1,200 balance, $35,000 income, not a student\n",
    "- Customer B: $2,800 balance, $25,000 income, student\n",
    "- Customer C: $800 balance, $65,000 income, not a student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# Create customer profiles\n",
    "# Get probability predictions\n",
    "# Provide business recommendations for each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Chapter Summary\n",
    "\n",
    "In this notebook, you practiced:\n",
    "\n",
    "- ‚úÖ Understanding why linear regression fails for classification problems\n",
    "- ‚úÖ Working with the logistic function and S-shaped probability curves\n",
    "- ‚úÖ Building simple and multiple logistic regression models with scikit-learn\n",
    "- ‚úÖ Interpreting coefficients in terms of log-odds and business impact\n",
    "- ‚úÖ Making probability-based predictions and applying classification thresholds\n",
    "- ‚úÖ Using proper train/test splits for model evaluation\n",
    "- ‚úÖ Recognizing class imbalance issues and their impact on accuracy\n",
    "\n",
    "## üîó Connections to Other Chapters\n",
    "\n",
    "- **Previous chapters**: Built on linear regression concepts from Chapter 21, extending them to classification problems\n",
    "- **Upcoming chapters**: Chapter 24 will introduce more sophisticated evaluation metrics (precision, recall, F1-score) that better handle class imbalance\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- [Scikit-learn Logistic Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [Understanding the Logistic Function](https://en.wikipedia.org/wiki/Logistic_function)\n",
    "- [Binary Classification Evaluation Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "1. **Review the chapter** to reinforce concepts about probability interpretation and business applications\n",
    "2. **Complete the end-of-chapter exercises** in the textbook using different ISLP datasets\n",
    "3. **Practice with your own datasets** to build confidence with real-world classification problems\n",
    "4. **Move on to Classification Evaluation** when ready to learn about advanced metrics for imbalanced data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
