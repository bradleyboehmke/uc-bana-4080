---
title: "Week 9 ‚Äì Correlation & Regression Foundations"
subtitle: "From Relationships to Predictions: Understanding What Drives Business Outcomes"
format:
  revealjs:
    slide-number: true
    preview-links: auto
    revealjs-plugins:
      - appearance
      - highlight-text
    css: styles.css
    mermaid:
      theme: neutral
footer: 'BANA 4080'
title-slide-attributes:
    data-background-image: images/correlation-regression-bg.jpeg
    data-background-size: cover
    data-background-opacity: "1.0"
filters: 
  - timer
execute:
    echo: true
---

## Welcome to Week 9

* Quick overview of today's plan:

  * Understanding relationships between business variables
  * Guess the Correlation team challenge 
  * From correlation to prediction with linear regression
  * Evaluating model performance for business decisions

# Discussion: Homework & Questions {background="#43464B"}

## Questions from Week 8? {.smaller}

* Midterm project feedback?
* Machine learning concepts and project planning?
* Anything confusing in the quiz or class lab?
* Time to ask!

. . .

::: {.callout}
## Activity

Converse with your neighbor and identify...

* 1 concept from last week that you thought was well explained
* 1 concept that is still confusing
:::

# Understanding Relationships in Business {background="#43464B"}

## Why Relationships Matter {.smaller}

In business, we rarely care about a single number in isolation. Leaders ask questions like:

* *Does increasing marketing spend actually increase sales?*
* *Are higher salaries associated with better employee retention?*
* *Do customers in certain regions spend more per transaction?*
* *Which factors drive customer satisfaction scores?*

**Today's Goal:** Learn to measure and model these relationships quantitatively.

## Think-Pair-Share: Business Relationships {.smaller}

:::: {.columns}
::: {.column width="70%"}

Think about your work experience, internships, or daily life:

- **What's one relationship between two things that you've noticed?** 
  - Example: "Study time and exam grades seem connected"
- **How strong do you think that relationship is?**
- **Could you use one to predict the other?**

Share your examples with your partner!

:::
::: {.column width="30%"}

<center>

<div id="4minWaiting"></div>
<script src="_extensions/produnis/timer/timer.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        initializeTimer("4minWaiting", 240, "slide"); 
    });
</script>
</center>

:::
::::

Then we'll take a few responses...

## What is Correlation? {.smaller}

:::: {.columns}
::: {.column}
**Correlation** measures how strongly two variables move together in a linear relationship.

* **Range**: -1 to +1
* **+1**: Perfect positive relationship
* **0**: No linear relationship  
* **-1**: Perfect negative relationship

**Key Insight:** Correlation is descriptive‚Äîit tells you variables move together, but **not why**.
:::
::: {.column}
```{python}
#| echo: false
#| fig-height: 8
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Set random seed for reproducibility
np.random.seed(42)

# Create figure with 3 subplots vertically
fig, axes = plt.subplots(3, 1, figsize=(6, 8))

# Positive correlation (~0.7)
n = 100
x1 = np.random.randn(n)
y1 = 0.7 * x1 + 0.5 * np.random.randn(n)
axes[0].scatter(x1, y1, alpha=0.6, color='blue')
axes[0].set_title('Positive Correlation (r ‚âà 0.7)', fontsize=12, fontweight='bold')
axes[0].set_xlabel('Variable X')
axes[0].set_ylabel('Variable Y')
axes[0].grid(True, alpha=0.3)

# No correlation (~0)
x2 = np.random.randn(n)
y2 = np.random.randn(n)
axes[1].scatter(x2, y2, alpha=0.6, color='gray')
axes[1].set_title('No Correlation (r ‚âà 0)', fontsize=12, fontweight='bold')
axes[1].set_xlabel('Variable X')
axes[1].set_ylabel('Variable Y')
axes[1].grid(True, alpha=0.3)

# Negative correlation (~-0.7)
x3 = np.random.randn(n)
y3 = -0.7 * x3 + 0.5 * np.random.randn(n)
axes[2].scatter(x3, y3, alpha=0.6, color='red')
axes[2].set_title('Negative Correlation (r ‚âà -0.7)', fontsize=12, fontweight='bold')
axes[2].set_xlabel('Variable X')
axes[2].set_ylabel('Variable Y')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```
:::
::::

::: {.notes}
Emphasize that correlation ‚â† causation. This is crucial for business decision-making.
:::

# Team Challenge: Guess the Correlation! {background="#43464B"}

## Team Challenge: Guess the Correlation! {.smaller}

::: {.callout}
## Setup & Game Link

**Teams:** I'm dividing the room into 2-3 teams

**Game:** We'll use the interactive game at [guessthecorrelation.com](https://www.guessthecorrelation.com/)

**How it works:**

- Teams view scatterplots and guess correlation coefficients (-1 to +1)
- Track your team's accuracy across multiple rounds
- See how close your intuition gets to the actual values!

**Scoring:** Keep track of your team's average accuracy

**Prize:** Winning team gets ~~extra credit~~ bragging rights! üéâ

**Let's play 5-10 rounds!**
:::

## Challenge Results & Key Insights {.smaller}

**Congratulations to our winning team!** üèÜ

**Key Takeaways from the Challenge:**

* **Visual patterns** help us estimate correlation strength
* **Business intuition** often aligns with statistical relationships
* **Perfect correlations** (¬±1.0) are rare in real business data
* **Direction matters**: Positive vs negative relationships tell different stories

**Next:** How do we move from measuring relationships to making predictions?

## Let's Practice: Grocery Chain Data {.smaller}

**Your turn!** Let's analyze some real grocery chain data.

```{python}
#| code-fold: true
#| code-summary: "Show code for creating grocery chain dataset"
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Example dataset: advertising spend vs. weekly sales
data = pd.DataFrame({
    "ad_spend": [400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300],
    "weekly_sales": [4200, 4400, 4100, 4800, 5600, 5200, 4900, 5500, 5300, 5900, 5700, 6300, 6900, 6200, 5800, 6600, 7100, 6800, 7300, 7800]
})

# Visualize the relationship
plt.figure(figsize=(8, 5))
plt.scatter(data["ad_spend"], data["weekly_sales"], alpha=0.7, s=60)
plt.xlabel("Advertising Spend ($)")
plt.ylabel("Weekly Sales")
plt.title("Ad Spend vs. Weekly Sales")
plt.grid(True, alpha=0.3)
plt.show()
```

**Question for you:** What do you think the correlation is? Write down your guess!

## Computing Correlation {.smaller}

Let's see how close your guess was!

```{python}
#| code-line-numbers: "7"
# Compute the correlation coefficient
correlation = data["ad_spend"].corr(data["weekly_sales"])
print(f"Correlation coefficient: {correlation:.3f}")

# Or we can see the full correlation matrix
print("\nFull correlation matrix:")
data.corr()
```

**Interpretation:** 

- Strong positive correlation (~0.9)
- As advertising spend increases, sales tend to increase
- **Remember:** Correlation ‚â† causation!

# From Correlation to Regression {background="#43464B"}

## From Correlation to Prediction {.smaller}

**We found a strong correlation (~0.9) between advertising and sales.**

**But now the business question is:** Can we use advertising spend to *predict* future sales?

**Question:** If you could draw a line to represent this relationship, what would it look like?

```{python}
#| code-fold: true
#| code-summary: "Show code for plot"
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Visualize the relationship
plt.figure(figsize=(8, 5))
plt.scatter(data["ad_spend"], data["weekly_sales"], alpha=0.7, s=60)
plt.xlabel("Advertising Spend ($)")
plt.ylabel("Weekly Sales")
plt.title("Ad Spend vs. Weekly Sales")
plt.grid(True, alpha=0.3)
plt.show()
```

## The Linear Regression Formula {.smaller}

**Remember from high school?** The equation of a line:

$$ y = mx + b $$

. . .

With linear regression, we follow a similar approach but the typical equation you'll see is:

$$ y = \beta_0 + \beta_1 x $$

Where:

- **y** = dependent variable (what we're predicting)
- **x** = independent variable (what we're using to predict)
- **Œ≤‚ÇÅ** = slope coefficient (how much y changes for each unit increase in x)
- **Œ≤‚ÇÄ** = intercept (value of y when x = 0)

::: {.callout-important}
**The goal:** Find the best values for slope and intercept that minimize prediction errors!
:::

## Fitting Our Regression Model {.smaller}

```{python}
from sklearn.linear_model import LinearRegression

# Prepare the data
X = data[["ad_spend"]]  # Feature matrix (note the double brackets)
y = data["weekly_sales"]  # Target variable

# Fit the model
model = LinearRegression()
model.fit(X, y)

# Extract the results
intercept = model.intercept_
slope = model.coef_[0]

print(f"Intercept (Œ≤‚ÇÄ): ${intercept:.0f}")
print(f"Slope (Œ≤‚ÇÅ): ${slope:.2f}")
```

**Our fitted equation:**
$$ \text{Weekly Sales} = 3552 + 1.68 \times \text{Ad Spend} $$

**Interpretation:** For every $1 increase in advertising, we expect weekly sales to increase by $1.68!

## Visualizing Our Predictions {.smaller}

```{python}
#| code-fold: true
#| code-summary: "Show code for regression line visualization"
# Create the regression line visualization
plt.figure(figsize=(10, 6))
plt.scatter(data["ad_spend"], data["weekly_sales"], alpha=0.7, s=60, label="Actual data")

# Add the fitted regression line
plt.plot(data["ad_spend"], model.predict(X), color="red", linewidth=3, label="Regression line")

# Show specific predictions
pred_x = [1500, 2000]
for x_val in pred_x:
    pred_y = model.predict([[x_val]])[0]
    plt.scatter(x_val, pred_y, color="orange", s=120, zorder=5)
    plt.annotate(f"${x_val} ‚Üí ${pred_y:.0f}", 
                xy=(x_val, pred_y), xytext=(5, -15), 
                textcoords='offset points', fontsize=10)

plt.xlabel("Advertising Spend ($)")
plt.ylabel("Weekly Sales")
plt.title("Linear Regression: Predictions on the Line")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

**Key insight:** Every prediction falls exactly on our regression line!

## Making Business Predictions {.smaller}

**Business value:** Now we can forecast sales for any advertising budget!

```{python}
#| code-line-numbers: "6"
# Make predictions for different scenarios
scenarios = pd.DataFrame({
    'ad_spend': [1500, 2500, 3000]
})

predictions = model.predict(scenarios)

print("Business Predictions:")
for spend, pred in zip(scenarios['ad_spend'], predictions):
    print(f"Spend ${spend:,} ‚Üí Predicted Sales: ${pred:,.0f}")
    
# Manual calculation to verify
print(f"\nManual check for $1,500:")
manual_pred = 1.68 * 1500 + 3552
print(f"1.68 √ó 1500 + 3552 = ${manual_pred:.0f}")
```

## Critical Question {.smaller}

**We now have a model that can make predictions...**

- Advertising spend of $1,500 ‚Üí $6,072 in sales
- Advertising spend of $2,500 ‚Üí $7,752 in sales  
- Advertising spend of $3,000 ‚Üí $8,592 in sales

::: {.callout-caution}
## Question ü§î

How do we know if this is a GOOD model?
:::

<br>

How would you assess prediction quality?

# Evaluating Model Performance {background="#43464B"}

## Why Model Evaluation Matters {.smaller .scrollable}

:::: {.columns}
::: {.column}
**Business Reality Check:**

* Building a model is only half the battle
* The real question: *How good is your model?*
* Without evaluation, you might deploy a model that makes terrible predictions!

**Think about it:**

* Would you trust a sales forecast that's typically off by 50%?
* How about one that's off by 5%?
:::
::: {.column}
```{mermaid}
%%| fig-align: center
%%| echo: false
flowchart TD
    A[Build Model] --> B[Make Predictions]
    B --> C[Compare with Actuals]
    C --> D[Calculate Metrics]
    D --> E{Good Performance?}
    E -->|Yes| F[Deploy Model]
    E -->|No| G[Improve Model]
    G --> A
    
    style A fill:#e1f5fe
    style D fill:#fff3e0
    style F fill:#c8e6c9
    style G fill:#ffcdd2
```
:::
::::

## Key Evaluation Metrics {.smaller}

**R¬≤ (R-squared):** "What percentage of the variation does my model explain?"

* Range: 0 to 1 (higher is better)
* Example: R¬≤ = 0.85 means the model explains 85% of sales variation

**RMSE (Root Mean Squared Error):** "How far off are my predictions, on average?"

* Same units as your outcome (dollars, customers, etc.)
* Example: RMSE = $347 means predictions are typically off by $347

::: {.callout-note}
## Additional Details in Readings

The readings will go into more details about these metrics and also introduce additional metrics like MAE (Mean Absolute Error) and MAPE (Mean Absolute Percentage Error). 

**Key takeaway:** All these metrics measure how our model's predicted values differ from the actual values.
:::

## Computing Metrics with Scikit-Learn {.smaller}

Let's see how to calculate these evaluation metrics using our advertising model:

```{python}
#| code-line-numbers: "1,3-8"
from sklearn.metrics import r2_score, root_mean_squared_error

# Make predictions on our data
predictions = model.predict(X)

# Calculate evaluation metrics
r2 = r2_score(y, predictions)
rmse = root_mean_squared_error(y, predictions)

print(f"R¬≤ Score: {r2:.3f}")
print(f"RMSE: ${rmse:.0f}")
```

**What do these numbers tell us about our model's performance?**

## The Problem: Training Data Evaluation {.smaller}

:::: {.columns}
::: {.column width="60%"}

**Wait a minute...** ü§î

We just evaluated our model on the **same data** we used to build it!

**This is like:**

- Grading your own homework
- A teacher giving students the test questions beforehand
- A chef only tasting their own cooking

:::
::: {.column width="40%"}

::: {.callout-warning}
## Generalization Problem

Training data evaluation can be overly optimistic!

We need to test on data the model has **never seen** before.
:::

:::
::::

::: {.callout-important}
## The Real Question ü§î

**Question**: How will our model perform on **new, unseen data**?

**Solution:** Train/Test splits simulate real-world deployment!
:::

## The Solution: Train/Test Split {.smaller .scrollable}

**The golden rule:** Never evaluate on the same data you used to train!

```{mermaid}
%%| fig-align: center
%%| fig-width: 5
%%| echo: false
flowchart TD
    A[Complete Dataset] --> B[Training Set<br/>70-80%]
    A --> C[Test Set<br/>20-30%]
    
    B --> D[Train Model]
    D --> E[Trained Model]
    
    C --> F[Evaluate Performance]
    E --> F
    F --> G[Unbiased Performance<br/>Estimate]
    
    style A fill:#f0f8ff
    style B fill:#e8f5e8
    style C fill:#ffe6e6
    style G fill:#fff2cc
```

**Key insight:** Test set simulates future, unseen data!

## Splitting Our Data {.smaller}

Let's split our advertising data into training and test sets:

```{python}
#| code-line-numbers: "1,3-6"
from sklearn.model_selection import train_test_split

# Split data: 70% training, 30% testing
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=30
)

# Check the sizes
print(f"Total data points: {len(X)}")
print(f"Training set: {len(X_train)} points ({len(X_train)/len(X):.1%})")
print(f"Test set: {len(X_test)} points ({len(X_test)/len(X):.1%})")
```

**Important:** `random_state=30` ensures reproducible results!

## Training and Evaluation {.smaller}

Now let's train on training data and evaluate on both sets:

```{python}
#| code-line-numbers: "1-3,6-9"
# Train on training data only
model_honest = LinearRegression()
model_honest.fit(X_train, y_train)

# Calculate RMSE on both sets
from sklearn.metrics import root_mean_squared_error
train_rmse = root_mean_squared_error(y_train, model_honest.predict(X_train))
test_rmse = root_mean_squared_error(y_test, model_honest.predict(X_test))

print(f"Training RMSE: ${train_rmse:.0f}")
print(f"Testing RMSE: ${test_rmse:.0f}")
print(f"Difference: ${test_rmse - train_rmse:.0f}")
```

**Business Insight:** The difference tells us how well our model will generalize to future data!

**Notice:** Test RMSE ($403) > Training RMSE ($330) - this is normal! Our model performs slightly worse on new data because it was optimized for the training set. However, the larger this difference becomes the more concern we should have on whether we have a good model or not.

# Putting It All Together {background="#43464B"}

## Complete Workflow: Start to Finish {.smaller}

Let's see the complete machine learning workflow using a fresh dataset:

```{python}
# Step 1: Import a dataset
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, root_mean_squared_error

# Load the classic Advertising dataset
advertising = pd.read_csv("../data/Advertising.csv")
print("Dataset shape:", advertising.shape)
advertising.head()
```

**Step 1 complete:** We have our data loaded and ready!

## Step 2: Split the Data {.smaller}

```{python}
#| code-line-numbers: "1-8"
# Step 2: Split data into features (X) and target (y), then train/test
X = advertising[['TV', 'radio', 'newspaper']]  # Multiple predictors
y = advertising['sales']  # Target variable

# Split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"Training set: {len(X_train)} samples")
print(f"Test set: {len(X_test)} samples")
```

**Step 2 complete:** Data is properly split for honest evaluation!

## Step 3: Train the Model {.smaller}

```{python}
#| code-line-numbers: "1-3"
# Step 3: Train our regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Look at our model coefficients
print("Model coefficients:")
for feature, coef in zip(X.columns, model.coef_):
    print(f"  {feature}: {coef:.3f}")
print(f"Intercept: {model.intercept_:.3f}")
```

**Step 3 complete:** Model is trained and ready to make predictions!

## Step 4: Evaluate Performance {.smaller}

```{python}
#| code-line-numbers: "1-9"
# Step 4: Evaluate model performance
train_predictions = model.predict(X_train)
test_predictions = model.predict(X_test)

# Calculate metrics
train_r2 = r2_score(y_train, train_predictions)
test_r2 = r2_score(y_test, test_predictions)
train_rmse = root_mean_squared_error(y_train, train_predictions)
test_rmse = root_mean_squared_error(y_test, test_predictions)

print("Performance Summary:")
print(f"Training R¬≤: {train_r2:.3f} | Training RMSE: {train_rmse:.2f}")
print(f"Test R¬≤: {test_r2:.3f} | Test RMSE: {test_rmse:.2f}")
```

**Complete workflow achieved!** üéâ We can now trust our model's performance estimates.

# What Else? {background="#43464B"}

## Additional Concepts You'll Learn {.smaller}

**We covered the core concepts, but there's more to explore in your readings:**

:::: {.columns}
::: {.column width="50%"}

- üìñ **Simple vs. Multiple Linear Regression**
   - When to use one predictor vs. many
   - Interpreting coefficients with multiple variables
- üìä **Categorical Variables with Dummy Encoding**
   - Converting categories (e.g., "Region") to numbers
   - Understanding baseline/reference groups
   
:::
::: {.column width="50%"}

- ‚öñÔ∏è **Underfitting vs. Overfitting**
   - Models that are too simple vs. too complex
   - Finding the "Goldilocks zone" of model complexity
- üíº **Business-Aligned Evaluation Metrics**
   - Choosing metrics that match business costs
   - When RMSE vs. MAE vs. MAPE makes sense

:::
::::

::: {.callout-tip}
**Bottom line:** Today gave you the foundation. The readings will deepen your understanding and show you more advanced techniques!
:::

# Key Takeaways {background="#43464B"}

## Key Takeaways

* **Correlation** ‚Äì Measures relationship strength but doesn't prove causation
* **Linear Regression** ‚Äì Provides prediction equations for business planning  
* **Model Evaluation** ‚Äì Essential for trusting your predictions in real business decisions
* **Train/Test Splits** ‚Äì The honest way to evaluate how models will perform on new data

**Remember:** A model that looks perfect on training data might be useless on new data!

## Connection to Thursday's Lab

::: {.callout-tip}
## This Week's Lab Preview

In Thursday's lab, you'll get hands-on practice with:

* Building regression models with real business data
* Calculating and interpreting evaluation metrics (R¬≤, RMSE, MAE)
* Using train/test splits to honestly evaluate model performance
* Connecting model results to actionable business recommendations

Come prepared to become a regression modeling expert!
:::

# Questions & Next Steps {background="#43464B"}

## Looking Ahead

**Next Tuesday:** Advanced machine learning concepts and ensemble methods

**Thursday Lab:** Hands-on regression modeling and evaluation practice

**Homework:** This week's Lab will serve as your homework!

## Any Final Questions?

* About correlation vs causation?
* About regression modeling for business?
* About model evaluation metrics?
* About Thursday's lab?

<br><br>

<center>
**See you Thursday for hands-on regression practice!**
</center>