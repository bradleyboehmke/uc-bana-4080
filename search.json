[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BANA 4080: Data Mining",
    "section": "",
    "text": "Welcome\nWelcome to BANA 4080: Introduction to Data Mining with Python. This course provides an immersive, hands-on introduction to the tools and techniques used in modern data science. You’ll learn how to explore, analyze, and model data using Python and gain practical experience through labs, projects, and real-world datasets.\nAlong the way you will develop core skills in data wrangling, exploratory data analysis, data visualization, and even key machine learning techniques such as supervised, unsupervised, and deep learning model. We’ll even take a quick detour into generative AI and large language models (LLMs). Throughout this process we’ll use real-world data and experiential learning to guide your learning.\nBy the end of the course, students will be able to:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#who-should-read-this",
    "href": "index.html#who-should-read-this",
    "title": "BANA 4080: Data Mining",
    "section": "Who should read this?",
    "text": "Who should read this?\nThis book is designed for upper-level undergraduate students who may have little to no prior programming experience but are eager to explore the world of data science using Python. It’s also an ideal resource for early-career professionals or students in analytics, business, or quantitative fields who are looking to upskill—whether by learning Python for the first time or by building a deeper understanding of how to explore, visualize, and model data. The content is structured to be accessible and hands-on, guiding readers step-by-step through the core tools and techniques used in modern data-driven problem solving.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-this-book-is-structured",
    "href": "index.html#how-this-book-is-structured",
    "title": "BANA 4080: Data Mining",
    "section": "How this book is structured",
    "text": "How this book is structured\nThis book is broken into 14 modules, each aligned with a week of instruction in the BANA 4080 course. Every module introduces key concepts or techniques in data science, combining concise explanations with interactive, hands-on code examples. Whether you’re reading independently or following along with the course, the modular structure makes it easy to work through the content at your own pace, week by week.\n\n\n\n\n\n\n\nModule & Topics\nSummary of Concepts Covered\n\n\n\n\n1. Fundamentals I\nCourse overview, coding environment setup, Python basics\n\n\n2. Fundamentals II\nUsing Jupyter notebooks, data structures, Python libraries\n\n\n3. Pandas DataFrames\nImporting data, DataFrame fundamentals, subsetting DataFrames\n\n\n4. Data Wrangling I\nCleaning, filtering, aggregating, and merging tabular data\n\n\n5. Data Wrangling II\nWorking with datetime, text data, and joining data like SQL\n\n\n6. Data Visualization\nCreating plots using matplotlib and seaborn, exploratory data analysis\n\n\n7. Writing Efficient Python Code\nControl flow, defining functions, loops, list comprehensions\n\n\n8. Introduction to Machine Learning\nOverview of ML, features/labels, train/test split, scikit-learn basics\n\n\n9. Unsupervised Learning\nClustering (k-means), PCA, dimensionality reduction, t-SNE visualization\n\n\n10. Supervised Learning\nRegression and classification models: linear, logistic regression\n\n\n11. Deep Learning & Neural Networks\nNeural networks using Keras; simple classification tasks\n\n\n12. Generative AI & Prompt Engineering\nWorking with LLMs, OpenAI API, prompt design, building AI agents\n\n\n13. Final Project Kickoff\nScoping and starting a capstone data science project\n\n\n14. Final Project Presentations & Wrap-Up\nPresenting project findings, course reflection, and next steps",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#conventions-used-in-this-book",
    "href": "index.html#conventions-used-in-this-book",
    "title": "BANA 4080: Data Mining",
    "section": "Conventions used in this book",
    "text": "Conventions used in this book\nThe following typographical conventions are used in this book:\n\nstrong italic: indicates new terms,\nbold: indicates package & file names,\ninline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user,\ncode chunk: indicates commands or other text that could be typed literally by the user\n\n\n1 + 2\n\n3\n\n\nIn addition to the general text used throughout, you will notice the following code chunks with images:\n\n\n\n\n\n\nSignifies a tip or suggestion\n\n\n\n\n\n\n\n\n\nSignifies a general note\n\n\n\n\n\n\n\n\n\nSignifies a warning or caution",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#software-used-throughout-this-book",
    "href": "index.html#software-used-throughout-this-book",
    "title": "BANA 4080: Data Mining",
    "section": "Software used throughout this book",
    "text": "Software used throughout this book\nThis book is built around an open-source Python-based data science ecosystem. While the list of tools evolves with the field, the examples and exercises in this book are designed to work with Python 3.x, currently using…\n\n\nCode\n# Display the Python version\nimport sys\nprint(\"Python version:\", sys.version.split()[0])\n\n\nPython version: 3.13.5\n\n\n…and are executed within Jupyter Notebooks, which provide an interactive, beginner-friendly environment for writing and running code.\nThroughout the modules, we use foundational Python libraries such as:\n\npandas and numpy for data wrangling and numerical computing,\nmatplotlib and seaborn for data visualization,\nscikit-learn and keras for machine learning and deep learning, and\nopenai and transformers for generative AI and large language model exploration.\n\nEach module explicitly introduces the relevant software and libraries, explains how and why they are used, and provides reproducible code so that readers can follow along and generate similar results in their own environment.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "BANA 4080: Data Mining",
    "section": "Additional resources",
    "text": "Additional resources\nTBD",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html",
    "href": "01-intro-data-mining.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Why are we here?\nWelcome to your journey into the world of data science. Whether you’re here out of curiosity, career aspirations, or because it’s a required course, you’re stepping into a field that blends logic, creativity, and curiosity to solve real-world problems using data. But let’s be honest—if you’ve never written a line of code before, the idea of learning Python or building machine learning models might feel a bit intimidating. You might even be wondering: “Why do I need to learn this when tools like ChatGPT or Copilot can just write code for me?” That’s a fair question — and one we’ll unpack in this chapter. Our goal is to give you a clear picture of why learning these foundational skills still matters, why Python is at the heart of modern data science, and how to approach the learning process with the right expectations and mindset.\nBy the end of this chapter, you will:\nLet’s start with a story.\nImagine you’re a college junior named Taylor who just landed a summer internship at a local marketing analytics firm. On your first day, your manager hands you a file — it’s a messy spreadsheet filled with customer purchase data — and says, “We’re trying to understand what drives repeat purchases. Can you dig into this and see what you find?”\nTaylor freezes.\nYou’ve taken calculus, stats, and maybe even a regression course. You’ve know Excel, used it to build a few dashboards in your business classes, and maybe dabbled in a little VBA. You know how to analyze formulas on paper and interpret statistical summaries. You’ve studied how marketing campaigns influence consumer behavior, how pricing affects demand, how supply chains function, and how companies generate profits.\nBut now, faced with a real dataset and a vague question, you’re not sure where to begin.\nYou know there’s a story in the data; something important about customer behavior that the business wants to uncover. But without a roadmap, the numbers feel more overwhelming than enlightening.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#why-are-we-here",
    "href": "01-intro-data-mining.html#why-are-we-here",
    "title": "1  Introduction",
    "section": "",
    "text": "Do you start sorting columns manually?\nWrite an IF statement?\nAsk ChatGPT to explain the data?\nLook for a “correlation” button in Excel?\n\n\n\nYou’ve built the foundation—now it’s time to apply it.\nThat feeling Taylor has? That’s not a sign of being unprepared, it’s a sign of the gap that exists between traditional classroom learning and the real-world application of data science.\nYou already know more than you think:\n\nYou understand business operations, customer behavior, and the bottom-line goals companies care about.\nYou’ve studied quantitative methods and statistical inference.\nYou know how to think critically and ask good questions.\n\nWhat you haven’t had (yet) is the experience of turning raw data into actionable insight using tools like Python to clean, explore, visualize, and model information in a repeatable, scalable way.\nThat’s where this course comes in.\n\n\nThis course is about doing.\nWe’re going to close the gap between the theory you’ve learned and the practice that’s expected in today’s data-driven workplace. You’ll work with messy datasets just like Taylor’s. You’ll write code to clean, organize, and analyze real data. And you’ll build up your own toolkit so that, the next time someone asks you to “see what you can find,” you won’t freeze — you’ll get to work.\nAnd yes, we’ll even talk about when to use tools like ChatGPT to help you along the way (and when not to).\n\n\n\n\n\n\nBy the end of this course, you’ll have the confidence and experience to tackle open-ended data problems, the skills to automate and scale your work, and the mindset to keep growing as a data-driven problem solver.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#isnt-ai-supposed-to-do-all-this-for-us-now",
    "href": "01-intro-data-mining.html#isnt-ai-supposed-to-do-all-this-for-us-now",
    "title": "1  Introduction",
    "section": "1.2 Isn’t AI supposed to do all this for us now?",
    "text": "1.2 Isn’t AI supposed to do all this for us now?\nLet’s address the question that’s probably been lingering in your mind since the moment you signed up for this course:\n\n“Why do I need to learn how to code when tools like ChatGPT, Copilot, and Claude can just do it for me?”\n\nIt’s a fair question.\nIn fact, you may have already used one of these tools to generate code. Maybe you asked ChatGPT to “write a Python script that reads a CSV file and finds the average,” and in seconds — boom — you had something that worked.\nSo… case closed, right? Not quite.\n\nAI is helpful—but it’s not a substitute for understanding.\nGenerative AI tools are incredible accelerators. They can save time, reduce friction, and provide helpful starting points. But they’re not magic. They don’t know your data. They don’t understand the business context. And they certainly don’t guarantee correct answers.\nThese tools work by predicting the next most likely word or line of code based on patterns in data they’ve seen. They don’t reason like humans. They can’t debug your logic or decide which metric is appropriate for your analysis. And sometimes? They just make stuff up.\n\n\n\n\n\n\nWarningCallout: It’s like autocorrect but for code!\n\n\n\n\n\nIf you’ve ever had your phone turn “on my way!” into “omg my weasel!” — you already understand how these tools work.\nGenAI models, like ChatGPT and Copilot, are basically super-powered autocomplete engines. They’re predicting what comes next based on what they’ve seen before. That means they can sometimes nail it… and other times leave you wondering, “What were you even trying to say?”\n\nJust like with autocorrect, the key is knowing when the suggestion is helpful—and when to hit backspace.\n\n\n\nIf you don’t have the foundational skills, you won’t know when they’re wrong—or worse, when they’re subtly wrong.\n\n\nAI tools are assistants, not autopilots.\nLearning how to code and analyze data yourself is what allows you to:\n\nSpot mistakes in generated code\nAsk better questions (better prompts = better results)\nCustomize and build on what AI gives you\nEvaluate whether an approach is valid or helpful\nExplain what your analysis means and how it should be used\n\nYou don’t need to fear these tools. But you also don’t want to depend on them blindly. This course will teach you the skills needed to use AI as a powerful assistant — not a crutch.\n\n\n\n\n\n\nNoteStudent Reflection Prompt\n\n\n\n\n\nHave you ever used a tool like ChatGPT, Claude, or Copilot to generate code or solve a problem?\n\nWhat did it get right?\nWhat did it miss?\nHow confident were you in the result?\n\n\n\n\nIn the chapters ahead, we’ll sometimes bring GenAI tools into the learning process, especially as helpers for things like brainstorming or debugging. But we’ll always emphasize the importance of understanding the code you’re working with. Otherwise, you’re just copying answers without learning anything—which is like using a calculator to solve a math problem you never actually learned.\nAnd trust us: if you ever find yourself in Taylor’s shoes—staring at a spreadsheet and needing to figure out what matters—you’ll want more than a chatbot. You’ll want skill.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#why-python-and-why-now",
    "href": "01-intro-data-mining.html#why-python-and-why-now",
    "title": "1  Introduction",
    "section": "1.3 Why Python, and why now?",
    "text": "1.3 Why Python, and why now?\nSo far, we’ve talked about why it’s important to build foundational skills and not overly rely on AI tools. But that probably leaves you with another question:\n\n“Out of all the programming languages out there, why are we learning Python?”\n\nIt’s a good question — and the answer is simple: Python is the language of modern data science.\n\nPython is beginner-friendly, but not just for beginners.\nPython was designed to be easy to read and write. Its syntax is clean and (mostly) intuitive, which means you won’t spend hours trying to remember obscure symbols or puzzling over where the semicolon went. That makes it a great first language for students who are new to programming.\nBut don’t mistake simplicity for lack of power. Python is also used by:\n\nData scientists at companies like Google, Netflix, and Spotify\nAnalysts at healthcare organizations, banks, and nonprofits\nResearchers running simulations and training machine learning models\nEngineers building large-scale data pipelines and AI tools\n\nSo while Python is accessible to beginners, it’s also respected and widely used by professionals. In fact, according to the 2023 Stack Overflow Developer Survey, Python ranks as one of the top three most commonly used programming languages overall, and is consistently a favorite among developers working in data science, machine learning, and academic research. Its combination of readability, power, and a rich ecosystem of libraries makes it a go-to language across industries and roles.\n\n\nIt’s not just the language, it’s the ecosystem.\nPython has a massive collection of libraries and packages built specifically for data work. Here are just a few you’ll get to know in this course but realize there are tens of thousands of open source Python libraries that you can use for various data science tasks:\n\npandas – for working with data tables\nnumpy – for efficient numerical computation\nmatplotlib and seaborn – for data visualization\nscikit-learn – for building machine learning models\n\nThis ecosystem means you won’t have to build everything from scratch. You’ll learn to stand on the shoulders of open-source giants so you can focus more on solving problems and less on reinventing the wheel.\n\n\nPython is your toolset. This course is your training ground.\nIn this course, you’ll learn how to:\n\nWrite Python code that manipulates and explores real datasets\nUse libraries like pandas and matplotlib to summarize and visualize your data\nBuild your first machine learning models with tools like scikit-learn\nGain confidence in reading and modifying code — whether it came from a textbook, a blog post, or a GenAI assistant\n\nBy the end of this course, Python won’t feel like some intimidating, foreign technical concept you’ve been avoiding or unsure how to approach. It will become a practical skill—a tool you know how to use to explore data, uncover insights, and drive real decisions. And here’s the best part: you’ll be learning the same tool that analysts, data scientists, and business teams across your future organization are already using. Instead of feeling left out of the conversation, you’ll be equipped to lead it.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#learning-to-code-a-reality-check",
    "href": "01-intro-data-mining.html#learning-to-code-a-reality-check",
    "title": "1  Introduction",
    "section": "1.4 Learning to code: a reality check",
    "text": "1.4 Learning to code: a reality check\nLet’s be real for a minute.\nLearning to code can be frustrating at first. It’s not always fun to stare at an error message that makes zero sense. It’s even less fun when you fix that error… only to get a brand-new one. You might feel stuck, confused, or like everyone else “gets it” but you.\nThat’s normal. In fact, it’s expected.\n\nLearning to code is like learning a new language (because it is)\nWhen you’re first learning a spoken language, you stumble over basic phrases. You forget vocabulary. You mess up grammar. But over time, with practice, you start to think in the new language — and eventually, it just clicks.\nCoding is the same way. You’ll start by copying examples and Googling error messages. That’s fine. That’s part of the process. Over time, those patterns will stick. You’ll stop memorizing and start thinking in code.\n\n\nThis course is designed for beginners\nYou don’t need prior programming experience to succeed here. We’ll start from the very beginning: writing simple statements, understanding variables and data types, building up to loops, functions, and eventually full data science workflows.\nYou’ll go from:\n\n“What’s a variable?”\n\nto…\n\n“I just built a machine learning model to predict customer churn.”\n\nStep by step. Week by week. You’ll get there. And don’t feel like you have to go it alone.\n\n\n\n\n\n\nTipLean on your classmates.\n\n\n\n\n\nIf you’re stuck, chances are someone else is too. Use the classroom discussion board to ask questions, share what you’re learning, and help each other out. This is a collaborative course, and we’re building a community of learners who grow together.\n\n\n\n\n\nIt’s okay to use AI tools but don’t skip the struggle\nYou’ll be encouraged to use tools like ChatGPT, Copilot, and Claude throughout this course. But here’s the rule: use them to learn, not to avoid learning.\nThink of these tools as tutors, not answer keys:\n\nUse them to check your understanding\nAsk them for help when you’re stuck\nCompare their answers to your own and figure out the differences\nBut always make sure you understand what the code is doing\n\nShortcuts don’t help if you skip the part where you build skill.\nLearning to code is like learning anything meaningful—it takes effort, patience, and a little bit of resilience. You won’t get everything right the first time, and that’s okay. You don’t need to be perfect. You just need to keep showing up and keep trying.\nAnd we’ll be right here to help every step of the way.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#wrap-up",
    "href": "01-intro-data-mining.html#wrap-up",
    "title": "1  Introduction",
    "section": "1.5 Wrap-Up",
    "text": "1.5 Wrap-Up\nYou’ve made it through your first chapter — nicely done!\nWe’ve talked about why data science matters, why Python is the language we’re using, and why learning these skills (even in the age of AI) still gives you a powerful edge. We’ve also tried to set realistic expectations: learning to code can be challenging, but it’s absolutely doable—with patience, practice, and support from your classmates, instructors, and tools like ChatGPT when used wisely.\nThe big takeaway? You’re not here to memorize formulas or passively watch someone else code. You’re here to build real skills that will help you ask better questions, explore real-world data, and create insights that drive decisions.\nAnd we’re not wasting any time. In the next chapter, we’ll jump right in — you’ll get your Python environment set up and write your first lines of code. This is where the hands-on part of the journey begins.\nThis course moves fast, so buckle up and enjoy the ride. Let’s get started!",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#exercise-can-you-help-taylor",
    "href": "01-intro-data-mining.html#exercise-can-you-help-taylor",
    "title": "1  Introduction",
    "section": "1.6 Exercise: “Can You Help Taylor?”",
    "text": "1.6 Exercise: “Can You Help Taylor?”\n\n\n\n\n\n\nNoneScenario\n\n\n\n\n\nYou’re stepping into Taylor’s shoes. You’ve just been handed two datasets by your manager at a local marketing analytics firm:\n\nTaylor’s Messy Retail Data — individual transactions from various customers, but… it’s messy.\nCustomer Info Data — customer-level information including age, gender, signup date, and loyalty tier.\n\nYour job is to begin exploring what factors might be influencing repeat purchases. But before you can even analyze anything, you’ll need to clean, combine, and make sense of the data.\nThroughout this exercise your objective is to:\n\nInspect and describe real-world “messy” data\nIdentify issues that must be addressed before analysis\nStrategize a plan for cleaning and joining datasets\nExperiment with using ChatGPT to assist your data work\nReflect on how foundational skills + GenAI can be used together\n\n\n\n\n\n\n\n\n\n\nNonePart 1 – Get Oriented\n\n\n\n\n\nStart by opening Taylor’s Messy Retail Data and take a few minutes to explore it.\nQuestions:\n\nWhat’s messy or confusing about this data?\nWhat would your first 2–3 steps be if you were asked to clean or prepare it for analysis?\nWhat do you notice about the Cust_ID and PurchaseDate columns specifically?\nBased on this table alone, what kinds of analyses might you try to do?\n\n\n\n\n\n\n\n\n\n\nNonePart 2 – Meet the Second Dataset\n\n\n\n\n\nNow open the Customer Info Data.\nQuestions:\n\nWhat types of information does this dataset contain that might help you understand customer behavior?\nHow would you combine this with Taylor’s Messy Retail Data?\nWhat issues do you foresee when trying to join them?\n\n\n\n\n\n\n\n\n\n\nNonePart 3 – Ask for Help (But Be Smart About It)\n\n\n\n\n\nUse ChatGPT (or any GenAI tool you’re comfortable with) to begin cleaning or analyzing the data.\nInstructions:\n\nTry writing 2–3 different prompts to help with basic data cleaning, joining, or summarizing the data.\nTry at least one prompt that combines both datasets.\n\nThen reflect:\n\nWhat prompts did you use?\nDid the AI return usable code?\nWere there any errors, misunderstandings, or surprises?\nDid you understand what the code was doing? If not, what helped you figure it out?\n\n\n\n\n\n\n\n\n\n\nNonePart 4 – Pull It All Together\n\n\n\n\n\nWrite a short summary (5–7 sentences) answering the following:\n\nWhat was your biggest takeaway from working with these datasets?\nHow did this exercise reinforce the ideas from Chapter 1?\nDid this change your perspective on what your role is when using AI tools like ChatGPT in data analysis?\nHow do you feel about starting your journey into Python after completing this activity?",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html",
    "href": "02-preparing-for-code.html",
    "title": "2  Setting Up Your Python Environment",
    "section": "",
    "text": "2.1 Hello World\nBefore we dive deep into the world of data science, we need to get you set up with the tools of the trade. Just like a carpenter needs a good workbench and set of tools, a data scientist needs a place to write and run code. The good news? There are several ways to do this—and even better, there’s no one-size-fits-all answer. You’ll get to choose the approach that works best for you.\nIn this chapter, we’ll introduce three common ways to set up a Python environment: Google Colab, the Anaconda distribution, and Visual Studio Code. We’ll start with the path of least resistance (Colab), then explore more powerful and flexible setups as we go.\nWe’ll kick things off by writing your very first Python program—a classic:\nBy the end of this chapter, you’ll not only have a working Python environment, but you’ll also understand the pros and cons of each approach and know how to get started coding in any of them. Let’s get your hands on the keyboard.\nWelcome to your first moment of writing real Python code. We’re not going to lecture you about the perfect setup right away. Instead, let’s just do something. This short activity gives you a quick win and shows you how easy it can be to get started with Python—no installations or configurations required.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html#hello-world",
    "href": "02-preparing-for-code.html#hello-world",
    "title": "2  Setting Up Your Python Environment",
    "section": "",
    "text": "Running Python in Google Colab\nGoogle Colab is a free, cloud-based tool that lets you run Python code in your browser. It requires no setup and is perfect for getting started.\n\nGo to: https://colab.research.google.com/\nSign in with your Google account (you’ll need one).\nClick on “New Notebook.”\nIn the cell that appears, type:\nprint(\"Hello World\")\nPress Shift + Enter to run the cell.\n\nYou should see:\nHello World\nYou just wrote and ran your first line of Python code! 🎉🎉🎉\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneExample Notebook\n\n\n\nLet’s put your new skills to the test by working through a simple example notebook.\n📓 We’ve created a sample notebook that introduces:\n\nThe print() function\nHow to format and write some basic text with Markdown\nHow to combine code and notes in one place\n\n👉 Click the “Open in Colab” link at the top of the notebook to launch it in Google Colab and run through the notebook.\n\n\n\n\nTry This: Generate Python Code with AI\nAfter you run your print(\"Hello World\"), take a moment to explore the built-in AI assistant in Google Colab. You might notice a prompt that says:\n\n“Start coding or generate with AI”\n\nThis is your chance to see how AI tools can help you write Python code. Try typing in a prompt or click on the suggestion box to let Colab help generate code for you. You can run the code to see what it does—and even tweak it to make it your own.\nFeel free to come up with your own prompt, or try one of these to get started:\n\n“Write Python code to compute the area of a 12-inch pizza.”\n“Write Python code to find all prime numbers between 2 and 100.”\n“Write a Python program that asks for a user’s name and prints a greeting.”\n\n\n\n\n\n\n\nNoneReflect: Your First Python Experience\n\n\n\n\n\nTake a few minutes to reflect on your first hands-on experience writing Python code and using an AI assistant. You can jot your answers in a notebook, a note-taking app, or even directly in your Colab notebook using a text cell (don’t know how to do this, that’s ok, ask Colab’s AI to help 😉).\nConsider the following questions:\n\nWas writing and running your first lines of Python code easier or harder than you expected?\nHow did it feel to use the AI assistant to generate code? Do you think tools like this can make you more productive? How confident are you that the code was actually correct?\nGoogle Colab makes it easy to get started, but do you think this is the kind of environment you would use at work? Why or why not?\n\nYou don’t need to write a long answer—just a few thoughtful sentences to capture your perspective at this early stage in your learning journey.\n\n\n\nNext, we’ll step back and look at what just happened. Then, we’ll dive into the different ways you can set up your Python environment, from easy to advanced.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html#development-environments",
    "href": "02-preparing-for-code.html#development-environments",
    "title": "2  Setting Up Your Python Environment",
    "section": "2.2 Development Environments",
    "text": "2.2 Development Environments\nNow that you’ve written and run your first line of Python code, it’s time to step back and understand where that code actually runs—and what your options are going forward.\nIn programming, the place where you write and run your code is called your development environment. Think of it like your digital workspace: it includes the tools, interface, and underlying systems that allow you to code, debug, and manage projects.\nThere’s no single “best” environment—just the one that’s best for your current needs. In this course, you’ll be exposed to three common Python environments used by data scientists, ranging from beginner-friendly to professional-grade. Each has its own strengths and trade-offs, and over time you may find yourself using all three depending on the situation.\nBelow is a quick overview of the three environments we’ll focus on in this course.\n\nGoogle Colab\nYou’ve already used this! Colab is a cloud-based environment that lets you run Python in your browser, no installation required. It’s perfect for beginners or anyone who wants to start quickly and painlessly.\nPros:\n\nNo installation needed—works in your browser.\nBuilt-in support for Jupyter notebooks.\nEasy to share and collaborate via Google Drive.\nIncludes access to an AI assistant to help generate code.\n\nCons:\n\nRequires an internet connection.\nLimited access to your local files or custom setups.\nNot commonly used in professional, production-level environments.\n\n\n\n\n\n\n\nWhile Colab is excellent for learning, prototyping, and sharing code, it isn’t typically used in workplace settings—especially for production code, version-controlled projects, or large-scale data workflows. Because of this, we encourage you to use Colab to get started quickly, but strive to set up one of the other environments (Anaconda or VS Code) as you progress through the course. These tools will better reflect the development environments you’re likely to use in internships, co-ops, or full-time roles\n\n\n\n\n\nAnaconda Distribution\nAnaconda is a local setup that installs Python along with most of the libraries and tools used in data science, including Jupyter Notebook and JupyterLab. It’s an excellent next step for learners who want more control while still keeping things simple.\n\n\n\n\n\n\nAnaconda\n\n\n\n\nFigure 2.1: Anaconda is one of the most widely used platforms for data science, offering an all-in-one distribution of Python, Jupyter, and essential packages for analysis and machine learning.\n\n\n\nPros:\n\nAll-in-one installation of Python + essential libraries.\nUser-friendly interface via Anaconda Navigator.\nIncludes JupyterLab for notebook-based development.\nWorks locally without internet access.\n\nCons:\n\nLarge download and install size (~3 GB).\nYou’ll need to learn how to manage packages and environments with Conda.\nSlightly more complex than Colab, but still beginner-friendly.\n\n\n\nVisual Studio Code (VS Code)\nVS Code is a lightweight but powerful code editor used by professional developers and data scientists. With the right extensions, it supports Python and Jupyter notebooks and is ideal for building larger or more complex projects.\n\n\n\n\n\n\nVS Code is my preferred environment for writing code, so throughout this course, most of the code you’ll see during lectures and demos will be displayed using VS Code. This will give you exposure to an industry-standard tool that’s commonly used across the kinds of organizations you’re likely to intern or work at.\n\n\n\n\n\n\n\n\n\nVS Code\n\n\n\n\nFigure 2.2: Visual Studio Code (VS Code) is one of the leading code editors used by data scientists and software developers. It’s widely adopted across industry and is likely the primary development environment you’ll encounter in internships or full-time roles.\n\n\n\nPros:\n\nHighly customizable and fast.\nGreat for real-world development workflows.\nIntegrated support for version control (Git), debugging, and extensions.\n\nCons:\n\nSteeper learning curve—more setup required.\nRequires separate installations of Python, extensions, and Jupyter support.\nBest suited for students who want to grow into more advanced tools.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html#choosing-your-path-forward",
    "href": "02-preparing-for-code.html#choosing-your-path-forward",
    "title": "2  Setting Up Your Python Environment",
    "section": "2.3 Choosing Your Path Forward",
    "text": "2.3 Choosing Your Path Forward\nHow you decide to move forward from here is up to you. For now, you’re welcome to continue using Google Colab, especially if it’s helping you build confidence and get comfortable writing Python code without worrying about software setup.\nThat said, by the end of this course, you’ll be expected to have a local Python development environment set up on your computer—either using Anaconda or Visual Studio Code. These environments are more reflective of what you’ll use in real-world internships, co-ops, or full-time roles, and they’ll give you greater flexibility and power as your projects grow in complexity.\nWhen you’re ready to make the leap, use the resources below to guide you:\n\nAnaconda Installation: Appendix 12\nVS Code Installation: Appendix 13\n\nYou don’t need to switch immediately, but the sooner you get comfortable working locally, the better prepared you’ll be for the kinds of tasks data scientists regularly tackle in the field.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html#exercise-pick-your-environment-and-make-it-yours",
    "href": "02-preparing-for-code.html#exercise-pick-your-environment-and-make-it-yours",
    "title": "2  Setting Up Your Python Environment",
    "section": "2.4 Exercise: Pick Your Environment and Make It Yours",
    "text": "2.4 Exercise: Pick Your Environment and Make It Yours\nChoose one of the following paths based on your current comfort level and curiosity. Your goal is to take one step toward becoming confident in your development environment.\n\n\n\n\n\n\nNoneOption 1: Stay with Colab (for now)\n\n\n\n\n\n\nOpen a new Colab notebook.\nUse the AI assistant to generate and run a short piece of Python code.\nTry modifying the code and run it again to see how it changes.\nAdd a Markdown cell where you reflect on:\n\nWhat the code does.\nWhat you changed.\nWhat you’re still unsure about.\n\n\nNote: Not sure what a Markdown cell is? Great opportunity to Google or ask ChatGPT! But don’t worry, we’ll discuss this more next week.\n\n\n\n\n\n\n\n\n\nNoneOption 2: Install Anaconda\n\n\n\n\n\n\nFollow the instructions in appendix 12 to install Anaconda and launch JupyterLab.\nCreate a new notebook and run print(\"Hello from Anaconda!\").\nTake a screenshot of your working notebook or write a short note on how it went.\n\n\n\n\n\n\n\n\n\n\nNoneOption 3: Set Up VS Code\n\n\n\n\n\n\nFollow the instructions in appendix 13 to install VS Code and set up Python and Jupyter support.\nOpen a Jupyter notebook and run print(\"Hello from VS Code!\").\nExplore the editor: try renaming a file, changing themes, or opening the terminal.\nWrite a brief reflection on what you liked or didn’t like about this environment.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html",
    "href": "03-python-basics.html",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "",
    "text": "3.1 Try It Before We Start!\nIf you’re new to Python or programming in general, welcome—you’re in the right place. This chapter is where we start turning ideas into code. Just like learning a new language starts with learning a few key words and phrases, learning Python begins with understanding how to work with basic building blocks: numbers, text, and data containers.\nWhy is this important? Because whether you’re analyzing customer data, building a machine learning model, or automating a task at work, you’ll rely on these foundational concepts every single time you write code. Think of this chapter as your toolbox—it may seem simple now, but you’ll keep coming back to these tools throughout your data science journey.\nAnd don’t worry if it doesn’t all click right away. Everyone struggles with syntax or logic at first. Learning to code is more like learning to solve puzzles than memorizing rules. Take your time, experiment, and remember: errors are part of the process (even experienced coders Google stuff constantly).\nBy the end of this chapter, you’ll be able to:\nLet’s dive in.\nLet’s write a few lines of Python code. Run the following in a Python environment (like Google Colab, Jupyter Notebook, or VS Code) and see what happens:\n# Store your name as a string\nname = \"Taylor\"\n\n# Store your age as a number\nage = 22\n\n# Print a personalized message\nprint(\"Hi \" + name + \"! You are \" + str(age) + \" years old.\")\n\n# Calculate how many years until you turn 30\nyears_to_30 = 30 - age\nprint(\"You'll turn 30 in \" + str(years_to_30) + \" years.\")\n\nHi Taylor! You are 22 years old.\nYou'll turn 30 in 8 years.\nWhat just happened?\nDon’t worry if this isn’t totally clear yet—that’s what this chapter is for. We’ll break it all down step by step.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#try-it-before-we-start",
    "href": "03-python-basics.html#try-it-before-we-start",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "",
    "text": "You stored information using variables\nYou worked with different data types (a string and a number)\nYou used basic math and printed a personalized message",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#python-data-types",
    "href": "03-python-basics.html#python-data-types",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.2 Python Data Types",
    "text": "3.2 Python Data Types\nEverything in Python is an object, and every object has a type. A data type tells Python what kind of value you’re working with—whether it’s a number, a piece of text, or a simple True/False flag. Understanding Python’s core data types is a fundamental step toward writing useful programs.\nLet’s walk through a quick introduction to the most common data types you’ll encounter early on. As we progress through this class and book, you’ll explore many more ways to work with and manipulate these data types.\n\n\nNumeric Types\nPython has two main types of numbers:\n\nint (integers): whole numbers like 5, 42, or -100\nfloat (floating-point): numbers with decimals like 3.14, 0.5, or -2.718\n\nYou can do all kinds of math with numbers in Python using simple operators:\n\n10 + 3.5   # Addition\n\n13.5\n\n\n\n10 * 3.5   # Multiplication\n\n35.0\n\n\nGo ahead and run these lines of code in your notebook:\n# Basic operations\n10 - 3.5   # Subtraction\n10 * 3.5   # Multiplication\n10 / 3.5   # Division (always returns a float)\n\n# More math\n10 ** 3.5  # Exponentiation (10^2 = 100)\n10 // 3.5  # Integer division (result is an int)\n10 % 3.5   # Modulus (remainder)\n\n\n\n\n\n\nWarningHeads Up: Python Math Isn’t Always Perfect\n\n\n\nSometimes, Python (like all programming languages) can give results that look a little… off. This usually happens because computers represent decimal numbers using floating-point approximation, which isn’t always exact.\nFor example, you’d expect the following to equal 0.3 but instead…\n\n0.1 + 0.1 + 0.1\n\n0.30000000000000004\n\n\nThis tiny difference is due to how numbers like 0.1 are stored in computer memory. It’s a common issue when doing calculations with decimal values, especially in financial or scientific applications. As we progress, you’ll learn best practices to handle imperfections like this. For now, you could just round it.\n\nround(0.1 + 0.1 + 0.1, ndigits=2)\n\n0.3\n\n\n\n\n\n\nStrings\nA string is a sequence of characters, enclosed in single or double quotes - whichever you use is personal preference but the output in the Python environment will contain single quotes:\n\n\"Hello Taylor!\"\n\n'Hello Taylor!'\n\n\nStrings can be combined and manipulated in many ways:\n\n# Concatenation\n\"Hello\" + \" \" + \"Taylor!\"   # Hello Taylor!\n\n'Hello Taylor!'\n\n\nGo ahead and run these lines of code in your notebook:\n# String repetition\n\"ha\" * 3   # 'hahaha'\n\n# Get first letter (starts at 0)\n\"Taylor\"[0] # 'T'\n\n# Get first three letters\n\"Taylor\"[:3]   # 'Tay'\nStrings are extremely common—you’ll use them to label, format, and present data in readable ways.\n\n\n\n\n\n\nIn fact, as organizations increasingly rely on data-driven decision-making, a significant portion of their data comes in the form of text. Examples include product descriptions, customer feedback, social media posts, and even log files. Understanding how to work with strings is essential for extracting insights and making sense of this unstructured data.\n\n\n\n\n\nBooleans\nA Boolean is a special data type with only two values: True and False.\nis_raining = True\nhas_umbrella = False\n\n\n\n\n\n\nIn Python, True and False are capitalized. Writing true or false (lowercase) will result in a NameError. Always ensure proper capitalization when working with booleans.\n\n\n\nYou’ll mostly use booleans when you write logical conditions, such as comparing values or checking if something is true. Don’t worry, we’ll discuss comparison operators (i.e. &gt;, ==) in a moment.\n\n5 &gt; 3      # True - 5 is greater than 3\n\nTrue\n\n\n\n5 == 3   # False - 5 does not equal 3\n\nFalse\n\n\nGo ahead and run these lines of code in your notebook:\n10 &lt; 9\n10 &gt; 9\n10 &lt;= 9\n10 &gt;= 9\n10 == 10\n\n\nType Checking and Conversion\nYou can check the type of any object using the type() function:\n\nprint(type(10))        # &lt;class 'int'&gt;\nprint(type(\"hello\"))   # &lt;class 'str'&gt;\n\n&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n\n\n\n\n\n\n\n\nTipA Quick Note on print()\n\n\n\n\n\nWhen working in a Jupyter notebook, Python will automatically display the result of the last line in a cell if it’s a value (like a string or number). For example:\n\ntype(10)        \ntype(\"hello\")   # will only show the output of this last line\n\nstr\n\n\nBut in most real Python programs—like scripts, functions, or when running multiple lines—you need to use print() to explicitly display something to the user:\n\nprint(type(10))        # will print print out this output...\nprint(type(\"hello\"))   # and this output\n\n&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n\n\nUsing print()` is a way to say, “Hey Python, show this to me in the output.” You’ll use it all the time for debugging, building interfaces, or just (as in this case) just to tell Python to display the output of each line of code.\n\n\n\nYou can also convert between types using built-in functions. For example, the following converts the integer value of 5 to a string '5'.\n\nstr(5) # '5' (string to int)\n\n'5'\n\n\nAnd the following converts a string '5' to an integer 5:\n\nint('5') # 5 (int to string)\n\n5\n\n\nGo ahead and run these lines of code in your notebook:\nprint(type(3.5))     # &lt;class 'float'&gt;\nprint(type(True))    # &lt;class 'bool'&gt;\nprint(str(3.14))     # '3.14' (convert decimal to string)\nprint(bool(0))       # False (0 is treated as False)\nprint(float(2))      # 2.0 (convert integer to decimal)\nThese conversions come in handy when working with user input or cleaning messy data.\n\n\nKnowledge check\nWork through the following tasks in your notebook.\n\n\n\n\n\n\nNone🍕 What’s the best deal?\n\n\n\n\n\nA 12-inch pizza costs $8. Use the formula for the area of a circle (\\(A = \\pi × r^2\\)) to calculate the cost per square inch of the pizza.\nHints:\n\nRadius (\\(r\\)) is half the diameter\nUse 3.14159 as your approximation for \\(\\pi\\)\nDivide the price by the area to get cost per square inch\n\nNow repeate for a 15-inch pizza that costs $12. Which is a better deal?\n\n\n\n\n\n\n\n\n\nNonePlay with ‘strings’\n\n\n\n\n\nFirst, guess what each line of code will result in. Then run them in your notebook. Were the results what you expected?\nprint(\"Python\" + \"Rocks\")\nprint(\"ha\" * 5)\nprint(\"banana\"[1])\nprint(\"banana\"[::-1])   # Can you guess what this does?\nExtra challenge: Can you use slicing to print just the word \"ana\" from \"banana\"?\n\n\n\n\n\n\n\n\n\nNone🕵🏻‍♂️ Data type detective\n\n\n\n\n\nBefore you run the following, what do you think the data types are for each line? Then, run the code in your notebook to check your answers. Were your predictions correct?\nprint(type(\"True\"))\nprint(type(True))",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#variables-and-the-assignment-operator",
    "href": "03-python-basics.html#variables-and-the-assignment-operator",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.3 Variables and the Assignment Operator",
    "text": "3.3 Variables and the Assignment Operator\nIn the last section, we did a lot of math using the same numbers — 10 and 3.5 — over and over again:\n# Basic operations\n10 - 3.5\n10 * 3.5\n10 / 3.5\nThat works fine for short examples, but imagine writing a program that needs to use the same values in dozens of different places. What if you want to change one of those values later? You’d have to find and update every instance in your code.\nThat’s where variables come in.\n\nWhat Is a Variable?\nA variable is a name that refers to a value. You can think of it like labeling a container that holds something useful—like a number, a word, or even a list of things.\nHere’s how we could rewrite the examples above using variables:\nx = 10\ny = 3.5\n\nprint(x - y)\nprint(x * y)\nprint(x / y)\nMuch cleaner, right?\n\n\n\n\n\n\nImportantWhy Use Variables?\n\n\n\nVariables help you:\n\nAvoid repeating values\nMake your code easier to read and maintain\nUpdate values in one place instead of many\n\nAnd when your programs get more complex, variables become essential for storing user input, results from calculations, or intermediate steps in a data analysis.\n\n\n\n\nThe Assignment Operator: =\nTo create a variable, we use the assignment operator (=). This tells Python to take the value on the right and assign it to the name on the left:\n\ngreeting = \"Hello, world!\"\n\nThis means - store the string \"Hello, world!\" in a variable called greeting\nYou can then reuse that variable:\n\nprint(greeting)\n\nHello, world!\n\n\n\n\n\n\n\n\nIn Python, = does not mean “equal to” like in math. It’s an instruction: assign the value.\n\n\n\n\n\nNaming Variables\nHere are basic rules for naming variables in Python:\n\n✅ Must start with a letter (or an underscore _ as in _name, though that’s typically reserved for special cases—so avoid starting with _ unless you know what you’re doing)\n✅ Can include letters, numbers, and underscores\n🚫 Cannot start with a number\n🚫 Cannot use built-in Python keywords (like if, True, print, etc.)\n\nSome good examples:\nage = 25\nstudent_name = \"Taylor\"\nis_logged_in = True\nUse descriptive names when possible—it makes your code easier for others (and future-you) to understand.\n\n\nReassigning Variables\nVariables can change! When you assign a new value to an existing variable, it overwrites the old one:\n\nx = 5\nx = x + 1  # Now x is 6\n\nprint(x)\n\n6\n\n\nPython always uses the most recent value.\n\n\nYou Can Store Any Type of Value\nYou can assign any data type to a variable—numbers, strings, booleans, and more:\nname = \"Taylor\"           # string\ngpa = 3.85                # float\nis_honors_student = True  # boolean\nAnd Python is flexible — you can even change what type a variable holds:\ngpa = \"3.85\"  # Now it's a string!\nThis is called dynamic typing, and it’s part of what makes Python beginner-friendly.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry It!\n\n\n\n\n\nLet’s return to our earlier example from the start of the chapter:\n# Store your name and age\nname = \"Taylor\"\nage = 22\n\n# Print a custom message\nprint(\"Hi \" + name + \"! You are \" + str(age) + \" years old.\")\n\n# Do a little math\nprint(\"You'll turn 30 in \" + str(30 - age) + \" years.\")\nTry updating the values of name and age to reflect your info. Then tweak the message to include your graduation year or your major. Play around; don’t worry you won’t break anything!",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#comparison-operators",
    "href": "03-python-basics.html#comparison-operators",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.4 Comparison Operators",
    "text": "3.4 Comparison Operators\nIn the previous section on booleans, we saw that comparison expressions—like 5 &gt; 3 — evaluate to either True or False. These expressions are powered by comparison operators, which are used to compare values in Python.\nYou’ll use comparison operators all the time when writing conditions, checking data, filtering results, or writing logic into your programs.\nHere’s a quick cheat sheet of the most common ones:\n\n\n\nOperator\nDescription\nExample\nResult\n\n\n\n\n==\nEqual to\n5 == 5\nTrue\n\n\n!=\nNot equal to\n5 != 3\nTrue\n\n\n&gt;\nGreater than\n10 &gt; 7\nTrue\n\n\n&lt;\nLess than\n4 &lt; 2\nFalse\n\n\n&gt;=\nGreater than or equal to\n3 &gt;= 3\nTrue\n\n\n&lt;=\nLess than or equal to\n8 &lt;= 6\nFalse\n\n\n\nAll of these expressions return a boolean value: True or False. Try running the following lines of code in your notebook:\nprint(10 &gt; 3)      # True\nprint(2 &lt; 1)       # False\nprint(4 == 4.0)    # True (int and float are treated as equal in value)\nprint(4 != 5)      # True\nprint(6 &gt;= 7)      # False\nprint(5 &lt;= 5)      # True\nYou can also compare strings:\nprint(\"apple\" == \"apple\")    # True\nprint(\"Apple\" == \"apple\")    # False (case matters!)\nprint(\"cat\" &lt; \"dog\")         # True (compares alphabetically)\n\n\n\n\n\n\nWarningCommon Pitfalls\n\n\n\nDon’t confuse = and ==:\n\n= is the assignment operator (used to assign a value to a variable)\n== is the comparison operator (used to check if two values are equal)\n\nx = 5           # assignment\nprint(x == 5)   # comparison → True\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneWhich Pizza is the Better Deal?\n\n\n\n\n\nLet’s build on a problem you saw earlier. This time, we’ll:\n\nuse variables to store the cost per square inch of two pizzas and\nthen use a comparison operator to see which one is the better deal.\n\nThe Setup\n\nA 12-inch pizza costs $8\nA 15-inch pizza costs $12\nUse the formula for the area of a circle:\n\n\\(A = \\pi \\times r^2\\)\nUse 3.14159 for π\n\n\nYour Task\n\nCompute the cost per square inch for each pizza\nStore the results in two variables: small_pizza and large_pizza\nUse a comparison operator to check if the smaller pizza is a better or equal deal\n\nHere’s a starting point:\n# Calculate cost per square inch for each pizza\nsmall_pizza = 8 / (3.14159 * (12 / 2) ** 2)\nlarge_pizza = 12 / (3.14159 * (15 / 2) ** 2)\n\n# Compare them (insert proper comparison operator in the blanks)\nprint(small_pizza __ large_pizza)\nWhat does the output of the comparison tell you? Try printing both values first to see how they compare. Which pizza gives you more for your money?\nprint(small_pizza)\nprint(large_pizza)",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#putting-it-all-together-basic-python-in-action",
    "href": "03-python-basics.html#putting-it-all-together-basic-python-in-action",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.5 Putting It All Together: Basic Python in Action",
    "text": "3.5 Putting It All Together: Basic Python in Action\nNow that you’ve learned about Python’s core data types, how to assign values to variables, and how to make comparisons, let’s put it all together into a small real-world example.\nImagine you’re helping manage event registration for a student club. You want to:\n\nStore the number of attendees and the cost per ticket\nCalculate total revenue from the event\nSet a goal for how much you wanted to make\nPrint a basic summary report\nUse comparison logic to see if you met your goal\n\nHere’s how you might write that in Python:\n\n# Number of attendees and ticket price\nattendees = 48\nticket_price = 12.50\n\n# Calculate total revenue\ntotal_revenue = attendees * ticket_price\n\n# Set a revenue goal\nrevenue_goal = 600\n\n# Print a summary message\nprint(\"You sold \" + str(attendees) + \" tickets at $\" + str(ticket_price) + \" each.\")\nprint(\"Total revenue: $\" + str(total_revenue))\n\n# Compare to revenue goal\ngoal_met = total_revenue &gt;= revenue_goal\nprint(\"Did we meet our revenue goal: \" + str(goal_met))\n\nYou sold 48 tickets at $12.5 each.\nTotal revenue: $600.0\nDid we meet our revenue goal: True\n\n\n\nWhat’s going on here?\nIn these 8 lines of code we’ve combined everything we learned across this chapter:\n\nWe used variables to store numbers and reused them in calculations\nWe performed basic math operations using multiplication\nWe used print() to display helpful messages, combining strings and numeric values\nWe used str() to convert numeric and boolean data types to strings\nWe used a comparison operator (&gt;=) to return True or False based on whether our total revenue met the goal\n\n\n\nTry it Yourself!\n\nChange the number of attendees or the ticket price. What happens to total revenue?\nTry changing the revenue goal and see if the result of the comparison changes.\nStretch: Use a GenAI tool like ChatGPT, Claude, or Copilot and ask it to expand upon your code so that it prints ‘Yaaah!’ if we met the revenue goal, or ‘Booo!’ if we didn’t. Then copy the AI’s suggestion into your notebook and test it out. Can you understand what it did? Does the code work the way you expected? If not—can you fix it?\n\n\n\n\n\n\n\nRemember: GenAI tools are great helpers, but not always correct. Always try to understand the code they give you!\n\n\n\nThis is your first step toward building programs that do real work. It may not seem fancy now—but you’ve already written a script that stores, processes, and evaluates real-world data.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#summary-and-whats-next",
    "href": "03-python-basics.html#summary-and-whats-next",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.6 Summary and What’s Next",
    "text": "3.6 Summary and What’s Next\nIn this chapter, you learned how to:\n\nIdentify and use Python’s most common data types—numbers, strings, and booleans\nUse the assignment operator (=) to store and reuse values with variables\nWrite and evaluate comparison expressions that return True or False\nUse print statements to combine and display information\nStart thinking like a programmer by working through simple real-world examples\n\nThese are the essential tools that will support everything you do moving forward—whether you’re analyzing a spreadsheet, building a model, or writing a script to automate a task.\n\nWhat’s Next: From Basics to Real Data\nNow that you’ve learned how to work with individual values and variables, it’s time to start thinking bigger—about how we structure and analyze real-world data. In the next module, we’ll go deeper into three key topics:\n\nJupyter Notebooks: You’ve already seen Jupyter Notebooks in action, but now we’ll explore just how powerful they are. Mastering Jupyter is about more than writing code—it’s about communicating insights clearly. You’ll learn how to:\n\nCombine text and code in the same document using Markdown\nFormat your notebook with headers, bullets, and even equations to make your work easier to understand\nStructure your notebooks as professional, reproducible reports, just like a real data scientist would\n\nPython Data Structures: So far, you’ve worked with individual values like one number or one string. But in data science, we almost never work with just one thing—we work with collections of data. Understanding data structures is essential as we start to clean, transform, and analyze datasets so we’ll cover:\n\nStoring and accessing data using lists (ordered sequences of items)\nOrganizing key-value pairs using dictionaries (think of them like labeled data bins)\nLooping through and manipulate these collections efficiently\n\nImporting Real-World Datasets with Pandas: You’ll also take your first step into real data science work—bringing in external datasets and exploring them with the Pandas library. Pandas is one of the most important tools in any data scientist’s toolbox. It makes it easy to:\n\nRead data from CSV files, Excel, databases, and more\nView, clean, and filter your data\nBegin asking real questions and discovering patterns\n\n\nYou’ve built a strong foundation. Next, we’ll build on it and start working with data the way real analysts and scientists do. Let’s keep going!",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#exercise-build-a-simple-event-summary",
    "href": "03-python-basics.html#exercise-build-a-simple-event-summary",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.7 Exercise: Build a Simple Event Summary",
    "text": "3.7 Exercise: Build a Simple Event Summary\n\n\n\n\n\n\nNoneThe Scenario\n\n\n\n\n\nYour student club is hosting an event and you’re in charge of summarizing registration data. Use what you’ve learned in this chapter to answer the following questions using Python.\nWrite all your code from scratch—no copy-pasting. Try to reason through the logic before typing.\n\nTickets sold: 56\nTicket price: $10.50\nRevenue goal: $600\nEvent name: “Python for Everyone”\n\n\n\n\n\n\n\n\n\n\nNoneYour Tasks\n\n\n\n\n\n\nCreate Variables: Assign appropriate values to variables for:\n\nEvent name\nTickets sold\nTicket price\nRevenue goal\n\nCalculate Total Revenue: Use math operations to calculate the total revenue earned from ticket sales.\nPrint a Summary Report: Use print() and string concatenation to display a message like:\nThe event \"Python for Everyone\" sold 56 tickets at $10.50 each.\nTotal revenue: $588.0\nMet or exceeded goal: False\nStretch Task (Optional): Add a comparison that checks whether your total revenue met or exceeded the revenue goal and prints:\n\n\"Yaaah! We met our goal!\" if the goal was met\n\"Booo! We missed our goal.\" if it was not\n\nHint: Ask a GenAI tool to help you construct the logic! If you can’t get it, don’t worry as we will talk about this later in the course.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html",
    "href": "04-jupyter.html",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "",
    "text": "4.1 Benefits:\nIn data science, we don’t just write code — we tell stories with data. Jupyter notebooks are an essential tool because they allow us to blend code, explanatory text, and visualizations in a single document. This makes it easier to understand, share, and reflect on your work.\nHere are a few key reasons why Jupyter notebooks are so widely used and appreciated in the data science world:",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#benefits",
    "href": "04-jupyter.html#benefits",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "",
    "text": "Blending code and context: Jupyter notebooks allow you to seamlessly combine executable Python code with rich text elements using Markdown. This means you can explain your thought process, document your workflow, and display results all in one place—making your analysis easier to follow and reproduce.\nExploratory-friendly: Notebooks are designed for experimentation. You can write and run code in small, manageable chunks (cells), see immediate feedback, and iteratively refine your approach. This makes it easy to test ideas, debug, and learn as you go.\nSharable and visual: Notebooks support inline visualizations, tables, and formatted text, making your work more engaging and accessible. You can easily export notebooks as HTML or PDF files to share with others, ensuring your analysis is both readable and reproducible.\n\n\n\n\n\n\n\nThink of a notebook as your digital lab notebook. Instead of jumping between different tools to write your analysis, calculate results, and explain your thinking, a Jupyter notebook lets you do it all in one place—cleanly combining code, commentary, and visuals in a single, easy-to-follow narrative.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#notebook-anatomy-and-core-features",
    "href": "04-jupyter.html#notebook-anatomy-and-core-features",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.2 Notebook Anatomy and Core Features",
    "text": "4.2 Notebook Anatomy and Core Features\n\n\n\n\n\n\nNote📓 Follow Along in Colab!\n\n\n\nAs you read through this chapter, we encourage you to follow along using the companion notebook in Google Colab (or other editor of choice). This interactive notebook lets you run code examples covered in the chapter—and experiment with your own ideas.\n👉 Open the Jupyter Basics Notebook in Colab.\n\n\n\nCode vs Markdown Cells\nJupyter notebooks are made up of cells, and there are two main types you’ll use regularly: code cells and Markdown cells.\n\nCode cells are where you write and run Python code. When executed, these cells display output directly beneath the cell—whether it’s a simple calculation, a table, or a visualization.\nMarkdown cells allow you to format text to explain your work, provide instructions, insert images, or even write mathematical formulas. These cells support formatting tools like headers, bullet points, bold/italic text, and LaTeX for equations.\n\nBy combining these two types of cells, you can create a clear, readable narrative that both documents your process and shows your results.\n\n\n\n\n\n\nWatch from 5:20-8:45 in this video to see simple examples of code cells and Markdown cells.\n\n\n\n\nTo select the type of a cell (Code or Markdown), use the dropdown menu typically found at the top of the notebook interface. In Jupyter Lab or classic Jupyter Notebook, it will say ‘Code’ or ‘Markdown’—click it to change. In VS Code, you can right-click a cell and choose the type, or use shortcuts like Cmd+M M (Markdown) and Cmd+M Y (Code) on Mac (Ctrl+M equivalents on Windows).\nTo run a cell: Shift + Enter\n\nKnowledge check\n\n\n\n\n\n\nNoneTry It!\n\n\n\n\nCreate and execute a Markdown cell and write the following: “Today I’m learning about Jupyter notebooks!”\nBelow it, create and execute a Code cell that:\n\nDefines a variable, like x = 5\nPrints a message that includes the variable (e.g., print(f'The value of x is {x}'))\n\n\n\n\n\n\n\nMarkdown Basics\nThere’s a lot you can do with Markdown to make your notebooks clearer and more engaging. You can add headings, lists, formatting, and even mathematical expressions to help document your analysis and guide readers through your work.\nHere are a few basics to get you started but check out the cheat sheet below to see more:\n\nHeaders: Use #, ##, ### to create headings of different sizes\nBold and Italic: **bold**, *italic*\nLists: - item for bullet points or 1. item for numbered lists\nEquations: Use LaTeX syntax between dollar signs, like $a^2 + b^2 = c^2$\n\n\n\n\nMarkdown Cheat Sheet (Datacamp)\n\n\nWant to go deeper? You can explore more Markdown syntax in these helpful guides:\n\nJupyter Docs on Markdown: A quick and easy introduction to get you started with basic Markdown syntax.\nMarkdown Guide: A more detailed reference for adding more structure and polish to your notebook explanations.\nColab Markdown Guide notebook: An example Colab notebook that allows you to experiment directly with Markdown syntax - a great playground for practicing different Markdown elements in a live notebook environment. \n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry It!\n\n\n\nBuild on the previous activity by enhancing your notebook with more Markdown elements:\n\nAt the very top of your notebook, add a title using a level-1 header (#).\nAdd a second-level header (##) introducing a new section.\nUnder that header, create a bulleted list—for example, a grocery shopping list.\n(Stretch Goal) Try writing a simple equation in Markdown, such as the formula for the area of a circle: $A = \\pi r^2$.\n\nThis will help you practice using Markdown to better organize and explain your work.\n\n\n\n\n\nCode Execution & State\nOne of the key features of Jupyter notebooks is that they allow you to execute code one cell at a time. This supports experimentation, but also introduces some complexity.\n\nCode is executed in the order you run the cells, not necessarily from top to bottom. This means you can define a variable in one cell and use it later—even if that later cell is above the original one. While this can be convenient, it can also lead to confusion or bugs if you’re not careful.\n\n\n\n\n\n\n\nCautionExample:\n\n\n\n\nIn one code cell, type: x = 42\nNow scroll up and create a new code cell above that one.\nIn that new cell, type: print(x) and run it.\n\nIt will work—because the variable x was already defined in memory. But if someone runs the notebook from top to bottom, it will break. This illustrates why executing cells out of order can lead to unpredictable behavior.\n\n\n\nThe notebook maintains a running memory called the kernel. As long as the kernel is active, all the variables and functions you’ve defined persist in memory. This makes it easy to build on previous work, but if you make a mistake or want a clean slate, you may need to restart the kernel.\n\nA good habit is to periodically restart the kernel and run all cells in order to ensure your notebook works as expected from top to bottom.\n\n\n\nManaging Your Notebook\nWorking in Jupyter notebooks involves managing both your work and the underlying system that runs it. Here are a few key features and best practices to help keep things running smoothly:\n\nSave often: Notebooks autosave regularly, especially when working in a browser-based environment like Jupyter Lab or Colab. However, it’s still a good habit to manually save using Ctrl+S or Cmd+S, especially before running all cells or restarting the kernel.\nRestarting the Kernel: If your notebook starts behaving unpredictably—due to memory overload, undefined variables, or bugs—it’s a good idea to restart the kernel. Restarting wipes the slate clean by clearing all variables from memory, allowing you to re-run the notebook from the top.\nClear Outputs: Before sharing your notebook with others or submitting it for review, consider clearing all outputs (cell results, print statements, plots, etc.). This makes your notebook easier to read and ensures the reader sees only the final results when they run it themselves. You can usually do this via the Kernel or Edit menu.\n\n\n\n\n\n\n\nAgain, can’t highlight this enough, a good practice is to “Restart and Run All” to make sure your notebook runs cleanly from start to finish without relying on any hidden state or cell order.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#when-not-to-use-notebooks",
    "href": "04-jupyter.html#when-not-to-use-notebooks",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.3 When Not to Use Notebooks",
    "text": "4.3 When Not to Use Notebooks\nNotebooks are powerful and flexible, but they aren’t the best fit for every type of task—especially as your work becomes more complex or transitions into production environments.\n\nLimitations:\n\nHard to modularize and test: Notebooks encourage an exploratory, linear style of development. But when you need reusable functions, testable components, or well-structured projects, notebooks can become unwieldy.\nPoor version control: While tools like Git work well with text files, notebooks save code and output together in JSON format. This makes it difficult to track changes or resolve merge conflicts. This is improving but is still less than stellar.\nExecution order issues: Since notebooks allow you to run cells out of order, it’s easy to accidentally create dependencies that break when someone else (or you later) tries to run the notebook top to bottom.\n\n\n\nWhen Notebooks Shine\nJupyter notebooks truly excel in scenarios where exploration, explanation, and communication are key. Here are a few situations where notebooks are particularly well-suited:\n\nLearning and teaching: The ability to mix code, narrative, and output in one place makes notebooks an ideal format for both instruction and self-study.\nExploratory data analysis (EDA): Notebooks allow you to quickly test hypotheses, visualize data, and keep track of your insights along the way.\nPrototyping and trying out ideas: Need to test a small function or compare two approaches? Notebooks provide a fast and flexible way to experiment.\nCreating interactive reports and visualizations: With support for charts, tables, and even interactive widgets, notebooks are a powerful medium for communicating data-driven stories.\n\n\n\nWhen to Use Something Else\nWhile notebooks are fantastic for interactive work, they aren’t ideal when software engineering discipline and scalability are required. Here are scenarios where other tools—like .py scripts, IDEs, or structured repositories—are often a better choice:\n\nBuilding and deploying production applications: Applications that need to be deployed on servers or integrated with other systems require a more modular and structured codebase than notebooks typically provide.\nCreating libraries, APIs, or packages: These projects need reusable components, proper documentation, and automated testing, which are easier to maintain outside of notebooks.\nCollaborating with others using complex Git workflows: Notebooks don’t lend themselves well to detailed version control, especially with branching and merging.\nDeveloping software that requires extensive testing or CI/CD pipelines: Production-grade software often relies on unit testing, linting, and automated builds, which are best handled in traditional .py environments.\n\n\n\n\n\n\n\nIf you don’t yet know what all the bullet points above mean — like Git, APIs, CI/CD, or modular code — that’s completely fine. You’ll encounter these concepts as you progress in your data science journey. For now, just understand that as projects get more complex and move closer to production, notebooks often aren’t the best tool for the job.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#using-.py-files-in-notebooks",
    "href": "04-jupyter.html#using-.py-files-in-notebooks",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.4 Using .py Files in Notebooks",
    "text": "4.4 Using .py Files in Notebooks\nMixing .py scripts with notebooks is common in real-world workflows, especially as your projects grow in complexity or require reusable functions. Instead of writing everything directly in a notebook, you can keep your reusable logic—like data cleaning functions, utility tools, or modeling pipelines—in a .py script and then import or run that script from within your notebook.\nThis approach offers several benefits: you avoid cluttering your notebook with repeated code, you can test pieces of logic more easily, and your codebase becomes easier to maintain and share.\n\nMethods:\nThere are a few different ways to work with .py files inside your notebook, each useful in different situations:\n\n%run your_script.py – This runs the entire Python script as if you had pasted the code into the notebook. Any variables, functions, or classes defined in that script will be available in your notebook’s environment after execution.\n%load your_script.py – This loads the contents of the script into a new notebook cell so you can review or edit it before running. It’s a nice way to examine the code without opening another file.\nimport your_script – This is the standard Python approach for using modules. If your script is in the same directory as your notebook, you can import functions or variables from it using this command. For example, if your script has a function called clean_data(), you could use it in your notebook with your_script.clean_data(). Don’t worry if this is new—we’ll cover imports and module structure later in the class.\n\n\n\n\n\n\n\nNoneExample\n\n\n\n\n\nSee an example of running a .py script with %run your_script.py in this video:\n\n\n\n\n\n\nWhy use .py files?\nOrganizing your code into .py files offers several advantages, especially as your projects grow beyond a single notebook:\n\nReusability: Functions or logic stored in a .py file can be reused across multiple notebooks or scripts without duplication.\nOrganization: Separating logic from analysis helps keep your notebooks clean and focused on storytelling, while your .py files house the supporting code.\nVersion control: Unlike notebooks, .py files are plain text and work much better with Git and other version control tools—making it easier to track changes, review code, and collaborate with others.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry It!\n\n\n\n\nOpen your editor of choice (such as Colab, JupyterLab, or VS Code).\nCreate a new .py file and type the following line into it:\nprint(\"Hello .py scripts!\")\nSave this file as my_first_script.py.\nNow open a new Jupyter notebook.\nIn a code cell in the notebook, type and run:\n%run my_first_script.py\n\nYou should see the message “Hello .py scripts!” printed in the notebook output.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#best-practices-and-pro-tips",
    "href": "04-jupyter.html#best-practices-and-pro-tips",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.5 Best Practices and Pro Tips",
    "text": "4.5 Best Practices and Pro Tips\nFollowing a few best practices can make your Jupyter notebooks more readable, reliable, and professional—both for yourself and others who use your work.\n\nName files clearly: Use descriptive and consistent naming conventions for your notebooks. For example, eda_customer_data.ipynb is much more informative than untitled3.ipynb.\nUse Markdown generously: Narrate your thought process. Include headers, bullet points, and equations to guide the reader through your analysis. Also, take time to format your writing as you would for any professional report—pay attention to grammar, clarity, and structure to ensure your notebook is polished and easy to follow.\nRestart and clear output before sharing: This ensures the notebook runs top-to-bottom and that readers aren’t confused by leftover output or state.\nExport as HTML or PDF: If you’re turning in a notebook or sharing it for review, exporting to a static format can ensure others see exactly what you intended.\n\n\n\n\n\n\n\nTipPro Tip\n\n\n\nStructure your notebook like a story—introduce the goal, describe your approach, show the results, and summarize your findings. Your future self will thank you!\nHere’s a great article that walks through best practices to help you make this a reality.\n\n\n\nWant to Learn More?\nHere are a few helpful references that expand on Jupyter notebook best practices:\n\nProject Jupyter Guidelines\nReal Python – Jupyter Notebook Tips, Tricks, and Shortcuts",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#summary-and-whats-next",
    "href": "04-jupyter.html#summary-and-whats-next",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.6 6. Summary and What’s Next",
    "text": "4.6 6. Summary and What’s Next\nLet’s recap what you’ve learned in this chapter:\n\nJupyter notebooks are an essential tool for blending code, narrative, and output in one place.\nYou now know how to use different cell types, write Markdown, and execute code.\nYou’ve seen how notebooks maintain memory with the kernel and why it’s important to restart and re-run code in order.\nWe covered notebook limitations and when it’s better to switch to .py scripts or more robust development tools.\nYou explored how to organize your code using .py files and how to run them inside your notebooks.\nFinally, you picked up several best practices for writing professional and readable notebooks.\n\n\nNext Up:\nIn the next chapter, we’ll dive into Python’s basic data structures—like lists and dictionaries. These tools are essential for organizing and manipulating data efficiently, and they will serve as the foundation for the rest of your data analysis journey.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#exercise-comparing-movie-theater-snacks",
    "href": "04-jupyter.html#exercise-comparing-movie-theater-snacks",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.7 Exercise: Comparing Movie Theater Snacks",
    "text": "4.7 Exercise: Comparing Movie Theater Snacks\n\n\n\n\n\n\nNoneScenario\n\n\n\n\n\nYou’re deciding between three snack combos at the movie theater. Each combo includes popcorn and a drink. Your task is to figure out which option gives you the most food value for your money, based on calories per dollar.\nCombo Details:\n\nCombo 1: $7.50 and 500 calories\nCombo 2: $9.00 and 700 calories\nCombo 3: $10.50 and 900 calories\n\n\n\n\n\n\n\n\n\n\nNoneCreate Notebook\n\n\n\n\n\nCreate a new Jupyter notebook and name it something meaningful and relevant for this analysis.\n\n\n\n\n\n\n\n\n\nNoneExecute Code\n\n\n\n\n\n\nUse a code cell to define the following variables for each combo:\n\ncombo_1_price = 7.50\ncombo_1_calories = 500\ncombo_2_price = 9.00\ncombo_2_calories = 700\ncombo_3_price = 10.50\ncombo_3_calories = 900\n\nUse another code cell to calculate the calories per dollar for each combo:\ncombo_1_value = combo_1_calories / combo_1_price\nUse a final code cell to print the results for each combo.\n\n\n\n\n\n\n\n\n\n\nNoneDocument Your Notebook\n\n\n\n\n\nUse Markdown cells to:\n\nGive your notebook a proper title and write a short introduction explaining the goal of the analysis.\nDisplay the formula used: \\(Value = \\frac{Calories}{Price}\\)\nReport your findings: Which combo offers the best value, and how did you determine this?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html",
    "href": "05-data-structures.html",
    "title": "5  Introduction to Data Structures",
    "section": "",
    "text": "5.1 Why Data Structures?\nIn the real world, we rarely work with individual pieces of data. Whether we’re analyzing a dataset, building a model, or writing a simulation, we typically work with collections of data. Python provides several built-in ways to store and organize these collections. These tools are called data structures.\nIn this chapter, you will learn how to:\nImagine you want to store the test scores of a student across multiple exams. Instead of creating separate variables for each score, it makes more sense to group them together:\n# instead of this\ngrade_1 = 85\ngrade_2 = 90\ngrade_3 = 88\ngrade_4 = 92\n\n# more convenient to group them together like this\ngrades = [85, 90, 88, 92]\nThis approach allows you to access, modify, and analyze data more efficiently. That’s the power of data structures: they help us organize and operate on collections of data.\nPython includes different types of data structures that allow us to organize data with specific characteristics suited to the task at hand. For example, some data structures preserve the order of elements, while others do not. Some allow us to modify their contents (mutable), and others do not (immutable). Still others let us associate labels or keys with each item, such as names linked to phone numbers. Choosing the right data structure depends on how we need to access and manage the data.\nBuilding on this, each structure in Python has tradeoffs that make it more suitable for specific situations. If you need to keep track of items in a particular order and expect the data to change, a list might be ideal. If you want to store a fixed grouping of values like GPS coordinates, a tuple ensures the data remains unchanged. And when you need to link one piece of information to another—like a person’s name to their phone number—a dictionary’s key-value format is the perfect tool. Understanding these attributes helps you pick the right structure for the task and write more efficient, readable code.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#why-data-structures",
    "href": "05-data-structures.html#why-data-structures",
    "title": "5  Introduction to Data Structures",
    "section": "",
    "text": "Think about a recent experience filling out an online form.\n\nWhat types of data did you need to input?\nDo you think this data should be stored in a structure that allows for changes (mutability)?\nDoes the order of the data matter for how it’s used or displayed? Were there any key-value pairs—like a name linked to an email address—that need to be stored and accessed together?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#lists-ordered-and-mutable",
    "href": "05-data-structures.html#lists-ordered-and-mutable",
    "title": "5  Introduction to Data Structures",
    "section": "5.2 Lists: Ordered and Mutable",
    "text": "5.2 Lists: Ordered and Mutable\n\nWhat is a List?\nA list is an ordered, changeable collection of items. You can add, remove, or modify elements in a list. Lists are among the most commonly used data structures in Python due to their flexibility and ease of use. When you need to keep items in a specific order—like daily stock prices, game scores, or survey responses—a list is often the right choice.\nLists are mutable, meaning you can change their contents after they’re created. This makes them ideal for scenarios where your data updates frequently, such as tracking website visits or logging sensor readings.\n\n\nCreating Lists\nTo create a list, use square brackets [] and separate items with commas. Lists can hold values of any type: numbers, strings, Booleans, or even other lists. Here are three simple examples:\n\nfruits = ['apple', 'banana', 'cherry']\nscores = [85, 90, 88, 92]\nmixed = ['blue', 42, True, 3.14]\n\nThe order in which you define the elements is preserved. We can see this when we evaluate a list - the output will always be in the same order as it was input:\n\nmixed\n\n['blue', 42, True, 3.14]\n\n\nThis makes lists especially helpful for keeping track of sequences where position matters, such as chronological data or ranked preferences. Because lists are mutable, you can also build them incrementally as your data grows or changes during the course of a program.\n\n\nAccessing List Items\nTo access a specific item in a list, you use indexing - which uses brackets ([]) along with the specified location. So, say we want to get the first item from a list:\n\nfruits[1]\n\n'banana'\n\n\nWait a minute! Shouldn’t fruits[1] give the first item in the list? It seems to give the second. This is because indexing in Python starts at zero.\n\n\n\n\n\n\nImportantZero-based Indexing\n\n\n\nPython uses zero-based indexing, which means that the first element in a list has an index of 0, the second has an index of 1, and so on. You can also use negative indexing to count from the end of the list, where -1 is the last item, -2 is the second-to-last, etc.\n\nprint(fruits[0])   # first item\nprint(fruits[2])   # second item\nprint(fruits[-1])  # last item\n\napple\ncherry\ncherry\n\n\nFun reading: Why Python uses 0-based indexing\n\n\nUnderstanding indexing is essential because it affects how you loop through lists, retrieve elements, and manipulate values. For example, if you mistakenly assume that indexing starts at 1, you might accidentally skip or mislabel elements in your data, or you may specify an invalid location which will raise an error like the following:\n\nfruits[4]\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 fruits[4]\n\nIndexError: list index out of range\n\n\n\n\n\nModifying Lists\nOnce a list is created, you can change its contents easily. This mutability makes lists incredibly useful for situations where your data evolves over time. You can add new items using methods like .append() or change existing elements by assigning new values using their index.\n\nfruits.append('orange')\nfruits[1] = 'blueberry'\n\nfruits\n\n['apple', 'blueberry', 'cherry', 'orange']\n\n\nThese operations reflect how dynamic lists can be—whether you’re updating a list of customer names, recording daily measurements, or tracking items in a to-do list, lists let you adjust your data in place without rebuilding the structure from scratch.\n\n\nCommon List Operations\nLists come with many built-in functions and methods that make it easy to analyze and manipulate data. These operations can help you answer questions like: How many elements are in the list? Does it contain a specific item? Can I sort or remove items from it?\n\nlen(fruits)            # 4 (returns the number of elements in the list)\n'apple' in fruits      # True (checks if 'apple' is in the list)\nfruits.remove('apple') # removes the first occurrence of 'apple'\nfruits.sort()          # sorts the list in place (alphabetically or numerically)\nfruits.pop()           # removes and returns the last item\n\nThese methods are especially useful in data wrangling tasks, such as filtering survey responses, cleaning up logs, or ranking scores.\n\n\n\n\n\n\nYou can find many other list operations here: https://docs.python.org/3/tutorial/datastructures.html#data-structures\n\n\n\n\n\nUse Cases\n\nTime series data\nCollections of records (e.g., names, prices, grades)\nMaintaining a dynamic list of inputs that might grow or shrink over time\nStoring results from computations or simulations\nCollecting user inputs or responses in interactive programs\nAnd others!\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry It!\n\n\n\nSay the last 5 days had daily high temperatures of 79, 83, 81, 89, 78.\n\nStore these values in a list.\nNow add a new item to this list that represents today’s high temp of 85.\nNext, suppose the first day’s reading was inaccurate and the actual high temp was 76 — update that value in the list.\nFinally, sort the list to see the temperatures in ascending order.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#tuples-ordered-and-immutable",
    "href": "05-data-structures.html#tuples-ordered-and-immutable",
    "title": "5  Introduction to Data Structures",
    "section": "5.3 Tuples: Ordered and Immutable",
    "text": "5.3 Tuples: Ordered and Immutable\n\nWhat is a Tuple?\nA tuple is similar to a list in that it is an ordered collection of items. However, unlike lists, tuples are immutable, meaning their contents cannot be changed after creation. This immutability makes tuples useful for representing fixed sets of data that should not be altered during a program’s execution.\n\n\n\n\n\n\nThink of tuples as a read-only list. This may be contentious, as described in this blog post. Tuples do have many other capabilities beyond what you would expect from just being “a read-only list,” but for us just beginning now, we can think of it that way.\n\n\n\nTuples are typically used when you want to ensure data integrity, such as storing constant values or configuration settings. They are also commonly used to return multiple values from a function, or to represent simple groupings like coordinates, RGB color values, or database records.\nBecause of their immutability, tuples can be used as keys in dictionaries (we’ll learn what these are shortly), unlike lists. Additionally, they offer slightly better performance than lists when it comes to iteration.\n\n\nCreating Tuples\nTo create a tuple, use parentheses () and separate values with commas. Like lists, tuples can hold values of different data types. However, because tuples are immutable, they are particularly well-suited for storing data that should remain constant throughout your program.\nFor example, if you’re working with geographic coordinates, a tuple ensures the latitude and longitude values stay paired and unchanged:\n\ncoordinates = (39.76, -84.19)\n\nOr you might use a tuple to store a birthdate, where the structure will never need to be modified:\n\nbirthday = (7, 14, 1998)\n\nTuples are also frequently used when functions need to return multiple values, making them both practical and efficient in everyday programming. We’ll see this in action in later sections.\n\n\nAccessing Tuple Items\nIndexing tuples works exactly the same way as indexing lists in Python. You use square brackets [] with a zero-based index to access elements. For example:\n\ncoordinates[0]\n\n39.76\n\n\nJust like lists, you can use negative indices to access elements from the end:\n\nbirthday[-1]\n\n1998\n\n\n\n\n\n\n\n\nWhile both lists and tuples support indexing, remember that tuples are immutable. This means you can read elements by index, but you cannot change them:\n\ncoordinates[0] = 41.62\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 coordinates[0] = 41.62\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\n\n\n\nTuple Unpacking\nTuple unpacking allows you to assign each item in a tuple to its own variable in a single line. This is especially useful when a function returns multiple values or when you’re working with grouped data like coordinates, dimensions, or ranges. It improves readability and simplifies your code when working with known-length tuples.\n\nx, y = coordinates\n\n\nprint(x)\n\n39.76\n\n\n\nprint(y)\n\n-84.19\n\n\nHere, the value of x will be 39.76 and y will be -84.19, corresponding to the first and second elements of the coordinates tuple, respectively. If you try to unpack a tuple into a different number of variables than it has elements, Python will raise an error.\n\n\nWhy Use Tuples?\nTuples are ideal when you want to group together related values that should not be changed once set. Their immutability provides a built-in safeguard against accidental modifications, which is especially helpful when the data must remain consistent throughout the execution of a program. Because they are more lightweight than lists, tuples also offer faster performance in scenarios that involve iteration or large numbers of data groupings. Furthermore, since tuples can be used as keys in dictionaries (unlike lists), they provide a reliable way to map compound keys to values in advanced data structures.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry It!\n\n\n\nGiven the following tuple schooling = ('UC', 'BANA', '4080')\n\nUse indexing to grab the word “BANA”.\nChange the value of “BANA” to “Business Analytics”. What happens?\nUnpack the schooling tuple into three variables: university, program, class_id.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#dictionaries-key-value-pairs",
    "href": "05-data-structures.html#dictionaries-key-value-pairs",
    "title": "5  Introduction to Data Structures",
    "section": "5.4 Dictionaries: Key-Value Pairs",
    "text": "5.4 Dictionaries: Key-Value Pairs\nA dictionary is a collection of key-value pairs, where each unique key maps to a specific value. This structure allows you to organize and retrieve data using meaningful identifiers rather than relying on position, as with lists or tuples. Dictionaries are especially helpful when you need to store data that has a clear label or attribute—such as a student’s name, ID, or grade—making it easy to access or update values using their corresponding keys. This makes dictionaries one of the most powerful and flexible tools for working with structured data in Python.\n\nCreating Dictionaries\nTo create a dictionary, use curly braces {} with key-value pairs separated by colons. Each key must be unique and is typically a string, though it can also be a number or other immutable type. Dictionaries are ideal when you want to store data that has meaningful labels. For instance, instead of remembering that index 0 in a list corresponds to a name and index 1 corresponds to a score, a dictionary lets you associate 'name' directly with a value.\n\nstudent = {\n    'name': 'Jordan',\n    'score': 95,\n    'major': 'Data Science'\n}\n\n\n\nAccessing and Modifying Dictionaries\nTo access a value in a dictionary, you reference its key in square brackets. This allows you to retrieve values without knowing their position, unlike lists or tuples.\n\n# access the value associated with the 'score' key\nstudent['score']\n\n95\n\n\nYou can also update the value of an existing key or add a completely new key-value pair to the dictionary. This mutability makes dictionaries ideal for dynamic data, such as user profiles, configuration settings, or database records.\n\nstudent['score'] = 98  # update the score\nstudent['grad_year'] = 2025  # add a new key-value pair\n\nstudent\n\n{'name': 'Jordan', 'score': 98, 'major': 'Data Science', 'grad_year': 2025}\n\n\n\n\nDictionary Methods\nDictionaries come with several built-in methods that allow you to efficiently access and interact with their contents. These methods help you retrieve just the keys, just the values, or both together. They are particularly useful when you’re iterating over a dictionary to analyze or transform its contents.\nTry the following operations and see what their outputs are:\nstudent.keys()         # dict_keys(['name', 'score', 'major', 'grad_year'])\nstudent.values()       # dict_values(['Jordan', 98, 'Data Science', 2025])\nstudent.items()        # dict_items([('name', 'Jordan'), ('score', 98), ...])\n'name' in student      # True (checks if a key exists in the dictionary)\ndel student['major']   # removes the 'major' key and its associated value\nUsing these methods allows you to build flexible programs that can dynamically explore or modify structured data—especially when working with JSON data from APIs, reading metadata from files, or updating attributes of users or records.\n\n\nUse Cases\nDictionaries are incredibly versatile and can be found in many practical programming and data science scenarios:\n\nLookup tables: Use dictionaries to map inputs to outputs, such as converting state abbreviations to full names or translating category codes to descriptions.\nStructured data: Store rows of data as dictionaries where keys represent column names—this mirrors how data is stored in JSON format and is common when working with web APIs or data from files.\nFeature storage in machine learning models: Organize features (like ‘age’, ‘income’, or ‘region’) with their corresponding values for each individual observation, allowing for dynamic construction and retrieval of input data for models.\nConfiguration settings: Store user preferences or system parameters that can be easily accessed or updated using descriptive keys.\nData merging and deduplication: Use keys to uniquely identify records, making it easier to combine and clean data from different sources.\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry It!\n\n\n\n\nCreate a dictionary that stores a classmate’s nickname, phone number, and age.\njohn_doe = {\n    'nickname': _______,\n    'phone_number': __,\n    'age': __\n}\nCreate another dictionary called classmate2 with similar information for a different classmate.\nCombine these two dictionaries into a new dictionary called contacts, where each key is the classmate’s name and the value is their corresponding dictionary.\nAdd a new entry to the contacts dictionary for a third classmate, including their name, phone number, and age.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#summary",
    "href": "05-data-structures.html#summary",
    "title": "5  Introduction to Data Structures",
    "section": "5.5 Summary",
    "text": "5.5 Summary\nIn this chapter, we focused on three of Python’s most commonly used data structures: lists, tuples, and dictionaries. These structures provide the foundation for how data is organized and accessed in most Python programs, especially in data science workflows.\n\nLists are ordered and mutable, making them ideal when you need to maintain and modify a sequence of items.\nTuples are ordered but immutable, which is helpful for fixed sets of values where data integrity is important.\nDictionaries store labeled data as key-value pairs, offering a flexible way to organize and retrieve data by meaningful identifiers.\n\n\n\n\n\n\n\n\n\n\nData Structure\nType (via type())\nDescription & When to Use\nExample\n\n\n\n\nList\nlist\nOrdered, mutable collection. Use when you need to keep items in sequence and change them later.\nscores = [85, 90, 88, 92]\n\n\nTuple\ntuple\nOrdered, immutable collection. Ideal for fixed groupings like coordinates or dates.\ncoordinates = (39.76, -84.19)\n\n\nDictionary\ndict\nUnordered, mutable key-value pairs. Great for labeled data or fast lookups.\nstudent = {'name': 'Jordan', 'score': 95}\n\n\n\nAs we move forward in this book, we’ll explore more advanced data structures that build on these basics and help you perform data mining tasks more efficiently. For now, having a solid understanding of these core structures will serve as a crucial building block for your continued work in Python and data science.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#exercise-student-records-management",
    "href": "05-data-structures.html#exercise-student-records-management",
    "title": "5  Introduction to Data Structures",
    "section": "5.6 Exercise: Student Records Management",
    "text": "5.6 Exercise: Student Records Management\nYou’ve been asked to build a simple data tracking system for a small classroom.\n\n\n\n\n\n\nNoneCreate a List\n\n\n\n\n\n\nMake a list called student_names that includes the names of 5 students.\nAdd a new student to the list.\nRemove the second student in the list.\nSort the list alphabetically and print the final list.\n\n\n\n\n\n\n\n\n\n\nNoneUse a Tuple\n\n\n\n\n\n\nCreate a tuple named classroom_location that stores the building name and room number, such as (\"Lindner Hall\", 315).\nUnpack the tuple into two variables: building and room, and print each one with a label.\n\n\n\n\n\n\n\n\n\n\nNoneBuild a Dictionary\n\n\n\n\n\n\nCreate a dictionary named student_info for one of the students, including their name, major, and graduation year.\nAdd a new key for GPA with a value of your choice.\nPrint out all the keys, all the values, and the full dictionary.\n\n\n\n\n\n\n\n\n\n\nNoneReflection\n\n\n\n\n\nThink through what you just did:\n\nWhat makes a list a good choice for student_names? Is there an alternative approach you could’ve taken?\nWhy would we use a tuple for classroom_location?\nWhat makes a dictionary a good choice for student_info?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "06-libraries.html",
    "href": "06-libraries.html",
    "title": "6  Packages, Libraries, and Modules",
    "section": "",
    "text": "6.1 Why Do We Care?\nAs a data scientist, you won’t be writing every piece of code from scratch. Python’s true power comes from its ecosystem of packages, libraries, and modules that extend its core functionality. These tools allow you to analyze data, build machine learning models, visualize patterns, and automate complex tasks with just a few lines of code.\nIn this chapter, we’ll explore what packages, libraries, and modules are, how to use them, and get hands-on with both built-in and third-party tools that will support you throughout this course.\nBy the end of this chapter, you will be able to:\nYou’ve probably heard that Python is one of the most popular languages in data science—but what makes it so powerful? One big reason is that Python gives you access to an enormous number of libraries that other people have written and shared.\nA library is a reusable collection of code—functions, tools, or entire frameworks—that someone else has written to make your life easier. Think of libraries as toolkits: instead of building a hammer every time you need to drive a nail, you just grab the hammer from your toolbox.\nPython libraries can help you:\nBy using the right library, you can accomplish in just a few lines of code what might otherwise take hours of work.\nLibraries are organized into modules, which are just files that group related code together. And when a collection of modules is bundled up and made shareable, we call that a package.\nThroughout this course, you’ll learn how to use some of the most powerful and popular Python libraries for data science. But first, let’s get familiar with the different types of libraries available and how to start using them.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#why-do-we-care",
    "href": "06-libraries.html#why-do-we-care",
    "title": "6  Packages, Libraries, and Modules",
    "section": "",
    "text": "Do complex math with a single function call.\nRead and clean messy data files.\nVisualize data with beautiful plots.\nTrain machine learning models.\n\n\n\n\n\n\n\n\n\n\nNoteTerminology: Modules, Libraries, and Packages\n\n\n\n\n\nThese three terms often get used interchangeably, but they have specific meanings in Python: * Module: A single .py file that contains Python code—functions, variables, classes—that you can reuse. For example, the math module lets you do mathematical calculations.\n\nModule: A single .py file that contains Python code—functions, variables, classes—that you can reuse. For example, the math module lets you do mathematical calculations.\nLibrary: A collection of related modules bundled together. For example, pandas is a library that includes several modules for data manipulation.\nPackage: A directory containing one or more modules or libraries, with an __init__.py file that tells Python it’s a package. You can think of a package as the container that holds libraries and modules.\n\nIn practice, you’ll often hear “library” and “package” used to refer to the same thing: something you install with pip and then import to use in your code. That’s okay! At this point in your learning, understanding the subtle differences between these terms is not critical. What’s more important is knowing that Python’s modular structure allows you to mix and match tools depending on your needs and that these tools make your work much more efficient.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#standard-library-vs-third-party-libraries",
    "href": "06-libraries.html#standard-library-vs-third-party-libraries",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.2 Standard Library vs Third-Party Libraries",
    "text": "6.2 Standard Library vs Third-Party Libraries\nOne of Python’s greatest strengths is its large collection of prebuilt tools. These tools fall into two broad categories: the standard library, which comes bundled with every Python installation, and third-party libraries, which you can download and install as needed.\n\nThe Standard Library\nThe standard library is like Python’s starter toolbox. It includes modules for doing math, generating random numbers, managing dates and times, reading and writing files, and even accessing the internet. Because it’s included with Python, you can use these modules right away—no installation required.\nFor example:\n\nWant to calculate square roots? Use the math module.\nNeed to simulate randomness? Try random.\nCurious where your code is running? The os module has answers.\n\nThese are great building blocks and perfect for learning foundational programming skills.\n\n\n\n\n\n\nTip📚 Want to Learn More?\n\n\n\nYou can explore the full list of available modules in Python’s standard library by visiting the official documentation here: https://docs.python.org/3/library/index.html\n\n\n\n\nThird-Party Libraries\nAs powerful as the standard library is, it doesn’t cover everything. That’s where third-party libraries come in. These are tools developed and maintained by the Python community to solve specific problems more efficiently. To use them, you’ll typically install them using a package manager called pip.\nFor example:\n\nNeed to work with large datasets? Use pandas.\nWant to make beautiful visualizations? Try matplotlib and seaborn.\nWant fast numerical computation? You’ll love numpy.\n\nThese libraries aren’t included by default, but they’re easy to install—and essential for doing real-world data science.\n\n\n\n\n\n\n\nStandard Library\nThird-Party Library\n\n\n\n\nAlready installed with Python\nInstalled manually (via pip)\n\n\nExamples: math, os, random, datetime\nExamples: pandas, numpy, matplotlib, seaborn\n\n\nNo internet needed to use\nRequires internet to install\n\n\n\nUnderstanding the difference between these two categories will help you know when to reach for built-in tools versus when to seek out more powerful external solutions.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneExploring Python Libraries\n\n\n\nVisit the following two documentation pages:\n\nPython math module (Standard Library)\nPandas library (Third-Party)\n\nThen answer the following questions:\n\nWhat is one function provided by the math module that you could use in one of your other classes? Briefly describe what it does.\nWhat is one feature or function of the pandas library that stands out to you? How might it help in data analysis?\nBased on your experience browsing both pages, what are some differences you notice between standard library documentation and third-party library documentation?\n\nTip: This exercise is not about memorizing everything. It’s about familiarizing yourself with how to explore documentation and recognizing the types of functionality different libraries provide.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#working-with-the-standard-library",
    "href": "06-libraries.html#working-with-the-standard-library",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.3 Working with the Standard Library",
    "text": "6.3 Working with the Standard Library\nThe standard library is like Python’s built-in Swiss Army knife. It includes dozens of modules for common programming tasks—and because it’s part of every Python installation, you can start using it immediately without needing to install anything.\n\nTo use a module from the standard library, you simply use the import statement. Once imported, you can access its functions and tools using dot notation.\nFor example, to use the math library to calculate the square root of a number we need to first import the math library and then we can access the square root function like below:\n\nimport math\n\nmath.sqrt(9)\n\n3.0\n\n\n\nThere are many useful standard libraries - here is an extremely incomplete list of some of the modules you might wish to explore and learn about:\n\nos and sys: Tools for interfacing with the operating system, including navigating file directory structures and executing shell commands\nmath and cmath: Mathematical functions and operations on real and complex numbers\nitertools: Tools for constructing and interacting with iterators and generators\nfunctools: Tools that assist with functional programming\nrandom: Tools for generating pseudorandom numbers\npickle: Tools for object persistence: saving objects to and loading objects from disk\njson and csv: Tools for reading JSON-formatted and CSV-formatted files.\nurllib: Tools for doing HTTP and other web requests.\ndatetime: Tools for working with dates, times, and time intervals\n\nLet’s see a few of these in action.\n\n🧮 math: Mathematical Functions\nThe math module gives you access to a wide variety of mathematical operations beyond basic arithmetic. These include square roots, exponentials, logarithms, trigonometric functions, and more.\nHere’s a simple example:\n\nimport math\n\nprint(math.ceil(9.2))        # Returns the smallest integer greater than or equal to 9.2 (i.e., 10)\nprint(math.factorial(6))     # Returns the factorial of 6 (i.e., 720)\nprint(math.sqrt(121))        # Returns the square root of 121 (i.e., 11.0)\n\n10\n720\n11.0\n\n\n\n\n📁 os: Interacting with the Operating System\nThe os module allows you to interact with your computer’s operating system. It’s helpful for navigating file paths, checking directories, and automating file-related tasks.\nExample:\n\nimport os\n\n# Get the current working directory\nprint(\"Current working directory:\", os.getcwd())\n\n# List the files and folders in that directory\nprint(\"Contents of the directory:\", os.listdir())\n\nCurrent working directory: /home/runner/work/uc-bana-4080/uc-bana-4080/book\nContents of the directory: ['10-manipulating-data.quarto_ipynb', '.quarto', 'images', '03-python-basics.qmd', 'index.quarto_ipynb', '06-libraries.quarto_ipynb', '05-data-structures.qmd', 'index.qmd', '01-intro-data-mining.qmd', '03-python-basics.html', '08-dataframes.qmd', 'references.bib', 'index.html', 'references.qmd', '_quarto.yml', '05-data-structures.quarto_ipynb', '07-importing-data.qmd', '09-subsetting.qmd', '08-dataframes.quarto_ipynb', '03-python-basics.quarto_ipynb', '07-importing-data.quarto_ipynb', 'summary.qmd', 'cover.png', '11_aggregating_data.quarto_ipynb', '05-data-structures.html', '02-preparing-for-code.html', '04-jupyter.qmd', '06-libraries.qmd', '01-intro-data-mining.html', '02-preparing-for-code.qmd', '10-manipulating-data.qmd', '09-subsetting.quarto_ipynb', '_book', '99-vscode-install.qmd', '11_aggregating_data.qmd', '.gitignore', '04-jupyter.html', 'site_libs', '99-anaconda-install.qmd']\n\n\n\n\n📅 datetime: Working with Dates and Times\nThe datetime module is essential for handling and manipulating dates and times. You can get the current date, format it in a specific way, or calculate time differences.\nExample:\n\nimport datetime\n\n# Get today's date\ntoday = datetime.date.today()\nprint(\"Today's date is:\", today)\n\n# Create a specific date\nbirthday = datetime.date(1980, 8, 24)\nprint(\"Birth date:\", birthday)\n\n# How many days have I been on earth\ndays_alive = (today-birthday).days\nprint(\"Days on earth:\", days_alive)\n\nToday's date is: 2025-07-12\nBirth date: 1980-08-24\nDays on earth: 16393\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneUsing the Standard Library\n\n\n\nStart a Jupyter notebook and write code that does the following using only the standard library:\n\nUses the datetime module to print today’s date.\nUses the math module to calculate the square root of 625.\nUses the random module to simulate rolling a 6-sided die five times.\nUses the os module to print your current working directory.\n\nTip: Try running each part separately and look up the documentation if you’re unsure how to use a module. This is great practice for solving problems using tools that are already built into Python! If you encounter any difficulties or have questions, don’t hesitate to ask ChatGPT or Copilot for assistance!",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#pythons-data-science-ecosystem-third-party-modules",
    "href": "06-libraries.html#pythons-data-science-ecosystem-third-party-modules",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.4 Python’s Data Science Ecosystem & Third-party Modules",
    "text": "6.4 Python’s Data Science Ecosystem & Third-party Modules\nThe standard library is powerful, but when you begin working on real-world data science tasks, you’ll quickly find yourself needing more specialized tools. This is where third-party libraries come in. One of the things that makes Python useful, especially within the world of data science, is its ecosystem of third-party modules. These are external packages developed by the Python community and are not included with Python by default.\nThese packages are typically hosted on a package manager and Python Package Index (PyPI for short) and Anaconda are the two primary public package managers for Python. As of June 2025 there was about 645,000 packages available through the Python Package Index (PyPI)! Usually, you can ask Google or ChatGPT about what you are trying to do, and there is often a third party module to help you do it.\nTo install packages from PyPI we can either type the following in our terminal:\n# install from PyPI\npip install pkg_name\nAlternatively, you can install Python packages directly from a code cell in Jupyter notebooks by prefixing the pip install command with an exclamation mark (!). For example:\n# install within Jupyter notebook cell\n!pip install pkg_name\nThis allows you to manage dependencies without leaving your notebook environment.\n\n\n\n\n\n\nNoteTry It!\n\n\n\nGo ahead and see if you can pip install the pandas library.\n\n\nOnce a package is installed, you can import the library using the import statement and optionally assign it an alias (a short nickname), which is a common convention:\n# install within Jupyter notebook cell\n!pip install pandas\n# import package using the `pd` alias\nimport pandas as pd\nThroughout this course we’ll use several third party libraries focused on data science – for example Numpy, SciPy, Pandas, Scikit-learn, among others. Let’s look at some examples of these third party packages to give you a flavor of what they do. Don’t worry, we’ll go into some of these more thoroughly in later lessons!\n\nNumPy\nNumPy provides an efficient way to store and manipulate multi-dimensional dense arrays in Python. The important features of NumPy are:\n\nIt provides an ndarray structure, which allows efficient storage and manipulation of vectors, matrices, and higher-dimensional datasets.\nIt provides a readable and efficient syntax for operating on this data, from simple element-wise arithmetic to more complicated linear algebraic operations.\n\n\n\n\n\n\n\nAlthough the package is officially spelled “NumPy” you will commonly see it referred to as Numpy and numpy across the Python ecosystem (and even within this course).\n\n\n\nIn the simplest case, NumPy arrays look a lot like Python lists. For example, here is an array containing the range of numbers 1 to 9:\n\nimport numpy as np\n\nx = np.arange(1, 10)\nx\n\narray([1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n\n\n\n\n\nStandard convention is to import numpy as the np alias.\n\n\n\nNumPy’s arrays offer both efficient storage of data, as well as efficient element-wise operations on the data. For example, to square each element of the array, we can apply the ** operator to the array directly:\n\nx ** 2\n\narray([ 1,  4,  9, 16, 25, 36, 49, 64, 81])\n\n\nThis element-wise operation capability (commonly referred to as vectorization) is extremely useful but is not available in base Python. In base Python, if you had a list of these same numbers you would have to loop through each element in the list and compute the square of each number:\n\nx_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n# not supported\nx_list ** 2\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[7], line 4\n      1 x_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n      3 # not supported\n----&gt; 4 x_list ** 2\n\nTypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'\n\n\n\nWe would need to use a non-vectorized approach that iterates through each element and computes the square. The below illustrates the much more verbose non-vectorized approach that produces the same result:\n\n\n\n\n\n\nDon’t worry about the syntax, you will learn about this in a later lesson. Just note how the above approach with Numpy is far more convenient!\n\n\n\n\nx_squared = [val ** 2 for val in x_list]\nx_squared\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nNumpy also provides a host of other vectorized arithmetic capabilities. For example, we can compute the mean of a list with the following:\n\nnp.mean([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\nnp.float64(5.5)\n\n\nUnlike Python lists (which are limited to one dimension), NumPy arrays can be multi-dimensional. For example, here we will reshape our x array into a 3x3 matrix:\n\nm = x.reshape((3, 3))\nm\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\nA two-dimensional array is one representation of a matrix, and NumPy knows how to efficiently do typical matrix operations. For example, you can compute the transpose using .T:\n\nm.T\n\narray([[1, 4, 7],\n       [2, 5, 8],\n       [3, 6, 9]])\n\n\nor a matrix-vector product using np.dot:\n\nnp.dot(m, [5, 6, 7])\n\narray([ 38,  92, 146])\n\n\nand even more sophisticated operations like eigenvalue decomposition:\n\nnp.linalg.eigvals(m)\n\narray([ 1.61168440e+01, -1.11684397e+00, -1.30367773e-15])\n\n\nSuch linear algebraic manipulation underpins much of modern data analysis, particularly when it comes to the fields of machine learning and data mining.\n\n\nPandas\nPandas is a much newer package than Numpy, and is in fact built on top of it. What Pandas provides is a labeled interface to multi-dimensional data, in the form of a DataFrame object that will feel very familiar to users of R and related languages. DataFrames in Pandas look something like the following.\n\n\n\n\n\n\nIt is a common convention to import Pandas with the pd alias.\n\n\n\n\nimport pandas as pd\n\ndf = pd.DataFrame({'label': ['A', 'B', 'C', 'A', 'B', 'C'],\n                   'value': [1, 2, 3, 4, 5, 6]})\n\nThe Pandas interface allows you to do things like select columns by name:\n\ndf['label']\n\n0    A\n1    B\n2    C\n3    A\n4    B\n5    C\nName: label, dtype: object\n\n\nApply string operations across string entries:\n\ndf['label'].str.lower()\n\n0    a\n1    b\n2    c\n3    a\n4    b\n5    c\nName: label, dtype: object\n\n\nCompute statistical aggregations for numerical columns:\n\ndf['value'].sum()\n\nnp.int64(21)\n\n\nAnd, perhaps most importantly, do efficient database-style joins and groupings:\n\ndf.groupby('label').sum()\n\n\n\n\n\n\n\n\nvalue\n\n\nlabel\n\n\n\n\n\nA\n5\n\n\nB\n7\n\n\nC\n9\n\n\n\n\n\n\n\nHere in one line we have computed the sum of all objects sharing the same label, something that is much more verbose (and much less efficient) using tools provided in Numpy and core Python.\n\n\n\n\n\n\nIn future lessons we will go much deeper into Pandas and you’ll also see a large dependency on using Pandas as we start exploring other parts of the statistical computing ecosystem (i.e. visualization, machine learning).\n\n\n\n\n\nMatplotlib\nMatplotlib is currently the most popular scientific visualization packages in Python. Even proponents admit that its interface is sometimes overly verbose, but it is a powerful library for creating a large range of plots.\n\n\n\n\n\n\nIt is a common convention to import Matplotlib with the plt alias.\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')  # make graphs in the style of R's ggplot\n\nNow let’s create some data and plot the results:\n\nx = np.linspace(0, 10)  # range of values from 0 to 10\ny = np.sin(x)           # sine of these values\nplt.plot(x, y);         # plot as a line\n\n\n\n\n\n\n\n\nThis is the simplest example of a Matplotlib plot; for ideas on the wide range of plot types available, see Matplotlib’s online gallery.\n\n\n\n\n\n\nAlthough you’ll be exposed to some Matplotlib throughout this course, we will tend to focus on other third-party visualization packages that are simpler to use.\n\n\n\n\n\nSciPy\nSciPy is a collection of scientific functionality that is built on Numpy. The package began as a set of Python wrappers to well-known Fortran libraries for numerical computing, and has grown from there. The package is arranged as a set of submodules, each implementing some class of numerical algorithms. Here is an incomplete sample of some of the more important ones for data science:\n\nscipy.fftpack: Fast Fourier transforms\nscipy.integrate: Numerical integration\nscipy.interpolate: Numerical interpolation\nscipy.linalg: Linear algebra routines\nscipy.optimize: Numerical optimization of functions\nscipy.sparse: Sparse matrix storage and linear algebra\nscipy.stats: Statistical analysis routines\n\nFor example, let’s take a look at interpolating a smooth curve between some data\n\nfrom scipy import interpolate\n\n# choose eight points between 0 and 10\nx = np.linspace(0, 10, 8)\ny = np.sin(x)\n\n# create a cubic interpolation function\nfunc = interpolate.interp1d(x, y, kind='cubic')\n\n# interpolate on a grid of 1,000 points\nx_interp = np.linspace(0, 10, 1000)\ny_interp = func(x_interp)\n\n# plot the results\nplt.figure()  # new figure\nplt.plot(x, y, 'o')\nplt.plot(x_interp, y_interp);\n\n\n\n\n\n\n\n\nWhat we see is a smooth interpolation between the points.\n\n\nOther Data Science Packages\nBuilt on top of these tools are a host of other data science packages, including general tools like Scikit-Learn for machine learning, Scikit-Image for image analysis, Seaborn for statistical visualization, and Statsmodels for statistical modeling; as well as more domain-specific packages like AstroPy for astronomy and astrophysics, NiPy for neuro-imaging, and many, many more.\nNo matter what type of scientific, numerical, or statistical problem you are facing, it’s likely there is a Python package out there that can help you solve it.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneInstalling and Using Third-Party Libraries\n\n\n\n\nUse pip (or !pip in a Jupyter notebook) to install the following third-party libraries:\n\nnumpy\nbokeh\n\nOnce installed, copy and run the following code in a Jupyter notebook:\nimport numpy as np\nimport bokeh.plotting\nimport bokeh.io\n\nbokeh.io.output_notebook()\n\n# Generate plotting values\nt = np.linspace(0, 2*np.pi, 200)\nx = 16 * np.sin(t)**3\ny = 13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t)\n\np = bokeh.plotting.figure(height=250, width=275)\np.line(x, y, color='red', line_width=3)\ntext = bokeh.models.Label(x=0, y=0, text='BANA 4080', text_align='center')\np.add_layout(text)\n\nbokeh.io.show(p)\nAfter executing the code:\n\nWhat shape is created by this visualization?\nWhat part of the code controls the label “BANA 4080”?\nHow does numpy assist in preparing the data for visualization?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#summary",
    "href": "06-libraries.html#summary",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.5 Summary",
    "text": "6.5 Summary\nIn this chapter, you learned that one of Python’s greatest strengths is its rich ecosystem of reusable code—organized into modules, libraries, and packages. These tools allow you to write less code, solve complex problems more efficiently, and leverage the collective efforts of the Python community.\nWe began by discussing the difference between Python’s standard library and third-party libraries. You learned how to use built-in tools like math, os, and datetime for essential tasks, and how to install and import third-party packages using pip.\nWe then explored Python’s thriving data science ecosystem—highlighting libraries like NumPy, Pandas, Matplotlib, Seaborn, and SciPy. These libraries will be your go-to tools for data wrangling, statistical modeling, and visualization throughout this course and beyond.\nYou don’t need to memorize every function from every package right now. Instead, focus on building awareness of what kinds of tools exist and how to access them. The more you practice, the more fluent you’ll become in navigating and using Python’s expansive ecosystem.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#end-of-chapter-exercise-putting-python-libraries-to-work",
    "href": "06-libraries.html#end-of-chapter-exercise-putting-python-libraries-to-work",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.6 End-of-Chapter Exercise: Putting Python Libraries to Work",
    "text": "6.6 End-of-Chapter Exercise: Putting Python Libraries to Work\nCreate a new Jupyter notebook titled chapter-6-libraries-practice.ipynb. This notebook should include markdown cells to describe each section of your work and code cells to perform the tasks below. Be sure to run your code and document your findings or observations.\n\n\n\n\n\n\nNonePart 1: Standard Library Practice\n\n\n\n\n\nUse the following standard libraries: math, os, datetime, and random.\n\nMath Practice: Compute the square root, factorial, and log (base 10) of any number you choose using the math module.\nWorking with Files: Use the os module to print your current working directory and list all files in it.\nRandom Simulation: Use the random module to simulate flipping a coin 20 times. Count how many times you get heads vs. tails.\nDate Math: Use the datetime module to:\n\nPrint today’s date.\nCreate your birthday as a date object.\nCalculate how many days old you are.\n\n\n\n\n\n\n\n\n\n\n\nNonePart 2: Installing and Using Third-Party Libraries\n\n\n\n\n\n\nUse !pip install to install the following:\n\nnumpy\nbokeh\n\nCopy and run the following code to generate a heart-shaped plot:\nimport numpy as np\nimport bokeh.plotting\nimport bokeh.io\n\nbokeh.io.output_notebook()\n\nt = np.linspace(0, 2*np.pi, 200)\nx = 16 * np.sin(t)**3\ny = 13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t)\n\np = bokeh.plotting.figure(height=250, width=275)\np.line(x, y, color='red', line_width=3)\ntext = bokeh.models.Label(x=0, y=0, text='BANA 4080', text_align='center')\np.add_layout(text)\n\nbokeh.io.show(p)\nAdd a markdown cell answering the following:\n\nWhat shape is drawn?\nHow does numpy help in generating the plot data?\nWhat part of the code adds the label “BANA 4080”?\n\n\n\n\n\n\n\n\n\n\n\nNonePart 3: Summary Reflection\n\n\n\n\n\nIn a markdown cell, write 3–5 sentences reflecting on what you learned in this chapter. Consider:\n\nWhat surprised you about Python’s library ecosystem?\nWhich module or package do you think you’ll use the most in this course?\nDoes knowing about these tools change the way you think about programming?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html",
    "href": "07-importing-data.html",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "",
    "text": "7.1 From Disk to DataFrame: How Data Enters Python\nImagine you’re working as a summer intern for a real estate analytics firm. On your first day, your manager hands you a file: “Here’s the raw data for the Ames, Iowa housing market. Let’s start by pulling it into Python and taking a quick look around.”\nYou double-click the file — it’s filled with rows and rows of numbers, codes, and column headers you don’t quite understand. Where do you even begin?\nIn this chapter, you’ll walk through the exact steps you’d take in that situation. You’ll load real data, explore it using Python, and start to build your intuition for what’s inside a dataset. You won’t be doing full analysis yet — but you will learn how to get your bearings using one of Python’s most powerful tools: Pandas.\nLater chapters will teach you how to clean, transform, and analyze data — but first, you need to bring it into Python and take a look around.\nBy the end of this chapter, you will be able to:\nPython stores its data in memory - this makes it relatively quickly accessible but can cause size limitations in certain fields. In this class we will mainly work with small to moderate data sets, which means we should not run into any space limitations.\nPython memory is session-specific, so quitting Python (i.e. shutting down JupyterLab) removes the data from memory. A general way to conceptualize data import into and use within Python:\nHere is a visualization of this process:",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#from-disk-to-dataframe-how-data-enters-python",
    "href": "07-importing-data.html#from-disk-to-dataframe-how-data-enters-python",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "",
    "text": "Python does provide tooling that allows you to work with big data via distributed data (i.e. Pyspark) and relational databrases (i.e. SQL).\n\n\n\n\n\nData sits in on the computer/server - this is frequently called “disk”\nPython code can be used to copy a data file from disk to the Python session’s memory\nPython data then sits within Python’s memory ready to be used by other Python code\n\n\n\n\n\nPython memory",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#importing-delimited-files-with-read_csv",
    "href": "07-importing-data.html#importing-delimited-files-with-read_csv",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.2 Importing Delimited Files with read_csv()",
    "text": "7.2 Importing Delimited Files with read_csv()\nText files are a popular way to store and exchange tabular data. Nearly every data application supports exporting to CSV (Comma Separated Values) format or another type of text-based format. These files use a delimiter — such as a comma, tab, or pipe symbol — to separate elements within each line. Because of this consistent structure, importing text files into Python typically follows a straightforward process once the delimiter is identified.\nPandas provides a very efficient and simple way to load these types of files using its read_csv() function. While there are other approaches available (such as Python’s built-in csv module), Pandas is preferred for its ease of use and direct creation of a DataFrame — the primary tabular data structure used throughout this course.\n\nIn the example below, we use read_csv() to load a dataset listing some information on aircraft.\n\n\n\n\n\n\nPlease note that you must have internet access for this example to work.\nIn this first example, we will demonstrate how to import data directly from a URL. This approach is useful when your data is hosted online and you want to access it directly within your analysis.\nLater in this chapter, we will discuss how to import data that resides on your local computer.\n\n\n\n\nimport pandas as pd\n\nplanes = pd.read_csv('https://tinyurl.com/planes-data')\n\nWe see that our imported data is represented as a DataFrame:\n\ntype(planes)\n\npandas.core.frame.DataFrame\n\n\nWe can look at it in the Jupyter notebook, since Jupyter will display it in a well-organized, pretty way.\n\nplanes\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3317\nN997AT\n2002.0\nFixed wing multi engine\nBOEING\n717-200\n2\n100\nNaN\nTurbo-fan\n\n\n3318\nN997DL\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS AIRCRAFT CO\nMD-88\n2\n142\nNaN\nTurbo-fan\n\n\n3319\nN998AT\n2002.0\nFixed wing multi engine\nBOEING\n717-200\n2\n100\nNaN\nTurbo-fan\n\n\n3320\nN998DL\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS CORPORATION\nMD-88\n2\n142\nNaN\nTurbo-jet\n\n\n3321\nN999DN\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS CORPORATION\nMD-88\n2\n142\nNaN\nTurbo-jet\n\n\n\n\n3322 rows × 9 columns\n\n\n\nThis is a nice representation of the data, but we really do not need to display that many rows of the DataFrame in order to understand its structure. Instead, we can use the head() method of data frplanes to look at the first few rows. This is more manageable and gives us an overview of what the columns are. Note also the the missing data was populated with NaN.\n\nplanes.head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneGetting Started with Ames Housing Data\n\n\n\nUse the Pandas library to complete the following tasks.\n\nLoad the Data: Import the Ames housing dataset using the following URL - https://tinyurl.com/ames-raw\nVerify the Object Type: What type of Python object is the result of your import? Use the type() function to confirm.\nPreview the Data: Use the .head() method to print the first few rows of the dataset. Based on the output:\n\nWhat are some of the column names you see?\nWhat kinds of values are stored in these columns?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#file-paths",
    "href": "07-importing-data.html#file-paths",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.3 File Paths",
    "text": "7.3 File Paths\nIn the previous example, we imported data directly from a URL; however, datasets often reside on our computer and we need to specify the file path to read them from. For example, rather than import the planes.csv data from the URL we used above, I can read in that same dataset as follows.\n\nplanes = pd.read_csv('../data/planes.csv')\n\nplanes.head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nBut to understand why I use '../data/planes.csv' in the code above, we need to spend a little time talking about file paths.\n\nIt’s important to understand where files exist on your computer and how to reference those paths. There are two main approaches:\n\nAbsolute paths\nRelative paths\n\nAn absolute path always contains the root elements and the complete list of directories to locate the specific file or folder. For the planes.csv file, the absolute path on my computer is:\n\nimport os\n\nabsolute_path = os.path.abspath('../data/planes.csv')\nabsolute_path\n\n'/home/runner/work/uc-bana-4080/uc-bana-4080/data/planes.csv'\n\n\nI can always use this absolute path in pd.read_csv():\n\nplanes = pd.read_csv(absolute_path)\n\nIn contrast, a relative path is a path built starting from the current location. For example, say that I am operating in a directory called “Project A”. If I’m working in “my_notebook.ipynb” and I have a “my_data.csv” file in that same directory:\n# illustration of the directory layout\nProject A\n├── my_notebook.ipynb\n└── my_data.csv\nThen I can use this relative path to import this file: pd.read_csv('my_data.csv'). This just means to look for the ‘my_data.csv’ file relative to the current directory that I am in.\nOften, people store data in a “data” directory. If this directory is a subdirectory within my Project A directory:\n# illustration of the directory layout\nProject A\n├── my_notebook.ipynb\n└── data\n    └── my_data.csv\nThen I can use this relative path to import this file: pd.read_csv('data/my_data.csv'). This just means to look for the ‘data’ subdirectory relative to the current directory that I am in and then look for the ‘my_data.csv’ file.\nSometimes, the data directory may not be in the current directory. Sometimes a project directory will look the following where there is a subdirectory containing multiple notebooks and then another subdirectory containing data assets. If you are working in “notebook1.ipynb” within the notebooks subdirectory, you will need to tell Pandas to go up one directory relative to the notebook you are working in to the main Project A directory and then go down into the data directory.\n# illustration of the directory layout\nProject A\n├── notebooks\n│   ├── notebook1.ipynb\n│   ├── notebook2.ipynb\n│   └── notebook3.ipynb\n└── data\n    └── my_data.csv\nI can do this by using dot-notation in my relative path specification - here I use ‘..’ to imply “go up one directory relative to my current location”: pd.read_csv('../data/my_data.csv'). And this is why is used '../data/planes.csv' in the code at the beginning of this section, because my directory layout is:\nProject A\n├── notebooks\n│   ├── notebook1.ipynb\n│   ├── notebook2.ipynb\n│   └── notebook3.ipynb\n└── data\n    └── planes.csv\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry Different File Paths\n\n\n\nDownload the ames_raw.csv dataset and save the file in a folder structure like this:\nProject A\n├── notebooks\n│   └── import_demo.ipynb\n└── data\n    └── ames_raw.csv\nNow, do the following:\n\nImport using a relative path: In import_demo.ipynb, write the code to import the file using a relative path.\nImport using an absolute path: Use Python’s os.path.abspath() to determine the full absolute path of the file. Then use that path in pd.read_csv().\nReflection questions (write your answers in a Markdown cell):\n\nWhich method feels easier or more flexible to you?\nIn what situations might you prefer absolute over relative paths?\n\n\n\n\nGreat idea — adding an example using Google Colab will help students who are working in the cloud and need to upload files directly from their local machine. Here’s a clean, student-friendly example you can drop into your chapter:",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#importing-data-in-google-colab",
    "href": "07-importing-data.html#importing-data-in-google-colab",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.4 Importing Data in Google Colab",
    "text": "7.4 Importing Data in Google Colab\nIf you’re working in Google Colab, your files aren’t stored on your local machine — you’re running code on a cloud-based virtual machine. That means reading in local files (like a .csv on your desktop) works a little differently.\nHere’s how you can upload a file directly from your computer into Colab:\nfrom google.colab import files\n\n# This will open a file picker in Colab\nuploaded = files.upload()\nOnce you select the file you want to upload (e.g., planes.csv), Colab will store it temporarily in your session and make it available to use just like any other file:\nimport pandas as pd\n\n# Now you can load the file into a DataFrame\nplanes = pd.read_csv('planes.csv')\n\n\n\n\n\n\nFiles uploaded this way only persist for the current Colab session. If you close the browser or restart your runtime, you’ll need to re-upload the file.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#inspecting-your-dataframe",
    "href": "07-importing-data.html#inspecting-your-dataframe",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.5 Inspecting Your DataFrame",
    "text": "7.5 Inspecting Your DataFrame\nAfter importing, the data is stored as a DataFrame — the core data structure in Pandas. And with DataFrames, there are several ways to start understanding some basic, descriptive information about our data. For example, we can get the dimensions of our DataFrame. Here, we see that we have 3,322 rows and 9 columns.\n\nplanes.shape\n\n(3322, 9)\n\n\nWe can also see what type of data each column is. For example, we see that the tailnum column data type is object, the year column is a floating point (float64), and engines is an integer (int64).\n\nplanes.dtypes\n\ntailnum          object\nyear            float64\ntype             object\nmanufacturer     object\nmodel            object\nengines           int64\nseats             int64\nspeed           float64\nengine           object\ndtype: object\n\n\nThe following are the most common data types that appear frequently in DataFrames.\n\nboolean - only two possible values, True and False\ninteger - whole numbers without decimals\nfloat - numbers with decimals\nobject - typically strings, but may contain any object\ndatetime - a specific date and time with nanosecond precision\n\n\n\n\n\n\n\nBooleans, integers, floats, and datetimes all use a particular amount of memory for each of their values. The memory is measured in bits. The number of bits used for each value is the number appended to the end of the data type name. For instance, integers can be either 8, 16, 32, or 64 bits while floats can be 16, 32, 64, or 128. A 128-bit float column will show up as float128. Technically a float128 is a different data type than a float64 but generally you will not have to worry about such a distinction as the operations between different float columns will be the same.\n\n\n\nWe can also use the info() method, which provides output similar to dtypes, but also shows the number of non-missing values in each column along with more info such as:\n\nType of object (always a DataFrame)\nThe type of index and number of rows\nThe number of columns\nThe data types of each column and the number of non-missing (a.k.a non-null)\nThe frequency count of all data types\nThe total memory usage\n\n\nplanes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3322 entries, 0 to 3321\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   tailnum       3322 non-null   object \n 1   year          3252 non-null   float64\n 2   type          3322 non-null   object \n 3   manufacturer  3322 non-null   object \n 4   model         3322 non-null   object \n 5   engines       3322 non-null   int64  \n 6   seats         3322 non-null   int64  \n 7   speed         23 non-null     float64\n 8   engine        3322 non-null   object \ndtypes: float64(2), int64(2), object(5)\nmemory usage: 233.7+ KB\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneInspecting the Ames Housing Data\n\n\n\nIn the previous section, you imported the ames_raw.csv dataset. Now let’s explore its structure using the tools you just learned.\n\nHow big is the dataset?: Use .shape to find out how many rows and columns are in the dataset.\nWhat kinds of data are stored?: Use .dtypes to print out the data types of all columns.\n\nHow many columns are object type?\nHow many are float or int?\n\nDig deeper with .info()\n\nHow many non-null values are in each column?\nHow much memory does the dataset use?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#attributes-methods",
    "href": "07-importing-data.html#attributes-methods",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.6 Attributes & Methods",
    "text": "7.6 Attributes & Methods\n\nWe’ve seen that we can use the dot-notation to access functions in libraries (i.e. pd.read_csv()). We can use this same approach to access things inside of objects. What’s an object? Basically, a variable that contains other data or functionality inside of it that is exposed to users. Consequently, our DataFrame item is an object.\nIn the above code, we saw that we can make different calls with our DataFrame such as planes.shape and planes.head(). An observant reader probably noticed the difference between the two – one has parentheses and the other does not.\nAn attribute inside an object is simply a variable that is unique to that object and a method is just a function inside an object that is unique to that object.\n\n\n\n\n\n\nVariables inside an object are often called attributes and functions inside objects are called methods.\nattribute: A variable associated with an object and is referenced by name using dotted expressions. For example, if an object o has an attribute a it would be referenced as o.a\nmethod: A function associated with an object and is also referenced using dotted expressions but will include parentheses. For example, if an object o has a method m it would be called as o.m()\n\n\n\nEarlier, we saw the attributes shape and dtypes. Another attribute is columns, which will list all column names in our DataFrame.\n\nplanes.columns\n\nIndex(['tailnum', 'year', 'type', 'manufacturer', 'model', 'engines', 'seats',\n       'speed', 'engine'],\n      dtype='object')\n\n\nSimilar to regular functions, methods are called with parentheses and often take arguments. For example, we can use the tail() method to see the last n rows in our DataFrame:\n\nplanes.tail(3)\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n3319\nN998AT\n2002.0\nFixed wing multi engine\nBOEING\n717-200\n2\n100\nNaN\nTurbo-fan\n\n\n3320\nN998DL\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS CORPORATION\nMD-88\n2\n142\nNaN\nTurbo-jet\n\n\n3321\nN999DN\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS CORPORATION\nMD-88\n2\n142\nNaN\nTurbo-jet\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will be exposed to many of the available DataFrame methods throughout this course!\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneAttributes vs. Methods in the Ames Housing Data\n\n\n\nUsing the ames_raw DataFrame you’ve already imported:\n\nList all column names: Use the appropriate attribute to display all the column names in the dataset.\nPreview the last 5 rows: Use the appropriate method to display the last five rows of the dataset.\nIdentify each line of code: For each of the following, decide whether it’s a method or an attribute:\n\names.columns\names.head()\names.shape\names.tail(10)\names.dtypes\n\nReflect In a markdown cell, briefly explain in your own words:\n\nWhat’s the difference between an attribute and a method?\nHow can you tell which one you’re using?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#common-dataframe-errors",
    "href": "07-importing-data.html#common-dataframe-errors",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.7 Common DataFrame Errors",
    "text": "7.7 Common DataFrame Errors\nAs you’re learning to work with DataFrames in Pandas, you’ll likely encounter a few common errors. Don’t worry — these are normal and part of the learning process. This section introduces a few of the most frequent issues and how to fix them.\n\nForgetting Parentheses When Using a Method\nOne of the most common mistakes is confusing methods with attributes. Remember: methods require parentheses () — even if they don’t take arguments.\names.head     # 🚫 Returns the method itself, not the data\names.head()   # ✅ Correct — this returns the first few rows\n\n\nTypos in Column Names\nColumn names in a DataFrame must be typed exactly as they appear. They’re case-sensitive and must match spacing and punctuation.\names['SalePrice']     # ✅ Correct\names['saleprice']     # 🚫 KeyError: 'saleprice'\n\n\n\n\n\n\nTip: Use ames.columns to check exact column names.\n\n\n\n\n\nFileNotFoundError When Loading a File\nThis happens when the file path is incorrect or the file isn’t in the expected location.\n# 🚫 Incorrect: file doesn't exist at this path\npd.read_csv('data.csv')\n\n# ✅ Correct (based on your current working directory)\npd.read_csv('../data/ames_raw.csv')\n\n\n\n\n\n\nTip: Use os.getcwd() to check your working directory, and os.path.abspath() to confirm the full path.\n\n\n\n\n\nUsing Dot Notation with Column Names that Contain Spaces or Special Characters\nPandas allows you to access columns using dot notation only if the column name is a valid Python variable name.\names.SalePrice     # ✅ Works (if column is named 'SalePrice')\names.MS Zoning     # 🚫 SyntaxError\n\n# ✅ Use bracket notation instead\names['MS Zoning']\n\n\nConfusing Methods That Don’t Exist\nSometimes, learners assume there’s a method for something that doesn’t exist.\names.rows()    # 🚫 AttributeError: 'DataFrame' object has no attribute 'rows'\n\n\n\n\n\n\nTip: Use dir(ames) to see a list of available methods and attributes, or use tab-completion in Jupyter to explore.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#other-file-types",
    "href": "07-importing-data.html#other-file-types",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.8 Other File Types",
    "text": "7.8 Other File Types\nSo far, we’ve focused on CSVs — the most common format for tabular data. But Python, especially through the Pandas library, can import a wide variety of file types beyond just delimited text files.\nHere are a few common formats you might encounter:\n\nExcel spreadsheets (.xls, .xlsx)\nJSON files — often used for APIs and web data\nPickle files — Python’s own format for saving data structures\nSQL databases — such as SQLite, PostgreSQL, or MySQL\nParquet and Feather — efficient storage formats for big data workflows\n\nIn most cases, Pandas provides a convenient read_ function to handle the import process. For example, let’s look at how we can import an Excel file directly — without converting it to a CSV first.\n\nImporting Excel Files with read_excel()\nExcel is still one of the most widely used tools for storing and sharing data. And while many users convert Excel files into CSVs before importing them into Python, Pandas allows you to skip that step entirely.\nTo import data directly from an Excel workbook, you can use the read_excel() function. But first, you may need to install an additional dependency:\n# Run this in your terminal if you haven’t already\npip install openpyxl\nIn this example, we’ll import a mock dataset of grocery store products stored in a file called products.xlsx (download here).\n\n# Preview the available sheets in the workbook\nproducts_excel = pd.ExcelFile('../data/products.xlsx')\nproducts_excel.sheet_names\n\n['metadata', 'products data', 'grocery list']\n\n\nTo load a specific sheet from this workbook:\n\n\n\n\n\n\nIf you don’t explicitly specify a sheet name, Pandas will default to importing the first worksheet in the file.\n\n\n\n\nproducts = pd.read_excel('../data/products.xlsx', sheet_name='products data')\nproducts.head()\n\n\n\n\n\n\n\n\nproduct_num\ndepartment\ncommodity\nbrand_ty\nx5\n\n\n\n\n0\n92993\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n1\n93924\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n2\n94272\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n3\n94299\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n4\n94594\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s an additional video provided by Corey Schafer that you might find useful. It covers importing and exporting data from multiple different sources.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#summary",
    "href": "07-importing-data.html#summary",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.9 Summary",
    "text": "7.9 Summary\nIn this chapter, you learned how to import real-world data into Python and begin exploring it using Pandas — one of the most important tools in the data science workflow.\nYou started with a realistic scenario: opening up a raw dataset and figuring out how to make sense of it. You learned how to read in delimited files (like CSVs), use relative and absolute file paths, and inspect your data using essential DataFrame attributes and methods like .shape, .dtypes, .info(), and .head().\nYou also saw how Pandas supports a wide variety of file formats beyond CSV, including Excel, JSON, Pickle, and SQL databases — making it a flexible tool for working with nearly any type of data source.\nThis chapter was focused on getting your data into Python and taking a first look around. In upcoming chapters, we’ll dig into the heart of data science: cleaning, wrangling, summarizing, and uncovering patterns in your data to support real-world decision making.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#exercise-covid-19-cases-at-u.s.-colleges",
    "href": "07-importing-data.html#exercise-covid-19-cases-at-u.s.-colleges",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.10 Exercise: COVID-19 Cases at U.S. Colleges",
    "text": "7.10 Exercise: COVID-19 Cases at U.S. Colleges\nThe New York Times published a dataset tracking COVID-19 cases at colleges and universities across the United States. You can read about this dataset here. In this exercise, you’ll download and explore that dataset to practice the skills you’ve learned in this chapter.\n\n\n\n\n\n\nNoneStep 1: Download the Data\n\n\n\n\n\nDownload the dataset directly from this GitHub URL: colleges.csv\nSet up your project directory to look like this:\nBANA 4080\n├── notebooks\n│   └── covid_analysis.ipynb\n└── data\n    └── colleges.csv\n\n\n\n\n\n\n\n\n\nNoneStep 2: Import the Data\n\n\n\n\n\nIn your covid_analysis.ipynb notebook:\n\nImport Pandas\nLoad the dataset using a relative path\nUse the type() function to confirm that the data is stored as a DataFrame\n\n\n\n\n\n\n\n\n\n\nNoneStep 3: Explore the Structure\n\n\n\n\n\nUse the following DataFrame attributes and methods:\n\n.shape — how many rows and columns are in the dataset?\n.columns — what variables are included?\n.head() — what do the first few rows of the dataset look like?\n.info() — are there any missing values?\n\n\n\n\n\n\n\n\n\n\nNoneStep 4: Reflect\n\n\n\n\n\nIn a Markdown cell, write a short summary addressing the following:\n\nWhat is this dataset tracking?\nWhat are some variables you’d want to explore further?\nWhat kinds of questions could you answer with this data in future chapters?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html",
    "href": "08-dataframes.html",
    "title": "8  Deeper Dive on DataFrames",
    "section": "",
    "text": "8.1 Learning objectives\nImagine you’ve just imported a new airline dataset into Python. It includes hundreds of rows listing airline names and their carrier codes. Your manager asks, “Can you quickly pull a list of all airline names? And which carrier code has the longest name?” Before you can answer, you need to understand what’s inside that DataFrame.\nIn the previous chapter, we focused on how to import datasets into Python using Pandas — a crucial first step in any data analysis workflow. Now that we can get data into Python, we need to understand what we’re actually working with. This chapter takes a closer look at Pandas DataFrames and their building blocks, the Series.\nBy deepening your understanding of the DataFrame structure and how to access and manipulate data within it, you’ll build a foundation for future chapters focused on cleaning, transforming, and analyzing real-world data.\nAt the end of this lesson you should be able to:\nTo illustrate our points throughout this lesson, we’ll use the following airlines data which includes the name of the airline carrier and the airline carrier code:\nimport pandas as pd\n\ndf = pd.read_csv('../data/airlines.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#learning-objectives",
    "href": "08-dataframes.html#learning-objectives",
    "title": "8  Deeper Dive on DataFrames",
    "section": "",
    "text": "Explain the difference between DataFrames and Series\nAccess and manipulate data within DataFrames and Series\nSet and manipulate index values\n\n\n\n\n\n\n\nNote📓 Follow Along in Colab!\n\n\n\nAs you read through this chapter, we encourage you to follow along using the companion notebook in Google Colab (or other editor of choice). This interactive notebook lets you run code examples covered in the chapter—and experiment with your own ideas.\n👉 Open the Dataframes Notebook in Colab.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#understanding-the-dataframe-structure",
    "href": "08-dataframes.html#understanding-the-dataframe-structure",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.2 Understanding the DataFrame Structure",
    "text": "8.2 Understanding the DataFrame Structure\nA DataFrame is a two-dimensional, labeled data structure—similar to a table in Excel or a SQL database—that is used to store and manipulate structured data. You can think of it as a spreadsheet-like object where the data is organized into rows and columns.\nEach column in a DataFrame is actually a special type of object in Pandas called a Series. A Series is a one-dimensional array-like object that holds data and a corresponding index for labeling each entry. While the concept of a Series might be new to you, it’s fundamental to how Pandas works under the hood. Understanding how Series operate is essential because much of your interaction with DataFrames involves accessing and manipulating individual Series.\n\ndf.info()\ndf.shape\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 16 entries, 0 to 15\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   carrier  16 non-null     object\n 1   name     16 non-null     object\ndtypes: object(2)\nmemory usage: 388.0+ bytes\n\n\n(16, 2)\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneExploring Airport Data\n\n\n\nDownload and import the airports data into a DataFrame called airports.\n\nHow many rows and columns are in the dataset? Use .shape and .info() to help.\nWhat are the column names and data types? Are any surprising?\nTry printing the first 5 and last 5 rows using .head() and .tail(). What types of data does this dataset contain?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#series-the-building-blocks-of-dataframes",
    "href": "08-dataframes.html#series-the-building-blocks-of-dataframes",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.3 Series: The Building Blocks of DataFrames",
    "text": "8.3 Series: The Building Blocks of DataFrames\n\nColumns (and rows) of a DataFrame are actually Series objects. A Series is a one-dimensional labeled array capable of holding any data type — such as integers, strings, or even objects. You can think of it as a single column of data with a label on every entry. When you select a single column from a DataFrame, Pandas returns that column as a Series object.\n\n\n\n\n\n\nExample Series\n\n\n\n\nFigure 8.1: Source: Python Programming for Data Science\n\n\n\nYou can extract a Series using bracket notation with the column name:\n\ncarrier_column = df['carrier']\ncarrier_column\n\n0     9E\n1     AA\n2     AS\n3     B6\n4     DL\n5     EV\n6     F9\n7     FL\n8     HA\n9     MQ\n10    OO\n11    UA\n12    US\n13    VX\n14    WN\n15    YV\nName: carrier, dtype: object\n\n\nThis looks very much like a column of data — and it is — but under the hood, it’s a different type of object than the full DataFrame:\n\ntype(carrier_column)\n\npandas.core.series.Series\n\n\nThis confirms that what you extracted is a Series. You can verify its one-dimensional nature with:\n\ncarrier_column.shape  # One dimension (just the number of rows)\n\n(16,)\n\n\nCompare this with the shape of the full DataFrame, which has both rows and columns:\n\ndf.shape  # Two dimensions (rows, columns)\n\n(16, 2)\n\n\nIt’s important to be familiar with Series because they are fundamentally the core of DataFrames.\n\n\n\n\n\n\nExample DataFrame\n\n\n\n\nFigure 8.2: Source: Python Programming for Data Science\n\n\n\nUnderstanding this distinction is important, because many of the functions and behaviors available to Series differ from those of DataFrames. As we continue working with data, we’ll frequently switch between viewing data as Series and viewing it as part of a full DataFrame.\nInterestingly, when you extract a single row using .loc[], Pandas also returns a Series. In that case, the index of the Series becomes the column names of the original DataFrame:\n\n\n\n\n\n\n.loc[] is an accessor that allows us to retrieve rows and columns by labels. We’ll explore accessors more in the next chapter, but for now, just know we’re using it here to get the first row of the DataFrame.\n\n\n\n\nfirst_row = df.loc[0]  # Using .loc to access the first row of the DataFrame\ntype(first_row)        # pandas.core.series.Series\n\npandas.core.series.Series\n\n\nThis reinforces the idea that both columns and rows, when isolated, are treated as Series.\nAnother difference lies in the methods available to each. Series come with some specialized methods, such as .to_list(), which converts the data to a basic Python list. This method is not available on DataFrames:\n\ncarrier_column.to_list()  # works\n\n['9E',\n 'AA',\n 'AS',\n 'B6',\n 'DL',\n 'EV',\n 'F9',\n 'FL',\n 'HA',\n 'MQ',\n 'OO',\n 'UA',\n 'US',\n 'VX',\n 'WN',\n 'YV']\n\n\n\n# This will raise an error\ndf.to_list()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/tmp/ipykernel_12496/1197199109.py in ?()\n      1 # This will raise an error\n----&gt; 2 df.to_list()\n\n/opt/hostedtoolcache/Python/3.13.5/x64/lib/python3.13/site-packages/pandas/core/generic.py in ?(self, name)\n   6314             and name not in self._accessors\n   6315             and self._info_axis._can_hold_identifiers_and_holds_name(name)\n   6316         ):\n   6317             return self[name]\n-&gt; 6318         return object.__getattribute__(self, name)\n\nAttributeError: 'DataFrame' object has no attribute 'to_list'\n\n\n\n\n\n\n\n\n\nAs you continue through this book, you’ll learn many methods that apply specifically to either Series or DataFrames, and some that work on both. Becoming familiar with the structure of each will help you develop the intuition to choose and apply the right methods in the right context.\n\n\n\nFinally, Series tend to print more compactly and are useful when you’re only interested in a single set of values — like a single variable or row. Their lightweight format makes them a convenient choice in many day-to-day data tasks. \n\nKnowledge Check\n\n\n\n\n\n\nNoneDigging into Columns and Rows\n\n\n\nUsing the airports DataFrame:\n\nSelect the alt column and assign it to a variable called altitudes. What type of object is altitudes?\nUse type() and .shape to confirm it’s a Series and check its dimensionality.\nExtract the first row using .loc[0]. What type of object is this? What is its index made up of?\nCompare the output of airports[\"alt\"] and airports[[\"alt\"]]. What do you think is the difference between the two?\nTry calling .to_list() on both the airports[\"alt\"] and airports[[\"alt\"]] objects. What happens?\nWhat happens if you try using dot notation like airports.alt? When does this work vs. when could you see this failing?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#creating-and-indexing-a-series",
    "href": "08-dataframes.html#creating-and-indexing-a-series",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.4 Creating and Indexing a Series",
    "text": "8.4 Creating and Indexing a Series\n\nFirst, let’s create our own Series object from scratch – they don’t always need to come from a DataFrame. Here, we pass a list in as an argument and it will be converted to a Series:\n\ns = pd.Series([10, 20, 30, 40, 50])\ns\n\n0    10\n1    20\n2    30\n3    40\n4    50\ndtype: int64\n\n\nThis gives us a one-dimensional structure that prints a little differently than a DataFrame. There are three main parts of the Series to pay attention to:\n\nThe values (10, 20, 30…)\nThe dtype, short for data type (in this case, int64)\nThe index (0, 1, 2… by default)\n\nValues are fairly self-explanatory — they’re the list elements we passed in. The data type describes what kind of values are being stored (numbers, strings, etc.). Series are often homogeneous — holding only one data type — though technically they can hold a mix (which we’ll avoid for clarity).\nThe index is where things get more interesting. Every Series has an index, which functions much like the keys in a dictionary — each label maps to a value. By default, Pandas assigns numeric labels starting at 0:\n\ns.index  # Default index (RangeIndex)\ns\n\n0    10\n1    20\n2    30\n3    40\n4    50\ndtype: int64\n\n\nBut we can change the index to be more meaningful. For example, we could relabel the values using letters:\n\ns.index = ['a', 'b', 'c', 'd', 'e']\ns\n\na    10\nb    20\nc    30\nd    40\ne    50\ndtype: int64\n\n\nNow, if we want to access the value 40, we can do so by label:\n\ns['d']\n\nnp.int64(40)\n\n\nThis flexibility is powerful. Recall how rows in a DataFrame are also Series. Let’s revisit our airline DataFrame:\n\ndf.head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\nIf we extract the first row using .loc[0], we get:\n\nfirst_row = df.loc[0]\nfirst_row\n\ncarrier                   9E\nname       Endeavor Air Inc.\nName: 0, dtype: object\n\n\nHere, the index labels of the Series are the original column names:\n\nfirst_row['carrier']  # returns '9E'\n\n'9E'\n\n\nThis demonstrates how Series behave consistently whether they’re extracted from columns, rows, or created from scratch. Understanding indexing will help you fluently navigate, reshape, and analyze your data as we move forward.\n\nKnowledge Check\n\n\n\n\n\n\nNoneBuild Your Own Series\n\n\n\n\nCreate a Series with the following values: [33, 64, 77, 22, 51]. Assign it to temps.\nWhat are the index labels? What is the data type?\nChange the index to letters: ['a', 'b', 'c', 'd', 'e'].\nWhat value is associated with the label 'c'?\nReassign the index back to numbers. What’s one reason you might prefer named indexes over numbers?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#indexing-in-dataframes",
    "href": "08-dataframes.html#indexing-in-dataframes",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.5 Indexing in DataFrames",
    "text": "8.5 Indexing in DataFrames\n\nIt’s not just Series that have indexes! DataFrames have them too. Take a look at the carrier DataFrame again and note the bold numbers on the left.\n\ndf.head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\nThese numbers are an index, just like the one we saw on our example Series. And DataFrame indexes support similar functionality.\n\n# Our index is a range from 0 (inclusive) to 16 (exclusive).\ndf.index\n\nRangeIndex(start=0, stop=16, step=1)\n\n\nWhen loading in a DataFrame, the default index will always be 0 to N-1, where N is the number of rows in your DataFrame. This is called a RangeIndex. Selecting individual rows by their index can also be done with the .loc accessor.\n\n# Get the row at index 4 (the fifth row).\ndf.loc[4]\n\ncarrier                      DL\nname       Delta Air Lines Inc.\nName: 4, dtype: object\n\n\nAs with Series, DataFrames support reassigning their index. However, with DataFrames it often makes sense to change one of your columns into the index. This is analogous to a primary key in relational databases: a way to rapidly look up rows within a table.\nIn our case, maybe we will often use the carrier code (carrier) to look up the full name of the airline. In that case, it would make sense to set the carrier column as our index.\n\ndf = df.set_index('carrier')\ndf.head()\n\n\n\n\n\n\n\n\nname\n\n\ncarrier\n\n\n\n\n\n9E\nEndeavor Air Inc.\n\n\nAA\nAmerican Airlines Inc.\n\n\nAS\nAlaska Airlines Inc.\n\n\nB6\nJetBlue Airways\n\n\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\nNow the RangeIndex has been replaced with a more meaningful index, and it’s possible to look up rows of the table by passing a carrier code to the .loc accessor.\n\ndf.loc['AA']\n\nname    American Airlines Inc.\nName: AA, dtype: object\n\n\n\n\n\n\n\n\nPandas does not require that indexes have unique values (that is, no duplicates) although many relational databases do have that requirement of a primary key. This means that it is possible to create a non-unique index, but highly inadvisable. Having duplicate values in your index can cause unexpected results when you refer to rows by index – but multiple rows have that index. Don’t do it if you can help it!\n\n\n\nWhen starting to work with a DataFrame, it’s often a good idea to determine what column makes sense as your index and to set it immediately. This will make your code nicer – by letting you directly look up values with the index – and also make your selections and filters faster, because Pandas is optimized for operations by index. If you want to change the index of your DataFrame later, you can always reset_index (and then assign a new one).\n\ndf.head() # DataFrame with carrier as the index\n\n\n\n\n\n\n\n\nname\n\n\ncarrier\n\n\n\n\n\n9E\nEndeavor Air Inc.\n\n\nAA\nAmerican Airlines Inc.\n\n\nAS\nAlaska Airlines Inc.\n\n\nB6\nJetBlue Airways\n\n\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\n\ndf = df.reset_index() # resetting the index to be 0:n-1\ndf.head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvoid non-unique indexes — they can lead to ambiguous behavior!\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneWorking with DataFrame Indexes\n\n\n\n\nWhat kind of index does the airports DataFrame currently use? Use .index to explore.\nIs this a good index? Why or why not?\nSet the faa column as the new index using .set_index().\nUse .loc to look up the row for FAA code '4G0'. What is the altitude of this airport?\nReset the index. What happens to the faa column after you reset it?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#summary",
    "href": "08-dataframes.html#summary",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.6 Summary",
    "text": "8.6 Summary\nIn this chapter, you took your first deep dive into Pandas DataFrames — the central data structure for data analysis in Python. You learned that a DataFrame is essentially a table of data, where each column is a one-dimensional object called a Series. While Series and DataFrames are closely related, they behave differently and support different operations, and it’s important to recognize which you’re working with.\nYou also saw how to create Series objects manually and explored the concept of indexing — both in Series and in DataFrames. Indexing allows you to label and quickly access specific rows or columns, and gives structure to your data. We covered how to extract single rows and columns (which are treated as Series), how to interpret data types (dtype), and how to assign or reset index values.\nFinally, you learned that Pandas provides accessors like .loc for retrieving data by label, and that setting a meaningful index can make your code more readable and efficient. As we continue through the book, you’ll see more examples of these techniques in action, and you’ll build the intuition for when and how to apply them in your own data projects.\n\n\n\nConcept\nWhat it is\nExample\n\n\n\n\nDataFrame\n2D table of data\ndf.head()\n\n\nSeries\n1D column/row\ndf['carrier'], df.loc[0]\n\n\nIndex\nLabels for rows\ndf.set_index('carrier')\n\n\n\nIn the next chapter, you’ll begin working with real-world messy data — and the skills you learned here will make that much easier to manage.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#exercise-exploring-covid-19-data-in-colleges",
    "href": "08-dataframes.html#exercise-exploring-covid-19-data-in-colleges",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.7 Exercise: Exploring COVID-19 Data in Colleges",
    "text": "8.7 Exercise: Exploring COVID-19 Data in Colleges\nIn the previous chapter’s exercise, you imported COVID-19 data related to U.S. colleges and universities using the New York Times dataset. Let’s now build on that by exploring and interacting with the structure of the DataFrame.\nYou can access the data again from this link: https://github.com/nytimes/covid-19-data/blob/master/colleges/colleges.csv\n\n\n\n\n\n\nNoneStep 1: Load the data\n\n\n\n\n\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/nytimes/covid-19-data/master/colleges/colleges.csv\"\ncovid = pd.read_csv(url)\ncovid.head()\n\n\n\n\n\n\n\n\ndate\nstate\ncounty\ncity\nipeds_id\ncollege\ncases\ncases_2021\nnotes\n\n\n\n\n0\n2021-05-26\nAlabama\nMadison\nHuntsville\n100654\nAlabama A&M University\n41\nNaN\nNaN\n\n\n1\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100724\nAlabama State University\n2\nNaN\nNaN\n\n\n2\n2021-05-26\nAlabama\nLimestone\nAthens\n100812\nAthens State University\n45\n10.0\nNaN\n\n\n3\n2021-05-26\nAlabama\nLee\nAuburn\n100858\nAuburn University\n2742\n567.0\nNaN\n\n\n4\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100830\nAuburn University at Montgomery\n220\n80.0\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneStep 2: Working with the data\n\n\n\n\n\n\nWhat is the current shape of the DataFrame? How many rows and columns does it have?\nUse .info() to inspect the column names and data types. Are any of them unexpected?\nSelect the column containing cumulative cases (cases) and check its type and structure. Is it a Series or a DataFrame?\nTry the following covid['cases'].sum(). What does this output represent? Try some other summary statistics. We’ll cover more summary statistics and aggregation methods in the next few chapters.\n\n\n\n\n\n\n\n\n\n\nNoneStep 3: Set and reset the index\n\n\n\n\n\n\nWhat kind of index does the full covid DataFrame currently use?\nWould any of the existing columns make a better index? If so, which one and why?\nSet the ipeds_id column as the new index.\nUse .loc[] to look up the row for where ipeds_id equals 201885 and report:\n\nWhich university is this?\nWhat is the total number of cases?\n\nReset the index back to its original form.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html",
    "href": "09-subsetting.html",
    "title": "9  Subsetting Data",
    "section": "",
    "text": "Learning Objectives\nThis activity gives you a glimpse of a common task in data science: subsetting a dataset to focus on the most relevant information. Whether you’re analyzing flight records, home prices, or COVID case data, you’ll frequently need to extract specific rows and columns before you can analyze or visualize anything.\nIn this lesson, you’ll learn how to do this efficiently using Python and the pandas library — a skill that will save you time, reduce errors, and set the foundation for deeper analysis later.\nBy the end of this lesson, you’ll be able to:",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#prerequisites",
    "href": "09-subsetting.html#prerequisites",
    "title": "9  Subsetting Data",
    "section": "9.1 Prerequisites",
    "text": "9.1 Prerequisites\nTo illustrate selecting and filtering let’s go ahead and load the pandas library and import our planes data we’ve been using:\n\nimport pandas as pd\n\nplanes_df = pd.read_csv('../data/planes.csv')\nplanes_df.head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote📓 Follow Along in Colab!\n\n\n\nAs you read through this chapter, we encourage you to follow along using the companion notebook in Google Colab (or other editor of choice). This interactive notebook lets you run code examples covered in the chapter—and experiment with your own ideas.\n👉 Open the Subsetting Notebook in Colab.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#subsetting-dimensions",
    "href": "09-subsetting.html#subsetting-dimensions",
    "title": "9  Subsetting Data",
    "section": "9.2 Subsetting dimensions",
    "text": "9.2 Subsetting dimensions\n\nWe don’t always want all of the data in a DataFrame, so we need to take subsets of the DataFrame. In general, subsetting is extracting a small portion of a DataFrame – making the DataFrame smaller. Since the DataFrame is two-dimensional, there are two dimensions on which to subset.\nDimension 1: We may only want to consider certain variables. For example, we may only care about the year and engines variables:\n\nWe call this selecting columns/variables – this is similar to SQL’s SELECT or R’s dplyr package’s select().\nDimension 2: We may only want to consider certain cases. For example, we may only care about the cases where the manufacturer is Embraer.\n\nWe call this filtering or slicing – this is similar to SQL’s WHERE or R’s dplyr package’s filter() or slice(). And we can combine these two options to subset in both dimensions – the year and engines variables where the manufacturer is Embraer:\n\nIn the previous example, we want to do two things using planes_df:\n\nselect the year and engines variables\nfilter to cases where the manufacturer is Embraer\n\nBut we also want to return a new DataFrame – not just highlight certain cells. In other words, we want to turn this:\n\n\nCode\nplanes_df.head()\n\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nInto this:\n\n\nCode\nplanes_df.head().loc[planes_df['manufacturer'] == 'EMBRAER', ['year', 'engines']]\n\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n4\n2002.0\n2\n\n\n\n\n\n\n\nSo we really have a third need: return the resulting DataFrame so we can continue our analysis:\n\nselect the year and engines variables\nfilter to cases where the manufacturer is Embraer\nReturn a DataFrame to continue the analysis",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#subsetting-variables",
    "href": "09-subsetting.html#subsetting-variables",
    "title": "9  Subsetting Data",
    "section": "9.3 Subsetting variables",
    "text": "9.3 Subsetting variables\nRecall that the subsetting of variables/columns is called selecting variables/columns. In a simple example, we can select a single variable using bracket subsetting notation:\n\nplanes_df['year'].head()\n\n0    2004.0\n1    1998.0\n2    1999.0\n3    1999.0\n4    2002.0\nName: year, dtype: float64\n\n\nNotice the head() method also works on planes_df['year'] to return the first five elements.\n\n\n\n\n\n\nNonePop quiz!\n\n\n\nWhat is the data type of planes_df['year']?\n\n\nThis returns pandas.core.series.Series, referred to simply as a “Series”, rather than a DataFrame.\n\ntype(planes_df['year'])\n\npandas.core.series.Series\n\n\nThis is okay – the Series is a popular data structure in Python. Recall from a previous lesson:\n\nA Series is a one-dimensional data structure – this is similar to a Python list\nNote that all objects in a Series are usually of the same type (but this isn’t a strict requirement)\nEach DataFrame can be thought of as a list of equal-length Series (plus an Index)\n\n\n\n\n\n\nSeries can be useful, but for now, we are interested in returning a DataFrame rather than a series. We can select a single variable and return a DataFrame by still using bracket subsetting notation, but this time we will pass a list of variables names:\n\nplanes_df[['year']].head()\n\n\n\n\n\n\n\n\nyear\n\n\n\n\n0\n2004.0\n\n\n1\n1998.0\n\n\n2\n1999.0\n\n\n3\n1999.0\n\n\n4\n2002.0\n\n\n\n\n\n\n\nAnd we can see that we’ve returned a DataFrame:\n\ntype(planes_df[['year']].head())\n\npandas.core.frame.DataFrame\n\n\n\n\n\n\n\n\nNonePop quiz!\n\n\n\nWhat do you think is another advantage of passing a list?\n\n\nPassing a list into the bracket subsetting notation allows us to select multiple variables at once:\n\nplanes_df[['year', 'engines']].head()\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n1\n1998.0\n2\n\n\n2\n1999.0\n2\n\n\n3\n1999.0\n2\n\n\n4\n2002.0\n2\n\n\n\n\n\n\n\nIn another example, assume we are interested in the model of plane, number of seats and engine type:\n\nplanes_df[['model', 'seats', 'engine']].head()\n\n\n\n\n\n\n\n\nmodel\nseats\nengine\n\n\n\n\n0\nEMB-145XR\n55\nTurbo-fan\n\n\n1\nA320-214\n182\nTurbo-fan\n\n\n2\nA320-214\n182\nTurbo-fan\n\n\n3\nA320-214\n182\nTurbo-fan\n\n\n4\nEMB-145LR\n55\nTurbo-fan\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This\n\n\n\n\n______ is a common term for subsetting DataFrame variables.\nWhat type of object is a DataFrame column?\nWhat will be returned by the following code?\nplanes_df['type', 'model']",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#subsetting-rows",
    "href": "09-subsetting.html#subsetting-rows",
    "title": "9  Subsetting Data",
    "section": "9.4 Subsetting rows",
    "text": "9.4 Subsetting rows\nWhen we subset rows (aka cases, records, observations) we primarily use two names: slicing and filtering, but these are not the same:\n\nslicing, similar to row indexing, subsets observations by the value of the Index\nfiltering subsets observations using a conditional test\n\n\nSlicing rows\nRemember that all DataFrames have an Index:\n\nplanes_df.head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nWe can slice cases/rows using the values in the Index and bracket subsetting notation. It’s common practice to use .loc to slice cases/rows:\n\nplanes_df.loc[0:5]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n5\nN105UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that since this is not “indexing”, the last element is inclusive.\n\n\n\nWe can also pass a list of Index values:\n\nplanes_df.loc[[0, 2, 4, 6, 8]]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n6\nN107US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n8\nN109UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\nFiltering rows\nWe can filter rows using a logical sequence equal in length to the number of rows in the DataFrame.\nContinuing our example, assume we want to determine whether each case’s manufacturer is Embraer. We can use the manufacturer Series and a logical equivalency test to find the result for each row:\n\nplanes_df['manufacturer'] == 'EMBRAER'\n\n0        True\n1       False\n2       False\n3       False\n4        True\n        ...  \n3317    False\n3318    False\n3319    False\n3320    False\n3321    False\nName: manufacturer, Length: 3322, dtype: bool\n\n\nWe can use this resulting logical sequence to test filter cases – rows that are True will be returned while those that are False will be removed:\n\nplanes_df[planes_df['manufacturer'] == 'EMBRAER'].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n10\nN11106\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n11\nN11107\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n12\nN11109\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nThis also works with .loc:\n\nplanes_df.loc[planes_df['manufacturer'] == 'EMBRAER'].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n10\nN11106\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n11\nN11107\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n12\nN11109\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nAny conditional test can be used to filter DataFrame rows:\n\n# Filter observations where year is greater than 2002\nplanes_df.loc[planes_df['year'] &gt; 2002].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n15\nN11121\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n16\nN11127\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n17\nN11137\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n18\nN11140\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nAnd multiple conditional tests can be combined using logical operators:\n\n# Filter observations where year is greater than 2002 and less than 2007\nplanes_df.loc[(planes_df['year'] &gt; 2002) & (planes_df['year'] &lt; 2007)].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n15\nN11121\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n16\nN11127\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n17\nN11137\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n18\nN11140\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that each condition is wrapped in parentheses – this is required.\n\n\n\nOften, as your condition gets more complex, it can be easier to read if you separate out the condition:\n\ncond = (planes_df['year'] &gt; 2002) & (planes_df['year'] &lt; 2004)\nplanes_df.loc[cond].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n15\nN11121\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n16\nN11127\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n17\nN11137\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n18\nN11140\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n19\nN11150\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This\n\n\n\n\nWhat’s the difference between slicing cases and filtering cases?\nFill in the blanks to fix the following code to find planes that have more than three engines:\n planes_df.loc[______['______'] &gt; 3]",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#selecting-variables-and-filtering-rows",
    "href": "09-subsetting.html#selecting-variables-and-filtering-rows",
    "title": "9  Subsetting Data",
    "section": "9.5 Selecting variables and filtering rows",
    "text": "9.5 Selecting variables and filtering rows\nIf we want to select variables and filter cases at the same time, we have a few options:\n\nSequential operations\nSimultaneous operations\n\n\nSequential Operations\nWe can use what we’ve previously learned to select variables and filter cases in multiple steps:\n\nplanes_df_filtered = planes_df.loc[planes_df['manufacturer'] == 'EMBRAER']\nplanes_df_filtered_and_selected = planes_df_filtered[['year', 'engines']]\nplanes_df_filtered_and_selected.head()\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n4\n2002.0\n2\n\n\n10\n2002.0\n2\n\n\n11\n2002.0\n2\n\n\n12\n2002.0\n2\n\n\n\n\n\n\n\nThis is a good way to learn how to select and filter independently, and it also reads very clearly.\n\n\nSimultaneous operations\nHowever, we can also do both selecting and filtering in a single step with .loc:\n\nplanes_df.loc[planes_df['manufacturer'] == 'EMBRAER', ['year', 'engines']].head()\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n4\n2002.0\n2\n\n\n10\n2002.0\n2\n\n\n11\n2002.0\n2\n\n\n12\n2002.0\n2\n\n\n\n\n\n\n\nThis option is more succinct and also reduces programming time. As before, as your filtering and selecting conditions get longer and/or more complex, it can make it easier to read to break it up into separate lines:\n\nrows = planes_df['manufacturer'] == 'EMBRAER'\ncols = ['year', 'engines']\nplanes_df.loc[rows, cols].head()\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n4\n2002.0\n2\n\n\n10\n2002.0\n2\n\n\n11\n2002.0\n2\n\n\n12\n2002.0\n2\n\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This\n\n\n\nSubset planes_df to only include planes made by Boeing and only return the seats and model variables.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#views-vs-copies",
    "href": "09-subsetting.html#views-vs-copies",
    "title": "9  Subsetting Data",
    "section": "9.6 Views vs copies",
    "text": "9.6 Views vs copies\nOne thing to be aware of, as you will likely experience it eventually, is the concept of returning a view (“looking” at a part of an existing object) versus a copy (making a new copy of the object in memory). This can be a bit abstract and even this section in the Pandas docs states “…it’s very hard to predict whether it will return a view or a copy.”\nThe main takeaway is that the most common warning you’ll encounter in Pandas is the SettingWithCopyWarning; Pandas raises it as a warning that you might not be doing what you think you’re doing or because the operation you are performing may behave unpredictably.\nLet’s look at an example. Say the number of seats on this particular plane was recorded incorrectly. Instead of 55 seats it should actually be 60 seats.\n\ntailnum_of_interest = planes_df['tailnum'] == 'N10156'\nplanes_df[tailnum_of_interest]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nInstead of using .iloc, we could actually filter and select this element in our DataFrame with the following bracket notation.\n\nplanes_df[tailnum_of_interest]['seats']\n\n0    55\nName: seats, dtype: int64\n\n\n\nplanes_df[tailnum_of_interest]['seats'] = 60\n\n/tmp/ipykernel_12525/2190037627.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  planes_df[tailnum_of_interest]['seats'] = 60\n\n\nSo what’s going on? Did our DataFrame get changed?\n\nplanes_df[tailnum_of_interest]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nNo it didn’t, even though you probably thought it did. What happened above is that planes_df[tailnum_of_interest]['seats'] was executed first and returned a copy of the DataFrame, which is an entirely different object. We can confirm by using id():\n\nprint(f\"The id of the original dataframe is: {id(planes_df)}\")\nprint(f\" The id of the indexed dataframe is: {id(planes_df[tailnum_of_interest])}\")\n\nThe id of the original dataframe is: 140520938092400\n The id of the indexed dataframe is: 140520937270288\n\n\nWe then tried to set a value on this new object by appending ['seats'] = 60. Pandas is warning us that we are doing that operation on a copy of the original dataframe, which is probably not what we want. To fix this, you need to index in a single go, using .loc[] for example:\n\nplanes_df.loc[tailnum_of_interest, 'seats'] = 60\n\nNo error this time! And let’s confirm the change:\n\nplanes_df[tailnum_of_interest]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n60\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe concept of views and copies is confusing and you can read more about it here.\nBut realize, this behavior is changing in pandas 3.0. A new system called Copy-on-Write will become the default, and it will prevent chained indexing from working at all — meaning instead of getting the SettingWithCopyWarning warning, pandas will simply raise an error.\nRegardless, always use .loc[] for combined filtering and selecting!",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#summary",
    "href": "09-subsetting.html#summary",
    "title": "9  Subsetting Data",
    "section": "9.7 Summary",
    "text": "9.7 Summary\nIn this chapter, you learned how to zoom in on the parts of a DataFrame that matter most. Whether you’re interested in just a few variables or a specific set of cases, being able to subset your data is a critical first step in any analysis.\nWe started by giving you a real-world task in Excel or Google Sheets — find aircraft built by Embraer in 2004 or later. That hands-on activity highlighted a common problem: manual filtering doesn’t scale. That’s where Python and pandas come in.\nIn this chapter, you learned how to use pandas for:\n\nSelecting columns using single or multiple variable names\nSlicing rows by index position\nFiltering rows using conditional logic\nCombining selection and filtering with .loc[] for efficient subsetting\n\n\n🧾 Quick Reference: Subsetting Techniques\n\n\n\n\n\n\n\n\nTask\nSyntax Example\nOutput Type\n\n\n\n\nSelect one column\ndf[\"col\"]\nSeries\n\n\nSelect multiple columns\ndf[[\"col1\", \"col2\"]]\nDataFrame\n\n\nSlice rows by index\ndf.loc[0:4]\nDataFrame\n\n\nFilter rows by condition\ndf[df[\"year\"] &gt; 2000]\nDataFrame\n\n\nCombine filter + select\ndf.loc[df[\"year\"] &gt; 2000, [\"col1\", \"col2\"]]\nDataFrame\n\n\nBest practice for assignment\ndf.loc[cond, \"col\"] = value\nSafe, avoids warnings\n\n\n\n\n\n🔁 Revisit the Challenge\nNow that you’ve learned how to subset data using pandas, go back to the original question:\n\nWhich aircraft were manufactured by Embraer in 2004 or later?\n\nThis time, solve it using Python instead of a spreadsheet. Use what you’ve learned in this chapter — filtering rows by conditions, and selecting only the columns you need — to create a clean, focused DataFrame.\nWhen you’re done, try to answer:\n\nHow many rows matched the condition?\nWhat columns did you choose to keep?\nCould you reuse your code later for different conditions?\n\nThis is how data scientists work: writing reusable, scalable code to extract insights from large datasets.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#exercise-subsetting-covid-college-data",
    "href": "09-subsetting.html#exercise-subsetting-covid-college-data",
    "title": "9  Subsetting Data",
    "section": "9.8 Exercise: Subsetting COVID College Data",
    "text": "9.8 Exercise: Subsetting COVID College Data\nIn this exercise, you’ll apply what you learned to subset the New York Times college COVID-19 dataset. The dataset tracks COVID cases reported by colleges across the U.S.\n\n📂 Download the data from this GitHub link or load it directly from a local copy if provided.\n\n\n\n\n\n\n\nNoneStep 1: Load the Data\n\n\n\n\n\n\nimport pandas as pd\n\ndata_url = \"https://raw.githubusercontent.com/nytimes/covid-19-data/refs/heads/master/colleges/colleges.csv\"\ncollege_df = pd.read_csv(data_url)\ncollege_df.head()\n\n\n\n\n\n\n\n\ndate\nstate\ncounty\ncity\nipeds_id\ncollege\ncases\ncases_2021\nnotes\n\n\n\n\n0\n2021-05-26\nAlabama\nMadison\nHuntsville\n100654\nAlabama A&M University\n41\nNaN\nNaN\n\n\n1\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100724\nAlabama State University\n2\nNaN\nNaN\n\n\n2\n2021-05-26\nAlabama\nLimestone\nAthens\n100812\nAthens State University\n45\n10.0\nNaN\n\n\n3\n2021-05-26\nAlabama\nLee\nAuburn\n100858\nAuburn University\n2742\n567.0\nNaN\n\n\n4\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100830\nAuburn University at Montgomery\n220\n80.0\nNaN\n\n\n\n\n\n\n\nThis dataset includes many columns, but for this exercise, we’re going to focus only on:\n\nstate\ncity\ncollege\ncases\n\n\n\n\n\n\n\n\n\n\nNoneStep 2: Subsetting Practice\n\n\n\n\n\n\nSelect only the columns: state, city, college, and cases.\nFilter the dataset to show only colleges in Ohio (state == \"OH\") that reported more than 100 cases.\nHow many Ohio colleges reported more than 100 cases? (Hint: Use .shape or len())\n\n\n\n\n\n\n\n\n\n\nNoneStep 3: Dig Into a Specific School\n\n\n\n\n\n\nFilter the dataset to find records where the college is “University of Cincinnati” (case sensitive).\nHow many cases were reported by the University of Cincinnati?\n\n\n\n\n\n\n\n\n\n\nNoneMake It Dynamic\n\n\n\n\n\nTry making your filtering logic more flexible by defining parameters at the top of your notebook:\nmy_state = \"OH\"\nthreshold = 100\nThen write code that will:\n\nFilter for all colleges in my_state with cases greater than threshold\nReturn only the college and cases columns\n\nTest your code with a few different states and thresholds. Can you reuse it to answer different questions (i.e. How many colleges in California reported more than 500 Covid cases?)",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html",
    "href": "10-manipulating-data.html",
    "title": "10  Manipulating Data",
    "section": "",
    "text": "The Ames Housing Data\nIn our previous chapter, we explored how to access, index, and subset data from a pandas DataFrame. These are essential skills for understanding and navigating real-world datasets. Now we take the next step: learning how to manipulate data.\nAs a data analyst or scientist, you will spend much of your time transforming raw data into something clean, interpretable, and analysis-ready. This chapter provides the foundation for doing just that. You’ll learn to rename columns, create new variables, deal with missing values, and apply functions to your data — all of which are fundamental skills in the data science workflow.\nBy the end of this chapter, you will be able to:\nYou first encountered the Ames Housing dataset back in Chapter 7, where you were challenged with the task of analyzing raw data for the Ames, Iowa housing market. In this chapter, we’ll return to the Ames data as our primary example to learn how to manipulate and prepare real-world data for analysis.\nBefore diving into the new concepts, take a few minutes to reacquaint yourself with the data:\nLet’s begin by loading the dataset and inspecting the first few rows:\nimport pandas as pd\n\names = pd.read_csv('../data/ames_raw.csv')\names.head()\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 82 columns\nWe’ll use this dataset throughout the chapter to demonstrate how to rename columns, compute new variables, handle missing data, and apply transformations — all crucial steps in cleaning and preparing data for analysis and modeling.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#renaming-columns",
    "href": "10-manipulating-data.html#renaming-columns",
    "title": "10  Manipulating Data",
    "section": "10.1 Renaming Columns",
    "text": "10.1 Renaming Columns\nOne of the first things you’ll often do when working with a new dataset is clean up the column names. Column names might contain spaces, inconsistent capitalization, or other formatting quirks that make them harder to work with in code. In this section, we’ll walk through a few ways to rename columns in a DataFrame using the Ames Housing dataset.\nLet’s start by looking at the current column names:\n\names.columns\n\nIndex(['Order', 'PID', 'MS SubClass', 'MS Zoning', 'Lot Frontage', 'Lot Area',\n       'Street', 'Alley', 'Lot Shape', 'Land Contour', 'Utilities',\n       'Lot Config', 'Land Slope', 'Neighborhood', 'Condition 1',\n       'Condition 2', 'Bldg Type', 'House Style', 'Overall Qual',\n       'Overall Cond', 'Year Built', 'Year Remod/Add', 'Roof Style',\n       'Roof Matl', 'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type',\n       'Mas Vnr Area', 'Exter Qual', 'Exter Cond', 'Foundation', 'Bsmt Qual',\n       'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin SF 1',\n       'BsmtFin Type 2', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Total Bsmt SF',\n       'Heating', 'Heating QC', 'Central Air', 'Electrical', '1st Flr SF',\n       '2nd Flr SF', 'Low Qual Fin SF', 'Gr Liv Area', 'Bsmt Full Bath',\n       'Bsmt Half Bath', 'Full Bath', 'Half Bath', 'Bedroom AbvGr',\n       'Kitchen AbvGr', 'Kitchen Qual', 'TotRms AbvGrd', 'Functional',\n       'Fireplaces', 'Fireplace Qu', 'Garage Type', 'Garage Yr Blt',\n       'Garage Finish', 'Garage Cars', 'Garage Area', 'Garage Qual',\n       'Garage Cond', 'Paved Drive', 'Wood Deck SF', 'Open Porch SF',\n       'Enclosed Porch', '3Ssn Porch', 'Screen Porch', 'Pool Area', 'Pool QC',\n       'Fence', 'Misc Feature', 'Misc Val', 'Mo Sold', 'Yr Sold', 'Sale Type',\n       'Sale Condition', 'SalePrice'],\n      dtype='object')\n\n\nYou might notice that some of these column names contain spaces or uppercase letters, such as \"MS SubClass\" and \"MS Zoning\". These formatting issues can be inconvenient when writing code — especially if you’re trying to access a column using dot notation (e.g., df.column_name) or when using string methods.\n\nRenaming Specific Columns\nWe can rename one or more columns using the .rename() method. This method accepts a dictionary where the keys are the original column names and the values are the new names you’d like to assign.\n\names.rename(columns={'MS SubClass': 'ms_subclass', 'MS Zoning': 'ms_zoning'})\n\n\n\n\n\n\n\n\nOrder\nPID\nms_subclass\nms_zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\n2926\n923275080\n80\nRL\n37.0\n7937\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nGdPrv\nNaN\n0\n3\n2006\nWD\nNormal\n142500\n\n\n2926\n2927\n923276100\n20\nRL\nNaN\n8885\nPave\nNaN\nIR1\nLow\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2006\nWD\nNormal\n131000\n\n\n2927\n2928\n923400125\n85\nRL\n62.0\n10441\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nShed\n700\n7\n2006\nWD\nNormal\n132000\n\n\n2928\n2929\n924100070\n20\nRL\n77.0\n10010\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2006\nWD\nNormal\n170000\n\n\n2929\n2930\n924151050\n60\nRL\n74.0\n9627\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n11\n2006\nWD\nNormal\n188000\n\n\n\n\n2930 rows × 82 columns\n\n\n\nThis command runs without error; and if we check out our data (below) nothing seems different. Why?\n\names.head(3)\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n\n\n3 rows × 82 columns\n\n\n\nThat’s because .rename() returns a new DataFrame with the updated names, but it does not modify the original DataFrame unless you explicitly tell it to.\nThere are two common ways to make these changes permanent:\n\nUse the inplace=True argument\nReassign the modified DataFrame to the same variable\n\nThe Pandas development team recommends the second approach (reassigning) for most use cases, as it leads to clearer and more predictable code.\n\names = ames.rename(columns={'MS SubClass': 'ms_subclass', 'MS Zoning': 'ms_zoning'})\names.head()\n\n\n\n\n\n\n\n\nOrder\nPID\nms_subclass\nms_zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 82 columns\n\n\n\n\n\n\n\n\n\nAlways include columns= when using .rename(). If you don’t, Pandas will assume you are renaming index values instead of column names — and it won’t raise a warning or error if you make this mistake.\n\n\n\n\n\nRenaming Many Columns at Once\nUsing .rename() works well for renaming one or two columns. But if you want to rename many columns — such as applying a consistent format across the entire DataFrame — it’s more efficient to use the .columns attribute and vectorized string methods.\nPandas provides powerful tools for working with strings through the .str accessor. For example, we can convert all column names to lowercase like this:\n\names.columns.str.lower()\n\nIndex(['order', 'pid', 'ms_subclass', 'ms_zoning', 'lot frontage', 'lot area',\n       'street', 'alley', 'lot shape', 'land contour', 'utilities',\n       'lot config', 'land slope', 'neighborhood', 'condition 1',\n       'condition 2', 'bldg type', 'house style', 'overall qual',\n       'overall cond', 'year built', 'year remod/add', 'roof style',\n       'roof matl', 'exterior 1st', 'exterior 2nd', 'mas vnr type',\n       'mas vnr area', 'exter qual', 'exter cond', 'foundation', 'bsmt qual',\n       'bsmt cond', 'bsmt exposure', 'bsmtfin type 1', 'bsmtfin sf 1',\n       'bsmtfin type 2', 'bsmtfin sf 2', 'bsmt unf sf', 'total bsmt sf',\n       'heating', 'heating qc', 'central air', 'electrical', '1st flr sf',\n       '2nd flr sf', 'low qual fin sf', 'gr liv area', 'bsmt full bath',\n       'bsmt half bath', 'full bath', 'half bath', 'bedroom abvgr',\n       'kitchen abvgr', 'kitchen qual', 'totrms abvgrd', 'functional',\n       'fireplaces', 'fireplace qu', 'garage type', 'garage yr blt',\n       'garage finish', 'garage cars', 'garage area', 'garage qual',\n       'garage cond', 'paved drive', 'wood deck sf', 'open porch sf',\n       'enclosed porch', '3ssn porch', 'screen porch', 'pool area', 'pool qc',\n       'fence', 'misc feature', 'misc val', 'mo sold', 'yr sold', 'sale type',\n       'sale condition', 'saleprice'],\n      dtype='object')\n\n\nOr, we can chain multiple string methods together to standardize our column names — converting them to lowercase and replacing all spaces with underscores:\n\names.columns = ames.columns.str.lower().str.replace(\" \", \"_\")\names.head()\n\n\n\n\n\n\n\n\norder\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\n...\npool_area\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 82 columns\n\n\n\nThis makes your column names easier to type, more consistent, and less prone to errors in your code.\n\n\n\n\n\n\nYou can explore other string operations that work with .str by checking out the Pandas string methods documentation.\n\n\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry This!\n\n\n\nLet’s practice cleaning up messy column names using the techniques from this section. Below is a small example dataset with inconsistent column names:\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'First Name': ['Alice', 'Bob', 'Charlie'],\n    'Last-Name': ['Smith', 'Jones', 'Brown'],\n    'AGE ': [25, 30, 22],\n    'Email Address': ['alice@example.com', 'bob@example.com', 'charlie@example.com']\n})\nYour task:\n\nPrint the current column names. What formatting issues do you see?\nClean the column names by doing the following:\n\nConvert all column names to lowercase\nReplace any spaces or hyphens with underscores\nStrip any leading or trailing whitespace\n\nAssign the cleaned column names back to the DataFrame.\nPrint the updated column names and confirm the changes.\n\nHint: .columns.str.lower(), .str.replace(), and .str.strip() will come in handy here!",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#performing-calculations-with-columns",
    "href": "10-manipulating-data.html#performing-calculations-with-columns",
    "title": "10  Manipulating Data",
    "section": "10.2 Performing Calculations with Columns",
    "text": "10.2 Performing Calculations with Columns\nOnce your data is loaded and your columns are cleaned up, the next common task is to perform calculations on your data. This might include creating new variables, transforming existing values, or applying arithmetic operations across columns.\nLet’s begin by focusing on the saleprice column in the Ames Housing dataset. This column records the sale price of each home in dollars. For example:\n\nsale_price = ames['saleprice']\nsale_price\n\n0       215000\n1       105000\n2       172000\n3       244000\n4       189900\n         ...  \n2925    142500\n2926    131000\n2927    132000\n2928    170000\n2929    188000\nName: saleprice, Length: 2930, dtype: int64\n\n\nThese numbers are fairly large — often six digits long. In many analyses or visualizations, it can be helpful to express values in thousands of dollars instead of raw dollar amounts.\nTo convert the sale price to thousands, we simply divide each value by 1,000:\n\nsale_price_k = sale_price / 1000\nsale_price_k\n\n0       215.0\n1       105.0\n2       172.0\n3       244.0\n4       189.9\n        ...  \n2925    142.5\n2926    131.0\n2927    132.0\n2928    170.0\n2929    188.0\nName: saleprice, Length: 2930, dtype: float64\n\n\nThis results in a new Series where each home’s price is now shown in thousands. For instance, a home that originally sold for $215,000 is now shown as 215.0.\nAt this point, sale_price_k is a new object that exists separately from the ames DataFrame. In the next section, we’ll learn how to add this new variable as a column in our DataFrame so we can use it in further analysis.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#adding-and-removing-columns",
    "href": "10-manipulating-data.html#adding-and-removing-columns",
    "title": "10  Manipulating Data",
    "section": "10.3 Adding and Removing Columns",
    "text": "10.3 Adding and Removing Columns\nOnce you’ve created a new variable — like sale_price_k in the previous section — you’ll often want to add it to your existing DataFrame so it becomes part of the dataset you’re working with.\n\nAdding Columns\nIn pandas, you can add a new column to a DataFrame using assignment syntax:\n# example syntax\ndf['new_column_name'] = new_column_series\nLet’s add the sale_price_k series (which represents sale prices in thousands) to the ames DataFrame:\n\names['sale_price_k'] = sale_price_k\names.head()\n\n\n\n\n\n\n\n\norder\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\n...\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nsale_price_k\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n215.0\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n105.0\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n172.0\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n244.0\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n189.9\n\n\n\n\n5 rows × 83 columns\n\n\n\nNow, you’ll see that a new column called \"sale_price_k\" appears at the end of the DataFrame.\n\n\n\n\n\n\nNotice how the column name ('sale_price_k') is placed in quotes inside the brackets on the left-hand side, while the Series providing the data goes on the right-hand side without quotes or brackets.\n\n\n\nThis entire process can be done in a single step, without creating an intermediate variable:\n\names['sale_price_k'] = ames['saleprice'] / 1000\n\nThis kind of operation is common in data science. What we’re doing here is applying vectorized math — performing arithmetic between a Series (a vector of values) and a scalar (a single constant value).\nHere are a few more examples:\n\n# Subtracting a scalar from a Series\n(ames['saleprice'] - 12).head()\n\n0    214988\n1    104988\n2    171988\n3    243988\n4    189888\nName: saleprice, dtype: int64\n\n\n\n# Multiplying a Series by a scalar\n(ames['saleprice'] * 10).head()\n\n0    2150000\n1    1050000\n2    1720000\n3    2440000\n4    1899000\nName: saleprice, dtype: int64\n\n\n\n# Raising a Series to a power\n(ames['saleprice'] ** 2).head()\n\n0    46225000000\n1    11025000000\n2    29584000000\n3    59536000000\n4    36062010000\nName: saleprice, dtype: int64\n\n\nVectorized operations like these are fast, efficient, and more readable than writing explicit loops.\n\n\nRemoving Columns\nJust as easily as we can add columns, we can also remove them. This is helpful when a column is no longer needed or was created only temporarily.\nTo drop one or more columns from a DataFrame, use the .drop() method with the columns= argument:\n\names = ames.drop(columns=['order', 'sale_price_k'])\names.head()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\npool_area\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 81 columns\n\n\n\nThis removes the \"order\" and \"sale_price_k\" columns from the DataFrame. Remember that most pandas methods return a new DataFrame by default, so you’ll need to reassign the result back to ames (or another variable) to make the change permanent.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneCreate a utility_space variable\n\n\n\n\nCreate a new column utility_space that is 1/5 of the above ground living space (gr_liv_area).\nYou will get fractional output with step #1. See if you can figure out how to round this output to the nearest integer.\nNow remove this column from your DataFrame",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#overwriting-columns",
    "href": "10-manipulating-data.html#overwriting-columns",
    "title": "10  Manipulating Data",
    "section": "10.4 Overwriting columns",
    "text": "10.4 Overwriting columns\nWhat if we discovered a systematic error in our data? Perhaps we find out that the “lot_area” column is not entirely accurate because the recording process includes an extra 50 square feet for every property. We could create a new column, “real_lot_area” but we’re not going to need the original “lot_area” column, and leaving it could cause confusion for others looking at our data.\nA better solution would be to replace the original column with the new, recalculated, values. We can do so using the same syntax as for creating a new column.\n\n# Subtract 50 from lot area, and then overwrite the original data.\names['lot_area'] = ames['lot_area'] - 50\names.head()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\npool_area\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31720\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n526350040\n20\nRH\n80.0\n11572\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n526351010\n20\nRL\n81.0\n14217\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n526353030\n20\nRL\n93.0\n11110\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n527105010\n60\nRL\n74.0\n13780\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 81 columns",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#calculating-with-multiple-columns",
    "href": "10-manipulating-data.html#calculating-with-multiple-columns",
    "title": "10  Manipulating Data",
    "section": "10.5 Calculating with Multiple Columns",
    "text": "10.5 Calculating with Multiple Columns\nUp to this point, we’ve focused on performing calculations between a column (Series) and a scalar — for example, dividing every value in saleprice by 1,000. But pandas also allows you to perform operations between columns — this is known as vector-vector arithmetic.\nLet’s look at an example where we calculate a new metric: price per square foot. We can compute this by dividing the sale price of each home by its above-ground living area (gr_liv_area):\n\nprice_per_sqft = ames['saleprice'] / ames['gr_liv_area']\nprice_per_sqft.head()\n\n0    129.830918\n1    117.187500\n2    129.420617\n3    115.639810\n4    116.574586\ndtype: float64\n\n\nNow that we’ve computed the new values, let’s add them to our DataFrame as a new column:\n\names['price_per_sqft'] = price_per_sqft\names.head()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31720\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n129.830918\n\n\n1\n526350040\n20\nRH\n80.0\n11572\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n117.187500\n\n\n2\n526351010\n20\nRL\n81.0\n14217\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n129.420617\n\n\n3\n526353030\n20\nRL\n93.0\n11110\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n115.639810\n\n\n4\n527105010\n60\nRL\n74.0\n13780\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n116.574586\n\n\n\n\n5 rows × 82 columns\n\n\n\nAs before, you could write this as a one-liner:\n\names['price_per_sqft'] = ames['saleprice'] / ames['gr_liv_area']\n\n\nCombining Multiple Operations\nYou can also combine multiple columns and scalars in more complex expressions. For example, the following line combines three columns and a constant:\n\names['nonsense'] = (ames['yr_sold'] + 12) * ames['gr_liv_area'] + ames['lot_area'] - 50\names.head()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\nnonsense\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31720\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n129.830918\n3380102\n\n\n1\n526350040\n20\nRH\n80.0\n11572\nPave\nNaN\nReg\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n117.187500\n1823234\n\n\n2\n526351010\n20\nRL\n81.0\n14217\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n129.420617\n2701405\n\n\n3\n526353030\n20\nRL\n93.0\n11110\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n115.639810\n4277480\n\n\n4\n527105010\n60\nRL\n74.0\n13780\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n116.574586\n3307568\n\n\n\n\n5 rows × 83 columns\n\n\n\nThis creates a column called \"nonsense\" using a mix of vector-vector and vector-scalar operations. While this particular example isn’t meaningful analytically, it shows how you can chain together multiple operations in a single expression.\nIn practice, you’ll often calculate new variables using a combination of existing columns — for example, calculating cost efficiency, total square footage, or ratios between two quantities. Being comfortable with these kinds of operations is essential for building features and preparing data for analysis or modeling.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneCreate a price_per_total_sqft variable\n\n\n\nCreate a new column price_per_total_sqft that is saleprice divided by the sum of gr_liv_area, total_bsmt_sf, wood_deck_sf, open_porch_sf.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#working-with-string-columns",
    "href": "10-manipulating-data.html#working-with-string-columns",
    "title": "10  Manipulating Data",
    "section": "10.6 Working with String Columns",
    "text": "10.6 Working with String Columns\nSo far, we’ve focused on numeric calculations — things like dividing, multiplying, and creating new variables based on numbers. But many datasets also contain non-numeric values, such as names, categories, or descriptive labels.\nIn pandas, string data is stored as object or string type columns, and you can perform operations on them just like you would with numbers. This is especially useful for cleaning, formatting, or combining text.\n\nString Concatenation\nA common operation with string data is concatenation — combining multiple strings together. For example, suppose we want to create a descriptive sentence using the neighborhood and sale condition of each home in the Ames dataset:\n\n'Home in ' + ames['neighborhood'] + ' neighborhood sold under ' + ames['sale_condition'] + ' condition'\n\n0       Home in NAmes neighborhood sold under Normal c...\n1       Home in NAmes neighborhood sold under Normal c...\n2       Home in NAmes neighborhood sold under Normal c...\n3       Home in NAmes neighborhood sold under Normal c...\n4       Home in Gilbert neighborhood sold under Normal...\n                              ...                        \n2925    Home in Mitchel neighborhood sold under Normal...\n2926    Home in Mitchel neighborhood sold under Normal...\n2927    Home in Mitchel neighborhood sold under Normal...\n2928    Home in Mitchel neighborhood sold under Normal...\n2929    Home in Mitchel neighborhood sold under Normal...\nLength: 2930, dtype: object\n\n\nThis works just like string addition in Python. Each piece of text is combined row by row across the DataFrame to generate a new sentence for each observation.\n\n\nString Methods with .str\nFor more advanced string operations, pandas provides a powerful set of tools through the .str accessor. This gives you access to many string-specific methods like .lower(), .replace(), .len(), and more.\nHere are a few examples:\n\n# Count the number of characters in each neighborhood name\names['neighborhood'].str.len()\n\n0       5\n1       5\n2       5\n3       5\n4       7\n       ..\n2925    7\n2926    7\n2927    7\n2928    7\n2929    7\nName: neighborhood, Length: 2930, dtype: int64\n\n\n\n# Standardize the format of garage type labels\names['garage_type'].str.lower().str.replace('tchd', 'tached')\n\n0       attached\n1       attached\n2       attached\n3       attached\n4       attached\n          ...   \n2925    detached\n2926    attached\n2927         NaN\n2928    attached\n2929    attached\nName: garage_type, Length: 2930, dtype: object\n\n\nThese methods are especially helpful when cleaning messy or inconsistent text data — for example, fixing capitalization, removing whitespace, or replacing substrings.\n\n\n\n\n\n\nIn this chapter, we’ve only scratched the surface of working with non-numeric data. In later chapters, we’ll take a deeper look at how to clean, transform, and analyze string values, as well as how to work with date and time data — including parsing timestamps, extracting components like month and day, and calculating time differences.\nFor now, if you want to dig into working with string columns some more, it’s worth exploring the official Pandas documentation on string methods to see the full range of capabilities.\n\n\n\nWhether you’re formatting text for a report, cleaning up inconsistent labels, or preparing inputs for machine learning models, working with string columns is a valuable part of your data wrangling skill set.\n\names\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\nnonsense\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31720\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n129.830918\n3380102\n\n\n1\n526350040\n20\nRH\n80.0\n11572\nPave\nNaN\nReg\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n117.187500\n1823234\n\n\n2\n526351010\n20\nRL\n81.0\n14217\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n129.420617\n2701405\n\n\n3\n526353030\n20\nRL\n93.0\n11110\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n115.639810\n4277480\n\n\n4\n527105010\n60\nRL\n74.0\n13780\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n116.574586\n3307568\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\n923275080\n80\nRL\n37.0\n7887\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nGdPrv\nNaN\n0\n3\n2006\nWD\nNormal\n142500\n142.073779\n2031891\n\n\n2926\n923276100\n20\nRL\nNaN\n8835\nPave\nNaN\nIR1\nLow\nAllPub\n...\nMnPrv\nNaN\n0\n6\n2006\nWD\nNormal\n131000\n145.232816\n1829021\n\n\n2927\n923400125\n85\nRL\n62.0\n10391\nPave\nNaN\nReg\nLvl\nAllPub\n...\nMnPrv\nShed\n700\n7\n2006\nWD\nNormal\n132000\n136.082474\n1967801\n\n\n2928\n924100070\n20\nRL\n77.0\n9960\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2006\nWD\nNormal\n170000\n122.390209\n2812912\n\n\n2929\n924151050\n60\nRL\n74.0\n9577\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n11\n2006\nWD\nNormal\n188000\n94.000000\n4045527\n\n\n\n\n2930 rows × 83 columns",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#more-complex-column-manipulation",
    "href": "10-manipulating-data.html#more-complex-column-manipulation",
    "title": "10  Manipulating Data",
    "section": "10.7 More Complex Column Manipulation",
    "text": "10.7 More Complex Column Manipulation\nAs you become more comfortable working with individual columns, you’ll often find yourself needing to do more than basic math. In this section, we’ll cover a few additional, common column operations:\n\nReplacing values using a mapping\nIdentifying and handling missing values\nApplying custom functions\n\nThese are core techniques that will serve you in any data cleaning or feature engineering workflow.\n\nReplacing Values\nOne fairly common situation in data wrangling is needing to convert one set of values to another, where there is a one-to-one correspondence between the values currently in the column and the new values that should replace them. This operation can be described as “mapping one set of values to another”.\nLet’s look at an example of this. In our Ames data the month sold is represented numerically:\n\names['mo_sold'].head()\n\n0    5\n1    6\n2    6\n3    4\n4    3\nName: mo_sold, dtype: int64\n\n\nSuppose we want to change this so that values are represented by the month name:\n\n1 = ‘Jan’\n2 = ‘Feb’\n…\n12 = ‘Dec’\n\nWe can express this mapping of old values to new values using a Python dictionary.\n\n# Only specify the values we want to replace; don't include the ones that should stay the same.\nvalue_mapping = {\n    1: 'Jan',\n    2: 'Feb',\n    3: 'Mar',\n    4: 'Apr',\n    5: 'May',\n    6: 'Jun',\n    7: 'Jul',\n    8: 'Aug',\n    9: 'Sep',\n    10: 'Oct',\n    11: 'Nov',\n    12: 'Dec'\n    }\n\nPandas provides a handy method on Series, .replace, that accepts this value mapping and updates the Series accordingly. We can use it to recode our values.\n\names['mo_sold'].replace(value_mapping).head()\n\n0    May\n1    Jun\n2    Jun\n3    Apr\n4    Mar\nName: mo_sold, dtype: object\n\n\n\n\n\n\n\n\nIf you are a SQL user, this workflow may look familiar to you; it’s quite similar to a CASE WHEN statement in SQL.\n\n\n\n\n\nMissing values\nIn real-world datasets, missing values are common. In pandas, these are usually represented as NaN (Not a Number).\nTo detect missing values in a DataFrame, use .isnull(). This returns a DataFrame of the same shape with True where values are missing:\n\names.isnull()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\nnonsense\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2926\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2927\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2928\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2929\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n2930 rows × 83 columns\n\n\n\nWe can use this to easily compute the total number of missing values in each column:\n\names.isnull().sum()\n\npid                 0\nms_subclass         0\nms_zoning           0\nlot_frontage      490\nlot_area            0\n                 ... \nsale_type           0\nsale_condition      0\nsaleprice           0\nprice_per_sqft      0\nnonsense            0\nLength: 83, dtype: int64\n\n\nRecall we also get this information with .info(). Actually, we get the inverse as .info() tells us how many non-null values exist in each column.\n\names.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2930 entries, 0 to 2929\nData columns (total 83 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   pid              2930 non-null   int64  \n 1   ms_subclass      2930 non-null   int64  \n 2   ms_zoning        2930 non-null   object \n 3   lot_frontage     2440 non-null   float64\n 4   lot_area         2930 non-null   int64  \n 5   street           2930 non-null   object \n 6   alley            198 non-null    object \n 7   lot_shape        2930 non-null   object \n 8   land_contour     2930 non-null   object \n 9   utilities        2930 non-null   object \n 10  lot_config       2930 non-null   object \n 11  land_slope       2930 non-null   object \n 12  neighborhood     2930 non-null   object \n 13  condition_1      2930 non-null   object \n 14  condition_2      2930 non-null   object \n 15  bldg_type        2930 non-null   object \n 16  house_style      2930 non-null   object \n 17  overall_qual     2930 non-null   int64  \n 18  overall_cond     2930 non-null   int64  \n 19  year_built       2930 non-null   int64  \n 20  year_remod/add   2930 non-null   int64  \n 21  roof_style       2930 non-null   object \n 22  roof_matl        2930 non-null   object \n 23  exterior_1st     2930 non-null   object \n 24  exterior_2nd     2930 non-null   object \n 25  mas_vnr_type     1155 non-null   object \n 26  mas_vnr_area     2907 non-null   float64\n 27  exter_qual       2930 non-null   object \n 28  exter_cond       2930 non-null   object \n 29  foundation       2930 non-null   object \n 30  bsmt_qual        2850 non-null   object \n 31  bsmt_cond        2850 non-null   object \n 32  bsmt_exposure    2847 non-null   object \n 33  bsmtfin_type_1   2850 non-null   object \n 34  bsmtfin_sf_1     2929 non-null   float64\n 35  bsmtfin_type_2   2849 non-null   object \n 36  bsmtfin_sf_2     2929 non-null   float64\n 37  bsmt_unf_sf      2929 non-null   float64\n 38  total_bsmt_sf    2929 non-null   float64\n 39  heating          2930 non-null   object \n 40  heating_qc       2930 non-null   object \n 41  central_air      2930 non-null   object \n 42  electrical       2929 non-null   object \n 43  1st_flr_sf       2930 non-null   int64  \n 44  2nd_flr_sf       2930 non-null   int64  \n 45  low_qual_fin_sf  2930 non-null   int64  \n 46  gr_liv_area      2930 non-null   int64  \n 47  bsmt_full_bath   2928 non-null   float64\n 48  bsmt_half_bath   2928 non-null   float64\n 49  full_bath        2930 non-null   int64  \n 50  half_bath        2930 non-null   int64  \n 51  bedroom_abvgr    2930 non-null   int64  \n 52  kitchen_abvgr    2930 non-null   int64  \n 53  kitchen_qual     2930 non-null   object \n 54  totrms_abvgrd    2930 non-null   int64  \n 55  functional       2930 non-null   object \n 56  fireplaces       2930 non-null   int64  \n 57  fireplace_qu     1508 non-null   object \n 58  garage_type      2773 non-null   object \n 59  garage_yr_blt    2771 non-null   float64\n 60  garage_finish    2771 non-null   object \n 61  garage_cars      2929 non-null   float64\n 62  garage_area      2929 non-null   float64\n 63  garage_qual      2771 non-null   object \n 64  garage_cond      2771 non-null   object \n 65  paved_drive      2930 non-null   object \n 66  wood_deck_sf     2930 non-null   int64  \n 67  open_porch_sf    2930 non-null   int64  \n 68  enclosed_porch   2930 non-null   int64  \n 69  3ssn_porch       2930 non-null   int64  \n 70  screen_porch     2930 non-null   int64  \n 71  pool_area        2930 non-null   int64  \n 72  pool_qc          13 non-null     object \n 73  fence            572 non-null    object \n 74  misc_feature     106 non-null    object \n 75  misc_val         2930 non-null   int64  \n 76  mo_sold          2930 non-null   int64  \n 77  yr_sold          2930 non-null   int64  \n 78  sale_type        2930 non-null   object \n 79  sale_condition   2930 non-null   object \n 80  saleprice        2930 non-null   int64  \n 81  price_per_sqft   2930 non-null   float64\n 82  nonsense         2930 non-null   int64  \ndtypes: float64(12), int64(28), object(43)\nmemory usage: 1.9+ MB\n\n\nWe can use any() to identify which columns have missing values. We can use this information for various reasons such as subsetting for just those columns that have missing values.\n\nmissing = ames.isnull().any() # identify if missing values exist in each column\names[missing[missing].index]  # subset for just those columns that have missing values\n\n\n\n\n\n\n\n\nlot_frontage\nalley\nmas_vnr_type\nmas_vnr_area\nbsmt_qual\nbsmt_cond\nbsmt_exposure\nbsmtfin_type_1\nbsmtfin_sf_1\nbsmtfin_type_2\n...\ngarage_type\ngarage_yr_blt\ngarage_finish\ngarage_cars\ngarage_area\ngarage_qual\ngarage_cond\npool_qc\nfence\nmisc_feature\n\n\n\n\n0\n141.0\nNaN\nStone\n112.0\nTA\nGd\nGd\nBLQ\n639.0\nUnf\n...\nAttchd\n1960.0\nFin\n2.0\n528.0\nTA\nTA\nNaN\nNaN\nNaN\n\n\n1\n80.0\nNaN\nNaN\n0.0\nTA\nTA\nNo\nRec\n468.0\nLwQ\n...\nAttchd\n1961.0\nUnf\n1.0\n730.0\nTA\nTA\nNaN\nMnPrv\nNaN\n\n\n2\n81.0\nNaN\nBrkFace\n108.0\nTA\nTA\nNo\nALQ\n923.0\nUnf\n...\nAttchd\n1958.0\nUnf\n1.0\n312.0\nTA\nTA\nNaN\nNaN\nGar2\n\n\n3\n93.0\nNaN\nNaN\n0.0\nTA\nTA\nNo\nALQ\n1065.0\nUnf\n...\nAttchd\n1968.0\nFin\n2.0\n522.0\nTA\nTA\nNaN\nNaN\nNaN\n\n\n4\n74.0\nNaN\nNaN\n0.0\nGd\nTA\nNo\nGLQ\n791.0\nUnf\n...\nAttchd\n1997.0\nFin\n2.0\n482.0\nTA\nTA\nNaN\nMnPrv\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\n37.0\nNaN\nNaN\n0.0\nTA\nTA\nAv\nGLQ\n819.0\nUnf\n...\nDetchd\n1984.0\nUnf\n2.0\n588.0\nTA\nTA\nNaN\nGdPrv\nNaN\n\n\n2926\nNaN\nNaN\nNaN\n0.0\nGd\nTA\nAv\nBLQ\n301.0\nALQ\n...\nAttchd\n1983.0\nUnf\n2.0\n484.0\nTA\nTA\nNaN\nMnPrv\nNaN\n\n\n2927\n62.0\nNaN\nNaN\n0.0\nGd\nTA\nAv\nGLQ\n337.0\nUnf\n...\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nMnPrv\nShed\n\n\n2928\n77.0\nNaN\nNaN\n0.0\nGd\nTA\nAv\nALQ\n1071.0\nLwQ\n...\nAttchd\n1975.0\nRFn\n2.0\n418.0\nTA\nTA\nNaN\nNaN\nNaN\n\n\n2929\n74.0\nNaN\nBrkFace\n94.0\nGd\nTA\nAv\nLwQ\n758.0\nUnf\n...\nAttchd\n1993.0\nFin\n3.0\n650.0\nTA\nTA\nNaN\nNaN\nNaN\n\n\n\n\n2930 rows × 27 columns\n\n\n\n\nDropping Missing Values\nWhen you have missing values, we usually either drop them or impute them.You can drop missing values with .dropna():\n\names.dropna()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\nnonsense\n\n\n\n\n\n\n0 rows × 83 columns\n\n\n\nWhoa! What just happened? Well, this data set actually has a missing value in every single row. .dropna() drops every row that contains a missing value so we end up dropping all observations. Consequently, we probably want to figure out what’s going on with these missing values and isolate the column causing the problem and imputing the values if possible.\n\n\n\n\n\n\nAnother “drop” method is .drop_duplcates() which will drop duplicated rows in your DataFrame.\n\n\n\n\n\nVisualizing Missingness\nSometimes visualizations help identify patterns in missing values. One thing I often do is print a heatmap of my dataframe to get a feel for where my missing values are. We’ll get into data visualization in future lessons but for now here is an example using the searborn library. We can see that several variables have a lot of missing values (alley, fireplace_qu, pool_qc, fence, misc_feature).\n\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(12, 8)})\n\n\names_missing = ames[missing[missing].index]\nsns.heatmap(ames_missing.isnull(), cmap='viridis', cbar=False);\n\n\n\n\n\n\n\n\n\n\nFilling Missing Values (Imputation)\nSince we can’t drop all missing values in this data set (since it leaves us with no rows), we need to impute (“fill”) them in. There are several approaches we can use to do this; one of which uses the .fillna() method. This method has various options for filling, you can use a fixed value, the mean of the column, the previous non-nan value, etc:\n\nimport numpy as np\n\n# example DataFrame with missing values\ndf = pd.DataFrame([[np.nan, 2, np.nan, 0],\n                   [3, 4, np.nan, 1],\n                   [np.nan, np.nan, np.nan, 5],\n                   [np.nan, 3, np.nan, 4]],\n                  columns=list('ABCD'))\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\nNaN\n2.0\nNaN\n0\n\n\n1\n3.0\n4.0\nNaN\n1\n\n\n2\nNaN\nNaN\nNaN\n5\n\n\n3\nNaN\n3.0\nNaN\n4\n\n\n\n\n\n\n\n\ndf.fillna(0)  # fill with 0\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n0.0\n2.0\n0.0\n0\n\n\n1\n3.0\n4.0\n0.0\n1\n\n\n2\n0.0\n0.0\n0.0\n5\n\n\n3\n0.0\n3.0\n0.0\n4\n\n\n\n\n\n\n\n\ndf.fillna(df.mean())  # fill with the mean\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n3.0\n2.0\nNaN\n0\n\n\n1\n3.0\n4.0\nNaN\n1\n\n\n2\n3.0\n3.0\nNaN\n5\n\n\n3\n3.0\n3.0\nNaN\n4\n\n\n\n\n\n\n\n\ndf.bfill()  # backward (upwards) fill from non-nan values\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n3.0\n2.0\nNaN\n0\n\n\n1\n3.0\n4.0\nNaN\n1\n\n\n2\nNaN\n3.0\nNaN\n5\n\n\n3\nNaN\n3.0\nNaN\n4\n\n\n\n\n\n\n\n\ndf.ffill()  # forward (downward) fill from non-nan values\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\nNaN\n2.0\nNaN\n0\n\n\n1\n3.0\n4.0\nNaN\n1\n\n\n2\n3.0\n4.0\nNaN\n5\n\n\n3\n3.0\n3.0\nNaN\n4\n\n\n\n\n\n\n\n\n\n\nApplying custom functions\nThere will be times when you want to apply a function that is not built-in to Pandas. For this, we have methods:\n\ndf.apply(), applies a function column-wise or row-wise across a dataframe (the function must be able to accept/return an array)\ndf.applymap(), applies a function element-wise (for functions that accept/return single values at a time)\nseries.apply()/series.map(), same as above but for Pandas series\n\nFor example, say you had the following custom function that defines if a home is considered a luxery home simply based on the price sold.\n\n\n\n\n\n\nDon’t worry, you’ll learn more about writing your own functions in future lessons!\n\n\n\n\ndef is_luxery_home(x):\n    if x &gt; 500000:\n        return 'Luxery'\n    else:\n        return 'Non-luxery'\n\names['saleprice'].apply(is_luxery_home)\n\n0       Non-luxery\n1       Non-luxery\n2       Non-luxery\n3       Non-luxery\n4       Non-luxery\n           ...    \n2925    Non-luxery\n2926    Non-luxery\n2927    Non-luxery\n2928    Non-luxery\n2929    Non-luxery\nName: saleprice, Length: 2930, dtype: object\n\n\nThis may have been better as a lambda function, which is just a shorter approach to writing functions. This may be a bit confusing but we’ll talk more about lambda functions in the writing functions lesson. For now, just think of it as being able to write a function for single use application on the fly.\n\names['saleprice'].apply(lambda x: 'Luxery' if x &gt; 500000 else 'Non-luxery')\n\n0       Non-luxery\n1       Non-luxery\n2       Non-luxery\n3       Non-luxery\n4       Non-luxery\n           ...    \n2925    Non-luxery\n2926    Non-luxery\n2927    Non-luxery\n2928    Non-luxery\n2929    Non-luxery\nName: saleprice, Length: 2930, dtype: object\n\n\nYou can even use functions that require additional arguments. Just specify the arguments in .apply():\n\ndef is_luxery_home(x, price):\n    if x &gt; price:\n        return 'Luxery'\n    else:\n        return 'Non-luxery'\n\names['saleprice'].apply(is_luxery_home, price=200000)\n\n0           Luxery\n1       Non-luxery\n2       Non-luxery\n3           Luxery\n4       Non-luxery\n           ...    \n2925    Non-luxery\n2926    Non-luxery\n2927    Non-luxery\n2928    Non-luxery\n2929    Non-luxery\nName: saleprice, Length: 2930, dtype: object\n\n\nSometimes we may have a function that we want to apply to every element across multiple columns. For example, say we wanted to convert several of the square footage variables to be represented as square meters. For this we can use the .applymap() method.\n\ndef convert_to_sq_meters(x):\n    return x*0.092903\n\names[['gr_liv_area', 'garage_area', 'lot_area']].map(convert_to_sq_meters)\n\n\n\n\n\n\n\n\ngr_liv_area\ngarage_area\nlot_area\n\n\n\n\n0\n153.847368\n49.052784\n2946.883160\n\n\n1\n83.241088\n67.819190\n1075.073516\n\n\n2\n123.468087\n28.985736\n1320.801951\n\n\n3\n196.025330\n48.495366\n1032.152330\n\n\n4\n151.338987\n44.779246\n1280.203340\n\n\n...\n...\n...\n...\n\n\n2925\n93.181709\n54.626964\n732.725961\n\n\n2926\n83.798506\n44.965052\n820.798005\n\n\n2927\n90.115910\n0.000000\n965.355073\n\n\n2928\n129.042267\n38.833454\n925.313880\n\n\n2929\n185.806000\n60.386950\n889.732031\n\n\n\n\n2930 rows × 3 columns",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#chapter-summary",
    "href": "10-manipulating-data.html#chapter-summary",
    "title": "10  Manipulating Data",
    "section": "10.8 Chapter Summary",
    "text": "10.8 Chapter Summary\nIn this chapter, you learned how to manipulate columns in a pandas DataFrame — a foundational skill for any kind of data analysis or modeling. You practiced working with both numeric and non-numeric data and explored common data wrangling tasks that analysts use every day.\nHere’s a recap of what you learned:\n\nHow to rename columns using .rename() or string methods like .str.lower() and .str.replace()\nHow to create new columns by performing arithmetic with scalars or other columns\nHow to remove columns using .drop()\nHow to work with text data, including string concatenation and using the .str accessor\nHow to replace values using a mapping (via .replace())\nHow to detect and handle missing values using .isnull(), .dropna(), and .fillna()\nHow to apply custom functions to transform your data using .apply() and .applymap()\n\nThese skills form the building blocks of effective data cleaning and transformation.\nIn the next few chapters, we’ll build on this foundation and introduce even more essential data wrangling techniques, including:\n\nComputing summary statistics and descriptive analytics\nGrouping and aggregating data\nJoining multiple datasets\nReshaping data with pivot tables and the .melt() and .pivot() methods\n\nBy the end of these upcoming lessons, you’ll be well-equipped to clean, prepare, and explore real-world datasets using pandas.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#exercise-heart-disease-data",
    "href": "10-manipulating-data.html#exercise-heart-disease-data",
    "title": "10  Manipulating Data",
    "section": "10.9 Exercise: Heart Disease Data",
    "text": "10.9 Exercise: Heart Disease Data\nIn this exercise, you’ll apply what you’ve learned in this chapter to a new dataset on heart disease, which includes various patient health indicators that may be predictive of cardiovascular conditions.\nRead more about this dataset on Kaggle, and you can download copy of the heart.csv file here.\nYour task is to perform several data wrangling steps to start cleaning and transforming this dataset.\n\n\n\n\n\n\nNoneImport the Data\n\n\n\n\n\n\nLoad the dataset into a DataFrame using pd.read_csv().\nPreview the first few rows with .head().\n\n\n\n\n\n\n\n\n\n\nNoneClean the Column Names\n\n\n\n\n\nCheck out the column names and think how you would standardize these names. Use .columns and string methods to:\n\nConvert all column names to lowercase\nReplace any spaces or dashes with underscores\nRemove any trailing or leading whitespace\n\n\n\n\n\n\n\n\n\n\nNoneHandle Missing Values\n\n\n\n\n\n\nCheck for missing values using .isnull().sum().\nIf any columns contain missing values:\n\nIdentify the mode (most frequent value) for those columns\nFill the missing values using .fillna()\n\n\n\n\n\n\n\n\nThe mode can be accessed with .mode().iloc[0] to retrieve the most frequent value.\n\n\n\n\n\n\n\n\n\n\n\n\nNoneCreate a New Column\n\n\n\n\n\nCreate a new column called risk, calculated as:\n\\[\n     \\text{risk} = \\frac{\\text{age}}{\\text{rest\\_bp} + \\text{chol} + \\text{max\\_hr}}\n\\]\n\n\n\n\n\n\nBe sure to use parentheses in your formula to ensure proper order of operations.\n\n\n\n\n\n\n\n\n\n\n\n\nNoneReplace Values in a Categorical Column\n\n\n\n\n\nThe rest_ecg column contains several text categories. Recode the values using .replace() and the following mapping:\n\n\n\nOriginal Value\nNew Value\n\n\n\n\nnormal\nnormal\n\n\nleft ventricular hypertrophy\nlvh\n\n\nST-T wave abnormality\nstt_wav_abn\n\n\n\n\n\n\n\n\n\nMake sure to overwrite the existing column with the updated values.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html",
    "href": "11_aggregating_data.html",
    "title": "11  Summarizing Data",
    "section": "",
    "text": "11.1 Simple aggregation\nAs datasets grow in size and complexity, the ability to summarize information becomes an essential skill in any data scientist’s toolkit. Summary statistics—like averages, medians, and group-level comparisons—help us cut through the noise and uncover meaningful patterns. Whether you’re reporting results to stakeholders or exploring data to form new hypotheses, knowing how to aggregate data is fundamental.\nThese types of questions will guide your thinking as we explore tools in Pandas for summarizing and grouping data. And by the end of this lesson, you will be able to:\nIn the previous lesson, we learned how to manipulate data across columns, typically by performing operations on two or more Series within the same row. For example, we might calculate a total by adding two columns together:\nFor example, we can calculate a total by adding two columns together with code that looks like this:\nThis adds the values in column A to the corresponding values in column B, row by row. You could also use other operators (e.g., subtraction, multiplication) as long as the result returns one value per row.\nHowever, sometimes we want to shift our focus from individual rows to the entire column. Instead of computing values across columns within a row, we want to aggregate values across rows within a column. This is the foundation of many summary statistics—like computing the average, median, or maximum of a column.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html#simple-aggregation",
    "href": "11_aggregating_data.html#simple-aggregation",
    "title": "11  Summarizing Data",
    "section": "",
    "text": "These types of operations return the same number of rows as the original DataFrame. This is sometimes called a window function—but you can think of it as performing calculations at the row level.\n\n\n\n\nDataFrame['A'] + DataFrame['B']\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese types of operations return a single value that summarizes multiple values. This is often referred to as a summary function, but you can think of it as aggregating values across rows.\n\n\n\n\nSummarizing a Series\nOnce we’ve loaded a dataset into a Pandas DataFrame, one of the simplest ways to explore it is by summarizing individual columns—also known as Series. Pandas makes this easy with built-in methods like .sum(), .mean(), and .median().\nSuppose we want to compute the total of all home sale prices. We can do that by selecting the SalePrice column and using the .sum() method:\n\names['SalePrice'].sum()\n\nnp.int64(529732456)\n\n\nThis returns a single value, which makes this a summary operation—we’re aggregating values across all rows in the SalePrice column.\nPandas includes many other helpful summary methods for numerical data:\n\n# Average sale price\names['SalePrice'].mean()\n\nnp.float64(180796.0600682594)\n\n\n\n# Median sale price\names['SalePrice'].median()\n\nnp.float64(160000.0)\n\n\n\n# Standard deviation of sale prices\names['SalePrice'].std()   \n\nnp.float64(79886.692356665)\n\n\nThese methods are designed for quantitative variables and won’t work as expected on text-based columns.\n\n\n\n\n\n\nPandas will include the data type in the summary statistic output preceeding the actual summary stat (i.e. np.float64). If you want to not see that you can just wrap it with print():\n\nprint(ames['SalePrice'].sum())\n\n529732456\n\n\n\n\n\nHowever, Pandas also provides summary methods that are useful for categorical variables (like neighborhood names):\n\n# Number of unique neighborhoods\names['Neighborhood'].nunique()\n\n28\n\n\n\n# Most frequent neighborhood\names['Neighborhood'].mode()    \n\n0    NAmes\nName: Neighborhood, dtype: object\n\n\nThese allow us to summarize the structure and frequency of categorical data—just as we do with numbers.\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nWhat is the difference between a window operation and a summary operation?\nWhat are the mean, median, and standard deviation of the Gr Liv Area (above ground square footage)?\nHow many times does each neighborhood appear in the dataset? (Hint: Try using the GenAI code assistant to figure this out. Ask it how to “count value frequency in a Pandas Series.”). Then reflect: Would this count as a summary operation? Why or why not?\n\n\n\n\n\n\n\nThe describe() Method\nWhen you’re first getting familiar with a dataset, it’s helpful to quickly view a variety of summary statistics all at once. Pandas provides the .describe() method for exactly this purpose.\nFor numeric variables, .describe() returns common summary statistics such as the count, mean, standard deviation, min, and max values:\n\names['SalePrice'].describe()\n\ncount      2930.000000\nmean     180796.060068\nstd       79886.692357\nmin       12789.000000\n25%      129500.000000\n50%      160000.000000\n75%      213500.000000\nmax      755000.000000\nName: SalePrice, dtype: float64\n\n\nThis is especially useful during exploratory data analysis (EDA) when you’re trying to get a sense of a column’s range, distribution, and central tendency.\n\n\n\n\n\n\nThe behavior of .describe() changes based on the data type of the Series.\n\n\n\nFor categorical variables, .describe() provides a different summary, showing the number of non-null entries, number of unique values, most frequent value, and its frequency:\n\names['Neighborhood'].describe()\n\ncount      2930\nunique       28\ntop       NAmes\nfreq        443\nName: Neighborhood, dtype: object\n\n\nIn both cases, .describe() gives you a quick and informative overview—whether you’re working with numbers or categories.\n\n\nSummarizing a DataFrame\nSo far, we’ve focused on summarizing individual columns (Series). But in many cases, we want to explore multiple variables at once. Fortunately, Pandas allows us to apply the same summary methods to a subset of columns in a DataFrame.\nFirst, recall how to select multiple columns using double brackets and a list of column names:\n\names[['SalePrice', 'Gr Liv Area']]\n\n\n\n\n\n\n\n\nSalePrice\nGr Liv Area\n\n\n\n\n0\n215000\n1656\n\n\n1\n105000\n896\n\n\n2\n172000\n1329\n\n\n3\n244000\n2110\n\n\n4\n189900\n1629\n\n\n...\n...\n...\n\n\n2925\n142500\n1003\n\n\n2926\n131000\n902\n\n\n2927\n132000\n970\n\n\n2928\n170000\n1389\n\n\n2929\n188000\n2000\n\n\n\n\n2930 rows × 2 columns\n\n\n\nThis returns a new DataFrame with just the SalePrice and Gr Liv Area columns.\nWe can now apply summary methods—just like we did with a single Series:\n\names[['SalePrice', 'Gr Liv Area']].mean()\n\nSalePrice      180796.060068\nGr Liv Area      1499.690444\ndtype: float64\n\n\n\names[['SalePrice', 'Gr Liv Area']].median()\n\nSalePrice      160000.0\nGr Liv Area      1442.0\ndtype: float64\n\n\nThese methods return a Series object, where:\n\nThe index contains the column names, and\nThe values are the summary statistics (e.g., mean or median) for each column.\n\nThis approach is useful when you’re interested in comparing summary values across several numeric variables at once.\n\n\n\n\n\n\nEven though you’re summarizing multiple columns, each summary method still operates column-by-column under the hood.\n\n\n\n\n\nThe .agg() Method\nSo far, we’ve used built-in summary methods like .mean() and .median() to compute statistics on one or more columns. While this approach works well, it has a few important limitations when applied to DataFrames:\n\nYou can only apply one summary method at a time.\nThe same method gets applied to every selected column.\nThe result is returned as a Series, which can be harder to work with later.\n\nTo overcome these limitations, Pandas provides a more flexible tool: the .agg() method (short for .aggregate()).\nHere’s a basic example:\n\names.agg({\n    'SalePrice': ['mean']\n})\n\n\n\n\n\n\n\n\nSalePrice\n\n\n\n\nmean\n180796.060068\n\n\n\n\n\n\n\nThis returns a DataFrame rather than a Series—and gives us more control over how we summarize each column.\nLet’s break it down:\n\nWe pass a dictionary to .agg()\nThe keys are column names\nThe values are lists of summary functions we want to apply\n\n\n\n\n\n\n\nThe .agg() method is just shorthand for .aggregate(). You can use either version—they’re equivalent!\n\n\n\n\n# Verbose version\names.aggregate({\n    'SalePrice': ['mean']\n})\n\n# Concise version\names.agg({\n    'SalePrice': ['mean']\n})\n\n\n\n\n\n\n\n\nSalePrice\n\n\n\n\nmean\n180796.060068\n\n\n\n\n\n\n\nWe can easily extend this to include multiple columns:\n\names.agg({\n    'SalePrice': ['mean'],\n    'Gr Liv Area': ['mean']\n})\n\n\n\n\n\n\n\n\nSalePrice\nGr Liv Area\n\n\n\n\nmean\n180796.060068\n1499.690444\n\n\n\n\n\n\n\nAnd because the values in the dictionary are lists, we can apply multiple summary functions to each column:\n\names.agg({\n    'SalePrice': ['mean', 'median'],\n    'Gr Liv Area': ['mean', 'min']\n})\n\n\n\n\n\n\n\n\nSalePrice\nGr Liv Area\n\n\n\n\nmean\n180796.060068\n1499.690444\n\n\nmedian\n160000.000000\nNaN\n\n\nmin\nNaN\n334.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou don’t have to apply the same summary functions to every variable. If a summary function isn’t applied to a particular column, Pandas will return a NaN in that spot.\n\n\n\nThe .agg() method is especially useful when you want to apply different aggregations to different columns and get the results in a clean, tabular format.\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nFill in the blanks to compute the average number of rooms above ground (TotRms AbvGrd) and the average number of bedrooms above ground (Bedroom AbvGr). What type of object is returned?\names[['______', '______']].______()\nUse the .agg() method to complete the same computation as above. How does the output differ?\nFill in the blanks in the below code to calculate the minimum and maximum year built (Year Built) and the mean and median number of garage stalls (Garage Cars):\names.agg({\n    '_____': ['min', '_____'],\n    '_____': ['_____', 'median']\n})\n\n\n\n\n\n\n\nThe describe() Method\nWhile .agg() is a powerful and flexible tool for customized summaries, the .describe() method offers a quick and convenient overview of your entire DataFrame—making it especially useful during exploratory data analysis (EDA).\nTry running .describe() on the full Ames dataset:\n\names.describe()\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nLot Frontage\nLot Area\nOverall Qual\nOverall Cond\nYear Built\nYear Remod/Add\nMas Vnr Area\n...\nWood Deck SF\nOpen Porch SF\nEnclosed Porch\n3Ssn Porch\nScreen Porch\nPool Area\nMisc Val\nMo Sold\nYr Sold\nSalePrice\n\n\n\n\ncount\n2930.00000\n2.930000e+03\n2930.000000\n2440.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2907.000000\n...\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n\n\nmean\n1465.50000\n7.144645e+08\n57.387372\n69.224590\n10147.921843\n6.094881\n5.563140\n1971.356314\n1984.266553\n101.896801\n...\n93.751877\n47.533447\n23.011604\n2.592491\n16.002048\n2.243345\n50.635154\n6.216041\n2007.790444\n180796.060068\n\n\nstd\n845.96247\n1.887308e+08\n42.638025\n23.365335\n7880.017759\n1.411026\n1.111537\n30.245361\n20.860286\n179.112611\n...\n126.361562\n67.483400\n64.139059\n25.141331\n56.087370\n35.597181\n566.344288\n2.714492\n1.316613\n79886.692357\n\n\nmin\n1.00000\n5.263011e+08\n20.000000\n21.000000\n1300.000000\n1.000000\n1.000000\n1872.000000\n1950.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n2006.000000\n12789.000000\n\n\n25%\n733.25000\n5.284770e+08\n20.000000\n58.000000\n7440.250000\n5.000000\n5.000000\n1954.000000\n1965.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n4.000000\n2007.000000\n129500.000000\n\n\n50%\n1465.50000\n5.354536e+08\n50.000000\n68.000000\n9436.500000\n6.000000\n5.000000\n1973.000000\n1993.000000\n0.000000\n...\n0.000000\n27.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n6.000000\n2008.000000\n160000.000000\n\n\n75%\n2197.75000\n9.071811e+08\n70.000000\n80.000000\n11555.250000\n7.000000\n6.000000\n2001.000000\n2004.000000\n164.000000\n...\n168.000000\n70.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n8.000000\n2009.000000\n213500.000000\n\n\nmax\n2930.00000\n1.007100e+09\n190.000000\n313.000000\n215245.000000\n10.000000\n9.000000\n2010.000000\n2010.000000\n1600.000000\n...\n1424.000000\n742.000000\n1012.000000\n508.000000\n576.000000\n800.000000\n17000.000000\n12.000000\n2010.000000\n755000.000000\n\n\n\n\n8 rows × 39 columns\n\n\n\nBy default, Pandas will summarize only the numeric columns, returning statistics like count, mean, standard deviation, min, max, and quartiles.\n\n\n\n\n\n\nBut wait, what’s missing from the output?\nString (object) columns — like Neighborhood — are excluded!\n\n\n\nIf you want to include other variable types, you can use the include parameter to tell Pandas what to summarize. For example:\n\names.describe(include=['int', 'float', 'object'])\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\ncount\n2930.00000\n2.930000e+03\n2930.000000\n2930\n2440.000000\n2930.000000\n2930\n198\n2930\n2930\n...\n2930.000000\n13\n572\n106\n2930.000000\n2930.000000\n2930.000000\n2930\n2930\n2930.000000\n\n\nunique\nNaN\nNaN\nNaN\n7\nNaN\nNaN\n2\n2\n4\n4\n...\nNaN\n4\n4\n5\nNaN\nNaN\nNaN\n10\n6\nNaN\n\n\ntop\nNaN\nNaN\nNaN\nRL\nNaN\nNaN\nPave\nGrvl\nReg\nLvl\n...\nNaN\nEx\nMnPrv\nShed\nNaN\nNaN\nNaN\nWD\nNormal\nNaN\n\n\nfreq\nNaN\nNaN\nNaN\n2273\nNaN\nNaN\n2918\n120\n1859\n2633\n...\nNaN\n4\n330\n95\nNaN\nNaN\nNaN\n2536\n2413\nNaN\n\n\nmean\n1465.50000\n7.144645e+08\n57.387372\nNaN\n69.224590\n10147.921843\nNaN\nNaN\nNaN\nNaN\n...\n2.243345\nNaN\nNaN\nNaN\n50.635154\n6.216041\n2007.790444\nNaN\nNaN\n180796.060068\n\n\nstd\n845.96247\n1.887308e+08\n42.638025\nNaN\n23.365335\n7880.017759\nNaN\nNaN\nNaN\nNaN\n...\n35.597181\nNaN\nNaN\nNaN\n566.344288\n2.714492\n1.316613\nNaN\nNaN\n79886.692357\n\n\nmin\n1.00000\n5.263011e+08\n20.000000\nNaN\n21.000000\n1300.000000\nNaN\nNaN\nNaN\nNaN\n...\n0.000000\nNaN\nNaN\nNaN\n0.000000\n1.000000\n2006.000000\nNaN\nNaN\n12789.000000\n\n\n25%\n733.25000\n5.284770e+08\n20.000000\nNaN\n58.000000\n7440.250000\nNaN\nNaN\nNaN\nNaN\n...\n0.000000\nNaN\nNaN\nNaN\n0.000000\n4.000000\n2007.000000\nNaN\nNaN\n129500.000000\n\n\n50%\n1465.50000\n5.354536e+08\n50.000000\nNaN\n68.000000\n9436.500000\nNaN\nNaN\nNaN\nNaN\n...\n0.000000\nNaN\nNaN\nNaN\n0.000000\n6.000000\n2008.000000\nNaN\nNaN\n160000.000000\n\n\n75%\n2197.75000\n9.071811e+08\n70.000000\nNaN\n80.000000\n11555.250000\nNaN\nNaN\nNaN\nNaN\n...\n0.000000\nNaN\nNaN\nNaN\n0.000000\n8.000000\n2009.000000\nNaN\nNaN\n213500.000000\n\n\nmax\n2930.00000\n1.007100e+09\n190.000000\nNaN\n313.000000\n215245.000000\nNaN\nNaN\nNaN\nNaN\n...\n800.000000\nNaN\nNaN\nNaN\n17000.000000\n12.000000\n2010.000000\nNaN\nNaN\n755000.000000\n\n\n\n\n11 rows × 82 columns\n\n\n\nThis will generate descriptive statistics for all numeric and categorical variables, including counts, unique values, top categories, and their frequencies.\n\n\n\n\n\n\nYou can also use include='all' to summarize every column, regardless of type. Just be aware that this can result in a mix of numeric and non-numeric statistics in the same table.\n\n\n\nThe .describe() method is a fast and effective way to get a high-level snapshot of your dataset—perfect for early-stage data exploration.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html#grouped-aggregation",
    "href": "11_aggregating_data.html#grouped-aggregation",
    "title": "11  Summarizing Data",
    "section": "11.2 Grouped Aggregation",
    "text": "11.2 Grouped Aggregation\nSo far, we’ve focused on summary operations that collapse an entire column—or a subset of columns—into a single result. But in many real-world analyses, we’re interested in summarizing within groups rather than across the whole dataset.\nThis is called a grouped aggregation. Instead of collapsing a DataFrame into a single row, we collapse it into one row per group.\nFor example, we might want to compute:\n\nTotal home sales by neighborhood\nAverage square footage by number of bedrooms\nMedian sale price by year\nMaximum temperature by month\n\nHere’s a simple illustration: suppose we want to compute the sum of column B for each unique category in column A:\n\n\n\n\n\n\nThe Groupby Model\nGrouped aggregation in Pandas always follows the same three-step process:\n\nGroup the data using groupby()\nApply a summary method like .sum(), .agg(), or .describe()\nReturn a DataFrame of group-level summaries\n\n\n\n\nCreating a Grouped Object\nWe use the groupby() method to define how we want to group the data. For example, to group homes by Neighborhood:\n\names_grp = ames.groupby('Neighborhood')\n\nThis creates a GroupBy object—it doesn’t return a DataFrame yet, but rather an internal structure that maps each group to its corresponding rows.\n\ntype(ames_grp)\n\npandas.core.groupby.generic.DataFrameGroupBy\n\n\nYou can inspect the structure of the groups:\n\names_grp.groups\n\n{'Blmngtn': [52, 53, 468, 469, 470, 471, 472, 473, 1080, 1081, 1082, 1083, 1084, 1741, 1742, 1743, 1744, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429], 'Blueste': [298, 299, 932, 933, 934, 935, 1542, 1543, 2225, 2227], 'BrDale': [29, 30, 31, 402, 403, 404, 405, 406, 407, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1675, 1676, 1677, 1678, 1679, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372], 'BrkSide': [129, 130, 191, 192, 193, 194, 195, 196, 197, 198, 199, 614, 615, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 1219, 1220, 1221, 1222, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1901, 1902, 1903, 1904, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2022, 2023, 2024, 2025, 2554, 2555, 2556, 2557, 2670, 2671, 2672, 2673, 2675, 2676, 2677, ...], 'ClearCr': [208, 209, 210, 228, 229, 232, 233, 255, 779, 781, 782, 785, 833, 834, 1374, 1392, 1395, 1398, 1399, 1400, 1401, 1402, 1406, 1429, 1430, 2045, 2071, 2072, 2073, 2077, 2115, 2116, 2117, 2118, 2701, 2725, 2726, 2727, 2730, 2731, 2764, 2765, 2766, 2767], 'CollgCr': [249, 250, 251, 252, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 1423, 1424, 1425, 1426, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, ...], 'Crawfor': [293, 294, 295, 296, 297, 300, 308, 912, 915, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 936, 937, 938, 944, 1523, 1524, 1525, 1526, 1528, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1559, 1560, 1561, 1562, 2196, 2197, 2198, 2199, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2226, 2228, 2229, 2230, 2245, 2246, 2850, 2853, 2854, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, ...], 'Edwards': [234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 287, 762, 763, 764, 765, 766, 767, 783, 784, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 1375, 1403, 1404, 1405, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, ...], 'Gilbert': [4, 5, 9, 10, 11, 12, 13, 16, 18, 51, 54, 55, 56, 57, 58, 344, 345, 346, 347, 348, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 369, 464, 465, 466, 467, 474, 475, 476, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 992, 993, 994, 995, 996, 997, 998, 1003, 1004, 1005, 1006, 1007, 1013, 1015, 1085, 1086, 1087, 1088, 1089, 1090, 1092, 1093, 1094, 1095, 1096, 1097, 1615, 1616, 1617, 1618, 1619, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1638, 1727, 1728, ...], 'Greens': [106, 107, 575, 1857, 2518, 2519, 2520, 2521], 'GrnHill': [2256, 2892], 'IDOTRR': [205, 206, 301, 302, 303, 304, 305, 306, 307, 726, 727, 754, 755, 758, 759, 760, 939, 940, 941, 942, 943, 945, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1368, 1369, 1370, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1610, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2043, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2669, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882], 'Landmrk': [2788], 'MeadowV': [326, 327, 328, 329, 330, 331, 332, 973, 975, 977, 978, 979, 1593, 1594, 1595, 1596, 1597, 1599, 1600, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2908, 2909, 2910, 2913, 2914, 2916, 2917, 2918, 2919, 2920], 'Mitchel': [309, 310, 311, 312, 313, 322, 323, 324, 325, 333, 334, 335, 336, 337, 338, 339, 340, 946, 947, 948, 949, 950, 951, 952, 953, 954, 969, 970, 971, 972, 974, 976, 980, 981, 982, 983, 984, 985, 986, 987, 988, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1588, 1589, 1590, 1591, 1592, 1598, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2277, 2278, 2279, 2280, 2281, 2282, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2885, 2886, 2887, 2888, 2889, 2890, 2903, 2904, 2905, ...], 'NAmes': [0, 1, 2, 3, 23, 24, 25, 26, 27, 28, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 341, 342, 343, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 418, 419, 593, 594, 595, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 616, 617, 618, 619, 620, 621, 622, 623, 624, ...], 'NPkVill': [32, 33, 34, 35, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 1046, 1047, 1048, 1680, 1681, 1682, 2373, 2376, 2377], 'NWAmes': [19, 20, 21, 110, 111, 112, 113, 114, 115, 116, 118, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 387, 388, 389, 390, 391, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 596, 597, 598, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1197, 1198, 1199, 1200, 1201, 1203, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1863, 1864, 1865, 1866, 1867, ...], 'NoRidge': [59, 60, 61, 62, 63, 64, 65, 90, 91, 92, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 564, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1157, 1158, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1832, 1833, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2499, 2500, 2501, 2502, 2503], 'NridgHt': [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 482, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1091, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, ...], 'OldTown': [158, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 200, 201, 202, 203, 204, 207, 650, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 749, 750, 751, 752, 753, 756, 757, 1254, 1255, 1257, 1258, 1259, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, ...], 'SWISU': [211, 212, 213, 214, 285, 286, 288, 289, 290, 291, 292, 905, 906, 907, 908, 909, 910, 911, 913, 914, 916, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1527, 1529, 2047, 2080, 2194, 2195, 2200, 2703, 2704, 2842, 2845, 2846, 2847, 2848, 2849, 2851, 2852, 2855, 2856], 'Sawyer': [83, 84, 85, 86, 87, 88, 89, 108, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 231, 554, 555, 556, 557, 558, 559, 560, 561, 562, 578, 761, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 780, 1149, 1150, 1151, 1152, 1153, 1154, 1156, 1371, 1372, 1373, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1393, 1394, 1396, 1397, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1829, 1830, 1831, 1861, 2044, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, ...], 'SawyerW': [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 244, 245, 246, 247, 248, 253, 254, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 832, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1418, 1419, 1420, 1421, 1422, 1427, 1428, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 2089, 2090, 2091, 2092, ...], 'Somerst': [22, 66, 67, 68, 69, 70, 71, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 109, 383, 384, 385, 386, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 565, 566, 567, 568, 569, 570, 571, 572, 573, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, ...], 'StoneBr': [6, 7, 8, 14, 15, 17, 349, 350, 351, 352, 365, 366, 367, 368, 999, 1000, 1001, 1002, 1008, 1009, 1010, 1011, 1012, 1014, 1620, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1639, 1640, 1641, 1642, 2322, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2339, 2340, 2341], 'Timber': [314, 315, 316, 317, 318, 319, 320, 321, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 2255, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2891, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902], 'Veenker': [563, 574, 576, 577, 1155, 1178, 1179, 1180, 1181, 1182, 1183, 1827, 1828, 1853, 1854, 1855, 1856, 1858, 1859, 1860, 2498, 2516, 2517, 2522]}\n\n\nAnd access a specific group:\n\n# Get the Bloomington neighborhood group\names_grp.get_group('Blmngtn').head()\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n52\n53\n528228285\n120\nRL\n43.0\n3203\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n1\n2010\nWD\nNormal\n160000\n\n\n53\n54\n528228440\n120\nRL\n43.0\n3182\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n192000\n\n\n468\n469\n528228290\n120\nRL\n53.0\n3684\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n6\n2009\nWD\nNormal\n174000\n\n\n469\n470\n528228295\n120\nRL\n51.0\n3635\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2009\nWD\nNormal\n175900\n\n\n470\n471\n528228435\n120\nRL\n43.0\n3182\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2009\nWD\nNormal\n192500\n\n\n\n\n5 rows × 82 columns\n\n\n\n\n\nApplying Aggregations to Groups\nOnce a group is defined, you can apply the same aggregation methods we used earlier:\n\names.groupby('Neighborhood').agg({'SalePrice': ['mean', 'median']}).head()\n\n\n\n\n\n\n\n\nSalePrice\n\n\n\nmean\nmedian\n\n\nNeighborhood\n\n\n\n\n\n\nBlmngtn\n196661.678571\n191500.0\n\n\nBlueste\n143590.000000\n130500.0\n\n\nBrDale\n105608.333333\n106000.0\n\n\nBrkSide\n124756.250000\n126750.0\n\n\nClearCr\n208662.090909\n197500.0\n\n\n\n\n\n\n\nThis returns a DataFrame where each row corresponds to a different neighborhood, and the columns represent the aggregated values.\n\n\nGroups as Index vs. Variables\n\n\n\n\n\n\nBy default, the grouped variable becomes the index of the resulting DataFrame.\n\n\n\n\names.groupby('Neighborhood').agg({'SalePrice': ['mean', 'median']}).index\n\nIndex(['Blmngtn', 'Blueste', 'BrDale', 'BrkSide', 'ClearCr', 'CollgCr',\n       'Crawfor', 'Edwards', 'Gilbert', 'Greens', 'GrnHill', 'IDOTRR',\n       'Landmrk', 'MeadowV', 'Mitchel', 'NAmes', 'NPkVill', 'NWAmes',\n       'NoRidge', 'NridgHt', 'OldTown', 'SWISU', 'Sawyer', 'SawyerW',\n       'Somerst', 'StoneBr', 'Timber', 'Veenker'],\n      dtype='object', name='Neighborhood')\n\n\nThis is the most efficient default behavior in Pandas. However, if you’d prefer the group column to remain a regular column instead of becoming the index, you can set as_index=False:\n\names.groupby('Neighborhood', as_index=False).agg({'SalePrice': ['mean', 'median']}).head()\n\n\n\n\n\n\n\n\nNeighborhood\nSalePrice\n\n\n\n\nmean\nmedian\n\n\n\n\n0\nBlmngtn\n196661.678571\n191500.0\n\n\n1\nBlueste\n143590.000000\n130500.0\n\n\n2\nBrDale\n105608.333333\n106000.0\n\n\n3\nBrkSide\n124756.250000\n126750.0\n\n\n4\nClearCr\n208662.090909\n197500.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing as_index=False can make your results easier to merge with other DataFrames or write to CSV later.\n\n\n\n\n\nGrouping by Multiple Variables\nYou can also group by more than one variable. For example, to compute the average sale price by both Neighborhood and Yr Sold:\n\names.groupby(['Neighborhood', 'Yr Sold'], as_index=False).agg({'SalePrice': 'mean'})\n\n\n\n\n\n\n\n\nNeighborhood\nYr Sold\nSalePrice\n\n\n\n\n0\nBlmngtn\n2006\n214424.454545\n\n\n1\nBlmngtn\n2007\n194671.500000\n\n\n2\nBlmngtn\n2008\n190714.400000\n\n\n3\nBlmngtn\n2009\n177266.666667\n\n\n4\nBlmngtn\n2010\n176000.000000\n\n\n...\n...\n...\n...\n\n\n125\nTimber\n2010\n224947.625000\n\n\n126\nVeenker\n2006\n270000.000000\n\n\n127\nVeenker\n2007\n253577.777778\n\n\n128\nVeenker\n2008\n225928.571429\n\n\n129\nVeenker\n2009\n253962.500000\n\n\n\n\n130 rows × 3 columns\n\n\n\nThis returns one row for each combination of Neighborhood and Yr Sold, giving you more granular insights.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nReframe the following question using Pandas’ grouped aggregation syntax. Which Pandas functions will you need?\n\nWhat is the average above-ground square footage of homes, grouped by neighborhood and number of bedrooms?\n\nNow compute the result using the Ames Housing dataset. 🔍 Hint…\n\nGr Liv Area = above-ground square footage\n\nNeighborhood = neighborhood name\n\nBedroom AbvGr = number of bedrooms\n\nUsing your results from #2, identify any neighborhoods where 1-bedroom homes have an average of more than 1500 square feet above ground. How many neighborhoods meet this condition?",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html#summary",
    "href": "11_aggregating_data.html#summary",
    "title": "11  Summarizing Data",
    "section": "11.3 Summary",
    "text": "11.3 Summary\nIn this chapter, we explored how to summarize and aggregate data using Pandas—a foundational skill in any data analysis workflow. You learned how to compute summary statistics on individual columns (Series), multiple columns in a DataFrame, and across groups using the powerful groupby() method. We introduced tools like .mean(), .median(), .describe(), and .agg() to help extract key insights from both numerical and categorical variables. These techniques allow us to make sense of large datasets by reducing complexity and identifying trends, patterns, and outliers.\nIn the chapters ahead, we’ll continue building on these data wrangling skills. You’ll learn how to combine datasets using joins, reshape data through pivoting and tidying, and prepare data for modeling and visualization. Mastering these techniques will give you the ability to transform raw data into a clean, structured form ready for deeper analysis.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html#exercise-aggregating-covid-college-data",
    "href": "11_aggregating_data.html#exercise-aggregating-covid-college-data",
    "title": "11  Summarizing Data",
    "section": "11.4 Exercise: Aggregating COVID College Data",
    "text": "11.4 Exercise: Aggregating COVID College Data\nNow that you’ve practiced subsetting and filtering, let’s dig deeper by computing some summary statistics from the college COVID-19 dataset.\n\n\n\n\n\n\nNoneStep 1: Load the Data\n\n\n\n\n\n\nimport pandas as pd\n\ndata_url = \"https://raw.githubusercontent.com/nytimes/covid-19-data/refs/heads/master/colleges/colleges.csv\"\ncollege_df = pd.read_csv(data_url)\ncollege_df.head()\n\n\n\n\n\n\n\n\ndate\nstate\ncounty\ncity\nipeds_id\ncollege\ncases\ncases_2021\nnotes\n\n\n\n\n0\n2021-05-26\nAlabama\nMadison\nHuntsville\n100654\nAlabama A&M University\n41\nNaN\nNaN\n\n\n1\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100724\nAlabama State University\n2\nNaN\nNaN\n\n\n2\n2021-05-26\nAlabama\nLimestone\nAthens\n100812\nAthens State University\n45\n10.0\nNaN\n\n\n3\n2021-05-26\nAlabama\nLee\nAuburn\n100858\nAuburn University\n2742\n567.0\nNaN\n\n\n4\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100830\nAuburn University at Montgomery\n220\n80.0\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneStep 2: Compute the average number of cases reported across all colleges\n\n\n\n\n\n\nWhat function will help you find the average (mean) value of a column?\nWhat is the average value of cases?\n\ncollege_df['cases'].____()\n\n\n\n\n\n\n\n\n\nNoneStep 3: Compute the total number of cases and total number of cases_2021\n\n\n\n\n\n\nWhat is the average value of cases?\nHow do these two numbers compare? Why might they be different? Hint: Read the variable definitions in the NYT GitHub repo. Are these columns measuring the same thing?\n\ncollege_df[['cases', 'cases_2021']].____()\n\n\n\n\n\n\n\n\n\nNoneStep 4: Compute the total number of cases by state\n\n\n\n\n\n\nUse groupby() and sum() to find the total cases reported by colleges in each state.\nWhich state had the most cases?\nWhich had the fewest?\n\ncollege_df.groupby('state')[['cases']].____()\n\n\n\n\n\n\n\n\n\nNoneStep 5: Focus on colleges in Ohio\n\n\n\n\n\nFilter the data to show only colleges in Ohio. Then compute:\n\nThe total number of cases\nThe average (mean) number of cases across Ohio colleges\n\nohio_df = college_df[college_df['state'] == '____']\n\n# Your aggregations go here",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html",
    "href": "99-anaconda-install.html",
    "title": "12  Anaconda Installation",
    "section": "",
    "text": "12.1 Installing Anaconda\nAnaconda Distribution is one of the most popular platforms for doing data science with Python. It’s designed to make it easier for beginners and professionals alike to set up their Python environment with all the essential tools for analysis, visualization, and machine learning.\nWhen you install Anaconda, you get much more than just Python, you get:\nHere’s why Anaconda is a great option for beginners:",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#installing-anaconda",
    "href": "99-anaconda-install.html#installing-anaconda",
    "title": "12  Anaconda Installation",
    "section": "",
    "text": "CautionWhat about Miniconda?\n\n\n\n\n\nYou may hear about a tool called Miniconda, which is a lightweight alternative to Anaconda. While Miniconda gives you more control by starting with a barebones Python environment, it requires you to install everything manually. In this course, we recommend Anaconda because it comes preloaded with everything you need for data science—and it’s much easier to get started with. If you’re new to Python, stick with Anaconda.\n\n\n\n\nBefore You Begin\nYou’ll need:\n\nA stable internet connection\nMake sure your operating system is current enough for the Anaconda install. See requirements here.\n~3–4 GB of free disk space\nA bit of patience—it’s a large install!\n\n\n\nFor Windows Users\n\nGo to https://www.anaconda.com/products/distribution\nClick “Download” and choose the Windows installer (64-bit) \nOnce downloaded, open the installer\nFollow the prompts:\n\nChoose “Just Me” (recommended)\nLeave default install location\nIMPORTANT: When asked about adding Anaconda to your PATH: Keep it unchecked (recommended)\n\nClick Install and wait (~5–10 mins)\nAfter installation, click “Finish”\nVerify your installation: The Anaconda Navigator should automatically open after successful installation of Anaconda Distribution. If it does not, you should be able to search for “Anaconda Navigator” in the taskbar search.\n\nFor more installation instructions you can find the Anaconda documentation for Windows installation here. You can also view the following video:\n\n\n\nFor Mac Users\n\nGo to https://www.anaconda.com/products/distribution\nDownload the macOS (Intel or M1/M2) installer that matches your hardware \nOpen the .pkg file and follow the installation instructions\n\nGrant permission when prompted\nLeave default settings\n\nVerify your installation: The Anaconda Navigator should automatically open after successful installation of Anaconda Distribution. If it does not, you should be able to search for “Anaconda Navigator” using Spotlight (Cmd + Space → “Anaconda Navigator”)\n\nFor more installation instructions you can find the Anaconda documentation for Windows installation here. You can also view the following video:",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#anaconda-navigator",
    "href": "99-anaconda-install.html#anaconda-navigator",
    "title": "12  Anaconda Installation",
    "section": "12.2 Anaconda Navigator",
    "text": "12.2 Anaconda Navigator\nAnaconda Navigator is the graphical interface included with Anaconda that makes it easier to access your development tools without needing to use the command line.\nWhen you launch Navigator, you’ll see a dashboard of available applications, including but not limited to:\n\nJupyter Notebook – A lightweight, web-based interface to write and run Python code interactively.\nJupyterLab – A more advanced interface for working with notebooks, code files, terminals, and data.\nSpyder – A scientific development environment similar to RStudio (not used in this course).\nVS Code – If installed, you may see a launcher for Visual Studio Code.\n\n\n\n\nAnaconda Navigator\n\n\nYou can also manage environments and packages through Navigator, but for now, your main focus will be on using it to launch Jupyter Notebook or JupyterLab.\nThis simple interface removes a lot of the friction of getting started with Python and is part of what makes Anaconda such a great option for beginners.\n\n\n\n\n\n\nTipLearn more about Anaconda Navigator\n\n\n\n\n\nAnaconda Navigator is a powerful application that can do much more than what we’ve covered here. If you’re curious and want to explore its full functionality, you can read more at here or watch this helpful overview video:\n\nThat said, don’t feel pressured to learn it all at once—some of the functionality may feel overwhelming at first, and that’s perfectly okay. We’ll ease into the tools you need as the course progresses.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#launching-jupyter-and-running-your-first-notebook",
    "href": "99-anaconda-install.html#launching-jupyter-and-running-your-first-notebook",
    "title": "12  Anaconda Installation",
    "section": "12.3 Launching Jupyter and Running Your First Notebook",
    "text": "12.3 Launching Jupyter and Running Your First Notebook\nNow that you’ve successfully installed Anaconda and explored the Navigator interface, it’s time to put it to use. One of the most common tools you’ll use as a data scientist is a Jupyter Notebook—an interactive coding environment that runs in your web browser and lets you combine code, text, and output in one place. In this section, we’ll walk you through how to launch a notebook and run your very first line of code using your newly installed Anaconda environment.\n\n\n\n\n\n\nNoteJupyter Notebooks vs. JupyterLab\n\n\n\nYou may notice that both Jupyter Notebook and JupyterLab are available in Anaconda Navigator. While they both allow you to write and execute code in the Jupyter Notebook format, JupyterLab is the newer, more modern interface. It provides a more flexible workspace, supports multiple tabs and file types, and integrates better with other tools. In this course, we’ll be using JupyterLab by default because of these added capabilities—but feel free to explore both if you’re curious!\n\n\nOnce you’ve installed Anaconda:\n\nOpen Anaconda Navigator: Start by opening the Anaconda Navigator application from your system’s start menu or application launcher.\nLocate JupyterLab: Within the Navigator interface, you’ll see a list of available applications. Find the “JupyterLab” icon. \nLaunch JupyterLab: Click the launch button at the bottom of the JupyterLab icon.\n\nThis will open a new tab in your default web browser at a local address like http://localhost:8888.\nThis web-based interface acts as your coding workspace. It allows you to create and run notebooks, write scripts, view files, and manage your data science projects—all in one place. And you interact with this interface through your browser.\n\nCreate a new notebook: In the Jupyter interface click New → Python 3 Notebook and this will launch a new Jupyter notebook. \nWrite Python code: In the first cell, type the following and press Shift + Enter to run the code:\nprint(\"Hello Anaconda\")\nYou should see:\nHello Anaconda\n\nThat’s it! You’ve set up your local Python data science environment.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#troubleshooting-tips",
    "href": "99-anaconda-install.html#troubleshooting-tips",
    "title": "12  Anaconda Installation",
    "section": "12.4 Troubleshooting Tips",
    "text": "12.4 Troubleshooting Tips\nIf you run into issues during installation or while launching tools like Anaconda Navigator or JupyterLab, don’t worry—it’s common when working with new software. Below are some tips that can help you troubleshoot the most common problems:\n\nAnaconda Installation Issues\n\nThe installer won’t open or crashes: Make sure your system meets the requirements and try re-downloading the installer from the official site.\nInstallation is stuck or taking too long: This process can take several minutes. If it seems frozen for more than 15–20 minutes, cancel the installation, restart your machine, and try again.\nAccidentally added Anaconda to PATH and things broke: Try uninstalling and reinstalling Anaconda, and leave the PATH option unchecked this time (recommended). You can follow the steps in this official guide to uninstall Anaconda properly:\n\nHow to uninstall Anaconda (official guide)\n\n\n\n\nIssues Launching Anaconda Navigator\n\nNavigator doesn’t launch:\n\nRestart your computer and try again.\nOn Windows, try launching the Anaconda Prompt and typing anaconda-navigator to launch it manually.\n\nNavigator opens but buttons are unresponsive or blank: This could be a display issue. Try updating your graphics drivers or restarting the app.\n\n\n\nProblems Starting JupyterLab\n\nClicking “Launch” in Navigator does nothing:\n\nRestart Navigator or your computer.\nTry launching JupyterLab via the command line:\n\nOn Windows: open Anaconda Prompt and type jupyter lab\nOn Mac: open Terminal and type jupyter lab\n\n\nBrowser doesn’t open automatically:\n\nCopy the URL shown in the terminal (e.g., http://localhost:8888) and paste it into your browser manually.\n\nJupyterLab opens but shows an error or doesn’t load:\n\nTry clearing your browser cache or switching to a different browser.\nMake sure no firewall or antivirus program is blocking access to localhost.\n\n\nIf all else fails, feel free to reach out to your instructor or teaching assistant for help!",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#additional-resources",
    "href": "99-anaconda-install.html#additional-resources",
    "title": "12  Anaconda Installation",
    "section": "12.5 Additional Resources",
    "text": "12.5 Additional Resources\nWant to explore more about Anaconda and JupyterLab beyond the basics? Here are some helpful resources if you’re curious to dig deeper:\n\nAnaconda Navigator Documentation: Learn about all the features available through Navigator, including managing environments and installing packages.\n\nhttps://www.anaconda.com/docs/tools/anaconda-navigator/main\n\nAnaconda Navigator Overview Video: A quick video tour of Navigator’s capabilities.\n\nhttps://youtu.be/PxMPl1x-qng\n\nJupyterLab Documentation: Explore JupyterLab’s interface, features, and how to extend its functionality.\n\nhttps://jupyterlab.readthedocs.io/en/stable/\n\n\nThese resources go into far more detail than we’ll need in this course—so don’t feel pressured to master everything right away. Use them as a reference when you’re ready to explore more.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html",
    "href": "99-vscode-install.html",
    "title": "13  VS Code Installation",
    "section": "",
    "text": "13.1 Installing VS Code\nVisual Studio Code (VS Code) is a free, lightweight, and powerful code editor used by developers and data scientists around the world. It’s highly customizable and supports many programming languages and tools—including Python.\nVS Code is more than just a text editor. With the right extensions, it can run code, work with Jupyter notebooks, help you manage projects, and even connect to Git for version control. It’s one of the most widely used environments in the industry and is likely the tool you’ll encounter in internships or full-time roles.\nWhile it may feel more advanced than Anaconda Navigator or JupyterLab, VS Code offers more flexibility and control as your projects grow. In this guide, we’ll walk you through getting started with VS Code for Python programming and data science.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#installing-vs-code",
    "href": "99-vscode-install.html#installing-vs-code",
    "title": "13  VS Code Installation",
    "section": "",
    "text": "Visit the official VS Code website: https://code.visualstudio.com\nClick Download and select the installer for your operating system (Windows, macOS, or Linux). \nRun the installer and follow the default prompts.\n\n\nWindows-Specific Notes:\n\nAllow the installer to add VS Code to your system PATH (this helps you launch it from the command line).\nYou can also enable options to open VS Code from the right-click context menu for added convenience.\n\n\n\nmacOS-Specific Notes:\n\nDrag and drop the VS Code application into your Applications folder.\nOpen it via Spotlight Search (Cmd + Space → “Visual Studio Code”).\n\nOnce installed, you should be able to open VS Code and access its interface.\n\nOn Windows: Click the Start menu, type “Visual Studio Code,” and press Enter. If you checked the box to add VS Code to the system PATH, you can also open a Command Prompt or PowerShell window and type code to launch it.\nOn macOS: Open Spotlight Search by pressing Cmd + Space, type “Visual Studio Code,” and press Enter. If you installed the command line tool, you can also open a Terminal window and type code.\n\nThis should open the VS Code editor which will look like:\n\n\n\nVS Code Editor\n\n\n\n\n\n\n\n\nIf you have any issues during setup, check out the VS Code setup docs for details around system requirements, more detailed setups for each operating system, along with common questions.\n\n\n\nIn the next section, we’ll help you install the necessary extensions to begin working with Python inside VS Code.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#installing-the-python-extension",
    "href": "99-vscode-install.html#installing-the-python-extension",
    "title": "13  VS Code Installation",
    "section": "13.2 Installing the Python Extension",
    "text": "13.2 Installing the Python Extension\nTo use Python inside VS Code, you’ll need two things:\n\nA Python interpreter: This is the program that actually runs your Python code. If you’ve already installed Anaconda, you’re all set—Anaconda includes a Python interpreter. If you haven’t (for example, if you’re coming straight from using Colab), you’ll need to install Python separately. You can download the latest version from https://www.python.org/downloads/.\nThe official Python extension provided by Microsoft: This extension adds support for writing, running, and debugging Python code in VS Code, and helps you work with virtual environments. To install this extension:\n\nOpen VS Code.\nClick on the Extensions icon on the left sidebar (or press Ctrl+Shift+X / Cmd+Shift+X).\nIn the search bar, type “Python” and look for the extension authored by Microsoft.\nClick Install. \n\n\nOnce installed, the Python extension will automatically detect Python interpreters on your system and provide features such as syntax highlighting, auto-complete, linting, and integrated terminal support.\n\nWhile You’re at It: Install These Additional Extensions\nTo get the most out of VS Code for Python and Jupyter work, we recommend installing the following extensions now:\n\nJupyter: Enables Jupyter Notebook support within VS Code. \nPylance: Improves code intelligence and type checking. \n\nAfter installing these extensions, you’re ready to start writing and running Python code in VS Code!\n\n\nUnderstanding Extensions in VS Code\nExtensions are small add-ons that enhance the functionality of VS Code. Think of them like plugins—they allow you to customize your coding environment based on your needs.\nSome extensions help you write and run code in specific languages, while others add support for tools like Git, Jupyter notebooks, or data visualization libraries.\nHere are a few common extensions for data science workflows:\n\nPython (by Microsoft) – Adds Python language support\nJupyter – Enables Jupyter Notebook support\nPylance – Adds fast, accurate code intelligence and type checking\nGitLens – Enhances Git capabilities directly in VS Code\nCode Runner – Allows you to run snippets of code from many languages\n\nTo explore more extensions:\n\nClick the Extensions icon in the sidebar.\nBrowse featured or recommended extensions.\nUse the search bar to find tools that match your interests.\n\n\n\n\n\n\n\nYou don’t need to install a lot of extensions to get started. As you become more experienced, you’ll naturally add tools that support your workflow and projects.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#creating-your-first-jupyter-notebook",
    "href": "99-vscode-install.html#creating-your-first-jupyter-notebook",
    "title": "13  VS Code Installation",
    "section": "13.3 Creating Your First Jupyter Notebook",
    "text": "13.3 Creating Your First Jupyter Notebook\nNow that VS Code is set up with Python and Jupyter support, let’s walk through creating your first Jupyter notebook and running a simple Python command.\nSteps to Create and Run a Notebook:\n\nOpen VS Code.\nClick File → New File → select Jupyter Notebook \nLet’s go ahead and save this file. Go to File → Save As and you can select where you want to save this file to and also rename it (e.g., hello.ipynb).\nVS Code will recognize this as a Jupyter notebook and open an interactive notebook interface.\nIn the first cell, type:\n\nprint(\"Hello VS Code\")\n\nClick the Run Cell button (the small ▶️ icon) to the left of the cell, or press Shift + Enter. You should see the output Hello VS Code as below.\n\n\n\n\nHello VS Code\n\n\nCongratulations! You’ve just run your first Jupyter notebook in VS Code. You’re now ready to use this environment for more advanced analysis and coding throughout the course.\n\n\n\n\n\n\nHere’s a great video to get you up and running with Jupyter notebooks in VS Code!",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#exploring-the-vs-code-interface",
    "href": "99-vscode-install.html#exploring-the-vs-code-interface",
    "title": "13  VS Code Installation",
    "section": "13.4 Exploring the VS Code Interface",
    "text": "13.4 Exploring the VS Code Interface\nVS Code is packed with features, but you don’t need to use them all right away. As you become more comfortable with VS Code and your data science skills grow, you’ll naturally start using more of its capabilities. Here are some of the features you’ll find yourself using most often:\n\nExplorer (📁 icon): View and manage your project files.\nRun and Debug (▶️ icon): Run code or set up debugging tools.\nExtensions (🔌 icon): Install and manage extensions.\nIntegrated Terminal: Open a terminal inside VS Code using Ctrl + `` orCmd + `` (backtick).\nNotebook Interface: When working in .ipynb files, you can use cells to write and run code interactively.\n\nIf the interface feels overwhelming at first, don’t worry. Start by focusing on writing and running Python code—over time, you’ll naturally grow into the more advanced features.\n\n\n\n\n\n\nHere is a nice video that will introduce you to some of VS Code’s features. Don’t worry about understanding the code shown or what all these features mean right now—just watch to get a sense of what VS Code can do!",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#troubleshooting-tips",
    "href": "99-vscode-install.html#troubleshooting-tips",
    "title": "13  VS Code Installation",
    "section": "13.5 Troubleshooting Tips",
    "text": "13.5 Troubleshooting Tips\nIf you run into issues while setting up or using VS Code, here are some tips:\n\nVS Code doesn’t install:: If you have any issues during setup, check out the VS Code setup docs for details around system requirements, more detailed setups for each operating system, along with common questions.\nVS Code installs but doesn’t launch:\n\nMake sure the installation finished successfully and you can find the application on your computer\n\nOn Windows: Click the Start menu, type “Visual Studio Code,” and press Enter. If you checked the box to add VS Code to the system PATH, you can also open a Command Prompt or PowerShell window and type code to launch it.\nOn macOS: Open Spotlight Search by pressing Cmd + Space, type “Visual Studio Code,” and press Enter. If you installed the command line tool, you can also open a Terminal window and type code.\n\nTry restarting your machine.\nReinstall VS Code from the official website.\n\nPython extension isn’t working or missing:\n\nMake sure it’s installed and enabled.\nReload the window (Cmd/Ctrl + Shift + P → type Reload Window).\n\nPython interpreter not found:\n\nPress Ctrl+Shift+P / Cmd+Shift+P and search for “Python: Select Interpreter”. Choose the appropriate Python version (often one installed with Anaconda).\nIf no Python options come up then you probably don’t have a Python interpreter installed. Go to https://www.python.org/downloads/ and download the latest version.\n\nNotebook cells not running:\n\nMake sure the Jupyter extension is installed.\nMake sure the file is saved with a .ipynb extension.\n\n\nIf all else fails, try searching the error message online or ask your instructor or teaching assistant for help.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#additional-resources",
    "href": "99-vscode-install.html#additional-resources",
    "title": "13  VS Code Installation",
    "section": "13.6 Additional Resources",
    "text": "13.6 Additional Resources\nIf you want to learn more about using VS Code for Python and data science, here are some helpful links:\n\nTutorial: Get started with Visual Studio Code\nVS Code Python Docs\nVS Code for Data Science Docs\nJupyter Notebooks in VS Code\nVarious VS Code Introductory Videos\n\nYou don’t need to master all of this at once—use these resources when you’re ready to explore more advanced features or troubleshoot issues.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  }
]