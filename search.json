[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BANA 4080: Data Mining",
    "section": "",
    "text": "Welcome\nWelcome to BANA 4080: Introduction to Data Mining with Python. This course provides an immersive, hands-on introduction to the tools and techniques used in modern data science. You’ll learn how to explore, analyze, and model data using Python and gain practical experience through labs, projects, and real-world datasets.\nAlong the way you will develop core skills in data wrangling, exploratory data analysis, data visualization, and even key machine learning techniques such as supervised, unsupervised, and deep learning model. We’ll even take a quick detour into generative AI and large language models (LLMs). Throughout this process we’ll use real-world data and experiential learning to guide your learning.\nBy the end of the course, students will be able to:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#who-should-read-this",
    "href": "index.html#who-should-read-this",
    "title": "BANA 4080: Data Mining",
    "section": "Who should read this?",
    "text": "Who should read this?\nThis book is designed for upper-level undergraduate students who may have little to no prior programming experience but are eager to explore the world of data science using Python. It’s also an ideal resource for early-career professionals or students in analytics, business, or quantitative fields who are looking to upskill—whether by learning Python for the first time or by building a deeper understanding of how to explore, visualize, and model data. The content is structured to be accessible and hands-on, guiding readers step-by-step through the core tools and techniques used in modern data-driven problem solving.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-this-book-is-structured",
    "href": "index.html#how-this-book-is-structured",
    "title": "BANA 4080: Data Mining",
    "section": "How this book is structured",
    "text": "How this book is structured\nThis book is broken into 14 modules, each aligned with a week of instruction in the BANA 4080 course. Every module introduces key concepts or techniques in data science, combining concise explanations with interactive, hands-on code examples. Whether you’re reading independently or following along with the course, the modular structure makes it easy to work through the content at your own pace, week by week.\n\n\n\n\n\n\n\nModule & Topics\nSummary of Concepts Covered\n\n\n\n\n1. Fundamentals I\nCourse overview, coding environment setup, Python basics\n\n\n2. Fundamentals II\nUsing Jupyter notebooks, data structures, Python libraries\n\n\n3. Pandas & Data Wrangling I\nImporting data, DataFrame fundamentals, subsetting, cleaning, filtering\n\n\n4. Pandas & Data Wrangling II\nWorking with datetime, text data, aggregating, merging, and joining data\n\n\n5. Data Visualization\nCreating plots using matplotlib and seaborn, exploratory data analysis\n\n\n6. Writing Efficient Python Code\nControl flow, defining functions, loops, list comprehensions\n\n\n7. Mid-term Project Kickoff\nProject scoping, planning, and initial development work\n\n\n8. Introduction to Machine Learning\nOverview of ML, features/labels, train/test split, key considerations\n\n\n9. Linear Regression & Correlation\nCorrelation analysis, simple & multiple linear regression, model evaluation\n\n\n10. Logistic Regression & Classification\nBinary classification, logistic regression, classification metrics\n\n\n11. Tree-Based Models & Feature Importance\nDecision trees, random forests, ensemble methods, model interpretability\n\n\n12. Model Optimization & Validation\nFeature engineering, cross-validation, hyperparameter tuning, best practices\n\n\n13. Unsupervised Learning & Advanced Topics\nClustering, dimensionality reduction, deep learning overview, generative AI\n\n\n14. Final Project Kickoff & Wrap-Up\nPresenting project findings, course reflection, and next steps",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#conventions-used-in-this-book",
    "href": "index.html#conventions-used-in-this-book",
    "title": "BANA 4080: Data Mining",
    "section": "Conventions used in this book",
    "text": "Conventions used in this book\nThe following typographical conventions are used in this book:\n\nstrong italic: indicates new terms,\nbold: indicates package & file names,\ninline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user,\ncode chunk: indicates commands or other text that could be typed literally by the user\n\n\n1 + 2\n\n3\n\n\nIn addition to the general text used throughout, you will notice the following code chunks with images:\n\n\n\n\n\n\nSignifies a tip or suggestion\n\n\n\n\n\n\n\n\n\nSignifies a general note\n\n\n\n\n\n\n\n\n\nSignifies a warning or caution",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#software-used-throughout-this-book",
    "href": "index.html#software-used-throughout-this-book",
    "title": "BANA 4080: Data Mining",
    "section": "Software used throughout this book",
    "text": "Software used throughout this book\nThis book is built around an open-source Python-based data science ecosystem. While the list of tools evolves with the field, the examples and exercises in this book are designed to work with Python 3.x, currently using…\n\n\nCode\n# Display the Python version\nimport sys\nprint(\"Python version:\", sys.version.split()[0])\n\n\nPython version: 3.13.7\n\n\n…and are executed within Jupyter Notebooks, which provide an interactive, beginner-friendly environment for writing and running code.\nThroughout the modules, we use foundational Python libraries such as:\n\npandas and numpy for data wrangling and numerical computing,\nmatplotlib and seaborn for data visualization,\nscikit-learn and keras for machine learning and deep learning, and\nopenai and transformers for generative AI and large language model exploration.\n\nEach module explicitly introduces the relevant software and libraries, explains how and why they are used, and provides reproducible code so that readers can follow along and generate similar results in their own environment.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "BANA 4080: Data Mining",
    "section": "Additional resources",
    "text": "Additional resources\nTBD",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html",
    "href": "01-intro-data-mining.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Why are we here?\nWelcome to your journey into the world of data science. Whether you’re here out of curiosity, career aspirations, or because it’s a required course, you’re stepping into a field that blends logic, creativity, and curiosity to solve real-world problems using data. But let’s be honest—if you’ve never written a line of code before, the idea of learning Python or building machine learning models might feel a bit intimidating. You might even be wondering: “Why do I need to learn this when tools like ChatGPT or Copilot can just write code for me?” That’s a fair question — and one we’ll unpack in this chapter. Our goal is to give you a clear picture of why learning these foundational skills still matters, why Python is at the heart of modern data science, and how to approach the learning process with the right expectations and mindset.\nBy the end of this chapter, you will:\nLet’s start with a story.\nImagine you’re a college junior named Taylor who just landed a summer internship at a local marketing analytics firm. On your first day, your manager hands you a file — it’s a messy spreadsheet filled with customer purchase data — and says, “We’re trying to understand what drives repeat purchases. Can you dig into this and see what you find?”\nTaylor freezes.\nYou’ve taken calculus, stats, and maybe even a regression course. You’ve know Excel, used it to build a few dashboards in your business classes, and maybe dabbled in a little VBA. You know how to analyze formulas on paper and interpret statistical summaries. You’ve studied how marketing campaigns influence consumer behavior, how pricing affects demand, how supply chains function, and how companies generate profits.\nBut now, faced with a real dataset and a vague question, you’re not sure where to begin.\nYou know there’s a story in the data; something important about customer behavior that the business wants to uncover. But without a roadmap, the numbers feel more overwhelming than enlightening.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#why-are-we-here",
    "href": "01-intro-data-mining.html#why-are-we-here",
    "title": "1  Introduction",
    "section": "",
    "text": "Do you start sorting columns manually?\nWrite an IF statement?\nAsk ChatGPT to explain the data?\nLook for a “correlation” button in Excel?\n\n\n\nYou’ve built the foundation—now it’s time to apply it.\nThat feeling Taylor has? That’s not a sign of being unprepared, it’s a sign of the gap that exists between traditional classroom learning and the real-world application of data science.\nYou already know more than you think:\n\nYou understand business operations, customer behavior, and the bottom-line goals companies care about.\nYou’ve studied quantitative methods and statistical inference.\nYou know how to think critically and ask good questions.\n\nWhat you haven’t had (yet) is the experience of turning raw data into actionable insight using tools like Python to clean, explore, visualize, and model information in a repeatable, scalable way.\nThat’s where this course comes in.\n\n\nThis course is about doing.\nWe’re going to close the gap between the theory you’ve learned and the practice that’s expected in today’s data-driven workplace. You’ll work with messy datasets just like Taylor’s. You’ll write code to clean, organize, and analyze real data. And you’ll build up your own toolkit so that, the next time someone asks you to “see what you can find,” you won’t freeze — you’ll get to work.\nAnd yes, we’ll even talk about when to use tools like ChatGPT to help you along the way (and when not to).\n\n\n\n\n\n\nBy the end of this course, you’ll have the confidence and experience to tackle open-ended data problems, the skills to automate and scale your work, and the mindset to keep growing as a data-driven problem solver.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#isnt-ai-supposed-to-do-all-this-for-us-now",
    "href": "01-intro-data-mining.html#isnt-ai-supposed-to-do-all-this-for-us-now",
    "title": "1  Introduction",
    "section": "1.2 Isn’t AI supposed to do all this for us now?",
    "text": "1.2 Isn’t AI supposed to do all this for us now?\nLet’s address the question that’s probably been lingering in your mind since the moment you signed up for this course:\n\n“Why do I need to learn how to code when tools like ChatGPT, Copilot, and Claude can just do it for me?”\n\nIt’s a fair question.\nIn fact, you may have already used one of these tools to generate code. Maybe you asked ChatGPT to “write a Python script that reads a CSV file and finds the average,” and in seconds — boom — you had something that worked.\nSo… case closed, right? Not quite.\n\nAI is helpful—but it’s not a substitute for understanding.\nGenerative AI tools are incredible accelerators. They can save time, reduce friction, and provide helpful starting points. But they’re not magic. They don’t know your data. They don’t understand the business context. And they certainly don’t guarantee correct answers.\nThese tools work by predicting the next most likely word or line of code based on patterns in data they’ve seen. They don’t reason like humans. They can’t debug your logic or decide which metric is appropriate for your analysis. And sometimes? They just make stuff up.\n\n\n\n\n\n\nWarningCallout: It’s like autocorrect but for code!\n\n\n\n\n\nIf you’ve ever had your phone turn “on my way!” into “omg my weasel!” — you already understand how these tools work.\nGenAI models, like ChatGPT and Copilot, are basically super-powered autocomplete engines. They’re predicting what comes next based on what they’ve seen before. That means they can sometimes nail it… and other times leave you wondering, “What were you even trying to say?”\n\nJust like with autocorrect, the key is knowing when the suggestion is helpful—and when to hit backspace.\n\n\n\nIf you don’t have the foundational skills, you won’t know when they’re wrong—or worse, when they’re subtly wrong.\n\n\nAI tools are assistants, not autopilots.\nLearning how to code and analyze data yourself is what allows you to:\n\nSpot mistakes in generated code\nAsk better questions (better prompts = better results)\nCustomize and build on what AI gives you\nEvaluate whether an approach is valid or helpful\nExplain what your analysis means and how it should be used\n\nYou don’t need to fear these tools. But you also don’t want to depend on them blindly. This course will teach you the skills needed to use AI as a powerful assistant — not a crutch.\n\n\n\n\n\n\nNoteStudent Reflection Prompt\n\n\n\n\n\nHave you ever used a tool like ChatGPT, Claude, or Copilot to generate code or solve a problem?\n\nWhat did it get right?\nWhat did it miss?\nHow confident were you in the result?\n\n\n\n\nIn the chapters ahead, we’ll sometimes bring GenAI tools into the learning process, especially as helpers for things like brainstorming or debugging. But we’ll always emphasize the importance of understanding the code you’re working with. Otherwise, you’re just copying answers without learning anything—which is like using a calculator to solve a math problem you never actually learned.\nAnd trust us: if you ever find yourself in Taylor’s shoes—staring at a spreadsheet and needing to figure out what matters—you’ll want more than a chatbot. You’ll want skill.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#why-python-and-why-now",
    "href": "01-intro-data-mining.html#why-python-and-why-now",
    "title": "1  Introduction",
    "section": "1.3 Why Python, and why now?",
    "text": "1.3 Why Python, and why now?\nSo far, we’ve talked about why it’s important to build foundational skills and not overly rely on AI tools. But that probably leaves you with another question:\n\n“Out of all the programming languages out there, why are we learning Python?”\n\nIt’s a good question — and the answer is simple: Python is the language of modern data science.\n\nPython is beginner-friendly, but not just for beginners.\nPython was designed to be easy to read and write. Its syntax is clean and (mostly) intuitive, which means you won’t spend hours trying to remember obscure symbols or puzzling over where the semicolon went. That makes it a great first language for students who are new to programming.\nBut don’t mistake simplicity for lack of power. Python is also used by:\n\nData scientists at companies like Google, Netflix, and Spotify\nAnalysts at healthcare organizations, banks, and nonprofits\nResearchers running simulations and training machine learning models\nEngineers building large-scale data pipelines and AI tools\n\nSo while Python is accessible to beginners, it’s also respected and widely used by professionals. In fact, according to the 2023 Stack Overflow Developer Survey, Python ranks as one of the top three most commonly used programming languages overall, and is consistently a favorite among developers working in data science, machine learning, and academic research. Its combination of readability, power, and a rich ecosystem of libraries makes it a go-to language across industries and roles.\n\n\nIt’s not just the language, it’s the ecosystem.\nPython has a massive collection of libraries and packages built specifically for data work. Here are just a few you’ll get to know in this course but realize there are tens of thousands of open source Python libraries that you can use for various data science tasks:\n\npandas – for working with data tables\nnumpy – for efficient numerical computation\nmatplotlib and seaborn – for data visualization\nscikit-learn – for building machine learning models\n\nThis ecosystem means you won’t have to build everything from scratch. You’ll learn to stand on the shoulders of open-source giants so you can focus more on solving problems and less on reinventing the wheel.\n\n\nPython is your toolset. This course is your training ground.\nIn this course, you’ll learn how to:\n\nWrite Python code that manipulates and explores real datasets\nUse libraries like pandas and matplotlib to summarize and visualize your data\nBuild your first machine learning models with tools like scikit-learn\nGain confidence in reading and modifying code — whether it came from a textbook, a blog post, or a GenAI assistant\n\nBy the end of this course, Python won’t feel like some intimidating, foreign technical concept you’ve been avoiding or unsure how to approach. It will become a practical skill—a tool you know how to use to explore data, uncover insights, and drive real decisions. And here’s the best part: you’ll be learning the same tool that analysts, data scientists, and business teams across your future organization are already using. Instead of feeling left out of the conversation, you’ll be equipped to lead it.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#learning-to-code-a-reality-check",
    "href": "01-intro-data-mining.html#learning-to-code-a-reality-check",
    "title": "1  Introduction",
    "section": "1.4 Learning to code: a reality check",
    "text": "1.4 Learning to code: a reality check\nLet’s be real for a minute.\nLearning to code can be frustrating at first. It’s not always fun to stare at an error message that makes zero sense. It’s even less fun when you fix that error… only to get a brand-new one. You might feel stuck, confused, or like everyone else “gets it” but you.\nThat’s normal. In fact, it’s expected.\n\nLearning to code is like learning a new language (because it is)\nWhen you’re first learning a spoken language, you stumble over basic phrases. You forget vocabulary. You mess up grammar. But over time, with practice, you start to think in the new language — and eventually, it just clicks.\nCoding is the same way. You’ll start by copying examples and Googling error messages. That’s fine. That’s part of the process. Over time, those patterns will stick. You’ll stop memorizing and start thinking in code.\n\n\nThis course is designed for beginners\nYou don’t need prior programming experience to succeed here. We’ll start from the very beginning: writing simple statements, understanding variables and data types, building up to loops, functions, and eventually full data science workflows.\nYou’ll go from:\n\n“What’s a variable?”\n\nto…\n\n“I just built a machine learning model to predict customer churn.”\n\nStep by step. Week by week. You’ll get there. And don’t feel like you have to go it alone.\n\n\n\n\n\n\nTipLean on your classmates.\n\n\n\n\n\nIf you’re stuck, chances are someone else is too. Use the classroom discussion board to ask questions, share what you’re learning, and help each other out. This is a collaborative course, and we’re building a community of learners who grow together.\n\n\n\n\n\nIt’s okay to use AI tools but don’t skip the struggle\nYou’ll be encouraged to use tools like ChatGPT, Copilot, and Claude throughout this course. But here’s the rule: use them to learn, not to avoid learning.\nThink of these tools as tutors, not answer keys:\n\nUse them to check your understanding\nAsk them for help when you’re stuck\nCompare their answers to your own and figure out the differences\nBut always make sure you understand what the code is doing\n\nShortcuts don’t help if you skip the part where you build skill.\nLearning to code is like learning anything meaningful—it takes effort, patience, and a little bit of resilience. You won’t get everything right the first time, and that’s okay. You don’t need to be perfect. You just need to keep showing up and keep trying.\nAnd we’ll be right here to help every step of the way.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#wrap-up",
    "href": "01-intro-data-mining.html#wrap-up",
    "title": "1  Introduction",
    "section": "1.5 Wrap-Up",
    "text": "1.5 Wrap-Up\nYou’ve made it through your first chapter — nicely done!\nWe’ve talked about why data science matters, why Python is the language we’re using, and why learning these skills (even in the age of AI) still gives you a powerful edge. We’ve also tried to set realistic expectations: learning to code can be challenging, but it’s absolutely doable—with patience, practice, and support from your classmates, instructors, and tools like ChatGPT when used wisely.\nThe big takeaway? You’re not here to memorize formulas or passively watch someone else code. You’re here to build real skills that will help you ask better questions, explore real-world data, and create insights that drive decisions.\nAnd we’re not wasting any time. In the next chapter, we’ll jump right in — you’ll get your Python environment set up and write your first lines of code. This is where the hands-on part of the journey begins.\nThis course moves fast, so buckle up and enjoy the ride. Let’s get started!",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#exercise-can-you-help-taylor",
    "href": "01-intro-data-mining.html#exercise-can-you-help-taylor",
    "title": "1  Introduction",
    "section": "1.6 Exercise: “Can You Help Taylor?”",
    "text": "1.6 Exercise: “Can You Help Taylor?”\n\n\n\n\n\n\nNoneScenario\n\n\n\n\n\nYou’re stepping into Taylor’s shoes. You’ve just been handed three datasets by your manager at a local marketing analytics firm:\n\ntransactions.csv — individual transactions from various customers, but… it’s a litle messy.\nhh_demographics.csv — customer-level information including age, household size, income level, etc.\nproduct.csv - product-level information such as manufacturer, brand, commodity type, package size, etc.\n\nYour job is to begin exploring what factors might be influencing repeat purchases. But before you can even analyze anything, you’ll need to clean, combine, and make sense of the data.\nThroughout this exercise your objective is to:\n\nInspect and describe real-world “messy” data\nIdentify issues that must be addressed before analysis\nStrategize a plan for cleaning and joining datasets\nExperiment with using ChatGPT to assist your data work\nReflect on how foundational skills + GenAI can be used together\n\n\n\n\n\n\n\n\n\n\nNonePart 1 – Get Oriented\n\n\n\n\n\nStart by opening the transactions data and take a few minutes to explore it.\nQuestions:\n\nWhat’s messy or confusing about this data?\nWhat would your first 2–3 steps be if you were asked to clean or prepare it for analysis?\nBased on this table alone, what kinds of analyses might you try to do?\n\n\n\n\n\n\n\n\n\n\nNonePart 2 – Meet the Second Dataset\n\n\n\n\n\nNow open the household demographic and the product information data.\nQuestions:\n\nWhat types of information does this dataset contain that might help you understand customer behavior?\nHow would you combine these with the transactions data?\nWhat issues do you foresee when trying to join them?\n\n\n\n\n\n\n\n\n\n\nNonePart 3 – Ask for Help (But Be Smart About It)\n\n\n\n\n\nUse ChatGPT (or any GenAI tool you’re comfortable with) to begin cleaning or analyzing the data.\nInstructions:\n\nTry writing 2–3 different prompts to help with basic data cleaning, joining, or summarizing the data.\nTry at least one prompt that combines both datasets.\n\nThen reflect:\n\nWhat prompts did you use?\nDid the AI return usable code?\nWere there any errors, misunderstandings, or surprises?\nDid you understand what the code was doing? If not, what helped you figure it out?\n\n\n\n\n\n\n\n\n\n\nNonePart 4 – Pull It All Together\n\n\n\n\n\nWrite a short summary (5–7 sentences) answering the following:\n\nWhat was your biggest takeaway from working with these datasets?\nHow did this exercise reinforce the ideas from Chapter 1?\nDid this change your perspective on what your role is when using AI tools like ChatGPT in data analysis?\nHow do you feel about starting your journey into Python after completing this activity?",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html",
    "href": "02-preparing-for-code.html",
    "title": "2  Setting Up Your Python Environment",
    "section": "",
    "text": "2.1 Hello World\nBefore we dive deep into the world of data science, we need to get you set up with the tools of the trade. Just like a carpenter needs a good workbench and set of tools, a data scientist needs a place to write and run code. The good news? There are several ways to do this—and even better, there’s no one-size-fits-all answer. You’ll get to choose the approach that works best for you.\nIn this chapter, we’ll introduce three common ways to set up a Python environment: Google Colab, the Anaconda distribution, and Visual Studio Code. We’ll start with the path of least resistance (Colab), then explore more powerful and flexible setups as we go.\nWe’ll kick things off by writing your very first Python program—a classic:\nBy the end of this chapter, you’ll not only have a working Python environment, but you’ll also understand the pros and cons of each approach and know how to get started coding in any of them. Let’s get your hands on the keyboard.\nWelcome to your first moment of writing real Python code. We’re not going to lecture you about the perfect setup right away. Instead, let’s just do something. This short activity gives you a quick win and shows you how easy it can be to get started with Python—no installations or configurations required.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html#hello-world",
    "href": "02-preparing-for-code.html#hello-world",
    "title": "2  Setting Up Your Python Environment",
    "section": "",
    "text": "Running Python in Google Colab\nGoogle Colab is a free, cloud-based tool that lets you run Python code in your browser. It requires no setup and is perfect for getting started.\n\nGo to: https://colab.research.google.com/\nSign in with your Google account (you’ll need one).\nClick on “New Notebook.”\nIn the cell that appears, type:\nprint(\"Hello World\")\nPress Shift + Enter to run the cell.\n\nYou should see:\nHello World\nYou just wrote and ran your first line of Python code! 🎉🎉🎉\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneExample Notebook\n\n\n\nLet’s put your new skills to the test by working through a simple example notebook.\n📓 We’ve created a sample notebook that introduces:\n\nThe print() function\nHow to format and write some basic text with Markdown\nHow to combine code and notes in one place\n\n👉 Click the “Open in Colab” link at the top of the notebook to launch it in Google Colab and run through the notebook.\n\n\n\n\nTry This: Generate Python Code with AI\nAfter you run your print(\"Hello World\"), take a moment to explore the built-in AI assistant in Google Colab. You might notice a prompt that says:\n\n“Start coding or generate with AI”\n\nThis is your chance to see how AI tools can help you write Python code. Try typing in a prompt or click on the suggestion box to let Colab help generate code for you. You can run the code to see what it does—and even tweak it to make it your own.\nFeel free to come up with your own prompt, or try one of these to get started:\n\n“Write Python code to compute the area of a 12-inch pizza.”\n“Write Python code to find all prime numbers between 2 and 100.”\n“Write a Python program that asks for a user’s name and prints a greeting.”\n\n\n\n\n\n\n\nNoneReflect: Your First Python Experience\n\n\n\n\n\nTake a few minutes to reflect on your first hands-on experience writing Python code and using an AI assistant. You can jot your answers in a notebook, a note-taking app, or even directly in your Colab notebook using a text cell (don’t know how to do this, that’s ok, ask Colab’s AI to help 😉).\nConsider the following questions:\n\nWas writing and running your first lines of Python code easier or harder than you expected?\nHow did it feel to use the AI assistant to generate code? Do you think tools like this can make you more productive? How confident are you that the code was actually correct?\nGoogle Colab makes it easy to get started, but do you think this is the kind of environment you would use at work? Why or why not?\n\nYou don’t need to write a long answer—just a few thoughtful sentences to capture your perspective at this early stage in your learning journey.\n\n\n\nNext, we’ll step back and look at what just happened. Then, we’ll dive into the different ways you can set up your Python environment, from easy to advanced.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html#development-environments",
    "href": "02-preparing-for-code.html#development-environments",
    "title": "2  Setting Up Your Python Environment",
    "section": "2.2 Development Environments",
    "text": "2.2 Development Environments\nNow that you’ve written and run your first line of Python code, it’s time to step back and understand where that code actually runs—and what your options are going forward.\nIn programming, the place where you write and run your code is called your development environment. Think of it like your digital workspace: it includes the tools, interface, and underlying systems that allow you to code, debug, and manage projects.\nThere’s no single “best” environment—just the one that’s best for your current needs. In this course, you’ll be exposed to three common Python environments used by data scientists, ranging from beginner-friendly to professional-grade. Each has its own strengths and trade-offs, and over time you may find yourself using all three depending on the situation.\nBelow is a quick overview of the three environments we’ll focus on in this course.\n\nGoogle Colab\nYou’ve already used this! Colab is a cloud-based environment that lets you run Python in your browser, no installation required. It’s perfect for beginners or anyone who wants to start quickly and painlessly.\nPros:\n\nNo installation needed—works in your browser.\nBuilt-in support for Jupyter notebooks.\nEasy to share and collaborate via Google Drive.\nIncludes access to an AI assistant to help generate code.\n\nCons:\n\nRequires an internet connection.\nLimited access to your local files or custom setups.\nNot commonly used in professional, production-level environments.\n\n\n\n\n\n\n\nWhile Colab is excellent for learning, prototyping, and sharing code, it isn’t typically used in workplace settings—especially for production code, version-controlled projects, or large-scale data workflows. Because of this, we encourage you to use Colab to get started quickly, but strive to set up one of the other environments (Anaconda or VS Code) as you progress through the course. These tools will better reflect the development environments you’re likely to use in internships, co-ops, or full-time roles\n\n\n\n\n\nAnaconda Distribution\nAnaconda is a local setup that installs Python along with most of the libraries and tools used in data science, including Jupyter Notebook and JupyterLab. It’s an excellent next step for learners who want more control while still keeping things simple.\n\n\n\n\n\n\nAnaconda\n\n\n\n\nFigure 2.1: Anaconda is one of the most widely used platforms for data science, offering an all-in-one distribution of Python, Jupyter, and essential packages for analysis and machine learning.\n\n\n\nPros:\n\nAll-in-one installation of Python + essential libraries.\nUser-friendly interface via Anaconda Navigator.\nIncludes JupyterLab for notebook-based development.\nWorks locally without internet access.\n\nCons:\n\nLarge download and install size (~3 GB).\nYou’ll need to learn how to manage packages and environments with Conda.\nSlightly more complex than Colab, but still beginner-friendly.\n\n\n\nVisual Studio Code (VS Code)\nVS Code is a lightweight but powerful code editor used by professional developers and data scientists. With the right extensions, it supports Python and Jupyter notebooks and is ideal for building larger or more complex projects.\n\n\n\n\n\n\nVS Code is my preferred environment for writing code, so throughout this course, most of the code you’ll see during lectures and demos will be displayed using VS Code. This will give you exposure to an industry-standard tool that’s commonly used across the kinds of organizations you’re likely to intern or work at.\n\n\n\n\n\n\n\n\n\nVS Code\n\n\n\n\nFigure 2.2: Visual Studio Code (VS Code) is one of the leading code editors used by data scientists and software developers. It’s widely adopted across industry and is likely the primary development environment you’ll encounter in internships or full-time roles.\n\n\n\nPros:\n\nHighly customizable and fast.\nGreat for real-world development workflows.\nIntegrated support for version control (Git), debugging, and extensions.\n\nCons:\n\nSteeper learning curve—more setup required.\nRequires separate installations of Python, extensions, and Jupyter support.\nBest suited for students who want to grow into more advanced tools.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html#choosing-your-path-forward",
    "href": "02-preparing-for-code.html#choosing-your-path-forward",
    "title": "2  Setting Up Your Python Environment",
    "section": "2.3 Choosing Your Path Forward",
    "text": "2.3 Choosing Your Path Forward\nHow you decide to move forward from here is up to you. For now, you’re welcome to continue using Google Colab, especially if it’s helping you build confidence and get comfortable writing Python code without worrying about software setup.\nThat said, by the end of this course, you’ll be expected to have a local Python development environment set up on your computer—either using Anaconda or Visual Studio Code. These environments are more reflective of what you’ll use in real-world internships, co-ops, or full-time roles, and they’ll give you greater flexibility and power as your projects grow in complexity.\nWhen you’re ready to make the leap, use the resources below to guide you:\n\nAnaconda Installation: Appendix 28\nVS Code Installation: Appendix 29\n\nYou don’t need to switch immediately, but the sooner you get comfortable working locally, the better prepared you’ll be for the kinds of tasks data scientists regularly tackle in the field.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html#exercise-pick-your-environment-and-make-it-yours",
    "href": "02-preparing-for-code.html#exercise-pick-your-environment-and-make-it-yours",
    "title": "2  Setting Up Your Python Environment",
    "section": "2.4 Exercise: Pick Your Environment and Make It Yours",
    "text": "2.4 Exercise: Pick Your Environment and Make It Yours\nChoose one of the following paths based on your current comfort level and curiosity. Your goal is to take one step toward becoming confident in your development environment.\n\n\n\n\n\n\nNoneOption 1: Stay with Colab (for now)\n\n\n\n\n\n\nOpen a new Colab notebook.\nUse the AI assistant to generate and run a short piece of Python code.\nTry modifying the code and run it again to see how it changes.\nAdd a Markdown cell where you reflect on:\n\nWhat the code does.\nWhat you changed.\nWhat you’re still unsure about.\n\n\nNote: Not sure what a Markdown cell is? Great opportunity to Google or ask ChatGPT! But don’t worry, we’ll discuss this more next week.\n\n\n\n\n\n\n\n\n\nNoneOption 2: Install Anaconda\n\n\n\n\n\n\nFollow the instructions in appendix 28 to install Anaconda and launch JupyterLab.\nCreate a new notebook and run print(\"Hello from Anaconda!\").\nTake a screenshot of your working notebook or write a short note on how it went.\n\n\n\n\n\n\n\n\n\n\nNoneOption 3: Set Up VS Code\n\n\n\n\n\n\nFollow the instructions in appendix 29 to install VS Code and set up Python and Jupyter support.\nOpen a Jupyter notebook and run print(\"Hello from VS Code!\").\nExplore the editor: try renaming a file, changing themes, or opening the terminal.\nWrite a brief reflection on what you liked or didn’t like about this environment.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html",
    "href": "03-python-basics.html",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "",
    "text": "3.1 Try It Before We Start!\nIf you’re new to Python or programming in general, welcome—you’re in the right place. This chapter is where we start turning ideas into code. Just like learning a new language starts with learning a few key words and phrases, learning Python begins with understanding how to work with basic building blocks: numbers, text, and data containers.\nWhy is this important? Because whether you’re analyzing customer data, building a machine learning model, or automating a task at work, you’ll rely on these foundational concepts every single time you write code. Think of this chapter as your toolbox—it may seem simple now, but you’ll keep coming back to these tools throughout your data science journey.\nAnd don’t worry if it doesn’t all click right away. Everyone struggles with syntax or logic at first. Learning to code is more like learning to solve puzzles than memorizing rules. Take your time, experiment, and remember: errors are part of the process (even experienced coders Google stuff constantly).\nBy the end of this chapter, you’ll be able to:\nLet’s dive in.\nLet’s write a few lines of Python code. Run the following in a Python environment (like Google Colab, Jupyter Notebook, or VS Code) and see what happens:\n# Store your name as a string\nname = \"Taylor\"\n\n# Store your age as a number\nage = 22\n\n# Print a personalized message\nprint(\"Hi \" + name + \"! You are \" + str(age) + \" years old.\")\n\n# Calculate how many years until you turn 30\nyears_to_30 = 30 - age\nprint(\"You'll turn 30 in \" + str(years_to_30) + \" years.\")\n\nHi Taylor! You are 22 years old.\nYou'll turn 30 in 8 years.\nWhat just happened?\nDon’t worry if this isn’t totally clear yet—that’s what this chapter is for. We’ll break it all down step by step.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#try-it-before-we-start",
    "href": "03-python-basics.html#try-it-before-we-start",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "",
    "text": "You stored information using variables\nYou worked with different data types (a string and a number)\nYou used basic math and printed a personalized message",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#python-data-types",
    "href": "03-python-basics.html#python-data-types",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.2 Python Data Types",
    "text": "3.2 Python Data Types\nEverything in Python is an object, and every object has a type. A data type tells Python what kind of value you’re working with—whether it’s a number, a piece of text, or a simple True/False flag. Understanding Python’s core data types is a fundamental step toward writing useful programs.\nLet’s walk through a quick introduction to the most common data types you’ll encounter early on. As we progress through this class and book, you’ll explore many more ways to work with and manipulate these data types.\n\n\nNumeric Types\nPython has two main types of numbers:\n\nint (integers): whole numbers like 5, 42, or -100\nfloat (floating-point): numbers with decimals like 3.14, 0.5, or -2.718\n\nYou can do all kinds of math with numbers in Python using simple operators:\n\n10 + 3.5   # Addition\n\n13.5\n\n\n\n10 * 3.5   # Multiplication\n\n35.0\n\n\nGo ahead and run these lines of code in your notebook:\n# Basic operations\n10 - 3.5   # Subtraction\n10 * 3.5   # Multiplication\n10 / 3.5   # Division (always returns a float)\n\n# More math\n10 ** 3.5  # Exponentiation (10^2 = 100)\n10 // 3.5  # Integer division (result is an int)\n10 % 3.5   # Modulus (remainder)\n\n\n\n\n\n\nWarningHeads Up: Python Math Isn’t Always Perfect\n\n\n\nSometimes, Python (like all programming languages) can give results that look a little… off. This usually happens because computers represent decimal numbers using floating-point approximation, which isn’t always exact.\nFor example, you’d expect the following to equal 0.3 but instead…\n\n0.1 + 0.1 + 0.1\n\n0.30000000000000004\n\n\nThis tiny difference is due to how numbers like 0.1 are stored in computer memory. It’s a common issue when doing calculations with decimal values, especially in financial or scientific applications. As we progress, you’ll learn best practices to handle imperfections like this. For now, you could just round it.\n\nround(0.1 + 0.1 + 0.1, ndigits=2)\n\n0.3\n\n\n\n\n\n\nStrings\nA string is a sequence of characters, enclosed in single or double quotes - whichever you use is personal preference but the output in the Python environment will contain single quotes:\n\n\"Hello Taylor!\"\n\n'Hello Taylor!'\n\n\nStrings can be combined and manipulated in many ways:\n\n# Concatenation\n\"Hello\" + \" \" + \"Taylor!\"   # Hello Taylor!\n\n'Hello Taylor!'\n\n\nGo ahead and run these lines of code in your notebook:\n# String repetition\n\"ha\" * 3   # 'hahaha'\n\n# Get first letter (starts at 0)\n\"Taylor\"[0] # 'T'\n\n# Get first three letters\n\"Taylor\"[:3]   # 'Tay'\nStrings are extremely common—you’ll use them to label, format, and present data in readable ways.\n\n\n\n\n\n\nIn fact, as organizations increasingly rely on data-driven decision-making, a significant portion of their data comes in the form of text. Examples include product descriptions, customer feedback, social media posts, and even log files. Understanding how to work with strings is essential for extracting insights and making sense of this unstructured data.\n\n\n\n\n\nBooleans\nA Boolean is a special data type with only two values: True and False.\nis_raining = True\nhas_umbrella = False\n\n\n\n\n\n\nIn Python, True and False are capitalized. Writing true or false (lowercase) will result in a NameError. Always ensure proper capitalization when working with booleans.\n\n\n\nYou’ll mostly use booleans when you write logical conditions, such as comparing values or checking if something is true. Don’t worry, we’ll discuss comparison operators (i.e. &gt;, ==) in a moment.\n\n5 &gt; 3      # True - 5 is greater than 3\n\nTrue\n\n\n\n5 == 3   # False - 5 does not equal 3\n\nFalse\n\n\nGo ahead and run these lines of code in your notebook:\n10 &lt; 9\n10 &gt; 9\n10 &lt;= 9\n10 &gt;= 9\n10 == 10\n\n\nType Checking and Conversion\nYou can check the type of any object using the type() function:\n\nprint(type(10))        # &lt;class 'int'&gt;\nprint(type(\"hello\"))   # &lt;class 'str'&gt;\n\n&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n\n\n\n\n\n\n\n\nTipA Quick Note on print()\n\n\n\n\n\nWhen working in a Jupyter notebook, Python will automatically display the result of the last line in a cell if it’s a value (like a string or number). For example:\n\ntype(10)        \ntype(\"hello\")   # will only show the output of this last line\n\nstr\n\n\nBut in most real Python programs—like scripts, functions, or when running multiple lines—you need to use print() to explicitly display something to the user:\n\nprint(type(10))        # will print print out this output...\nprint(type(\"hello\"))   # and this output\n\n&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n\n\nUsing print()` is a way to say, “Hey Python, show this to me in the output.” You’ll use it all the time for debugging, building interfaces, or just (as in this case) just to tell Python to display the output of each line of code.\n\n\n\nYou can also convert between types using built-in functions. For example, the following converts the integer value of 5 to a string '5'.\n\nstr(5) # '5' (string to int)\n\n'5'\n\n\nAnd the following converts a string '5' to an integer 5:\n\nint('5') # 5 (int to string)\n\n5\n\n\nGo ahead and run these lines of code in your notebook:\nprint(type(3.5))     # &lt;class 'float'&gt;\nprint(type(True))    # &lt;class 'bool'&gt;\nprint(str(3.14))     # '3.14' (convert decimal to string)\nprint(bool(0))       # False (0 is treated as False)\nprint(float(2))      # 2.0 (convert integer to decimal)\nThese conversions come in handy when working with user input or cleaning messy data.\n\n\nKnowledge check\nWork through the following tasks in your notebook.\n\n\n\n\n\n\nNone🍕 What’s the best deal?\n\n\n\n\n\nA 12-inch pizza costs $8. Use the formula for the area of a circle (\\(A = \\pi × r^2\\)) to calculate the cost per square inch of the pizza.\nHints:\n\nRadius (\\(r\\)) is half the diameter\nUse 3.14159 as your approximation for \\(\\pi\\)\nDivide the price by the area to get cost per square inch\n\nNow repeate for a 15-inch pizza that costs $12. Which is a better deal?\n\n\n\n\n\n\n\n\n\nNonePlay with ‘strings’\n\n\n\n\n\nFirst, guess what each line of code will result in. Then run them in your notebook. Were the results what you expected?\nprint(\"Python\" + \"Rocks\")\nprint(\"ha\" * 5)\nprint(\"banana\"[1])\nprint(\"banana\"[::-1])   # Can you guess what this does?\nExtra challenge: Can you use slicing to print just the word \"ana\" from \"banana\"?\n\n\n\n\n\n\n\n\n\nNone🕵🏻‍♂️ Data type detective\n\n\n\n\n\nBefore you run the following, what do you think the data types are for each line? Then, run the code in your notebook to check your answers. Were your predictions correct?\nprint(type(\"True\"))\nprint(type(True))",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#variables-and-the-assignment-operator",
    "href": "03-python-basics.html#variables-and-the-assignment-operator",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.3 Variables and the Assignment Operator",
    "text": "3.3 Variables and the Assignment Operator\nIn the last section, we did a lot of math using the same numbers — 10 and 3.5 — over and over again:\n# Basic operations\n10 - 3.5\n10 * 3.5\n10 / 3.5\nThat works fine for short examples, but imagine writing a program that needs to use the same values in dozens of different places. What if you want to change one of those values later? You’d have to find and update every instance in your code.\nThat’s where variables come in.\n\nWhat Is a Variable?\nA variable is a name that refers to a value. You can think of it like labeling a container that holds something useful—like a number, a word, or even a list of things.\nHere’s how we could rewrite the examples above using variables:\nx = 10\ny = 3.5\n\nprint(x - y)\nprint(x * y)\nprint(x / y)\nMuch cleaner, right?\n\n\n\n\n\n\nImportantWhy Use Variables?\n\n\n\nVariables help you:\n\nAvoid repeating values\nMake your code easier to read and maintain\nUpdate values in one place instead of many\n\nAnd when your programs get more complex, variables become essential for storing user input, results from calculations, or intermediate steps in a data analysis.\n\n\n\n\nThe Assignment Operator: =\nTo create a variable, we use the assignment operator (=). This tells Python to take the value on the right and assign it to the name on the left:\n\ngreeting = \"Hello, world!\"\n\nThis means - store the string \"Hello, world!\" in a variable called greeting\nYou can then reuse that variable:\n\nprint(greeting)\n\nHello, world!\n\n\n\n\n\n\n\n\nIn Python, = does not mean “equal to” like in math. It’s an instruction: assign the value.\n\n\n\n\n\nNaming Variables\nHere are basic rules for naming variables in Python:\n\n✅ Must start with a letter (or an underscore _ as in _name, though that’s typically reserved for special cases—so avoid starting with _ unless you know what you’re doing)\n✅ Can include letters, numbers, and underscores\n🚫 Cannot start with a number\n🚫 Cannot use built-in Python keywords (like if, True, print, etc.)\n\nSome good examples:\nage = 25\nstudent_name = \"Taylor\"\nis_logged_in = True\nUse descriptive names when possible—it makes your code easier for others (and future-you) to understand.\n\n\nReassigning Variables\nVariables can change! When you assign a new value to an existing variable, it overwrites the old one:\n\nx = 5\nx = x + 1  # Now x is 6\n\nprint(x)\n\n6\n\n\nPython always uses the most recent value.\n\n\nYou Can Store Any Type of Value\nYou can assign any data type to a variable—numbers, strings, booleans, and more:\nname = \"Taylor\"           # string\ngpa = 3.85                # float\nis_honors_student = True  # boolean\nAnd Python is flexible — you can even change what type a variable holds:\ngpa = \"3.85\"  # Now it's a string!\nThis is called dynamic typing, and it’s part of what makes Python beginner-friendly.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry It!\n\n\n\n\n\nLet’s return to our earlier example from the start of the chapter:\n# Store your name and age\nname = \"Taylor\"\nage = 22\n\n# Print a custom message\nprint(\"Hi \" + name + \"! You are \" + str(age) + \" years old.\")\n\n# Do a little math\nprint(\"You'll turn 30 in \" + str(30 - age) + \" years.\")\nTry updating the values of name and age to reflect your info. Then tweak the message to include your graduation year or your major. Play around; don’t worry you won’t break anything!",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#comparison-operators",
    "href": "03-python-basics.html#comparison-operators",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.4 Comparison Operators",
    "text": "3.4 Comparison Operators\nIn the previous section on booleans, we saw that comparison expressions—like 5 &gt; 3 — evaluate to either True or False. These expressions are powered by comparison operators, which are used to compare values in Python.\nYou’ll use comparison operators all the time when writing conditions, checking data, filtering results, or writing logic into your programs.\nHere’s a quick cheat sheet of the most common ones:\n\n\n\nOperator\nDescription\nExample\nResult\n\n\n\n\n==\nEqual to\n5 == 5\nTrue\n\n\n!=\nNot equal to\n5 != 3\nTrue\n\n\n&gt;\nGreater than\n10 &gt; 7\nTrue\n\n\n&lt;\nLess than\n4 &lt; 2\nFalse\n\n\n&gt;=\nGreater than or equal to\n3 &gt;= 3\nTrue\n\n\n&lt;=\nLess than or equal to\n8 &lt;= 6\nFalse\n\n\n\nAll of these expressions return a boolean value: True or False. Try running the following lines of code in your notebook:\nprint(10 &gt; 3)      # True\nprint(2 &lt; 1)       # False\nprint(4 == 4.0)    # True (int and float are treated as equal in value)\nprint(4 != 5)      # True\nprint(6 &gt;= 7)      # False\nprint(5 &lt;= 5)      # True\nYou can also compare strings:\nprint(\"apple\" == \"apple\")    # True\nprint(\"Apple\" == \"apple\")    # False (case matters!)\nprint(\"cat\" &lt; \"dog\")         # True (compares alphabetically)\n\n\n\n\n\n\nWarningCommon Pitfalls\n\n\n\nDon’t confuse = and ==:\n\n= is the assignment operator (used to assign a value to a variable)\n== is the comparison operator (used to check if two values are equal)\n\nx = 5           # assignment\nprint(x == 5)   # comparison → True\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneWhich Pizza is the Better Deal?\n\n\n\n\n\nLet’s build on a problem you saw earlier. This time, we’ll:\n\nuse variables to store the cost per square inch of two pizzas and\nthen use a comparison operator to see which one is the better deal.\n\nThe Setup\n\nA 12-inch pizza costs $8\nA 15-inch pizza costs $12\nUse the formula for the area of a circle:\n\n\\(A = \\pi \\times r^2\\)\nUse 3.14159 for π\n\n\nYour Task\n\nCompute the cost per square inch for each pizza\nStore the results in two variables: small_pizza and large_pizza\nUse a comparison operator to check if the smaller pizza is a better or equal deal\n\nHere’s a starting point:\n# Calculate cost per square inch for each pizza\nsmall_pizza = 8 / (3.14159 * (12 / 2) ** 2)\nlarge_pizza = 12 / (3.14159 * (15 / 2) ** 2)\n\n# Compare them (insert proper comparison operator in the blanks)\nprint(small_pizza __ large_pizza)\nWhat does the output of the comparison tell you? Try printing both values first to see how they compare. Which pizza gives you more for your money?\nprint(small_pizza)\nprint(large_pizza)",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#putting-it-all-together-basic-python-in-action",
    "href": "03-python-basics.html#putting-it-all-together-basic-python-in-action",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.5 Putting It All Together: Basic Python in Action",
    "text": "3.5 Putting It All Together: Basic Python in Action\nNow that you’ve learned about Python’s core data types, how to assign values to variables, and how to make comparisons, let’s put it all together into a small real-world example.\nImagine you’re helping manage event registration for a student club. You want to:\n\nStore the number of attendees and the cost per ticket\nCalculate total revenue from the event\nSet a goal for how much you wanted to make\nPrint a basic summary report\nUse comparison logic to see if you met your goal\n\nHere’s how you might write that in Python:\n\n# Number of attendees and ticket price\nattendees = 48\nticket_price = 12.50\n\n# Calculate total revenue\ntotal_revenue = attendees * ticket_price\n\n# Set a revenue goal\nrevenue_goal = 600\n\n# Print a summary message\nprint(\"You sold \" + str(attendees) + \" tickets at $\" + str(ticket_price) + \" each.\")\nprint(\"Total revenue: $\" + str(total_revenue))\n\n# Compare to revenue goal\ngoal_met = total_revenue &gt;= revenue_goal\nprint(\"Did we meet our revenue goal: \" + str(goal_met))\n\nYou sold 48 tickets at $12.5 each.\nTotal revenue: $600.0\nDid we meet our revenue goal: True\n\n\n\nWhat’s going on here?\nIn these 8 lines of code we’ve combined everything we learned across this chapter:\n\nWe used variables to store numbers and reused them in calculations\nWe performed basic math operations using multiplication\nWe used print() to display helpful messages, combining strings and numeric values\nWe used str() to convert numeric and boolean data types to strings\nWe used a comparison operator (&gt;=) to return True or False based on whether our total revenue met the goal\n\n\n\nTry it Yourself!\n\nChange the number of attendees or the ticket price. What happens to total revenue?\nTry changing the revenue goal and see if the result of the comparison changes.\nStretch: Use a GenAI tool like ChatGPT, Claude, or Copilot and ask it to expand upon your code so that it prints ‘Yaaah!’ if we met the revenue goal, or ‘Booo!’ if we didn’t. Then copy the AI’s suggestion into your notebook and test it out. Can you understand what it did? Does the code work the way you expected? If not—can you fix it?\n\n\n\n\n\n\n\nRemember: GenAI tools are great helpers, but not always correct. Always try to understand the code they give you!\n\n\n\nThis is your first step toward building programs that do real work. It may not seem fancy now—but you’ve already written a script that stores, processes, and evaluates real-world data.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#summary-and-whats-next",
    "href": "03-python-basics.html#summary-and-whats-next",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.6 Summary and What’s Next",
    "text": "3.6 Summary and What’s Next\nIn this chapter, you learned how to:\n\nIdentify and use Python’s most common data types—numbers, strings, and booleans\nUse the assignment operator (=) to store and reuse values with variables\nWrite and evaluate comparison expressions that return True or False\nUse print statements to combine and display information\nStart thinking like a programmer by working through simple real-world examples\n\nThese are the essential tools that will support everything you do moving forward—whether you’re analyzing a spreadsheet, building a model, or writing a script to automate a task.\n\nWhat’s Next: From Basics to Real Data\nNow that you’ve learned how to work with individual values and variables, it’s time to start thinking bigger—about how we structure and analyze real-world data. In the next module, we’ll go deeper into three key topics:\n\nJupyter Notebooks: You’ve already seen Jupyter Notebooks in action, but now we’ll explore just how powerful they are. Mastering Jupyter is about more than writing code—it’s about communicating insights clearly. You’ll learn how to:\n\nCombine text and code in the same document using Markdown\nFormat your notebook with headers, bullets, and even equations to make your work easier to understand\nStructure your notebooks as professional, reproducible reports, just like a real data scientist would\n\nPython Data Structures: So far, you’ve worked with individual values like one number or one string. But in data science, we almost never work with just one thing—we work with collections of data. Understanding data structures is essential as we start to clean, transform, and analyze datasets so we’ll cover:\n\nStoring and accessing data using lists (ordered sequences of items)\nOrganizing key-value pairs using dictionaries (think of them like labeled data bins)\nLooping through and manipulate these collections efficiently\n\nImporting Real-World Datasets with Pandas: You’ll also take your first step into real data science work—bringing in external datasets and exploring them with the Pandas library. Pandas is one of the most important tools in any data scientist’s toolbox. It makes it easy to:\n\nRead data from CSV files, Excel, databases, and more\nView, clean, and filter your data\nBegin asking real questions and discovering patterns\n\n\nYou’ve built a strong foundation. Next, we’ll build on it and start working with data the way real analysts and scientists do. Let’s keep going!",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#exercise-build-a-simple-event-summary",
    "href": "03-python-basics.html#exercise-build-a-simple-event-summary",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.7 Exercise: Build a Simple Event Summary",
    "text": "3.7 Exercise: Build a Simple Event Summary\n\n\n\n\n\n\nNoneThe Scenario\n\n\n\n\n\nYour student club is hosting an event and you’re in charge of summarizing registration data. Use what you’ve learned in this chapter to answer the following questions using Python.\nWrite all your code from scratch—no copy-pasting. Try to reason through the logic before typing.\n\nTickets sold: 56\nTicket price: $10.50\nRevenue goal: $600\nEvent name: “Python for Everyone”\n\n\n\n\n\n\n\n\n\n\nNoneYour Tasks\n\n\n\n\n\n\nCreate Variables: Assign appropriate values to variables for:\n\nEvent name\nTickets sold\nTicket price\nRevenue goal\n\nCalculate Total Revenue: Use math operations to calculate the total revenue earned from ticket sales.\nPrint a Summary Report: Use print() and string concatenation to display a message like:\nThe event \"Python for Everyone\" sold 56 tickets at $10.50 each.\nTotal revenue: $588.0\nMet or exceeded goal: False\nStretch Task (Optional): Add a comparison that checks whether your total revenue met or exceeded the revenue goal and prints:\n\n\"Yaaah! We met our goal!\" if the goal was met\n\"Booo! We missed our goal.\" if it was not\n\nHint: Ask a GenAI tool to help you construct the logic! If you can’t get it, don’t worry as we will talk about this later in the course.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html",
    "href": "04-jupyter.html",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "",
    "text": "4.1 Benefits:\nIn data science, we don’t just write code — we tell stories with data. Jupyter notebooks are an essential tool because they allow us to blend code, explanatory text, and visualizations in a single document. This makes it easier to understand, share, and reflect on your work.\nHere are a few key reasons why Jupyter notebooks are so widely used and appreciated in the data science world:",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#benefits",
    "href": "04-jupyter.html#benefits",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "",
    "text": "Blending code and context: Jupyter notebooks allow you to seamlessly combine executable Python code with rich text elements using Markdown. This means you can explain your thought process, document your workflow, and display results all in one place—making your analysis easier to follow and reproduce.\nExploratory-friendly: Notebooks are designed for experimentation. You can write and run code in small, manageable chunks (cells), see immediate feedback, and iteratively refine your approach. This makes it easy to test ideas, debug, and learn as you go.\nSharable and visual: Notebooks support inline visualizations, tables, and formatted text, making your work more engaging and accessible. You can easily export notebooks as HTML or PDF files to share with others, ensuring your analysis is both readable and reproducible.\n\n\n\n\n\n\n\nThink of a notebook as your digital lab notebook. Instead of jumping between different tools to write your analysis, calculate results, and explain your thinking, a Jupyter notebook lets you do it all in one place—cleanly combining code, commentary, and visuals in a single, easy-to-follow narrative.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#notebook-anatomy-and-core-features",
    "href": "04-jupyter.html#notebook-anatomy-and-core-features",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.2 Notebook Anatomy and Core Features",
    "text": "4.2 Notebook Anatomy and Core Features\n\n\n\n\n\n\nNote📓 Follow Along in Colab!\n\n\n\nAs you read through this chapter, we encourage you to follow along using the companion notebook in Google Colab (or other editor of choice). This interactive notebook lets you run code examples covered in the chapter—and experiment with your own ideas.\n👉 Open the Jupyter Basics Notebook in Colab.\n\n\n\nCode vs Markdown Cells\nJupyter notebooks are made up of cells, and there are two main types you’ll use regularly: code cells and Markdown cells.\n\nCode cells are where you write and run Python code. When executed, these cells display output directly beneath the cell—whether it’s a simple calculation, a table, or a visualization.\nMarkdown cells allow you to format text to explain your work, provide instructions, insert images, or even write mathematical formulas. These cells support formatting tools like headers, bullet points, bold/italic text, and LaTeX for equations.\n\nBy combining these two types of cells, you can create a clear, readable narrative that both documents your process and shows your results.\n\n\n\n\n\n\nWatch from 5:20-8:45 in this video to see simple examples of code cells and Markdown cells.\n\n\n\n\nTo select the type of a cell (Code or Markdown), use the dropdown menu typically found at the top of the notebook interface. In Jupyter Lab or classic Jupyter Notebook, it will say ‘Code’ or ‘Markdown’—click it to change. In VS Code, you can right-click a cell and choose the type, or use shortcuts like Cmd+M M (Markdown) and Cmd+M Y (Code) on Mac (Ctrl+M equivalents on Windows).\nTo run a cell: Shift + Enter\n\nKnowledge check\n\n\n\n\n\n\nNoneTry It!\n\n\n\n\nCreate and execute a Markdown cell and write the following: “Today I’m learning about Jupyter notebooks!”\nBelow it, create and execute a Code cell that:\n\nDefines a variable, like x = 5\nPrints a message that includes the variable (e.g., print(f'The value of x is {x}'))\n\n\n\n\n\n\n\nMarkdown Basics\nThere’s a lot you can do with Markdown to make your notebooks clearer and more engaging. You can add headings, lists, formatting, and even mathematical expressions to help document your analysis and guide readers through your work.\nHere are a few basics to get you started but check out the cheat sheet below to see more:\n\nHeaders: Use #, ##, ### to create headings of different sizes\nBold and Italic: **bold**, *italic*\nLists: - item for bullet points or 1. item for numbered lists\nEquations: Use LaTeX syntax between dollar signs, like $a^2 + b^2 = c^2$\n\n\n\n\nMarkdown Cheat Sheet (Datacamp)\n\n\nWant to go deeper? You can explore more Markdown syntax in these helpful guides:\n\nJupyter Docs on Markdown: A quick and easy introduction to get you started with basic Markdown syntax.\nMarkdown Guide: A more detailed reference for adding more structure and polish to your notebook explanations.\nColab Markdown Guide notebook: An example Colab notebook that allows you to experiment directly with Markdown syntax - a great playground for practicing different Markdown elements in a live notebook environment. \n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry It!\n\n\n\nBuild on the previous activity by enhancing your notebook with more Markdown elements:\n\nAt the very top of your notebook, add a title using a level-1 header (#).\nAdd a second-level header (##) introducing a new section.\nUnder that header, create a bulleted list—for example, a grocery shopping list.\n(Stretch Goal) Try writing a simple equation in Markdown, such as the formula for the area of a circle: $A = \\pi r^2$.\n\nThis will help you practice using Markdown to better organize and explain your work.\n\n\n\n\n\nCode Execution & State\nOne of the key features of Jupyter notebooks is that they allow you to execute code one cell at a time. This supports experimentation, but also introduces some complexity.\n\nCode is executed in the order you run the cells, not necessarily from top to bottom. This means you can define a variable in one cell and use it later—even if that later cell is above the original one. While this can be convenient, it can also lead to confusion or bugs if you’re not careful.\n\n\n\n\n\n\n\nCautionExample:\n\n\n\n\nIn one code cell, type: x = 42\nNow scroll up and create a new code cell above that one.\nIn that new cell, type: print(x) and run it.\n\nIt will work—because the variable x was already defined in memory. But if someone runs the notebook from top to bottom, it will break. This illustrates why executing cells out of order can lead to unpredictable behavior.\n\n\n\nThe notebook maintains a running memory called the kernel. As long as the kernel is active, all the variables and functions you’ve defined persist in memory. This makes it easy to build on previous work, but if you make a mistake or want a clean slate, you may need to restart the kernel.\n\nA good habit is to periodically restart the kernel and run all cells in order to ensure your notebook works as expected from top to bottom.\n\n\n\nManaging Your Notebook\nWorking in Jupyter notebooks involves managing both your work and the underlying system that runs it. Here are a few key features and best practices to help keep things running smoothly:\n\nSave often: Notebooks autosave regularly, especially when working in a browser-based environment like Jupyter Lab or Colab. However, it’s still a good habit to manually save using Ctrl+S or Cmd+S, especially before running all cells or restarting the kernel.\nRestarting the Kernel: If your notebook starts behaving unpredictably—due to memory overload, undefined variables, or bugs—it’s a good idea to restart the kernel. Restarting wipes the slate clean by clearing all variables from memory, allowing you to re-run the notebook from the top.\nClear Outputs: Before sharing your notebook with others or submitting it for review, consider clearing all outputs (cell results, print statements, plots, etc.). This makes your notebook easier to read and ensures the reader sees only the final results when they run it themselves. You can usually do this via the Kernel or Edit menu.\n\n\n\n\n\n\n\nAgain, can’t highlight this enough, a good practice is to “Restart and Run All” to make sure your notebook runs cleanly from start to finish without relying on any hidden state or cell order.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#when-not-to-use-notebooks",
    "href": "04-jupyter.html#when-not-to-use-notebooks",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.3 When Not to Use Notebooks",
    "text": "4.3 When Not to Use Notebooks\nNotebooks are powerful and flexible, but they aren’t the best fit for every type of task—especially as your work becomes more complex or transitions into production environments.\n\nLimitations:\n\nHard to modularize and test: Notebooks encourage an exploratory, linear style of development. But when you need reusable functions, testable components, or well-structured projects, notebooks can become unwieldy.\nPoor version control: While tools like Git work well with text files, notebooks save code and output together in JSON format. This makes it difficult to track changes or resolve merge conflicts. This is improving but is still less than stellar.\nExecution order issues: Since notebooks allow you to run cells out of order, it’s easy to accidentally create dependencies that break when someone else (or you later) tries to run the notebook top to bottom.\n\n\n\nWhen Notebooks Shine\nJupyter notebooks truly excel in scenarios where exploration, explanation, and communication are key. Here are a few situations where notebooks are particularly well-suited:\n\nLearning and teaching: The ability to mix code, narrative, and output in one place makes notebooks an ideal format for both instruction and self-study.\nExploratory data analysis (EDA): Notebooks allow you to quickly test hypotheses, visualize data, and keep track of your insights along the way.\nPrototyping and trying out ideas: Need to test a small function or compare two approaches? Notebooks provide a fast and flexible way to experiment.\nCreating interactive reports and visualizations: With support for charts, tables, and even interactive widgets, notebooks are a powerful medium for communicating data-driven stories.\n\n\n\nWhen to Use Something Else\nWhile notebooks are fantastic for interactive work, they aren’t ideal when software engineering discipline and scalability are required. Here are scenarios where other tools—like .py scripts, IDEs, or structured repositories—are often a better choice:\n\nBuilding and deploying production applications: Applications that need to be deployed on servers or integrated with other systems require a more modular and structured codebase than notebooks typically provide.\nCreating libraries, APIs, or packages: These projects need reusable components, proper documentation, and automated testing, which are easier to maintain outside of notebooks.\nCollaborating with others using complex Git workflows: Notebooks don’t lend themselves well to detailed version control, especially with branching and merging.\nDeveloping software that requires extensive testing or CI/CD pipelines: Production-grade software often relies on unit testing, linting, and automated builds, which are best handled in traditional .py environments.\n\n\n\n\n\n\n\nIf you don’t yet know what all the bullet points above mean — like Git, APIs, CI/CD, or modular code — that’s completely fine. You’ll encounter these concepts as you progress in your data science journey. For now, just understand that as projects get more complex and move closer to production, notebooks often aren’t the best tool for the job.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#using-.py-files-in-notebooks",
    "href": "04-jupyter.html#using-.py-files-in-notebooks",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.4 Using .py Files in Notebooks",
    "text": "4.4 Using .py Files in Notebooks\nMixing .py scripts with notebooks is common in real-world workflows, especially as your projects grow in complexity or require reusable functions. Instead of writing everything directly in a notebook, you can keep your reusable logic—like data cleaning functions, utility tools, or modeling pipelines—in a .py script and then import or run that script from within your notebook.\nThis approach offers several benefits: you avoid cluttering your notebook with repeated code, you can test pieces of logic more easily, and your codebase becomes easier to maintain and share.\n\nMethods:\nThere are a few different ways to work with .py files inside your notebook, each useful in different situations:\n\n%run your_script.py – This runs the entire Python script as if you had pasted the code into the notebook. Any variables, functions, or classes defined in that script will be available in your notebook’s environment after execution.\n%load your_script.py – This loads the contents of the script into a new notebook cell so you can review or edit it before running. It’s a nice way to examine the code without opening another file.\nimport your_script – This is the standard Python approach for using modules. If your script is in the same directory as your notebook, you can import functions or variables from it using this command. For example, if your script has a function called clean_data(), you could use it in your notebook with your_script.clean_data(). Don’t worry if this is new—we’ll cover imports and module structure later in the class.\n\n\n\n\n\n\n\nNoneExample\n\n\n\n\n\nSee an example of running a .py script with %run your_script.py in this video:\n\n\n\n\n\n\nWhy use .py files?\nOrganizing your code into .py files offers several advantages, especially as your projects grow beyond a single notebook:\n\nReusability: Functions or logic stored in a .py file can be reused across multiple notebooks or scripts without duplication.\nOrganization: Separating logic from analysis helps keep your notebooks clean and focused on storytelling, while your .py files house the supporting code.\nVersion control: Unlike notebooks, .py files are plain text and work much better with Git and other version control tools—making it easier to track changes, review code, and collaborate with others.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry It!\n\n\n\n\nOpen your editor of choice (such as Colab, JupyterLab, or VS Code).\nCreate a new .py file and type the following line into it:\nprint(\"Hello .py scripts!\")\nSave this file as my_first_script.py.\nNow open a new Jupyter notebook.\nIn a code cell in the notebook, type and run:\n%run my_first_script.py\n\nYou should see the message “Hello .py scripts!” printed in the notebook output.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#best-practices-and-pro-tips",
    "href": "04-jupyter.html#best-practices-and-pro-tips",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.5 Best Practices and Pro Tips",
    "text": "4.5 Best Practices and Pro Tips\nFollowing a few best practices can make your Jupyter notebooks more readable, reliable, and professional—both for yourself and others who use your work.\n\nName files clearly: Use descriptive and consistent naming conventions for your notebooks. For example, eda_customer_data.ipynb is much more informative than untitled3.ipynb.\nUse Markdown generously: Narrate your thought process. Include headers, bullet points, and equations to guide the reader through your analysis. Also, take time to format your writing as you would for any professional report—pay attention to grammar, clarity, and structure to ensure your notebook is polished and easy to follow.\nRestart and clear output before sharing: This ensures the notebook runs top-to-bottom and that readers aren’t confused by leftover output or state.\nExport as HTML or PDF: If you’re turning in a notebook or sharing it for review, exporting to a static format can ensure others see exactly what you intended.\n\n\n\n\n\n\n\nTipPro Tip\n\n\n\nStructure your notebook like a story—introduce the goal, describe your approach, show the results, and summarize your findings. Your future self will thank you!\nHere’s a great article that walks through best practices to help you make this a reality.\n\n\n\nWant to Learn More?\nHere are a few helpful references that expand on Jupyter notebook best practices:\n\nProject Jupyter Guidelines\nReal Python – Jupyter Notebook Tips, Tricks, and Shortcuts",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#summary-and-whats-next",
    "href": "04-jupyter.html#summary-and-whats-next",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.6 6. Summary and What’s Next",
    "text": "4.6 6. Summary and What’s Next\nLet’s recap what you’ve learned in this chapter:\n\nJupyter notebooks are an essential tool for blending code, narrative, and output in one place.\nYou now know how to use different cell types, write Markdown, and execute code.\nYou’ve seen how notebooks maintain memory with the kernel and why it’s important to restart and re-run code in order.\nWe covered notebook limitations and when it’s better to switch to .py scripts or more robust development tools.\nYou explored how to organize your code using .py files and how to run them inside your notebooks.\nFinally, you picked up several best practices for writing professional and readable notebooks.\n\n\nNext Up:\nIn the next chapter, we’ll dive into Python’s basic data structures—like lists and dictionaries. These tools are essential for organizing and manipulating data efficiently, and they will serve as the foundation for the rest of your data analysis journey.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#exercise-comparing-movie-theater-snacks",
    "href": "04-jupyter.html#exercise-comparing-movie-theater-snacks",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.7 Exercise: Comparing Movie Theater Snacks",
    "text": "4.7 Exercise: Comparing Movie Theater Snacks\n\n\n\n\n\n\nNoneScenario\n\n\n\n\n\nYou’re deciding between three snack combos at the movie theater. Each combo includes popcorn and a drink. Your task is to figure out which option gives you the most food value for your money, based on calories per dollar.\nCombo Details:\n\nCombo 1: $7.50 and 500 calories\nCombo 2: $9.00 and 700 calories\nCombo 3: $10.50 and 900 calories\n\n\n\n\n\n\n\n\n\n\nNoneCreate Notebook\n\n\n\n\n\nCreate a new Jupyter notebook and name it something meaningful and relevant for this analysis.\n\n\n\n\n\n\n\n\n\nNoneExecute Code\n\n\n\n\n\n\nUse a code cell to define the following variables for each combo:\n\ncombo_1_price = 7.50\ncombo_1_calories = 500\ncombo_2_price = 9.00\ncombo_2_calories = 700\ncombo_3_price = 10.50\ncombo_3_calories = 900\n\nUse another code cell to calculate the calories per dollar for each combo:\ncombo_1_value = combo_1_calories / combo_1_price\nUse a final code cell to print the results for each combo.\n\n\n\n\n\n\n\n\n\n\nNoneDocument Your Notebook\n\n\n\n\n\nUse Markdown cells to:\n\nGive your notebook a proper title and write a short introduction explaining the goal of the analysis.\nDisplay the formula used: \\(Value = \\frac{Calories}{Price}\\)\nReport your findings: Which combo offers the best value, and how did you determine this?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html",
    "href": "05-data-structures.html",
    "title": "5  Introduction to Data Structures",
    "section": "",
    "text": "5.1 Why Data Structures?\nIn the real world, we rarely work with individual pieces of data. Whether we’re analyzing a dataset, building a model, or writing a simulation, we typically work with collections of data. Python provides several built-in ways to store and organize these collections. These tools are called data structures.\nIn this chapter, you will learn how to:\nImagine you want to store the test scores of a student across multiple exams. Instead of creating separate variables for each score, it makes more sense to group them together:\n# instead of this\ngrade_1 = 85\ngrade_2 = 90\ngrade_3 = 88\ngrade_4 = 92\n\n# more convenient to group them together like this\ngrades = [85, 90, 88, 92]\nThis approach allows you to access, modify, and analyze data more efficiently. That’s the power of data structures: they help us organize and operate on collections of data.\nPython includes different types of data structures that allow us to organize data with specific characteristics suited to the task at hand. For example, some data structures preserve the order of elements, while others do not. Some allow us to modify their contents (mutable), and others do not (immutable). Still others let us associate labels or keys with each item, such as names linked to phone numbers. Choosing the right data structure depends on how we need to access and manage the data.\nBuilding on this, each structure in Python has tradeoffs that make it more suitable for specific situations. If you need to keep track of items in a particular order and expect the data to change, a list might be ideal. If you want to store a fixed grouping of values like GPS coordinates, a tuple ensures the data remains unchanged. And when you need to link one piece of information to another—like a person’s name to their phone number—a dictionary’s key-value format is the perfect tool. Understanding these attributes helps you pick the right structure for the task and write more efficient, readable code.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#why-data-structures",
    "href": "05-data-structures.html#why-data-structures",
    "title": "5  Introduction to Data Structures",
    "section": "",
    "text": "Think about a recent experience filling out an online form.\n\nWhat types of data did you need to input?\nDo you think this data should be stored in a structure that allows for changes (mutability)?\nDoes the order of the data matter for how it’s used or displayed? Were there any key-value pairs—like a name linked to an email address—that need to be stored and accessed together?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#lists-ordered-and-mutable",
    "href": "05-data-structures.html#lists-ordered-and-mutable",
    "title": "5  Introduction to Data Structures",
    "section": "5.2 Lists: Ordered and Mutable",
    "text": "5.2 Lists: Ordered and Mutable\n\nWhat is a List?\nA list is an ordered, changeable collection of items. You can add, remove, or modify elements in a list. Lists are among the most commonly used data structures in Python due to their flexibility and ease of use. When you need to keep items in a specific order—like daily stock prices, game scores, or survey responses—a list is often the right choice.\nLists are mutable, meaning you can change their contents after they’re created. This makes them ideal for scenarios where your data updates frequently, such as tracking website visits or logging sensor readings.\n\n\nCreating Lists\nTo create a list, use square brackets [] and separate items with commas. Lists can hold values of any type: numbers, strings, Booleans, or even other lists. Here are three simple examples:\n\nfruits = ['apple', 'banana', 'cherry']\nscores = [85, 90, 88, 92]\nmixed = ['blue', 42, True, 3.14]\n\nThe order in which you define the elements is preserved. We can see this when we evaluate a list - the output will always be in the same order as it was input:\n\nmixed\n\n['blue', 42, True, 3.14]\n\n\nThis makes lists especially helpful for keeping track of sequences where position matters, such as chronological data or ranked preferences. Because lists are mutable, you can also build them incrementally as your data grows or changes during the course of a program.\n\n\nAccessing List Items\nTo access a specific item in a list, you use indexing - which uses brackets ([]) along with the specified location. So, say we want to get the first item from a list:\n\nfruits[1]\n\n'banana'\n\n\nWait a minute! Shouldn’t fruits[1] give the first item in the list? It seems to give the second. This is because indexing in Python starts at zero.\n\n\n\n\n\n\nImportantZero-based Indexing\n\n\n\nPython uses zero-based indexing, which means that the first element in a list has an index of 0, the second has an index of 1, and so on. You can also use negative indexing to count from the end of the list, where -1 is the last item, -2 is the second-to-last, etc.\n\nprint(fruits[0])   # first item\nprint(fruits[2])   # second item\nprint(fruits[-1])  # last item\n\napple\ncherry\ncherry\n\n\nFun reading: Why Python uses 0-based indexing\n\n\nUnderstanding indexing is essential because it affects how you loop through lists, retrieve elements, and manipulate values. For example, if you mistakenly assume that indexing starts at 1, you might accidentally skip or mislabel elements in your data, or you may specify an invalid location which will raise an error like the following:\n\nfruits[4]\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 fruits[4]\n\nIndexError: list index out of range\n\n\n\n\n\nModifying Lists\nOnce a list is created, you can change its contents easily. This mutability makes lists incredibly useful for situations where your data evolves over time. You can add new items using methods like .append() or change existing elements by assigning new values using their index.\n\nfruits.append('orange')\nfruits[1] = 'blueberry'\n\nfruits\n\n['apple', 'blueberry', 'cherry', 'orange']\n\n\nThese operations reflect how dynamic lists can be—whether you’re updating a list of customer names, recording daily measurements, or tracking items in a to-do list, lists let you adjust your data in place without rebuilding the structure from scratch.\n\n\nCommon List Operations\nLists come with many built-in functions and methods that make it easy to analyze and manipulate data. These operations can help you answer questions like: How many elements are in the list? Does it contain a specific item? Can I sort or remove items from it?\n\nlen(fruits)            # 4 (returns the number of elements in the list)\n'apple' in fruits      # True (checks if 'apple' is in the list)\nfruits.remove('apple') # removes the first occurrence of 'apple'\nfruits.sort()          # sorts the list in place (alphabetically or numerically)\nfruits.pop()           # removes and returns the last item\n\nThese methods are especially useful in data wrangling tasks, such as filtering survey responses, cleaning up logs, or ranking scores.\n\n\n\n\n\n\nYou can find many other list operations here: https://docs.python.org/3/tutorial/datastructures.html#data-structures\n\n\n\n\n\nUse Cases\n\nTime series data\nCollections of records (e.g., names, prices, grades)\nMaintaining a dynamic list of inputs that might grow or shrink over time\nStoring results from computations or simulations\nCollecting user inputs or responses in interactive programs\nAnd others!\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry It!\n\n\n\nSay the last 5 days had daily high temperatures of 79, 83, 81, 89, 78.\n\nStore these values in a list.\nNow add a new item to this list that represents today’s high temp of 85.\nNext, suppose the first day’s reading was inaccurate and the actual high temp was 76 — update that value in the list.\nFinally, sort the list to see the temperatures in ascending order.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#tuples-ordered-and-immutable",
    "href": "05-data-structures.html#tuples-ordered-and-immutable",
    "title": "5  Introduction to Data Structures",
    "section": "5.3 Tuples: Ordered and Immutable",
    "text": "5.3 Tuples: Ordered and Immutable\n\nWhat is a Tuple?\nA tuple is similar to a list in that it is an ordered collection of items. However, unlike lists, tuples are immutable, meaning their contents cannot be changed after creation. This immutability makes tuples useful for representing fixed sets of data that should not be altered during a program’s execution.\n\n\n\n\n\n\nThink of tuples as a read-only list. This may be contentious, as described in this blog post. Tuples do have many other capabilities beyond what you would expect from just being “a read-only list,” but for us just beginning now, we can think of it that way.\n\n\n\nTuples are typically used when you want to ensure data integrity, such as storing constant values or configuration settings. They are also commonly used to return multiple values from a function, or to represent simple groupings like coordinates, RGB color values, or database records.\nBecause of their immutability, tuples can be used as keys in dictionaries (we’ll learn what these are shortly), unlike lists. Additionally, they offer slightly better performance than lists when it comes to iteration.\n\n\nCreating Tuples\nTo create a tuple, use parentheses () and separate values with commas. Like lists, tuples can hold values of different data types. However, because tuples are immutable, they are particularly well-suited for storing data that should remain constant throughout your program.\nFor example, if you’re working with geographic coordinates, a tuple ensures the latitude and longitude values stay paired and unchanged:\n\ncoordinates = (39.76, -84.19)\n\nOr you might use a tuple to store a birthdate, where the structure will never need to be modified:\n\nbirthday = (7, 14, 1998)\n\nTuples are also frequently used when functions need to return multiple values, making them both practical and efficient in everyday programming. We’ll see this in action in later sections.\n\n\nAccessing Tuple Items\nIndexing tuples works exactly the same way as indexing lists in Python. You use square brackets [] with a zero-based index to access elements. For example:\n\ncoordinates[0]\n\n39.76\n\n\nJust like lists, you can use negative indices to access elements from the end:\n\nbirthday[-1]\n\n1998\n\n\n\n\n\n\n\n\nWhile both lists and tuples support indexing, remember that tuples are immutable. This means you can read elements by index, but you cannot change them:\n\ncoordinates[0] = 41.62\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 coordinates[0] = 41.62\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\n\n\n\nTuple Unpacking\nTuple unpacking allows you to assign each item in a tuple to its own variable in a single line. This is especially useful when a function returns multiple values or when you’re working with grouped data like coordinates, dimensions, or ranges. It improves readability and simplifies your code when working with known-length tuples.\n\nx, y = coordinates\n\n\nprint(x)\n\n39.76\n\n\n\nprint(y)\n\n-84.19\n\n\nHere, the value of x will be 39.76 and y will be -84.19, corresponding to the first and second elements of the coordinates tuple, respectively. If you try to unpack a tuple into a different number of variables than it has elements, Python will raise an error.\n\n\nWhy Use Tuples?\nTuples are ideal when you want to group together related values that should not be changed once set. Their immutability provides a built-in safeguard against accidental modifications, which is especially helpful when the data must remain consistent throughout the execution of a program. Because they are more lightweight than lists, tuples also offer faster performance in scenarios that involve iteration or large numbers of data groupings. Furthermore, since tuples can be used as keys in dictionaries (unlike lists), they provide a reliable way to map compound keys to values in advanced data structures.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry It!\n\n\n\nGiven the following tuple schooling = ('UC', 'BANA', '4080')\n\nUse indexing to grab the word “BANA”.\nChange the value of “BANA” to “Business Analytics”. What happens?\nUnpack the schooling tuple into three variables: university, program, class_id.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#dictionaries-key-value-pairs",
    "href": "05-data-structures.html#dictionaries-key-value-pairs",
    "title": "5  Introduction to Data Structures",
    "section": "5.4 Dictionaries: Key-Value Pairs",
    "text": "5.4 Dictionaries: Key-Value Pairs\nA dictionary is a collection of key-value pairs, where each unique key maps to a specific value. This structure allows you to organize and retrieve data using meaningful identifiers rather than relying on position, as with lists or tuples. Dictionaries are especially helpful when you need to store data that has a clear label or attribute—such as a student’s name, ID, or grade—making it easy to access or update values using their corresponding keys. This makes dictionaries one of the most powerful and flexible tools for working with structured data in Python.\n\nCreating Dictionaries\nTo create a dictionary, use curly braces {} with key-value pairs separated by colons. Each key must be unique and is typically a string, though it can also be a number or other immutable type. Dictionaries are ideal when you want to store data that has meaningful labels. For instance, instead of remembering that index 0 in a list corresponds to a name and index 1 corresponds to a score, a dictionary lets you associate 'name' directly with a value.\n\nstudent = {\n    'name': 'Jordan',\n    'score': 95,\n    'major': 'Data Science'\n}\n\n\n\nAccessing and Modifying Dictionaries\nTo access a value in a dictionary, you reference its key in square brackets. This allows you to retrieve values without knowing their position, unlike lists or tuples.\n\n# access the value associated with the 'score' key\nstudent['score']\n\n95\n\n\nYou can also update the value of an existing key or add a completely new key-value pair to the dictionary. This mutability makes dictionaries ideal for dynamic data, such as user profiles, configuration settings, or database records.\n\nstudent['score'] = 98  # update the score\nstudent['grad_year'] = 2025  # add a new key-value pair\n\nstudent\n\n{'name': 'Jordan', 'score': 98, 'major': 'Data Science', 'grad_year': 2025}\n\n\n\n\nDictionary Methods\nDictionaries come with several built-in methods that allow you to efficiently access and interact with their contents. These methods help you retrieve just the keys, just the values, or both together. They are particularly useful when you’re iterating over a dictionary to analyze or transform its contents.\nTry the following operations and see what their outputs are:\nstudent.keys()         # dict_keys(['name', 'score', 'major', 'grad_year'])\nstudent.values()       # dict_values(['Jordan', 98, 'Data Science', 2025])\nstudent.items()        # dict_items([('name', 'Jordan'), ('score', 98), ...])\n'name' in student      # True (checks if a key exists in the dictionary)\ndel student['major']   # removes the 'major' key and its associated value\nUsing these methods allows you to build flexible programs that can dynamically explore or modify structured data—especially when working with JSON data from APIs, reading metadata from files, or updating attributes of users or records.\n\n\nUse Cases\nDictionaries are incredibly versatile and can be found in many practical programming and data science scenarios:\n\nLookup tables: Use dictionaries to map inputs to outputs, such as converting state abbreviations to full names or translating category codes to descriptions.\nStructured data: Store rows of data as dictionaries where keys represent column names—this mirrors how data is stored in JSON format and is common when working with web APIs or data from files.\nFeature storage in machine learning models: Organize features (like ‘age’, ‘income’, or ‘region’) with their corresponding values for each individual observation, allowing for dynamic construction and retrieval of input data for models.\nConfiguration settings: Store user preferences or system parameters that can be easily accessed or updated using descriptive keys.\nData merging and deduplication: Use keys to uniquely identify records, making it easier to combine and clean data from different sources.\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry It!\n\n\n\n\nCreate a dictionary that stores a classmate’s nickname, phone number, and age.\njohn_doe = {\n    'nickname': _______,\n    'phone_number': __,\n    'age': __\n}\nCreate another dictionary called classmate2 with similar information for a different classmate.\nCombine these two dictionaries into a new dictionary called contacts, where each key is the classmate’s name and the value is their corresponding dictionary.\nAdd a new entry to the contacts dictionary for a third classmate, including their name, phone number, and age.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#summary",
    "href": "05-data-structures.html#summary",
    "title": "5  Introduction to Data Structures",
    "section": "5.5 Summary",
    "text": "5.5 Summary\nIn this chapter, we focused on three of Python’s most commonly used data structures: lists, tuples, and dictionaries. These structures provide the foundation for how data is organized and accessed in most Python programs, especially in data science workflows.\n\nLists are ordered and mutable, making them ideal when you need to maintain and modify a sequence of items.\nTuples are ordered but immutable, which is helpful for fixed sets of values where data integrity is important.\nDictionaries store labeled data as key-value pairs, offering a flexible way to organize and retrieve data by meaningful identifiers.\n\n\n\n\n\n\n\n\n\n\nData Structure\nType (via type())\nDescription & When to Use\nExample\n\n\n\n\nList\nlist\nOrdered, mutable collection. Use when you need to keep items in sequence and change them later.\nscores = [85, 90, 88, 92]\n\n\nTuple\ntuple\nOrdered, immutable collection. Ideal for fixed groupings like coordinates or dates.\ncoordinates = (39.76, -84.19)\n\n\nDictionary\ndict\nUnordered, mutable key-value pairs. Great for labeled data or fast lookups.\nstudent = {'name': 'Jordan', 'score': 95}\n\n\n\nAs we move forward in this book, we’ll explore more advanced data structures that build on these basics and help you perform data mining tasks more efficiently. For now, having a solid understanding of these core structures will serve as a crucial building block for your continued work in Python and data science.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#exercise-student-records-management",
    "href": "05-data-structures.html#exercise-student-records-management",
    "title": "5  Introduction to Data Structures",
    "section": "5.6 Exercise: Student Records Management",
    "text": "5.6 Exercise: Student Records Management\nYou’ve been asked to build a simple data tracking system for a small classroom.\n\n\n\n\n\n\nNoneCreate a List\n\n\n\n\n\n\nMake a list called student_names that includes the names of 5 students.\nAdd a new student to the list.\nRemove the second student in the list.\nSort the list alphabetically and print the final list.\n\n\n\n\n\n\n\n\n\n\nNoneUse a Tuple\n\n\n\n\n\n\nCreate a tuple named classroom_location that stores the building name and room number, such as (\"Lindner Hall\", 315).\nUnpack the tuple into two variables: building and room, and print each one with a label.\n\n\n\n\n\n\n\n\n\n\nNoneBuild a Dictionary\n\n\n\n\n\n\nCreate a dictionary named student_info for one of the students, including their name, major, and graduation year.\nAdd a new key for GPA with a value of your choice.\nPrint out all the keys, all the values, and the full dictionary.\n\n\n\n\n\n\n\n\n\n\nNoneReflection\n\n\n\n\n\nThink through what you just did:\n\nWhat makes a list a good choice for student_names? Is there an alternative approach you could’ve taken?\nWhy would we use a tuple for classroom_location?\nWhat makes a dictionary a good choice for student_info?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "06-libraries.html",
    "href": "06-libraries.html",
    "title": "6  Packages, Libraries, and Modules",
    "section": "",
    "text": "6.1 Why Do We Care?\nAs a data scientist, you won’t be writing every piece of code from scratch. Python’s true power comes from its ecosystem of packages, libraries, and modules that extend its core functionality. These tools allow you to analyze data, build machine learning models, visualize patterns, and automate complex tasks with just a few lines of code.\nIn this chapter, we’ll explore what packages, libraries, and modules are, how to use them, and get hands-on with both built-in and third-party tools that will support you throughout this course.\nBy the end of this chapter, you will be able to:\nYou’ve probably heard that Python is one of the most popular languages in data science—but what makes it so powerful? One big reason is that Python gives you access to an enormous number of libraries that other people have written and shared.\nA library is a reusable collection of code—functions, tools, or entire frameworks—that someone else has written to make your life easier. Think of libraries as toolkits: instead of building a hammer every time you need to drive a nail, you just grab the hammer from your toolbox.\nPython libraries can help you:\nBy using the right library, you can accomplish in just a few lines of code what might otherwise take hours of work.\nLibraries are organized into modules, which are just files that group related code together. And when a collection of modules is bundled up and made shareable, we call that a package.\nThroughout this course, you’ll learn how to use some of the most powerful and popular Python libraries for data science. But first, let’s get familiar with the different types of libraries available and how to start using them.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#why-do-we-care",
    "href": "06-libraries.html#why-do-we-care",
    "title": "6  Packages, Libraries, and Modules",
    "section": "",
    "text": "Do complex math with a single function call.\nRead and clean messy data files.\nVisualize data with beautiful plots.\nTrain machine learning models.\n\n\n\n\n\n\n\n\n\n\nNoteTerminology: Modules, Libraries, and Packages\n\n\n\n\n\nThese three terms often get used interchangeably, but they have specific meanings in Python: * Module: A single .py file that contains Python code—functions, variables, classes—that you can reuse. For example, the math module lets you do mathematical calculations.\n\nModule: A single .py file that contains Python code—functions, variables, classes—that you can reuse. For example, the math module lets you do mathematical calculations.\nLibrary: A collection of related modules bundled together. For example, pandas is a library that includes several modules for data manipulation.\nPackage: A directory containing one or more modules or libraries, with an __init__.py file that tells Python it’s a package. You can think of a package as the container that holds libraries and modules.\n\nIn practice, you’ll often hear “library” and “package” used to refer to the same thing: something you install with pip and then import to use in your code. That’s okay! At this point in your learning, understanding the subtle differences between these terms is not critical. What’s more important is knowing that Python’s modular structure allows you to mix and match tools depending on your needs and that these tools make your work much more efficient.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#standard-library-vs-third-party-libraries",
    "href": "06-libraries.html#standard-library-vs-third-party-libraries",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.2 Standard Library vs Third-Party Libraries",
    "text": "6.2 Standard Library vs Third-Party Libraries\nOne of Python’s greatest strengths is its large collection of prebuilt tools. These tools fall into two broad categories: the standard library, which comes bundled with every Python installation, and third-party libraries, which you can download and install as needed.\n\nThe Standard Library\nThe standard library is like Python’s starter toolbox. It includes modules for doing math, generating random numbers, managing dates and times, reading and writing files, and even accessing the internet. Because it’s included with Python, you can use these modules right away—no installation required.\nFor example:\n\nWant to calculate square roots? Use the math module.\nNeed to simulate randomness? Try random.\nCurious where your code is running? The os module has answers.\n\nThese are great building blocks and perfect for learning foundational programming skills.\n\n\n\n\n\n\nTip📚 Want to Learn More?\n\n\n\nYou can explore the full list of available modules in Python’s standard library by visiting the official documentation here: https://docs.python.org/3/library/index.html\n\n\n\n\nThird-Party Libraries\nAs powerful as the standard library is, it doesn’t cover everything. That’s where third-party libraries come in. These are tools developed and maintained by the Python community to solve specific problems more efficiently. To use them, you’ll typically install them using a package manager called pip.\nFor example:\n\nNeed to work with large datasets? Use pandas.\nWant to make beautiful visualizations? Try matplotlib and seaborn.\nWant fast numerical computation? You’ll love numpy.\n\nThese libraries aren’t included by default, but they’re easy to install—and essential for doing real-world data science.\n\n\n\n\n\n\n\nStandard Library\nThird-Party Library\n\n\n\n\nAlready installed with Python\nInstalled manually (via pip)\n\n\nExamples: math, os, random, datetime\nExamples: pandas, numpy, matplotlib, seaborn\n\n\nNo internet needed to use\nRequires internet to install\n\n\n\nUnderstanding the difference between these two categories will help you know when to reach for built-in tools versus when to seek out more powerful external solutions.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneExploring Python Libraries\n\n\n\nVisit the following two documentation pages:\n\nPython math module (Standard Library)\nPandas library (Third-Party)\n\nThen answer the following questions:\n\nWhat is one function provided by the math module that you could use in one of your other classes? Briefly describe what it does.\nWhat is one feature or function of the pandas library that stands out to you? How might it help in data analysis?\nBased on your experience browsing both pages, what are some differences you notice between standard library documentation and third-party library documentation?\n\nTip: This exercise is not about memorizing everything. It’s about familiarizing yourself with how to explore documentation and recognizing the types of functionality different libraries provide.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#working-with-the-standard-library",
    "href": "06-libraries.html#working-with-the-standard-library",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.3 Working with the Standard Library",
    "text": "6.3 Working with the Standard Library\nThe standard library is like Python’s built-in Swiss Army knife. It includes dozens of modules for common programming tasks—and because it’s part of every Python installation, you can start using it immediately without needing to install anything.\n\nTo use a module from the standard library, you simply use the import statement. Once imported, you can access its functions and tools using dot notation.\nFor example, to use the math library to calculate the square root of a number we need to first import the math library and then we can access the square root function like below:\n\nimport math\n\nmath.sqrt(9)\n\n3.0\n\n\n\nThere are many useful standard libraries - here is an extremely incomplete list of some of the modules you might wish to explore and learn about:\n\nos and sys: Tools for interfacing with the operating system, including navigating file directory structures and executing shell commands\nmath and cmath: Mathematical functions and operations on real and complex numbers\nitertools: Tools for constructing and interacting with iterators and generators\nfunctools: Tools that assist with functional programming\nrandom: Tools for generating pseudorandom numbers\npickle: Tools for object persistence: saving objects to and loading objects from disk\njson and csv: Tools for reading JSON-formatted and CSV-formatted files.\nurllib: Tools for doing HTTP and other web requests.\ndatetime: Tools for working with dates, times, and time intervals\n\nLet’s see a few of these in action.\n\n🧮 math: Mathematical Functions\nThe math module gives you access to a wide variety of mathematical operations beyond basic arithmetic. These include square roots, exponentials, logarithms, trigonometric functions, and more.\nHere’s a simple example:\n\nimport math\n\nprint(math.ceil(9.2))        # Returns the smallest integer greater than or equal to 9.2 (i.e., 10)\nprint(math.factorial(6))     # Returns the factorial of 6 (i.e., 720)\nprint(math.sqrt(121))        # Returns the square root of 121 (i.e., 11.0)\n\n10\n720\n11.0\n\n\n\n\n📁 os: Interacting with the Operating System\nThe os module allows you to interact with your computer’s operating system. It’s helpful for navigating file paths, checking directories, and automating file-related tasks.\nExample:\n\nimport os\n\n# Get the current working directory\nprint(\"Current working directory:\", os.getcwd())\n\n# List the files and folders in that directory\nprint(\"Contents of the directory:\", os.listdir())\n\nCurrent working directory: /home/runner/work/uc-bana-4080/uc-bana-4080/book\nContents of the directory: ['17-iteration-statements.quarto_ipynb', '06-libraries.qmd', '17-iteration-statements.qmd', '18-functions.quarto_ipynb', '25-decision-trees.qmd', 'summary.qmd', '11_aggregating_data.quarto_ipynb', '15-data-viz-bokeh.qmd', '23-logistic-regression.qmd', 'index.qmd', '16-control-statements.quarto_ipynb', '05-data-structures.html', '21-correlation-regression.qmd', '99-anaconda-install.qmd', '01-intro-data-mining.html', 'index.html', '26-random-forests.quarto_ipynb', '14-data-viz-matplotlib.qmd', '04-jupyter.qmd', '16-control-statements.qmd', '08-dataframes.quarto_ipynb', '05-data-structures.qmd', '13-data-viz-pandas.qmd', '26-random-forests.qmd', 'references.bib', '02-preparing-for-code.qmd', '14-data-viz-matplotlib.quarto_ipynb', 'images', '02-preparing-for-code.html', '.gitignore', 'site_libs', '27-feature-importance.quarto_ipynb', '12-joining-data.qmd', '19-intro-ml-ai.qmd', '06-libraries.quarto_ipynb', '_book', '24-classification-evaluation.quarto_ipynb', '.quarto', '24-classification-evaluation.qmd', '04-jupyter.html', '99-vscode-install.qmd', '15-data-viz-bokeh.quarto_ipynb', '22-regression-evaluation.qmd', '09-subsetting.quarto_ipynb', '05-data-structures.quarto_ipynb', 'index.quarto_ipynb', '21-correlation-regression.quarto_ipynb', '07-importing-data.quarto_ipynb', 'cover.png', '12-joining-data.quarto_ipynb', '22-regression-evaluation.quarto_ipynb', '09-subsetting.qmd', '11_aggregating_data.qmd', '19-intro-ml-ai.quarto_ipynb', '07-importing-data.qmd', '08-dataframes.qmd', '13-data-viz-pandas.quarto_ipynb', '03-python-basics.qmd', '03-python-basics.quarto_ipynb', '23-logistic-regression.quarto_ipynb', 'references.qmd', '10-manipulating-data.quarto_ipynb', '18-functions.qmd', '10-manipulating-data.qmd', '01-intro-data-mining.qmd', '25-decision-trees.quarto_ipynb', '03-python-basics.html', '20-before-we-build.qmd', '_quarto.yml', '27-feature-importance.qmd']\n\n\n\n\n📅 datetime: Working with Dates and Times\nThe datetime module is essential for handling and manipulating dates and times. You can get the current date, format it in a specific way, or calculate time differences.\nExample:\n\nimport datetime\n\n# Get today's date\ntoday = datetime.date.today()\nprint(\"Today's date is:\", today)\n\n# Create a specific date\nbirthday = datetime.date(1980, 8, 24)\nprint(\"Birth date:\", birthday)\n\n# How many days have I been on earth\ndays_alive = (today-birthday).days\nprint(\"Days on earth:\", days_alive)\n\nToday's date is: 2025-10-22\nBirth date: 1980-08-24\nDays on earth: 16495\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneUsing the Standard Library\n\n\n\nStart a Jupyter notebook and write code that does the following using only the standard library:\n\nUses the datetime module to print today’s date.\nUses the math module to calculate the square root of 625.\nUses the random module to simulate rolling a 6-sided die five times.\nUses the os module to print your current working directory.\n\nTip: Try running each part separately and look up the documentation if you’re unsure how to use a module. This is great practice for solving problems using tools that are already built into Python! If you encounter any difficulties or have questions, don’t hesitate to ask ChatGPT or Copilot for assistance!",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#pythons-data-science-ecosystem-third-party-modules",
    "href": "06-libraries.html#pythons-data-science-ecosystem-third-party-modules",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.4 Python’s Data Science Ecosystem & Third-party Modules",
    "text": "6.4 Python’s Data Science Ecosystem & Third-party Modules\nThe standard library is powerful, but when you begin working on real-world data science tasks, you’ll quickly find yourself needing more specialized tools. This is where third-party libraries come in. One of the things that makes Python useful, especially within the world of data science, is its ecosystem of third-party modules. These are external packages developed by the Python community and are not included with Python by default.\nThese packages are typically hosted on a package manager and Python Package Index (PyPI for short) and Anaconda are the two primary public package managers for Python. As of June 2025 there was about 645,000 packages available through the Python Package Index (PyPI)! Usually, you can ask Google or ChatGPT about what you are trying to do, and there is often a third party module to help you do it.\nTo install packages from PyPI we can either type the following in our terminal:\n# install from PyPI\npip install pkg_name\nAlternatively, you can install Python packages directly from a code cell in Jupyter notebooks by prefixing the pip install command with an exclamation mark (!). For example:\n# install within Jupyter notebook cell\n!pip install pkg_name\nThis allows you to manage dependencies without leaving your notebook environment.\n\n\n\n\n\n\nNoteTry It!\n\n\n\nGo ahead and see if you can pip install the pandas library.\n\n\nOnce a package is installed, you can import the library using the import statement and optionally assign it an alias (a short nickname), which is a common convention:\n# install within Jupyter notebook cell\n!pip install pandas\n# import package using the `pd` alias\nimport pandas as pd\nThroughout this course we’ll use several third party libraries focused on data science – for example Numpy, SciPy, Pandas, Scikit-learn, among others. Let’s look at some examples of these third party packages to give you a flavor of what they do. Don’t worry, we’ll go into some of these more thoroughly in later lessons!\n\nNumPy\nNumPy provides an efficient way to store and manipulate multi-dimensional dense arrays in Python. The important features of NumPy are:\n\nIt provides an ndarray structure, which allows efficient storage and manipulation of vectors, matrices, and higher-dimensional datasets.\nIt provides a readable and efficient syntax for operating on this data, from simple element-wise arithmetic to more complicated linear algebraic operations.\n\n\n\n\n\n\n\nAlthough the package is officially spelled “NumPy” you will commonly see it referred to as Numpy and numpy across the Python ecosystem (and even within this course).\n\n\n\nIn the simplest case, NumPy arrays look a lot like Python lists. For example, here is an array containing the range of numbers 1 to 9:\n\nimport numpy as np\n\nx = np.arange(1, 10)\nx\n\narray([1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n\n\n\n\n\nStandard convention is to import numpy as the np alias.\n\n\n\nNumPy’s arrays offer both efficient storage of data, as well as efficient element-wise operations on the data. For example, to square each element of the array, we can apply the ** operator to the array directly:\n\nx ** 2\n\narray([ 1,  4,  9, 16, 25, 36, 49, 64, 81])\n\n\nThis element-wise operation capability (commonly referred to as vectorization) is extremely useful but is not available in base Python. In base Python, if you had a list of these same numbers you would have to loop through each element in the list and compute the square of each number:\n\nx_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n# not supported\nx_list ** 2\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[7], line 4\n      1 x_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n      3 # not supported\n----&gt; 4 x_list ** 2\n\nTypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'\n\n\n\nWe would need to use a non-vectorized approach that iterates through each element and computes the square. The below illustrates the much more verbose non-vectorized approach that produces the same result:\n\n\n\n\n\n\nDon’t worry about the syntax, you will learn about this in a later lesson. Just note how the above approach with Numpy is far more convenient!\n\n\n\n\nx_squared = [val ** 2 for val in x_list]\nx_squared\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nNumpy also provides a host of other vectorized arithmetic capabilities. For example, we can compute the mean of a list with the following:\n\nnp.mean([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\nnp.float64(5.5)\n\n\nUnlike Python lists (which are limited to one dimension), NumPy arrays can be multi-dimensional. For example, here we will reshape our x array into a 3x3 matrix:\n\nm = x.reshape((3, 3))\nm\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\nA two-dimensional array is one representation of a matrix, and NumPy knows how to efficiently do typical matrix operations. For example, you can compute the transpose using .T:\n\nm.T\n\narray([[1, 4, 7],\n       [2, 5, 8],\n       [3, 6, 9]])\n\n\nor a matrix-vector product using np.dot:\n\nnp.dot(m, [5, 6, 7])\n\narray([ 38,  92, 146])\n\n\nand even more sophisticated operations like eigenvalue decomposition:\n\nnp.linalg.eigvals(m)\n\narray([ 1.61168440e+01, -1.11684397e+00, -1.30367773e-15])\n\n\nSuch linear algebraic manipulation underpins much of modern data analysis, particularly when it comes to the fields of machine learning and data mining.\n\n\nPandas\nPandas is a much newer package than Numpy, and is in fact built on top of it. What Pandas provides is a labeled interface to multi-dimensional data, in the form of a DataFrame object that will feel very familiar to users of R and related languages. DataFrames in Pandas look something like the following.\n\n\n\n\n\n\nIt is a common convention to import Pandas with the pd alias.\n\n\n\n\nimport pandas as pd\n\ndf = pd.DataFrame({'label': ['A', 'B', 'C', 'A', 'B', 'C'],\n                   'value': [1, 2, 3, 4, 5, 6]})\n\nThe Pandas interface allows you to do things like select columns by name:\n\ndf['label']\n\n0    A\n1    B\n2    C\n3    A\n4    B\n5    C\nName: label, dtype: object\n\n\nApply string operations across string entries:\n\ndf['label'].str.lower()\n\n0    a\n1    b\n2    c\n3    a\n4    b\n5    c\nName: label, dtype: object\n\n\nCompute statistical aggregations for numerical columns:\n\ndf['value'].sum()\n\nnp.int64(21)\n\n\nAnd, perhaps most importantly, do efficient database-style joins and groupings:\n\ndf.groupby('label').sum()\n\n\n\n\n\n\n\n\nvalue\n\n\nlabel\n\n\n\n\n\nA\n5\n\n\nB\n7\n\n\nC\n9\n\n\n\n\n\n\n\nHere in one line we have computed the sum of all objects sharing the same label, something that is much more verbose (and much less efficient) using tools provided in Numpy and core Python.\n\n\n\n\n\n\nIn future lessons we will go much deeper into Pandas and you’ll also see a large dependency on using Pandas as we start exploring other parts of the statistical computing ecosystem (i.e. visualization, machine learning).\n\n\n\n\n\nMatplotlib\nMatplotlib is currently the most popular scientific visualization packages in Python. Even proponents admit that its interface is sometimes overly verbose, but it is a powerful library for creating a large range of plots.\n\n\n\n\n\n\nIt is a common convention to import Matplotlib with the plt alias.\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')  # make graphs in the style of R's ggplot\n\nNow let’s create some data and plot the results:\n\nx = np.linspace(0, 10)  # range of values from 0 to 10\ny = np.sin(x)           # sine of these values\nplt.plot(x, y);         # plot as a line\n\n\n\n\n\n\n\n\nThis is the simplest example of a Matplotlib plot; for ideas on the wide range of plot types available, see Matplotlib’s online gallery.\n\n\n\n\n\n\nAlthough you’ll be exposed to some Matplotlib throughout this course, we will tend to focus on other third-party visualization packages that are simpler to use.\n\n\n\n\n\nSciPy\nSciPy is a collection of scientific functionality that is built on Numpy. The package began as a set of Python wrappers to well-known Fortran libraries for numerical computing, and has grown from there. The package is arranged as a set of submodules, each implementing some class of numerical algorithms. Here is an incomplete sample of some of the more important ones for data science:\n\nscipy.fftpack: Fast Fourier transforms\nscipy.integrate: Numerical integration\nscipy.interpolate: Numerical interpolation\nscipy.linalg: Linear algebra routines\nscipy.optimize: Numerical optimization of functions\nscipy.sparse: Sparse matrix storage and linear algebra\nscipy.stats: Statistical analysis routines\n\nFor example, let’s take a look at interpolating a smooth curve between some data\n\nfrom scipy import interpolate\n\n# choose eight points between 0 and 10\nx = np.linspace(0, 10, 8)\ny = np.sin(x)\n\n# create a cubic interpolation function\nfunc = interpolate.interp1d(x, y, kind='cubic')\n\n# interpolate on a grid of 1,000 points\nx_interp = np.linspace(0, 10, 1000)\ny_interp = func(x_interp)\n\n# plot the results\nplt.figure()  # new figure\nplt.plot(x, y, 'o')\nplt.plot(x_interp, y_interp);\n\n\n\n\n\n\n\n\nWhat we see is a smooth interpolation between the points.\n\n\nOther Data Science Packages\nBuilt on top of these tools are a host of other data science packages, including general tools like Scikit-Learn for machine learning, Scikit-Image for image analysis, Seaborn for statistical visualization, and Statsmodels for statistical modeling; as well as more domain-specific packages like AstroPy for astronomy and astrophysics, NiPy for neuro-imaging, and many, many more.\nNo matter what type of scientific, numerical, or statistical problem you are facing, it’s likely there is a Python package out there that can help you solve it.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneInstalling and Using Third-Party Libraries\n\n\n\n\nUse pip (or !pip in a Jupyter notebook) to install the following third-party libraries:\n\nnumpy\nbokeh\n\nOnce installed, copy and run the following code in a Jupyter notebook:\nimport numpy as np\nimport bokeh.plotting\nimport bokeh.io\n\nbokeh.io.output_notebook()\n\n# Generate plotting values\nt = np.linspace(0, 2*np.pi, 200)\nx = 16 * np.sin(t)**3\ny = 13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t)\n\np = bokeh.plotting.figure(height=250, width=275)\np.line(x, y, color='red', line_width=3)\ntext = bokeh.models.Label(x=0, y=0, text='BANA 4080', text_align='center')\np.add_layout(text)\n\nbokeh.io.show(p)\nAfter executing the code:\n\nWhat shape is created by this visualization?\nWhat part of the code controls the label “BANA 4080”?\nHow does numpy assist in preparing the data for visualization?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#summary",
    "href": "06-libraries.html#summary",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.5 Summary",
    "text": "6.5 Summary\nIn this chapter, you learned that one of Python’s greatest strengths is its rich ecosystem of reusable code—organized into modules, libraries, and packages. These tools allow you to write less code, solve complex problems more efficiently, and leverage the collective efforts of the Python community.\nWe began by discussing the difference between Python’s standard library and third-party libraries. You learned how to use built-in tools like math, os, and datetime for essential tasks, and how to install and import third-party packages using pip.\nWe then explored Python’s thriving data science ecosystem—highlighting libraries like NumPy, Pandas, Matplotlib, Seaborn, and SciPy. These libraries will be your go-to tools for data wrangling, statistical modeling, and visualization throughout this course and beyond.\nYou don’t need to memorize every function from every package right now. Instead, focus on building awareness of what kinds of tools exist and how to access them. The more you practice, the more fluent you’ll become in navigating and using Python’s expansive ecosystem.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#end-of-chapter-exercise-putting-python-libraries-to-work",
    "href": "06-libraries.html#end-of-chapter-exercise-putting-python-libraries-to-work",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.6 End-of-Chapter Exercise: Putting Python Libraries to Work",
    "text": "6.6 End-of-Chapter Exercise: Putting Python Libraries to Work\nCreate a new Jupyter notebook titled chapter-6-libraries-practice.ipynb. This notebook should include markdown cells to describe each section of your work and code cells to perform the tasks below. Be sure to run your code and document your findings or observations.\n\n\n\n\n\n\nNonePart 1: Standard Library Practice\n\n\n\n\n\nUse the following standard libraries: math, os, datetime, and random.\n\nMath Practice: Compute the square root, factorial, and log (base 10) of any number you choose using the math module.\nWorking with Files: Use the os module to print your current working directory and list all files in it.\nRandom Simulation: Use the random module to simulate flipping a coin 20 times. Count how many times you get heads vs. tails.\nDate Math: Use the datetime module to:\n\nPrint today’s date.\nCreate your birthday as a date object.\nCalculate how many days old you are.\n\n\n\n\n\n\n\n\n\n\n\nNonePart 2: Installing and Using Third-Party Libraries\n\n\n\n\n\n\nUse !pip install to install the following:\n\nnumpy\nbokeh\n\nCopy and run the following code to generate a heart-shaped plot:\nimport numpy as np\nimport bokeh.plotting\nimport bokeh.io\n\nbokeh.io.output_notebook()\n\nt = np.linspace(0, 2*np.pi, 200)\nx = 16 * np.sin(t)**3\ny = 13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t)\n\np = bokeh.plotting.figure(height=250, width=275)\np.line(x, y, color='red', line_width=3)\ntext = bokeh.models.Label(x=0, y=0, text='BANA 4080', text_align='center')\np.add_layout(text)\n\nbokeh.io.show(p)\nAdd a markdown cell answering the following:\n\nWhat shape is drawn?\nHow does numpy help in generating the plot data?\nWhat part of the code adds the label “BANA 4080”?\n\n\n\n\n\n\n\n\n\n\n\nNonePart 3: Summary Reflection\n\n\n\n\n\nIn a markdown cell, write 3–5 sentences reflecting on what you learned in this chapter. Consider:\n\nWhat surprised you about Python’s library ecosystem?\nWhich module or package do you think you’ll use the most in this course?\nDoes knowing about these tools change the way you think about programming?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html",
    "href": "07-importing-data.html",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "",
    "text": "7.1 From Disk to DataFrame: How Data Enters Python\nImagine you’re working as a summer intern for a real estate analytics firm. On your first day, your manager hands you a file: “Here’s the raw data for the Ames, Iowa housing market. Let’s start by pulling it into Python and taking a quick look around.”\nYou double-click the file — it’s filled with rows and rows of numbers, codes, and column headers you don’t quite understand. Where do you even begin?\nIn this chapter, you’ll walk through the exact steps you’d take in that situation. You’ll load real data, explore it using Python, and start to build your intuition for what’s inside a dataset. You won’t be doing full analysis yet — but you will learn how to get your bearings using one of Python’s most powerful tools: Pandas.\nLater chapters will teach you how to clean, transform, and analyze data — but first, you need to bring it into Python and take a look around.\nBy the end of this chapter, you will be able to:\nPython stores its data in memory - this makes it relatively quickly accessible but can cause size limitations in certain fields. In this class we will mainly work with small to moderate data sets, which means we should not run into any space limitations.\nPython memory is session-specific, so quitting Python (i.e. shutting down JupyterLab) removes the data from memory. A general way to conceptualize data import into and use within Python:\nHere is a visualization of this process:",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#from-disk-to-dataframe-how-data-enters-python",
    "href": "07-importing-data.html#from-disk-to-dataframe-how-data-enters-python",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "",
    "text": "Python does provide tooling that allows you to work with big data via distributed data (i.e. Pyspark) and relational databrases (i.e. SQL).\n\n\n\n\n\nData sits in on the computer/server - this is frequently called “disk”\nPython code can be used to copy a data file from disk to the Python session’s memory\nPython data then sits within Python’s memory ready to be used by other Python code\n\n\n\n\n\nPython memory",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#importing-delimited-files-with-read_csv",
    "href": "07-importing-data.html#importing-delimited-files-with-read_csv",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.2 Importing Delimited Files with read_csv()",
    "text": "7.2 Importing Delimited Files with read_csv()\nText files are a popular way to store and exchange tabular data. Nearly every data application supports exporting to CSV (Comma Separated Values) format or another type of text-based format. These files use a delimiter — such as a comma, tab, or pipe symbol — to separate elements within each line. Because of this consistent structure, importing text files into Python typically follows a straightforward process once the delimiter is identified.\nPandas provides a very efficient and simple way to load these types of files using its read_csv() function. While there are other approaches available (such as Python’s built-in csv module), Pandas is preferred for its ease of use and direct creation of a DataFrame — the primary tabular data structure used throughout this course.\n\nIn the example below, we use read_csv() to load a dataset listing some information on aircraft.\n\n\n\n\n\n\nPlease note that you must have internet access for this example to work.\nIn this first example, we will demonstrate how to import data directly from a URL. This approach is useful when your data is hosted online and you want to access it directly within your analysis.\nLater in this chapter, we will discuss how to import data that resides on your local computer.\n\n\n\n\nimport pandas as pd\n\nplanes = pd.read_csv('https://tinyurl.com/planes-data')\n\nWe see that our imported data is represented as a DataFrame:\n\ntype(planes)\n\npandas.core.frame.DataFrame\n\n\nWe can look at it in the Jupyter notebook, since Jupyter will display it in a well-organized, pretty way.\n\nplanes\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3317\nN997AT\n2002.0\nFixed wing multi engine\nBOEING\n717-200\n2\n100\nNaN\nTurbo-fan\n\n\n3318\nN997DL\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS AIRCRAFT CO\nMD-88\n2\n142\nNaN\nTurbo-fan\n\n\n3319\nN998AT\n2002.0\nFixed wing multi engine\nBOEING\n717-200\n2\n100\nNaN\nTurbo-fan\n\n\n3320\nN998DL\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS CORPORATION\nMD-88\n2\n142\nNaN\nTurbo-jet\n\n\n3321\nN999DN\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS CORPORATION\nMD-88\n2\n142\nNaN\nTurbo-jet\n\n\n\n\n3322 rows × 9 columns\n\n\n\nThis is a nice representation of the data, but we really do not need to display that many rows of the DataFrame in order to understand its structure. Instead, we can use the head() method of data frplanes to look at the first few rows. This is more manageable and gives us an overview of what the columns are. Note also the the missing data was populated with NaN.\n\nplanes.head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneGetting Started with Ames Housing Data\n\n\n\nUse the Pandas library to complete the following tasks.\n\nLoad the Data: Import the Ames housing dataset using the following URL - https://tinyurl.com/ames-raw\nVerify the Object Type: What type of Python object is the result of your import? Use the type() function to confirm.\nPreview the Data: Use the .head() method to print the first few rows of the dataset. Based on the output:\n\nWhat are some of the column names you see?\nWhat kinds of values are stored in these columns?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#file-paths",
    "href": "07-importing-data.html#file-paths",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.3 File Paths",
    "text": "7.3 File Paths\nIn the previous example, we imported data directly from a URL; however, datasets often reside on our computer and we need to specify the file path to read them from. For example, rather than import the planes.csv data from the URL we used above, I can read in that same dataset as follows.\n\nplanes = pd.read_csv('../data/planes.csv')\n\nplanes.head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nBut to understand why I use '../data/planes.csv' in the code above, we need to spend a little time talking about file paths.\n\nIt’s important to understand where files exist on your computer and how to reference those paths. There are two main approaches:\n\nAbsolute paths\nRelative paths\n\nAn absolute path always contains the root elements and the complete list of directories to locate the specific file or folder. For the planes.csv file, the absolute path on my computer is:\n\nimport os\n\nabsolute_path = os.path.abspath('../data/planes.csv')\nabsolute_path\n\n'/home/runner/work/uc-bana-4080/uc-bana-4080/data/planes.csv'\n\n\nI can always use this absolute path in pd.read_csv():\n\nplanes = pd.read_csv(absolute_path)\n\nIn contrast, a relative path is a path built starting from the current location. For example, say that I am operating in a directory called “Project A”. If I’m working in “my_notebook.ipynb” and I have a “my_data.csv” file in that same directory:\n# illustration of the directory layout\nProject A\n├── my_notebook.ipynb\n└── my_data.csv\nThen I can use this relative path to import this file: pd.read_csv('my_data.csv'). This just means to look for the ‘my_data.csv’ file relative to the current directory that I am in.\nOften, people store data in a “data” directory. If this directory is a subdirectory within my Project A directory:\n# illustration of the directory layout\nProject A\n├── my_notebook.ipynb\n└── data\n    └── my_data.csv\nThen I can use this relative path to import this file: pd.read_csv('data/my_data.csv'). This just means to look for the ‘data’ subdirectory relative to the current directory that I am in and then look for the ‘my_data.csv’ file.\nSometimes, the data directory may not be in the current directory. Sometimes a project directory will look the following where there is a subdirectory containing multiple notebooks and then another subdirectory containing data assets. If you are working in “notebook1.ipynb” within the notebooks subdirectory, you will need to tell Pandas to go up one directory relative to the notebook you are working in to the main Project A directory and then go down into the data directory.\n# illustration of the directory layout\nProject A\n├── notebooks\n│   ├── notebook1.ipynb\n│   ├── notebook2.ipynb\n│   └── notebook3.ipynb\n└── data\n    └── my_data.csv\nI can do this by using dot-notation in my relative path specification - here I use ‘..’ to imply “go up one directory relative to my current location”: pd.read_csv('../data/my_data.csv'). And this is why is used '../data/planes.csv' in the code at the beginning of this section, because my directory layout is:\nProject A\n├── notebooks\n│   ├── notebook1.ipynb\n│   ├── notebook2.ipynb\n│   └── notebook3.ipynb\n└── data\n    └── planes.csv\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry Different File Paths\n\n\n\nDownload the ames_raw.csv dataset and save the file in a folder structure like this:\nProject A\n├── notebooks\n│   └── import_demo.ipynb\n└── data\n    └── ames_raw.csv\nNow, do the following:\n\nImport using a relative path: In import_demo.ipynb, write the code to import the file using a relative path.\nImport using an absolute path: Use Python’s os.path.abspath() to determine the full absolute path of the file. Then use that path in pd.read_csv().\nReflection questions (write your answers in a Markdown cell):\n\nWhich method feels easier or more flexible to you?\nIn what situations might you prefer absolute over relative paths?\n\n\n\n\nGreat idea — adding an example using Google Colab will help students who are working in the cloud and need to upload files directly from their local machine. Here’s a clean, student-friendly example you can drop into your chapter:",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#importing-data-in-google-colab",
    "href": "07-importing-data.html#importing-data-in-google-colab",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.4 Importing Data in Google Colab",
    "text": "7.4 Importing Data in Google Colab\nIf you’re working in Google Colab, your files aren’t stored on your local machine — you’re running code on a cloud-based virtual machine. That means reading in local files (like a .csv on your desktop) works a little differently.\nHere’s how you can upload a file directly from your computer into Colab:\nfrom google.colab import files\n\n# This will open a file picker in Colab\nuploaded = files.upload()\nOnce you select the file you want to upload (e.g., planes.csv), Colab will store it temporarily in your session and make it available to use just like any other file:\nimport pandas as pd\n\n# Now you can load the file into a DataFrame\nplanes = pd.read_csv('planes.csv')\n\n\n\n\n\n\nFiles uploaded this way only persist for the current Colab session. If you close the browser or restart your runtime, you’ll need to re-upload the file.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#inspecting-your-dataframe",
    "href": "07-importing-data.html#inspecting-your-dataframe",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.5 Inspecting Your DataFrame",
    "text": "7.5 Inspecting Your DataFrame\nAfter importing, the data is stored as a DataFrame — the core data structure in Pandas. And with DataFrames, there are several ways to start understanding some basic, descriptive information about our data. For example, we can get the dimensions of our DataFrame. Here, we see that we have 3,322 rows and 9 columns.\n\nplanes.shape\n\n(3322, 9)\n\n\nWe can also see what type of data each column is. For example, we see that the tailnum column data type is object, the year column is a floating point (float64), and engines is an integer (int64).\n\nplanes.dtypes\n\ntailnum          object\nyear            float64\ntype             object\nmanufacturer     object\nmodel            object\nengines           int64\nseats             int64\nspeed           float64\nengine           object\ndtype: object\n\n\nThe following are the most common data types that appear frequently in DataFrames.\n\nboolean - only two possible values, True and False\ninteger - whole numbers without decimals\nfloat - numbers with decimals\nobject - typically strings, but may contain any object\ndatetime - a specific date and time with nanosecond precision\n\n\n\n\n\n\n\nBooleans, integers, floats, and datetimes all use a particular amount of memory for each of their values. The memory is measured in bits. The number of bits used for each value is the number appended to the end of the data type name. For instance, integers can be either 8, 16, 32, or 64 bits while floats can be 16, 32, 64, or 128. A 128-bit float column will show up as float128. Technically a float128 is a different data type than a float64 but generally you will not have to worry about such a distinction as the operations between different float columns will be the same.\n\n\n\nWe can also use the info() method, which provides output similar to dtypes, but also shows the number of non-missing values in each column along with more info such as:\n\nType of object (always a DataFrame)\nThe type of index and number of rows\nThe number of columns\nThe data types of each column and the number of non-missing (a.k.a non-null)\nThe frequency count of all data types\nThe total memory usage\n\n\nplanes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3322 entries, 0 to 3321\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   tailnum       3322 non-null   object \n 1   year          3252 non-null   float64\n 2   type          3322 non-null   object \n 3   manufacturer  3322 non-null   object \n 4   model         3322 non-null   object \n 5   engines       3322 non-null   int64  \n 6   seats         3322 non-null   int64  \n 7   speed         23 non-null     float64\n 8   engine        3322 non-null   object \ndtypes: float64(2), int64(2), object(5)\nmemory usage: 233.7+ KB\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneInspecting the Ames Housing Data\n\n\n\nIn the previous section, you imported the ames_raw.csv dataset. Now let’s explore its structure using the tools you just learned.\n\nHow big is the dataset?: Use .shape to find out how many rows and columns are in the dataset.\nWhat kinds of data are stored?: Use .dtypes to print out the data types of all columns.\n\nHow many columns are object type?\nHow many are float or int?\n\nDig deeper with .info()\n\nHow many non-null values are in each column?\nHow much memory does the dataset use?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#attributes-methods",
    "href": "07-importing-data.html#attributes-methods",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.6 Attributes & Methods",
    "text": "7.6 Attributes & Methods\n\nWe’ve seen that we can use the dot-notation to access functions in libraries (i.e. pd.read_csv()). We can use this same approach to access things inside of objects. What’s an object? Basically, a variable that contains other data or functionality inside of it that is exposed to users. Consequently, our DataFrame item is an object.\nIn the above code, we saw that we can make different calls with our DataFrame such as planes.shape and planes.head(). An observant reader probably noticed the difference between the two – one has parentheses and the other does not.\nAn attribute inside an object is simply a variable that is unique to that object and a method is just a function inside an object that is unique to that object.\n\n\n\n\n\n\nVariables inside an object are often called attributes and functions inside objects are called methods.\nattribute: A variable associated with an object and is referenced by name using dotted expressions. For example, if an object o has an attribute a it would be referenced as o.a\nmethod: A function associated with an object and is also referenced using dotted expressions but will include parentheses. For example, if an object o has a method m it would be called as o.m()\n\n\n\nEarlier, we saw the attributes shape and dtypes. Another attribute is columns, which will list all column names in our DataFrame.\n\nplanes.columns\n\nIndex(['tailnum', 'year', 'type', 'manufacturer', 'model', 'engines', 'seats',\n       'speed', 'engine'],\n      dtype='object')\n\n\nSimilar to regular functions, methods are called with parentheses and often take arguments. For example, we can use the tail() method to see the last n rows in our DataFrame:\n\nplanes.tail(3)\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n3319\nN998AT\n2002.0\nFixed wing multi engine\nBOEING\n717-200\n2\n100\nNaN\nTurbo-fan\n\n\n3320\nN998DL\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS CORPORATION\nMD-88\n2\n142\nNaN\nTurbo-jet\n\n\n3321\nN999DN\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS CORPORATION\nMD-88\n2\n142\nNaN\nTurbo-jet\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will be exposed to many of the available DataFrame methods throughout this course!\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneAttributes vs. Methods in the Ames Housing Data\n\n\n\nUsing the ames_raw DataFrame you’ve already imported:\n\nList all column names: Use the appropriate attribute to display all the column names in the dataset.\nPreview the last 5 rows: Use the appropriate method to display the last five rows of the dataset.\nIdentify each line of code: For each of the following, decide whether it’s a method or an attribute:\n\names.columns\names.head()\names.shape\names.tail(10)\names.dtypes\n\nReflect In a markdown cell, briefly explain in your own words:\n\nWhat’s the difference between an attribute and a method?\nHow can you tell which one you’re using?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#common-dataframe-errors",
    "href": "07-importing-data.html#common-dataframe-errors",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.7 Common DataFrame Errors",
    "text": "7.7 Common DataFrame Errors\nAs you’re learning to work with DataFrames in Pandas, you’ll likely encounter a few common errors. Don’t worry — these are normal and part of the learning process. This section introduces a few of the most frequent issues and how to fix them.\n\nForgetting Parentheses When Using a Method\nOne of the most common mistakes is confusing methods with attributes. Remember: methods require parentheses () — even if they don’t take arguments.\names.head     # 🚫 Returns the method itself, not the data\names.head()   # ✅ Correct — this returns the first few rows\n\n\nTypos in Column Names\nColumn names in a DataFrame must be typed exactly as they appear. They’re case-sensitive and must match spacing and punctuation.\names['SalePrice']     # ✅ Correct\names['saleprice']     # 🚫 KeyError: 'saleprice'\n\n\n\n\n\n\nTip: Use ames.columns to check exact column names.\n\n\n\n\n\nFileNotFoundError When Loading a File\nThis happens when the file path is incorrect or the file isn’t in the expected location.\n# 🚫 Incorrect: file doesn't exist at this path\npd.read_csv('data.csv')\n\n# ✅ Correct (based on your current working directory)\npd.read_csv('../data/ames_raw.csv')\n\n\n\n\n\n\nTip: Use os.getcwd() to check your working directory, and os.path.abspath() to confirm the full path.\n\n\n\n\n\nUsing Dot Notation with Column Names that Contain Spaces or Special Characters\nPandas allows you to access columns using dot notation only if the column name is a valid Python variable name.\names.SalePrice     # ✅ Works (if column is named 'SalePrice')\names.MS Zoning     # 🚫 SyntaxError\n\n# ✅ Use bracket notation instead\names['MS Zoning']\n\n\nConfusing Methods That Don’t Exist\nSometimes, learners assume there’s a method for something that doesn’t exist.\names.rows()    # 🚫 AttributeError: 'DataFrame' object has no attribute 'rows'\n\n\n\n\n\n\nTip: Use dir(ames) to see a list of available methods and attributes, or use tab-completion in Jupyter to explore.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#other-file-types",
    "href": "07-importing-data.html#other-file-types",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.8 Other File Types",
    "text": "7.8 Other File Types\nSo far, we’ve focused on CSVs — the most common format for tabular data. But Python, especially through the Pandas library, can import a wide variety of file types beyond just delimited text files.\nHere are a few common formats you might encounter:\n\nExcel spreadsheets (.xls, .xlsx)\nJSON files — often used for APIs and web data\nPickle files — Python’s own format for saving data structures\nSQL databases — such as SQLite, PostgreSQL, or MySQL\nParquet and Feather — efficient storage formats for big data workflows\n\nIn most cases, Pandas provides a convenient read_ function to handle the import process. For example, let’s look at how we can import an Excel file directly — without converting it to a CSV first.\n\nImporting Excel Files with read_excel()\nExcel is still one of the most widely used tools for storing and sharing data. And while many users convert Excel files into CSVs before importing them into Python, Pandas allows you to skip that step entirely.\nTo import data directly from an Excel workbook, you can use the read_excel() function. But first, you may need to install an additional dependency:\n# Run this in your terminal if you haven’t already\npip install openpyxl\nIn this example, we’ll import a mock dataset of grocery store products stored in a file called products.xlsx (download here).\n\n# Preview the available sheets in the workbook\nproducts_excel = pd.ExcelFile('../data/products.xlsx')\nproducts_excel.sheet_names\n\n['metadata', 'products data', 'grocery list']\n\n\nTo load a specific sheet from this workbook:\n\n\n\n\n\n\nIf you don’t explicitly specify a sheet name, Pandas will default to importing the first worksheet in the file.\n\n\n\n\nproducts = pd.read_excel('../data/products.xlsx', sheet_name='products data')\nproducts.head()\n\n\n\n\n\n\n\n\nproduct_num\ndepartment\ncommodity\nbrand_ty\nx5\n\n\n\n\n0\n92993\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n1\n93924\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n2\n94272\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n3\n94299\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n4\n94594\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s an additional video provided by Corey Schafer that you might find useful. It covers importing and exporting data from multiple different sources.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#summary",
    "href": "07-importing-data.html#summary",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.9 Summary",
    "text": "7.9 Summary\nIn this chapter, you learned how to import real-world data into Python and begin exploring it using Pandas — one of the most important tools in the data science workflow.\nYou started with a realistic scenario: opening up a raw dataset and figuring out how to make sense of it. You learned how to read in delimited files (like CSVs), use relative and absolute file paths, and inspect your data using essential DataFrame attributes and methods like .shape, .dtypes, .info(), and .head().\nYou also saw how Pandas supports a wide variety of file formats beyond CSV, including Excel, JSON, Pickle, and SQL databases — making it a flexible tool for working with nearly any type of data source.\nThis chapter was focused on getting your data into Python and taking a first look around. In upcoming chapters, we’ll dig into the heart of data science: cleaning, wrangling, summarizing, and uncovering patterns in your data to support real-world decision making.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#exercise-covid-19-cases-at-u.s.-colleges",
    "href": "07-importing-data.html#exercise-covid-19-cases-at-u.s.-colleges",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.10 Exercise: COVID-19 Cases at U.S. Colleges",
    "text": "7.10 Exercise: COVID-19 Cases at U.S. Colleges\nThe New York Times published a dataset tracking COVID-19 cases at colleges and universities across the United States. You can read about this dataset here. In this exercise, you’ll download and explore that dataset to practice the skills you’ve learned in this chapter.\n\n\n\n\n\n\nNoneStep 1: Download the Data\n\n\n\n\n\nDownload the dataset directly from this GitHub URL: colleges.csv\nSet up your project directory to look like this:\nBANA 4080\n├── notebooks\n│   └── covid_analysis.ipynb\n└── data\n    └── colleges.csv\n\n\n\n\n\n\n\n\n\nNoneStep 2: Import the Data\n\n\n\n\n\nIn your covid_analysis.ipynb notebook:\n\nImport Pandas\nLoad the dataset using a relative path\nUse the type() function to confirm that the data is stored as a DataFrame\n\n\n\n\n\n\n\n\n\n\nNoneStep 3: Explore the Structure\n\n\n\n\n\nUse the following DataFrame attributes and methods:\n\n.shape — how many rows and columns are in the dataset?\n.columns — what variables are included?\n.head() — what do the first few rows of the dataset look like?\n.info() — are there any missing values?\n\n\n\n\n\n\n\n\n\n\nNoneStep 4: Reflect\n\n\n\n\n\nIn a Markdown cell, write a short summary addressing the following:\n\nWhat is this dataset tracking?\nWhat are some variables you’d want to explore further?\nWhat kinds of questions could you answer with this data in future chapters?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html",
    "href": "08-dataframes.html",
    "title": "8  Deeper Dive on DataFrames",
    "section": "",
    "text": "8.1 Learning objectives\nImagine you’ve just imported a new airline dataset into Python. It includes hundreds of rows listing airline names and their carrier codes. Your manager asks, “Can you quickly pull a list of all airline names? And which carrier code has the longest name?” Before you can answer, you need to understand what’s inside that DataFrame.\nIn the previous chapter, we focused on how to import datasets into Python using Pandas — a crucial first step in any data analysis workflow. Now that we can get data into Python, we need to understand what we’re actually working with. This chapter takes a closer look at Pandas DataFrames and their building blocks, the Series.\nBy deepening your understanding of the DataFrame structure and how to access and manipulate data within it, you’ll build a foundation for future chapters focused on cleaning, transforming, and analyzing real-world data.\nAt the end of this lesson you should be able to:\nTo illustrate our points throughout this lesson, we’ll use the following airlines data which includes the name of the airline carrier and the airline carrier code:\nimport pandas as pd\n\ndf = pd.read_csv('../data/airlines.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#learning-objectives",
    "href": "08-dataframes.html#learning-objectives",
    "title": "8  Deeper Dive on DataFrames",
    "section": "",
    "text": "Explain the difference between DataFrames and Series\nAccess and manipulate data within DataFrames and Series\nSet and manipulate index values\n\n\n\n\n\n\n\nNote📓 Follow Along in Colab!\n\n\n\nAs you read through this chapter, we encourage you to follow along using the companion notebook in Google Colab (or other editor of choice). This interactive notebook lets you run code examples covered in the chapter—and experiment with your own ideas.\n👉 Open the Dataframes Notebook in Colab.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#understanding-the-dataframe-structure",
    "href": "08-dataframes.html#understanding-the-dataframe-structure",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.2 Understanding the DataFrame Structure",
    "text": "8.2 Understanding the DataFrame Structure\nA DataFrame is a two-dimensional, labeled data structure—similar to a table in Excel or a SQL database—that is used to store and manipulate structured data. You can think of it as a spreadsheet-like object where the data is organized into rows and columns.\nEach column in a DataFrame is actually a special type of object in Pandas called a Series. A Series is a one-dimensional array-like object that holds data and a corresponding index for labeling each entry. While the concept of a Series might be new to you, it’s fundamental to how Pandas works under the hood. Understanding how Series operate is essential because much of your interaction with DataFrames involves accessing and manipulating individual Series.\n\ndf.info()\ndf.shape\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 16 entries, 0 to 15\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   carrier  16 non-null     object\n 1   name     16 non-null     object\ndtypes: object(2)\nmemory usage: 388.0+ bytes\n\n\n(16, 2)\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneExploring Airport Data\n\n\n\nDownload and import the airports data into a DataFrame called airports.\n\nHow many rows and columns are in the dataset? Use .shape and .info() to help.\nWhat are the column names and data types? Are any surprising?\nTry printing the first 5 and last 5 rows using .head() and .tail(). What types of data does this dataset contain?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#series-the-building-blocks-of-dataframes",
    "href": "08-dataframes.html#series-the-building-blocks-of-dataframes",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.3 Series: The Building Blocks of DataFrames",
    "text": "8.3 Series: The Building Blocks of DataFrames\n\nColumns (and rows) of a DataFrame are actually Series objects. A Series is a one-dimensional labeled array capable of holding any data type — such as integers, strings, or even objects. You can think of it as a single column of data with a label on every entry. When you select a single column from a DataFrame, Pandas returns that column as a Series object.\n\n\n\n\n\n\nExample Series\n\n\n\n\nFigure 8.1: Source: Python Programming for Data Science\n\n\n\nYou can extract a Series using bracket notation with the column name:\n\ncarrier_column = df['carrier']\ncarrier_column\n\n0     9E\n1     AA\n2     AS\n3     B6\n4     DL\n5     EV\n6     F9\n7     FL\n8     HA\n9     MQ\n10    OO\n11    UA\n12    US\n13    VX\n14    WN\n15    YV\nName: carrier, dtype: object\n\n\nThis looks very much like a column of data — and it is — but under the hood, it’s a different type of object than the full DataFrame:\n\ntype(carrier_column)\n\npandas.core.series.Series\n\n\nThis confirms that what you extracted is a Series. You can verify its one-dimensional nature with:\n\ncarrier_column.shape  # One dimension (just the number of rows)\n\n(16,)\n\n\nCompare this with the shape of the full DataFrame, which has both rows and columns:\n\ndf.shape  # Two dimensions (rows, columns)\n\n(16, 2)\n\n\nIt’s important to be familiar with Series because they are fundamentally the core of DataFrames.\n\n\n\n\n\n\nExample DataFrame\n\n\n\n\nFigure 8.2: Source: Python Programming for Data Science\n\n\n\nUnderstanding this distinction is important, because many of the functions and behaviors available to Series differ from those of DataFrames. As we continue working with data, we’ll frequently switch between viewing data as Series and viewing it as part of a full DataFrame.\nInterestingly, when you extract a single row using .loc[], Pandas also returns a Series. In that case, the index of the Series becomes the column names of the original DataFrame:\n\n\n\n\n\n\n.loc[] is an accessor that allows us to retrieve rows and columns by labels. We’ll explore accessors more in the next chapter, but for now, just know we’re using it here to get the first row of the DataFrame.\n\n\n\n\nfirst_row = df.loc[0]  # Using .loc to access the first row of the DataFrame\ntype(first_row)        # pandas.core.series.Series\n\npandas.core.series.Series\n\n\nThis reinforces the idea that both columns and rows, when isolated, are treated as Series.\nAnother difference lies in the methods available to each. Series come with some specialized methods, such as .to_list(), which converts the data to a basic Python list. This method is not available on DataFrames:\n\ncarrier_column.to_list()  # works\n\n['9E',\n 'AA',\n 'AS',\n 'B6',\n 'DL',\n 'EV',\n 'F9',\n 'FL',\n 'HA',\n 'MQ',\n 'OO',\n 'UA',\n 'US',\n 'VX',\n 'WN',\n 'YV']\n\n\n\n# This will raise an error\ndf.to_list()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/tmp/ipykernel_12556/1197199109.py in ?()\n      1 # This will raise an error\n----&gt; 2 df.to_list()\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/generic.py in ?(self, name)\n   6314             and name not in self._accessors\n   6315             and self._info_axis._can_hold_identifiers_and_holds_name(name)\n   6316         ):\n   6317             return self[name]\n-&gt; 6318         return object.__getattribute__(self, name)\n\nAttributeError: 'DataFrame' object has no attribute 'to_list'\n\n\n\n\n\n\n\n\n\nAs you continue through this book, you’ll learn many methods that apply specifically to either Series or DataFrames, and some that work on both. Becoming familiar with the structure of each will help you develop the intuition to choose and apply the right methods in the right context.\n\n\n\nFinally, Series tend to print more compactly and are useful when you’re only interested in a single set of values — like a single variable or row. Their lightweight format makes them a convenient choice in many day-to-day data tasks. \n\nKnowledge Check\n\n\n\n\n\n\nNoneDigging into Columns and Rows\n\n\n\nUsing the airports DataFrame:\n\nSelect the alt column and assign it to a variable called altitudes. What type of object is altitudes?\nUse type() and .shape to confirm it’s a Series and check its dimensionality.\nExtract the first row using .loc[0]. What type of object is this? What is its index made up of?\nCompare the output of airports[\"alt\"] and airports[[\"alt\"]]. What do you think is the difference between the two?\nTry calling .to_list() on both the airports[\"alt\"] and airports[[\"alt\"]] objects. What happens?\nWhat happens if you try using dot notation like airports.alt? When does this work vs. when could you see this failing?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#creating-and-indexing-a-series",
    "href": "08-dataframes.html#creating-and-indexing-a-series",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.4 Creating and Indexing a Series",
    "text": "8.4 Creating and Indexing a Series\n\nFirst, let’s create our own Series object from scratch – they don’t always need to come from a DataFrame. Here, we pass a list in as an argument and it will be converted to a Series:\n\ns = pd.Series([10, 20, 30, 40, 50])\ns\n\n0    10\n1    20\n2    30\n3    40\n4    50\ndtype: int64\n\n\nThis gives us a one-dimensional structure that prints a little differently than a DataFrame. There are three main parts of the Series to pay attention to:\n\nThe values (10, 20, 30…)\nThe dtype, short for data type (in this case, int64)\nThe index (0, 1, 2… by default)\n\nValues are fairly self-explanatory — they’re the list elements we passed in. The data type describes what kind of values are being stored (numbers, strings, etc.). Series are often homogeneous — holding only one data type — though technically they can hold a mix (which we’ll avoid for clarity).\nThe index is where things get more interesting. Every Series has an index, which functions much like the keys in a dictionary — each label maps to a value. By default, Pandas assigns numeric labels starting at 0:\n\ns.index  # Default index (RangeIndex)\ns\n\n0    10\n1    20\n2    30\n3    40\n4    50\ndtype: int64\n\n\nBut we can change the index to be more meaningful. For example, we could relabel the values using letters:\n\ns.index = ['a', 'b', 'c', 'd', 'e']\ns\n\na    10\nb    20\nc    30\nd    40\ne    50\ndtype: int64\n\n\nNow, if we want to access the value 40, we can do so by label:\n\ns['d']\n\nnp.int64(40)\n\n\nThis flexibility is powerful. Recall how rows in a DataFrame are also Series. Let’s revisit our airline DataFrame:\n\ndf.head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\nIf we extract the first row using .loc[0], we get:\n\nfirst_row = df.loc[0]\nfirst_row\n\ncarrier                   9E\nname       Endeavor Air Inc.\nName: 0, dtype: object\n\n\nHere, the index labels of the Series are the original column names:\n\nfirst_row['carrier']  # returns '9E'\n\n'9E'\n\n\nThis demonstrates how Series behave consistently whether they’re extracted from columns, rows, or created from scratch. Understanding indexing will help you fluently navigate, reshape, and analyze your data as we move forward.\n\nKnowledge Check\n\n\n\n\n\n\nNoneBuild Your Own Series\n\n\n\n\nCreate a Series with the following values: [33, 64, 77, 22, 51]. Assign it to temps.\nWhat are the index labels? What is the data type?\nChange the index to letters: ['a', 'b', 'c', 'd', 'e'].\nWhat value is associated with the label 'c'?\nReassign the index back to numbers. What’s one reason you might prefer named indexes over numbers?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#indexing-in-dataframes",
    "href": "08-dataframes.html#indexing-in-dataframes",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.5 Indexing in DataFrames",
    "text": "8.5 Indexing in DataFrames\n\nIt’s not just Series that have indexes! DataFrames have them too. Take a look at the carrier DataFrame again and note the bold numbers on the left.\n\ndf.head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\nThese numbers are an index, just like the one we saw on our example Series. And DataFrame indexes support similar functionality.\n\n# Our index is a range from 0 (inclusive) to 16 (exclusive).\ndf.index\n\nRangeIndex(start=0, stop=16, step=1)\n\n\nWhen loading in a DataFrame, the default index will always be 0 to N-1, where N is the number of rows in your DataFrame. This is called a RangeIndex. Selecting individual rows by their index can also be done with the .loc accessor.\n\n# Get the row at index 4 (the fifth row).\ndf.loc[4]\n\ncarrier                      DL\nname       Delta Air Lines Inc.\nName: 4, dtype: object\n\n\nAs with Series, DataFrames support reassigning their index. However, with DataFrames it often makes sense to change one of your columns into the index. This is analogous to a primary key in relational databases: a way to rapidly look up rows within a table.\nIn our case, maybe we will often use the carrier code (carrier) to look up the full name of the airline. In that case, it would make sense to set the carrier column as our index.\n\ndf = df.set_index('carrier')\ndf.head()\n\n\n\n\n\n\n\n\nname\n\n\ncarrier\n\n\n\n\n\n9E\nEndeavor Air Inc.\n\n\nAA\nAmerican Airlines Inc.\n\n\nAS\nAlaska Airlines Inc.\n\n\nB6\nJetBlue Airways\n\n\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\nNow the RangeIndex has been replaced with a more meaningful index, and it’s possible to look up rows of the table by passing a carrier code to the .loc accessor.\n\ndf.loc['AA']\n\nname    American Airlines Inc.\nName: AA, dtype: object\n\n\n\n\n\n\n\n\nPandas does not require that indexes have unique values (that is, no duplicates) although many relational databases do have that requirement of a primary key. This means that it is possible to create a non-unique index, but highly inadvisable. Having duplicate values in your index can cause unexpected results when you refer to rows by index – but multiple rows have that index. Don’t do it if you can help it!\n\n\n\nWhen starting to work with a DataFrame, it’s often a good idea to determine what column makes sense as your index and to set it immediately. This will make your code nicer – by letting you directly look up values with the index – and also make your selections and filters faster, because Pandas is optimized for operations by index. If you want to change the index of your DataFrame later, you can always reset_index (and then assign a new one).\n\ndf.head() # DataFrame with carrier as the index\n\n\n\n\n\n\n\n\nname\n\n\ncarrier\n\n\n\n\n\n9E\nEndeavor Air Inc.\n\n\nAA\nAmerican Airlines Inc.\n\n\nAS\nAlaska Airlines Inc.\n\n\nB6\nJetBlue Airways\n\n\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\n\ndf = df.reset_index() # resetting the index to be 0:n-1\ndf.head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvoid non-unique indexes — they can lead to ambiguous behavior!\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneWorking with DataFrame Indexes\n\n\n\n\nWhat kind of index does the airports DataFrame currently use? Use .index to explore.\nIs this a good index? Why or why not?\nSet the faa column as the new index using .set_index().\nUse .loc to look up the row for FAA code '4G0'. What is the altitude of this airport?\nReset the index. What happens to the faa column after you reset it?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#summary",
    "href": "08-dataframes.html#summary",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.6 Summary",
    "text": "8.6 Summary\nIn this chapter, you took your first deep dive into Pandas DataFrames — the central data structure for data analysis in Python. You learned that a DataFrame is essentially a table of data, where each column is a one-dimensional object called a Series. While Series and DataFrames are closely related, they behave differently and support different operations, and it’s important to recognize which you’re working with.\nYou also saw how to create Series objects manually and explored the concept of indexing — both in Series and in DataFrames. Indexing allows you to label and quickly access specific rows or columns, and gives structure to your data. We covered how to extract single rows and columns (which are treated as Series), how to interpret data types (dtype), and how to assign or reset index values.\nFinally, you learned that Pandas provides accessors like .loc for retrieving data by label, and that setting a meaningful index can make your code more readable and efficient. As we continue through the book, you’ll see more examples of these techniques in action, and you’ll build the intuition for when and how to apply them in your own data projects.\n\n\n\nConcept\nWhat it is\nExample\n\n\n\n\nDataFrame\n2D table of data\ndf.head()\n\n\nSeries\n1D column/row\ndf['carrier'], df.loc[0]\n\n\nIndex\nLabels for rows\ndf.set_index('carrier')\n\n\n\nIn the next chapter, you’ll begin working with real-world messy data — and the skills you learned here will make that much easier to manage.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#exercise-exploring-covid-19-data-in-colleges",
    "href": "08-dataframes.html#exercise-exploring-covid-19-data-in-colleges",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.7 Exercise: Exploring COVID-19 Data in Colleges",
    "text": "8.7 Exercise: Exploring COVID-19 Data in Colleges\nIn the previous chapter’s exercise, you imported COVID-19 data related to U.S. colleges and universities using the New York Times dataset. Let’s now build on that by exploring and interacting with the structure of the DataFrame.\nYou can access the data again from this link: https://github.com/nytimes/covid-19-data/blob/master/colleges/colleges.csv\n\n\n\n\n\n\nNoneStep 1: Load the data\n\n\n\n\n\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/nytimes/covid-19-data/master/colleges/colleges.csv\"\ncovid = pd.read_csv(url)\ncovid.head()\n\n\n\n\n\n\n\n\ndate\nstate\ncounty\ncity\nipeds_id\ncollege\ncases\ncases_2021\nnotes\n\n\n\n\n0\n2021-05-26\nAlabama\nMadison\nHuntsville\n100654\nAlabama A&M University\n41\nNaN\nNaN\n\n\n1\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100724\nAlabama State University\n2\nNaN\nNaN\n\n\n2\n2021-05-26\nAlabama\nLimestone\nAthens\n100812\nAthens State University\n45\n10.0\nNaN\n\n\n3\n2021-05-26\nAlabama\nLee\nAuburn\n100858\nAuburn University\n2742\n567.0\nNaN\n\n\n4\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100830\nAuburn University at Montgomery\n220\n80.0\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneStep 2: Working with the data\n\n\n\n\n\n\nWhat is the current shape of the DataFrame? How many rows and columns does it have?\nUse .info() to inspect the column names and data types. Are any of them unexpected?\nSelect the column containing cumulative cases (cases) and check its type and structure. Is it a Series or a DataFrame?\nTry the following covid['cases'].sum(). What does this output represent? Try some other summary statistics. We’ll cover more summary statistics and aggregation methods in the next few chapters.\n\n\n\n\n\n\n\n\n\n\nNoneStep 3: Set and reset the index\n\n\n\n\n\n\nWhat kind of index does the full covid DataFrame currently use?\nWould any of the existing columns make a better index? If so, which one and why?\nSet the ipeds_id column as the new index.\nUse .loc[] to look up the row for where ipeds_id equals 201885 and report:\n\nWhich university is this?\nWhat is the total number of cases?\n\nReset the index back to its original form.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html",
    "href": "09-subsetting.html",
    "title": "9  Subsetting Data",
    "section": "",
    "text": "Learning Objectives\nThis activity gives you a glimpse of a common task in data science: subsetting a dataset to focus on the most relevant information. Whether you’re analyzing flight records, home prices, or COVID case data, you’ll frequently need to extract specific rows and columns before you can analyze or visualize anything.\nIn this lesson, you’ll learn how to do this efficiently using Python and the pandas library — a skill that will save you time, reduce errors, and set the foundation for deeper analysis later.\nBy the end of this lesson, you’ll be able to:",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#prerequisites",
    "href": "09-subsetting.html#prerequisites",
    "title": "9  Subsetting Data",
    "section": "9.1 Prerequisites",
    "text": "9.1 Prerequisites\nTo illustrate selecting and filtering let’s go ahead and load the pandas library and import our planes data we’ve been using:\n\nimport pandas as pd\n\nplanes_df = pd.read_csv('../data/planes.csv')\nplanes_df.head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote📓 Follow Along in Colab!\n\n\n\nAs you read through this chapter, we encourage you to follow along using the companion notebook in Google Colab (or other editor of choice). This interactive notebook lets you run code examples covered in the chapter—and experiment with your own ideas.\n👉 Open the Subsetting Notebook in Colab.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#subsetting-dimensions",
    "href": "09-subsetting.html#subsetting-dimensions",
    "title": "9  Subsetting Data",
    "section": "9.2 Subsetting dimensions",
    "text": "9.2 Subsetting dimensions\n\nWe don’t always want all of the data in a DataFrame, so we need to take subsets of the DataFrame. In general, subsetting is extracting a small portion of a DataFrame – making the DataFrame smaller. Since the DataFrame is two-dimensional, there are two dimensions on which to subset.\nDimension 1: We may only want to consider certain variables. For example, we may only care about the year and engines variables:\n\nWe call this selecting columns/variables – this is similar to SQL’s SELECT or R’s dplyr package’s select().\nDimension 2: We may only want to consider certain cases. For example, we may only care about the cases where the manufacturer is Embraer.\n\nWe call this filtering or slicing – this is similar to SQL’s WHERE or R’s dplyr package’s filter() or slice(). And we can combine these two options to subset in both dimensions – the year and engines variables where the manufacturer is Embraer:\n\nIn the previous example, we want to do two things using planes_df:\n\nselect the year and engines variables\nfilter to cases where the manufacturer is Embraer\n\nBut we also want to return a new DataFrame – not just highlight certain cells. In other words, we want to turn this:\n\n\nCode\nplanes_df.head()\n\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nInto this:\n\n\nCode\nplanes_df.head().loc[planes_df['manufacturer'] == 'EMBRAER', ['year', 'engines']]\n\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n4\n2002.0\n2\n\n\n\n\n\n\n\nSo we really have a third need: return the resulting DataFrame so we can continue our analysis:\n\nselect the year and engines variables\nfilter to cases where the manufacturer is Embraer\nReturn a DataFrame to continue the analysis",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#subsetting-variables",
    "href": "09-subsetting.html#subsetting-variables",
    "title": "9  Subsetting Data",
    "section": "9.3 Subsetting variables",
    "text": "9.3 Subsetting variables\nRecall that the subsetting of variables/columns is called selecting variables/columns. In a simple example, we can select a single variable using bracket subsetting notation:\n\nplanes_df['year'].head()\n\n0    2004.0\n1    1998.0\n2    1999.0\n3    1999.0\n4    2002.0\nName: year, dtype: float64\n\n\nNotice the head() method also works on planes_df['year'] to return the first five elements.\n\n\n\n\n\n\nNonePop quiz!\n\n\n\nWhat is the data type of planes_df['year']?\n\n\nThis returns pandas.core.series.Series, referred to simply as a “Series”, rather than a DataFrame.\n\ntype(planes_df['year'])\n\npandas.core.series.Series\n\n\nThis is okay – the Series is a popular data structure in Python. Recall from a previous lesson:\n\nA Series is a one-dimensional data structure – this is similar to a Python list\nNote that all objects in a Series are usually of the same type (but this isn’t a strict requirement)\nEach DataFrame can be thought of as a list of equal-length Series (plus an Index)\n\n\n\n\n\n\nSeries can be useful, but for now, we are interested in returning a DataFrame rather than a series. We can select a single variable and return a DataFrame by still using bracket subsetting notation, but this time we will pass a list of variables names:\n\nplanes_df[['year']].head()\n\n\n\n\n\n\n\n\nyear\n\n\n\n\n0\n2004.0\n\n\n1\n1998.0\n\n\n2\n1999.0\n\n\n3\n1999.0\n\n\n4\n2002.0\n\n\n\n\n\n\n\nAnd we can see that we’ve returned a DataFrame:\n\ntype(planes_df[['year']].head())\n\npandas.core.frame.DataFrame\n\n\n\n\n\n\n\n\nNonePop quiz!\n\n\n\nWhat do you think is another advantage of passing a list?\n\n\nPassing a list into the bracket subsetting notation allows us to select multiple variables at once:\n\nplanes_df[['year', 'engines']].head()\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n1\n1998.0\n2\n\n\n2\n1999.0\n2\n\n\n3\n1999.0\n2\n\n\n4\n2002.0\n2\n\n\n\n\n\n\n\nIn another example, assume we are interested in the model of plane, number of seats and engine type:\n\nplanes_df[['model', 'seats', 'engine']].head()\n\n\n\n\n\n\n\n\nmodel\nseats\nengine\n\n\n\n\n0\nEMB-145XR\n55\nTurbo-fan\n\n\n1\nA320-214\n182\nTurbo-fan\n\n\n2\nA320-214\n182\nTurbo-fan\n\n\n3\nA320-214\n182\nTurbo-fan\n\n\n4\nEMB-145LR\n55\nTurbo-fan\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This\n\n\n\n\n______ is a common term for subsetting DataFrame variables.\nWhat type of object is a DataFrame column?\nWhat will be returned by the following code?\nplanes_df['type', 'model']",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#subsetting-rows",
    "href": "09-subsetting.html#subsetting-rows",
    "title": "9  Subsetting Data",
    "section": "9.4 Subsetting rows",
    "text": "9.4 Subsetting rows\nWhen we subset rows (aka cases, records, observations) we primarily use two names: slicing and filtering, but these are not the same:\n\nslicing, similar to row indexing, subsets observations by the value of the Index\nfiltering subsets observations using a conditional test\n\n\nSlicing rows\nRemember that all DataFrames have an Index:\n\nplanes_df.head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nWe can slice cases/rows using the values in the Index and bracket subsetting notation. It’s common practice to use .loc to slice cases/rows:\n\nplanes_df.loc[0:5]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n5\nN105UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that since this is not “indexing”, the last element is inclusive.\n\n\n\nWe can also pass a list of Index values:\n\nplanes_df.loc[[0, 2, 4, 6, 8]]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n6\nN107US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n8\nN109UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\nFiltering rows\nWe can filter rows using a logical sequence equal in length to the number of rows in the DataFrame.\nContinuing our example, assume we want to determine whether each case’s manufacturer is Embraer. We can use the manufacturer Series and a logical equivalency test to find the result for each row:\n\nplanes_df['manufacturer'] == 'EMBRAER'\n\n0        True\n1       False\n2       False\n3       False\n4        True\n        ...  \n3317    False\n3318    False\n3319    False\n3320    False\n3321    False\nName: manufacturer, Length: 3322, dtype: bool\n\n\nWe can use this resulting logical sequence to test filter cases – rows that are True will be returned while those that are False will be removed:\n\nplanes_df[planes_df['manufacturer'] == 'EMBRAER'].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n10\nN11106\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n11\nN11107\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n12\nN11109\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nThis also works with .loc:\n\nplanes_df.loc[planes_df['manufacturer'] == 'EMBRAER'].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n10\nN11106\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n11\nN11107\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n12\nN11109\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nAny conditional test can be used to filter DataFrame rows:\n\n# Filter observations where year is greater than 2002\nplanes_df.loc[planes_df['year'] &gt; 2002].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n15\nN11121\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n16\nN11127\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n17\nN11137\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n18\nN11140\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nAnd multiple conditional tests can be combined using logical operators:\n\n# Filter observations where year is greater than 2002 and less than 2007\nplanes_df.loc[(planes_df['year'] &gt; 2002) & (planes_df['year'] &lt; 2007)].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n15\nN11121\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n16\nN11127\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n17\nN11137\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n18\nN11140\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that each condition is wrapped in parentheses – this is required.\n\n\n\nOften, as your condition gets more complex, it can be easier to read if you separate out the condition:\n\ncond = (planes_df['year'] &gt; 2002) & (planes_df['year'] &lt; 2004)\nplanes_df.loc[cond].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n15\nN11121\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n16\nN11127\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n17\nN11137\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n18\nN11140\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n19\nN11150\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This\n\n\n\n\nWhat’s the difference between slicing cases and filtering cases?\nFill in the blanks to fix the following code to find planes that have more than three engines:\n planes_df.loc[______['______'] &gt; 3]",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#selecting-variables-and-filtering-rows",
    "href": "09-subsetting.html#selecting-variables-and-filtering-rows",
    "title": "9  Subsetting Data",
    "section": "9.5 Selecting variables and filtering rows",
    "text": "9.5 Selecting variables and filtering rows\nIf we want to select variables and filter cases at the same time, we have a few options:\n\nSequential operations\nSimultaneous operations\n\n\nSequential Operations\nWe can use what we’ve previously learned to select variables and filter cases in multiple steps:\n\nplanes_df_filtered = planes_df.loc[planes_df['manufacturer'] == 'EMBRAER']\nplanes_df_filtered_and_selected = planes_df_filtered[['year', 'engines']]\nplanes_df_filtered_and_selected.head()\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n4\n2002.0\n2\n\n\n10\n2002.0\n2\n\n\n11\n2002.0\n2\n\n\n12\n2002.0\n2\n\n\n\n\n\n\n\nThis is a good way to learn how to select and filter independently, and it also reads very clearly.\n\n\nSimultaneous operations\nHowever, we can also do both selecting and filtering in a single step with .loc:\n\nplanes_df.loc[planes_df['manufacturer'] == 'EMBRAER', ['year', 'engines']].head()\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n4\n2002.0\n2\n\n\n10\n2002.0\n2\n\n\n11\n2002.0\n2\n\n\n12\n2002.0\n2\n\n\n\n\n\n\n\nThis option is more succinct and also reduces programming time. As before, as your filtering and selecting conditions get longer and/or more complex, it can make it easier to read to break it up into separate lines:\n\nrows = planes_df['manufacturer'] == 'EMBRAER'\ncols = ['year', 'engines']\nplanes_df.loc[rows, cols].head()\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n4\n2002.0\n2\n\n\n10\n2002.0\n2\n\n\n11\n2002.0\n2\n\n\n12\n2002.0\n2\n\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This\n\n\n\nSubset planes_df to only include planes made by Boeing and only return the seats and model variables.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#views-vs-copies",
    "href": "09-subsetting.html#views-vs-copies",
    "title": "9  Subsetting Data",
    "section": "9.6 Views vs copies",
    "text": "9.6 Views vs copies\nOne thing to be aware of, as you will likely experience it eventually, is the concept of returning a view (“looking” at a part of an existing object) versus a copy (making a new copy of the object in memory). This can be a bit abstract and even this section in the Pandas docs states “…it’s very hard to predict whether it will return a view or a copy.”\nThe main takeaway is that the most common warning you’ll encounter in Pandas is the SettingWithCopyWarning; Pandas raises it as a warning that you might not be doing what you think you’re doing or because the operation you are performing may behave unpredictably.\nLet’s look at an example. Say the number of seats on this particular plane was recorded incorrectly. Instead of 55 seats it should actually be 60 seats.\n\ntailnum_of_interest = planes_df['tailnum'] == 'N10156'\nplanes_df[tailnum_of_interest]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nInstead of using .iloc, we could actually filter and select this element in our DataFrame with the following bracket notation.\n\nplanes_df[tailnum_of_interest]['seats']\n\n0    55\nName: seats, dtype: int64\n\n\n\nplanes_df[tailnum_of_interest]['seats'] = 60\n\n/tmp/ipykernel_12587/2190037627.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  planes_df[tailnum_of_interest]['seats'] = 60\n\n\nSo what’s going on? Did our DataFrame get changed?\n\nplanes_df[tailnum_of_interest]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nNo it didn’t, even though you probably thought it did. What happened above is that planes_df[tailnum_of_interest]['seats'] was executed first and returned a copy of the DataFrame, which is an entirely different object. We can confirm by using id():\n\nprint(f\"The id of the original dataframe is: {id(planes_df)}\")\nprint(f\" The id of the indexed dataframe is: {id(planes_df[tailnum_of_interest])}\")\n\nThe id of the original dataframe is: 140444060292432\n The id of the indexed dataframe is: 140444061085248\n\n\nWe then tried to set a value on this new object by appending ['seats'] = 60. Pandas is warning us that we are doing that operation on a copy of the original dataframe, which is probably not what we want. To fix this, you need to index in a single go, using .loc[] for example:\n\nplanes_df.loc[tailnum_of_interest, 'seats'] = 60\n\nNo error this time! And let’s confirm the change:\n\nplanes_df[tailnum_of_interest]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n60\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe concept of views and copies is confusing and you can read more about it here.\nBut realize, this behavior is changing in pandas 3.0. A new system called Copy-on-Write will become the default, and it will prevent chained indexing from working at all — meaning instead of getting the SettingWithCopyWarning warning, pandas will simply raise an error.\nRegardless, always use .loc[] for combined filtering and selecting!",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#summary",
    "href": "09-subsetting.html#summary",
    "title": "9  Subsetting Data",
    "section": "9.7 Summary",
    "text": "9.7 Summary\nIn this chapter, you learned how to zoom in on the parts of a DataFrame that matter most. Whether you’re interested in just a few variables or a specific set of cases, being able to subset your data is a critical first step in any analysis.\nWe started by giving you a real-world task in Excel or Google Sheets — find aircraft built by Embraer in 2004 or later. That hands-on activity highlighted a common problem: manual filtering doesn’t scale. That’s where Python and pandas come in.\nIn this chapter, you learned how to use pandas for:\n\nSelecting columns using single or multiple variable names\nSlicing rows by index position\nFiltering rows using conditional logic\nCombining selection and filtering with .loc[] for efficient subsetting\n\n\n🧾 Quick Reference: Subsetting Techniques\n\n\n\n\n\n\n\n\nTask\nSyntax Example\nOutput Type\n\n\n\n\nSelect one column\ndf[\"col\"]\nSeries\n\n\nSelect multiple columns\ndf[[\"col1\", \"col2\"]]\nDataFrame\n\n\nSlice rows by index\ndf.loc[0:4]\nDataFrame\n\n\nFilter rows by condition\ndf[df[\"year\"] &gt; 2000]\nDataFrame\n\n\nCombine filter + select\ndf.loc[df[\"year\"] &gt; 2000, [\"col1\", \"col2\"]]\nDataFrame\n\n\nBest practice for assignment\ndf.loc[cond, \"col\"] = value\nSafe, avoids warnings\n\n\n\n\n\n🔁 Revisit the Challenge\nNow that you’ve learned how to subset data using pandas, go back to the original question:\n\nWhich aircraft were manufactured by Embraer in 2004 or later?\n\nThis time, solve it using Python instead of a spreadsheet. Use what you’ve learned in this chapter — filtering rows by conditions, and selecting only the columns you need — to create a clean, focused DataFrame.\nWhen you’re done, try to answer:\n\nHow many rows matched the condition?\nWhat columns did you choose to keep?\nCould you reuse your code later for different conditions?\n\nThis is how data scientists work: writing reusable, scalable code to extract insights from large datasets.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#exercise-subsetting-covid-college-data",
    "href": "09-subsetting.html#exercise-subsetting-covid-college-data",
    "title": "9  Subsetting Data",
    "section": "9.8 Exercise: Subsetting COVID College Data",
    "text": "9.8 Exercise: Subsetting COVID College Data\nIn this exercise, you’ll apply what you learned to subset the New York Times college COVID-19 dataset. The dataset tracks COVID cases reported by colleges across the U.S.\n\n📂 Download the data from this GitHub link or load it directly from a local copy if provided.\n\n\n\n\n\n\n\nNoneStep 1: Load the Data\n\n\n\n\n\n\nimport pandas as pd\n\ndata_url = \"https://raw.githubusercontent.com/nytimes/covid-19-data/refs/heads/master/colleges/colleges.csv\"\ncollege_df = pd.read_csv(data_url)\ncollege_df.head()\n\n\n\n\n\n\n\n\ndate\nstate\ncounty\ncity\nipeds_id\ncollege\ncases\ncases_2021\nnotes\n\n\n\n\n0\n2021-05-26\nAlabama\nMadison\nHuntsville\n100654\nAlabama A&M University\n41\nNaN\nNaN\n\n\n1\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100724\nAlabama State University\n2\nNaN\nNaN\n\n\n2\n2021-05-26\nAlabama\nLimestone\nAthens\n100812\nAthens State University\n45\n10.0\nNaN\n\n\n3\n2021-05-26\nAlabama\nLee\nAuburn\n100858\nAuburn University\n2742\n567.0\nNaN\n\n\n4\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100830\nAuburn University at Montgomery\n220\n80.0\nNaN\n\n\n\n\n\n\n\nThis dataset includes many columns, but for this exercise, we’re going to focus only on:\n\nstate\ncity\ncollege\ncases\n\n\n\n\n\n\n\n\n\n\nNoneStep 2: Subsetting Practice\n\n\n\n\n\n\nSelect only the columns: state, city, college, and cases.\nFilter the dataset to show only colleges in Ohio (state == \"OH\") that reported more than 100 cases.\nHow many Ohio colleges reported more than 100 cases? (Hint: Use .shape or len())\n\n\n\n\n\n\n\n\n\n\nNoneStep 3: Dig Into a Specific School\n\n\n\n\n\n\nFilter the dataset to find records where the college is “University of Cincinnati” (case sensitive).\nHow many cases were reported by the University of Cincinnati?\n\n\n\n\n\n\n\n\n\n\nNoneMake It Dynamic\n\n\n\n\n\nTry making your filtering logic more flexible by defining parameters at the top of your notebook:\nmy_state = \"OH\"\nthreshold = 100\nThen write code that will:\n\nFilter for all colleges in my_state with cases greater than threshold\nReturn only the college and cases columns\n\nTest your code with a few different states and thresholds. Can you reuse it to answer different questions (i.e. How many colleges in California reported more than 500 Covid cases?)",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html",
    "href": "10-manipulating-data.html",
    "title": "10  Manipulating Data",
    "section": "",
    "text": "The Ames Housing Data\nIn our previous chapter, we explored how to access, index, and subset data from a pandas DataFrame. These are essential skills for understanding and navigating real-world datasets. Now we take the next step: learning how to manipulate data.\nAs a data analyst or scientist, you will spend much of your time transforming raw data into something clean, interpretable, and analysis-ready. This chapter provides the foundation for doing just that. You’ll learn to rename columns, create new variables, deal with missing values, and apply functions to your data — all of which are fundamental skills in the data science workflow.\nBy the end of this chapter, you will be able to:\nYou first encountered the Ames Housing dataset back in Chapter 7, where you were challenged with the task of analyzing raw data for the Ames, Iowa housing market. In this chapter, we’ll return to the Ames data as our primary example to learn how to manipulate and prepare real-world data for analysis.\nBefore diving into the new concepts, take a few minutes to reacquaint yourself with the data:\nLet’s begin by loading the dataset and inspecting the first few rows:\nimport pandas as pd\n\names = pd.read_csv('../data/ames_raw.csv')\names.head()\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 82 columns\nWe’ll use this dataset throughout the chapter to demonstrate how to rename columns, compute new variables, handle missing data, and apply transformations — all crucial steps in cleaning and preparing data for analysis and modeling.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#renaming-columns",
    "href": "10-manipulating-data.html#renaming-columns",
    "title": "10  Manipulating Data",
    "section": "10.1 Renaming Columns",
    "text": "10.1 Renaming Columns\nOne of the first things you’ll often do when working with a new dataset is clean up the column names. Column names might contain spaces, inconsistent capitalization, or other formatting quirks that make them harder to work with in code. In this section, we’ll walk through a few ways to rename columns in a DataFrame using the Ames Housing dataset.\nLet’s start by looking at the current column names:\n\names.columns\n\nIndex(['Order', 'PID', 'MS SubClass', 'MS Zoning', 'Lot Frontage', 'Lot Area',\n       'Street', 'Alley', 'Lot Shape', 'Land Contour', 'Utilities',\n       'Lot Config', 'Land Slope', 'Neighborhood', 'Condition 1',\n       'Condition 2', 'Bldg Type', 'House Style', 'Overall Qual',\n       'Overall Cond', 'Year Built', 'Year Remod/Add', 'Roof Style',\n       'Roof Matl', 'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type',\n       'Mas Vnr Area', 'Exter Qual', 'Exter Cond', 'Foundation', 'Bsmt Qual',\n       'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin SF 1',\n       'BsmtFin Type 2', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Total Bsmt SF',\n       'Heating', 'Heating QC', 'Central Air', 'Electrical', '1st Flr SF',\n       '2nd Flr SF', 'Low Qual Fin SF', 'Gr Liv Area', 'Bsmt Full Bath',\n       'Bsmt Half Bath', 'Full Bath', 'Half Bath', 'Bedroom AbvGr',\n       'Kitchen AbvGr', 'Kitchen Qual', 'TotRms AbvGrd', 'Functional',\n       'Fireplaces', 'Fireplace Qu', 'Garage Type', 'Garage Yr Blt',\n       'Garage Finish', 'Garage Cars', 'Garage Area', 'Garage Qual',\n       'Garage Cond', 'Paved Drive', 'Wood Deck SF', 'Open Porch SF',\n       'Enclosed Porch', '3Ssn Porch', 'Screen Porch', 'Pool Area', 'Pool QC',\n       'Fence', 'Misc Feature', 'Misc Val', 'Mo Sold', 'Yr Sold', 'Sale Type',\n       'Sale Condition', 'SalePrice'],\n      dtype='object')\n\n\nYou might notice that some of these column names contain spaces or uppercase letters, such as \"MS SubClass\" and \"MS Zoning\". These formatting issues can be inconvenient when writing code — especially if you’re trying to access a column using dot notation (e.g., df.column_name) or when using string methods.\n\nRenaming Specific Columns\nWe can rename one or more columns using the .rename() method. This method accepts a dictionary where the keys are the original column names and the values are the new names you’d like to assign.\n\names.rename(columns={'MS SubClass': 'ms_subclass', 'MS Zoning': 'ms_zoning'})\n\n\n\n\n\n\n\n\nOrder\nPID\nms_subclass\nms_zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\n2926\n923275080\n80\nRL\n37.0\n7937\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nGdPrv\nNaN\n0\n3\n2006\nWD\nNormal\n142500\n\n\n2926\n2927\n923276100\n20\nRL\nNaN\n8885\nPave\nNaN\nIR1\nLow\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2006\nWD\nNormal\n131000\n\n\n2927\n2928\n923400125\n85\nRL\n62.0\n10441\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nShed\n700\n7\n2006\nWD\nNormal\n132000\n\n\n2928\n2929\n924100070\n20\nRL\n77.0\n10010\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2006\nWD\nNormal\n170000\n\n\n2929\n2930\n924151050\n60\nRL\n74.0\n9627\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n11\n2006\nWD\nNormal\n188000\n\n\n\n\n2930 rows × 82 columns\n\n\n\nThis command runs without error; and if we check out our data (below) nothing seems different. Why?\n\names.head(3)\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n\n\n3 rows × 82 columns\n\n\n\nThat’s because .rename() returns a new DataFrame with the updated names, but it does not modify the original DataFrame unless you explicitly tell it to.\nThere are two common ways to make these changes permanent:\n\nUse the inplace=True argument\nReassign the modified DataFrame to the same variable\n\nThe Pandas development team recommends the second approach (reassigning) for most use cases, as it leads to clearer and more predictable code.\n\names = ames.rename(columns={'MS SubClass': 'ms_subclass', 'MS Zoning': 'ms_zoning'})\names.head()\n\n\n\n\n\n\n\n\nOrder\nPID\nms_subclass\nms_zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 82 columns\n\n\n\n\n\n\n\n\n\nAlways include columns= when using .rename(). If you don’t, Pandas will assume you are renaming index values instead of column names — and it won’t raise a warning or error if you make this mistake.\n\n\n\n\n\nRenaming Many Columns at Once\nUsing .rename() works well for renaming one or two columns. But if you want to rename many columns — such as applying a consistent format across the entire DataFrame — it’s more efficient to use the .columns attribute and vectorized string methods.\nPandas provides powerful tools for working with strings through the .str accessor. For example, we can convert all column names to lowercase like this:\n\names.columns.str.lower()\n\nIndex(['order', 'pid', 'ms_subclass', 'ms_zoning', 'lot frontage', 'lot area',\n       'street', 'alley', 'lot shape', 'land contour', 'utilities',\n       'lot config', 'land slope', 'neighborhood', 'condition 1',\n       'condition 2', 'bldg type', 'house style', 'overall qual',\n       'overall cond', 'year built', 'year remod/add', 'roof style',\n       'roof matl', 'exterior 1st', 'exterior 2nd', 'mas vnr type',\n       'mas vnr area', 'exter qual', 'exter cond', 'foundation', 'bsmt qual',\n       'bsmt cond', 'bsmt exposure', 'bsmtfin type 1', 'bsmtfin sf 1',\n       'bsmtfin type 2', 'bsmtfin sf 2', 'bsmt unf sf', 'total bsmt sf',\n       'heating', 'heating qc', 'central air', 'electrical', '1st flr sf',\n       '2nd flr sf', 'low qual fin sf', 'gr liv area', 'bsmt full bath',\n       'bsmt half bath', 'full bath', 'half bath', 'bedroom abvgr',\n       'kitchen abvgr', 'kitchen qual', 'totrms abvgrd', 'functional',\n       'fireplaces', 'fireplace qu', 'garage type', 'garage yr blt',\n       'garage finish', 'garage cars', 'garage area', 'garage qual',\n       'garage cond', 'paved drive', 'wood deck sf', 'open porch sf',\n       'enclosed porch', '3ssn porch', 'screen porch', 'pool area', 'pool qc',\n       'fence', 'misc feature', 'misc val', 'mo sold', 'yr sold', 'sale type',\n       'sale condition', 'saleprice'],\n      dtype='object')\n\n\nOr, we can chain multiple string methods together to standardize our column names — converting them to lowercase and replacing all spaces with underscores:\n\names.columns = ames.columns.str.lower().str.replace(\" \", \"_\")\names.head()\n\n\n\n\n\n\n\n\norder\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\n...\npool_area\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 82 columns\n\n\n\nThis makes your column names easier to type, more consistent, and less prone to errors in your code.\n\n\n\n\n\n\nYou can explore other string operations that work with .str by checking out the Pandas string methods documentation.\n\n\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry This!\n\n\n\nLet’s practice cleaning up messy column names using the techniques from this section. Below is a small example dataset with inconsistent column names:\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'First Name': ['Alice', 'Bob', 'Charlie'],\n    'Last-Name': ['Smith', 'Jones', 'Brown'],\n    'AGE ': [25, 30, 22],\n    'Email Address': ['alice@example.com', 'bob@example.com', 'charlie@example.com']\n})\nYour task:\n\nPrint the current column names. What formatting issues do you see?\nClean the column names by doing the following:\n\nConvert all column names to lowercase\nReplace any spaces or hyphens with underscores\nStrip any leading or trailing whitespace\n\nAssign the cleaned column names back to the DataFrame.\nPrint the updated column names and confirm the changes.\n\nHint: .columns.str.lower(), .str.replace(), and .str.strip() will come in handy here!",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#performing-calculations-with-columns",
    "href": "10-manipulating-data.html#performing-calculations-with-columns",
    "title": "10  Manipulating Data",
    "section": "10.2 Performing Calculations with Columns",
    "text": "10.2 Performing Calculations with Columns\nOnce your data is loaded and your columns are cleaned up, the next common task is to perform calculations on your data. This might include creating new variables, transforming existing values, or applying arithmetic operations across columns.\nLet’s begin by focusing on the saleprice column in the Ames Housing dataset. This column records the sale price of each home in dollars. For example:\n\nsale_price = ames['saleprice']\nsale_price\n\n0       215000\n1       105000\n2       172000\n3       244000\n4       189900\n         ...  \n2925    142500\n2926    131000\n2927    132000\n2928    170000\n2929    188000\nName: saleprice, Length: 2930, dtype: int64\n\n\nThese numbers are fairly large — often six digits long. In many analyses or visualizations, it can be helpful to express values in thousands of dollars instead of raw dollar amounts.\nTo convert the sale price to thousands, we simply divide each value by 1,000:\n\nsale_price_k = sale_price / 1000\nsale_price_k\n\n0       215.0\n1       105.0\n2       172.0\n3       244.0\n4       189.9\n        ...  \n2925    142.5\n2926    131.0\n2927    132.0\n2928    170.0\n2929    188.0\nName: saleprice, Length: 2930, dtype: float64\n\n\nThis results in a new Series where each home’s price is now shown in thousands. For instance, a home that originally sold for $215,000 is now shown as 215.0.\nAt this point, sale_price_k is a new object that exists separately from the ames DataFrame. In the next section, we’ll learn how to add this new variable as a column in our DataFrame so we can use it in further analysis.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#adding-and-removing-columns",
    "href": "10-manipulating-data.html#adding-and-removing-columns",
    "title": "10  Manipulating Data",
    "section": "10.3 Adding and Removing Columns",
    "text": "10.3 Adding and Removing Columns\nOnce you’ve created a new variable — like sale_price_k in the previous section — you’ll often want to add it to your existing DataFrame so it becomes part of the dataset you’re working with.\n\nAdding Columns\nIn pandas, you can add a new column to a DataFrame using assignment syntax:\n# example syntax\ndf['new_column_name'] = new_column_series\nLet’s add the sale_price_k series (which represents sale prices in thousands) to the ames DataFrame:\n\names['sale_price_k'] = sale_price_k\names.head()\n\n\n\n\n\n\n\n\norder\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\n...\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nsale_price_k\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n215.0\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n105.0\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n172.0\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n244.0\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n189.9\n\n\n\n\n5 rows × 83 columns\n\n\n\nNow, you’ll see that a new column called \"sale_price_k\" appears at the end of the DataFrame.\n\n\n\n\n\n\nNotice how the column name ('sale_price_k') is placed in quotes inside the brackets on the left-hand side, while the Series providing the data goes on the right-hand side without quotes or brackets.\n\n\n\nThis entire process can be done in a single step, without creating an intermediate variable:\n\names['sale_price_k'] = ames['saleprice'] / 1000\n\nThis kind of operation is common in data science. What we’re doing here is applying vectorized math — performing arithmetic between a Series (a vector of values) and a scalar (a single constant value).\nHere are a few more examples:\n\n# Subtracting a scalar from a Series\n(ames['saleprice'] - 12).head()\n\n0    214988\n1    104988\n2    171988\n3    243988\n4    189888\nName: saleprice, dtype: int64\n\n\n\n# Multiplying a Series by a scalar\n(ames['saleprice'] * 10).head()\n\n0    2150000\n1    1050000\n2    1720000\n3    2440000\n4    1899000\nName: saleprice, dtype: int64\n\n\n\n# Raising a Series to a power\n(ames['saleprice'] ** 2).head()\n\n0    46225000000\n1    11025000000\n2    29584000000\n3    59536000000\n4    36062010000\nName: saleprice, dtype: int64\n\n\nVectorized operations like these are fast, efficient, and more readable than writing explicit loops.\n\n\nRemoving Columns\nJust as easily as we can add columns, we can also remove them. This is helpful when a column is no longer needed or was created only temporarily.\nTo drop one or more columns from a DataFrame, use the .drop() method with the columns= argument:\n\names = ames.drop(columns=['order', 'sale_price_k'])\names.head()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\npool_area\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 81 columns\n\n\n\nThis removes the \"order\" and \"sale_price_k\" columns from the DataFrame. Remember that most pandas methods return a new DataFrame by default, so you’ll need to reassign the result back to ames (or another variable) to make the change permanent.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneCreate a utility_space variable\n\n\n\n\nCreate a new column utility_space that is 1/5 of the above ground living space (gr_liv_area).\nYou will get fractional output with step #1. See if you can figure out how to round this output to the nearest integer.\nNow remove this column from your DataFrame",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#overwriting-columns",
    "href": "10-manipulating-data.html#overwriting-columns",
    "title": "10  Manipulating Data",
    "section": "10.4 Overwriting columns",
    "text": "10.4 Overwriting columns\nWhat if we discovered a systematic error in our data? Perhaps we find out that the “lot_area” column is not entirely accurate because the recording process includes an extra 50 square feet for every property. We could create a new column, “real_lot_area” but we’re not going to need the original “lot_area” column, and leaving it could cause confusion for others looking at our data.\nA better solution would be to replace the original column with the new, recalculated, values. We can do so using the same syntax as for creating a new column.\n\n# Subtract 50 from lot area, and then overwrite the original data.\names['lot_area'] = ames['lot_area'] - 50\names.head()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\npool_area\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31720\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n526350040\n20\nRH\n80.0\n11572\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n526351010\n20\nRL\n81.0\n14217\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n526353030\n20\nRL\n93.0\n11110\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n527105010\n60\nRL\n74.0\n13780\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 81 columns",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#calculating-with-multiple-columns",
    "href": "10-manipulating-data.html#calculating-with-multiple-columns",
    "title": "10  Manipulating Data",
    "section": "10.5 Calculating with Multiple Columns",
    "text": "10.5 Calculating with Multiple Columns\nUp to this point, we’ve focused on performing calculations between a column (Series) and a scalar — for example, dividing every value in saleprice by 1,000. But pandas also allows you to perform operations between columns — this is known as vector-vector arithmetic.\nLet’s look at an example where we calculate a new metric: price per square foot. We can compute this by dividing the sale price of each home by its above-ground living area (gr_liv_area):\n\nprice_per_sqft = ames['saleprice'] / ames['gr_liv_area']\nprice_per_sqft.head()\n\n0    129.830918\n1    117.187500\n2    129.420617\n3    115.639810\n4    116.574586\ndtype: float64\n\n\nNow that we’ve computed the new values, let’s add them to our DataFrame as a new column:\n\names['price_per_sqft'] = price_per_sqft\names.head()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31720\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n129.830918\n\n\n1\n526350040\n20\nRH\n80.0\n11572\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n117.187500\n\n\n2\n526351010\n20\nRL\n81.0\n14217\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n129.420617\n\n\n3\n526353030\n20\nRL\n93.0\n11110\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n115.639810\n\n\n4\n527105010\n60\nRL\n74.0\n13780\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n116.574586\n\n\n\n\n5 rows × 82 columns\n\n\n\nAs before, you could write this as a one-liner:\n\names['price_per_sqft'] = ames['saleprice'] / ames['gr_liv_area']\n\n\nCombining Multiple Operations\nYou can also combine multiple columns and scalars in more complex expressions. For example, the following line combines three columns and a constant:\n\names['nonsense'] = (ames['yr_sold'] + 12) * ames['gr_liv_area'] + ames['lot_area'] - 50\names.head()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\nnonsense\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31720\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n129.830918\n3380102\n\n\n1\n526350040\n20\nRH\n80.0\n11572\nPave\nNaN\nReg\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n117.187500\n1823234\n\n\n2\n526351010\n20\nRL\n81.0\n14217\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n129.420617\n2701405\n\n\n3\n526353030\n20\nRL\n93.0\n11110\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n115.639810\n4277480\n\n\n4\n527105010\n60\nRL\n74.0\n13780\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n116.574586\n3307568\n\n\n\n\n5 rows × 83 columns\n\n\n\nThis creates a column called \"nonsense\" using a mix of vector-vector and vector-scalar operations. While this particular example isn’t meaningful analytically, it shows how you can chain together multiple operations in a single expression.\nIn practice, you’ll often calculate new variables using a combination of existing columns — for example, calculating cost efficiency, total square footage, or ratios between two quantities. Being comfortable with these kinds of operations is essential for building features and preparing data for analysis or modeling.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneCreate a price_per_total_sqft variable\n\n\n\nCreate a new column price_per_total_sqft that is saleprice divided by the sum of gr_liv_area, total_bsmt_sf, wood_deck_sf, open_porch_sf.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#working-with-string-columns",
    "href": "10-manipulating-data.html#working-with-string-columns",
    "title": "10  Manipulating Data",
    "section": "10.6 Working with String Columns",
    "text": "10.6 Working with String Columns\nSo far, we’ve focused on numeric calculations — things like dividing, multiplying, and creating new variables based on numbers. But many datasets also contain non-numeric values, such as names, categories, or descriptive labels.\nIn pandas, string data is stored as object or string type columns, and you can perform operations on them just like you would with numbers. This is especially useful for cleaning, formatting, or combining text.\n\nString Concatenation\nA common operation with string data is concatenation — combining multiple strings together. For example, suppose we want to create a descriptive sentence using the neighborhood and sale condition of each home in the Ames dataset:\n\n'Home in ' + ames['neighborhood'] + ' neighborhood sold under ' + ames['sale_condition'] + ' condition'\n\n0       Home in NAmes neighborhood sold under Normal c...\n1       Home in NAmes neighborhood sold under Normal c...\n2       Home in NAmes neighborhood sold under Normal c...\n3       Home in NAmes neighborhood sold under Normal c...\n4       Home in Gilbert neighborhood sold under Normal...\n                              ...                        \n2925    Home in Mitchel neighborhood sold under Normal...\n2926    Home in Mitchel neighborhood sold under Normal...\n2927    Home in Mitchel neighborhood sold under Normal...\n2928    Home in Mitchel neighborhood sold under Normal...\n2929    Home in Mitchel neighborhood sold under Normal...\nLength: 2930, dtype: object\n\n\nThis works just like string addition in Python. Each piece of text is combined row by row across the DataFrame to generate a new sentence for each observation.\n\n\nString Methods with .str\nFor more advanced string operations, pandas provides a powerful set of tools through the .str accessor. This gives you access to many string-specific methods like .lower(), .replace(), .len(), and more.\nHere are a few examples:\n\n# Count the number of characters in each neighborhood name\names['neighborhood'].str.len()\n\n0       5\n1       5\n2       5\n3       5\n4       7\n       ..\n2925    7\n2926    7\n2927    7\n2928    7\n2929    7\nName: neighborhood, Length: 2930, dtype: int64\n\n\n\n# Standardize the format of garage type labels\names['garage_type'].str.lower().str.replace('tchd', 'tached')\n\n0       attached\n1       attached\n2       attached\n3       attached\n4       attached\n          ...   \n2925    detached\n2926    attached\n2927         NaN\n2928    attached\n2929    attached\nName: garage_type, Length: 2930, dtype: object\n\n\nThese methods are especially helpful when cleaning messy or inconsistent text data — for example, fixing capitalization, removing whitespace, or replacing substrings.\n\n\n\n\n\n\nIn this chapter, we’ve only scratched the surface of working with non-numeric data. In later chapters, we’ll take a deeper look at how to clean, transform, and analyze string values, as well as how to work with date and time data — including parsing timestamps, extracting components like month and day, and calculating time differences.\nFor now, if you want to dig into working with string columns some more, it’s worth exploring the official Pandas documentation on string methods to see the full range of capabilities.\n\n\n\nWhether you’re formatting text for a report, cleaning up inconsistent labels, or preparing inputs for machine learning models, working with string columns is a valuable part of your data wrangling skill set.\n\names\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\nnonsense\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31720\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n129.830918\n3380102\n\n\n1\n526350040\n20\nRH\n80.0\n11572\nPave\nNaN\nReg\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n117.187500\n1823234\n\n\n2\n526351010\n20\nRL\n81.0\n14217\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n129.420617\n2701405\n\n\n3\n526353030\n20\nRL\n93.0\n11110\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n115.639810\n4277480\n\n\n4\n527105010\n60\nRL\n74.0\n13780\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n116.574586\n3307568\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\n923275080\n80\nRL\n37.0\n7887\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nGdPrv\nNaN\n0\n3\n2006\nWD\nNormal\n142500\n142.073779\n2031891\n\n\n2926\n923276100\n20\nRL\nNaN\n8835\nPave\nNaN\nIR1\nLow\nAllPub\n...\nMnPrv\nNaN\n0\n6\n2006\nWD\nNormal\n131000\n145.232816\n1829021\n\n\n2927\n923400125\n85\nRL\n62.0\n10391\nPave\nNaN\nReg\nLvl\nAllPub\n...\nMnPrv\nShed\n700\n7\n2006\nWD\nNormal\n132000\n136.082474\n1967801\n\n\n2928\n924100070\n20\nRL\n77.0\n9960\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2006\nWD\nNormal\n170000\n122.390209\n2812912\n\n\n2929\n924151050\n60\nRL\n74.0\n9577\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n11\n2006\nWD\nNormal\n188000\n94.000000\n4045527\n\n\n\n\n2930 rows × 83 columns",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#more-complex-column-manipulation",
    "href": "10-manipulating-data.html#more-complex-column-manipulation",
    "title": "10  Manipulating Data",
    "section": "10.7 More Complex Column Manipulation",
    "text": "10.7 More Complex Column Manipulation\nAs you become more comfortable working with individual columns, you’ll often find yourself needing to do more than basic math. In this section, we’ll cover a few additional, common column operations:\n\nReplacing values using a mapping\nIdentifying and handling missing values\nApplying custom functions\n\nThese are core techniques that will serve you in any data cleaning or feature engineering workflow.\n\nReplacing Values\nOne fairly common situation in data wrangling is needing to convert one set of values to another, where there is a one-to-one correspondence between the values currently in the column and the new values that should replace them. This operation can be described as “mapping one set of values to another”.\nLet’s look at an example of this. In our Ames data the month sold is represented numerically:\n\names['mo_sold'].head()\n\n0    5\n1    6\n2    6\n3    4\n4    3\nName: mo_sold, dtype: int64\n\n\nSuppose we want to change this so that values are represented by the month name:\n\n1 = ‘Jan’\n2 = ‘Feb’\n…\n12 = ‘Dec’\n\nWe can express this mapping of old values to new values using a Python dictionary.\n\n# Only specify the values we want to replace; don't include the ones that should stay the same.\nvalue_mapping = {\n    1: 'Jan',\n    2: 'Feb',\n    3: 'Mar',\n    4: 'Apr',\n    5: 'May',\n    6: 'Jun',\n    7: 'Jul',\n    8: 'Aug',\n    9: 'Sep',\n    10: 'Oct',\n    11: 'Nov',\n    12: 'Dec'\n    }\n\nPandas provides a handy method on Series, .replace, that accepts this value mapping and updates the Series accordingly. We can use it to recode our values.\n\names['mo_sold'].replace(value_mapping).head()\n\n0    May\n1    Jun\n2    Jun\n3    Apr\n4    Mar\nName: mo_sold, dtype: object\n\n\n\n\n\n\n\n\nIf you are a SQL user, this workflow may look familiar to you; it’s quite similar to a CASE WHEN statement in SQL.\n\n\n\n\n\nMissing values\nIn real-world datasets, missing values are common. In pandas, these are usually represented as NaN (Not a Number).\nTo detect missing values in a DataFrame, use .isnull(). This returns a DataFrame of the same shape with True where values are missing:\n\names.isnull()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\nnonsense\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2926\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2927\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2928\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2929\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n2930 rows × 83 columns\n\n\n\nWe can use this to easily compute the total number of missing values in each column:\n\names.isnull().sum()\n\npid                 0\nms_subclass         0\nms_zoning           0\nlot_frontage      490\nlot_area            0\n                 ... \nsale_type           0\nsale_condition      0\nsaleprice           0\nprice_per_sqft      0\nnonsense            0\nLength: 83, dtype: int64\n\n\nRecall we also get this information with .info(). Actually, we get the inverse as .info() tells us how many non-null values exist in each column.\n\names.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2930 entries, 0 to 2929\nData columns (total 83 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   pid              2930 non-null   int64  \n 1   ms_subclass      2930 non-null   int64  \n 2   ms_zoning        2930 non-null   object \n 3   lot_frontage     2440 non-null   float64\n 4   lot_area         2930 non-null   int64  \n 5   street           2930 non-null   object \n 6   alley            198 non-null    object \n 7   lot_shape        2930 non-null   object \n 8   land_contour     2930 non-null   object \n 9   utilities        2930 non-null   object \n 10  lot_config       2930 non-null   object \n 11  land_slope       2930 non-null   object \n 12  neighborhood     2930 non-null   object \n 13  condition_1      2930 non-null   object \n 14  condition_2      2930 non-null   object \n 15  bldg_type        2930 non-null   object \n 16  house_style      2930 non-null   object \n 17  overall_qual     2930 non-null   int64  \n 18  overall_cond     2930 non-null   int64  \n 19  year_built       2930 non-null   int64  \n 20  year_remod/add   2930 non-null   int64  \n 21  roof_style       2930 non-null   object \n 22  roof_matl        2930 non-null   object \n 23  exterior_1st     2930 non-null   object \n 24  exterior_2nd     2930 non-null   object \n 25  mas_vnr_type     1155 non-null   object \n 26  mas_vnr_area     2907 non-null   float64\n 27  exter_qual       2930 non-null   object \n 28  exter_cond       2930 non-null   object \n 29  foundation       2930 non-null   object \n 30  bsmt_qual        2850 non-null   object \n 31  bsmt_cond        2850 non-null   object \n 32  bsmt_exposure    2847 non-null   object \n 33  bsmtfin_type_1   2850 non-null   object \n 34  bsmtfin_sf_1     2929 non-null   float64\n 35  bsmtfin_type_2   2849 non-null   object \n 36  bsmtfin_sf_2     2929 non-null   float64\n 37  bsmt_unf_sf      2929 non-null   float64\n 38  total_bsmt_sf    2929 non-null   float64\n 39  heating          2930 non-null   object \n 40  heating_qc       2930 non-null   object \n 41  central_air      2930 non-null   object \n 42  electrical       2929 non-null   object \n 43  1st_flr_sf       2930 non-null   int64  \n 44  2nd_flr_sf       2930 non-null   int64  \n 45  low_qual_fin_sf  2930 non-null   int64  \n 46  gr_liv_area      2930 non-null   int64  \n 47  bsmt_full_bath   2928 non-null   float64\n 48  bsmt_half_bath   2928 non-null   float64\n 49  full_bath        2930 non-null   int64  \n 50  half_bath        2930 non-null   int64  \n 51  bedroom_abvgr    2930 non-null   int64  \n 52  kitchen_abvgr    2930 non-null   int64  \n 53  kitchen_qual     2930 non-null   object \n 54  totrms_abvgrd    2930 non-null   int64  \n 55  functional       2930 non-null   object \n 56  fireplaces       2930 non-null   int64  \n 57  fireplace_qu     1508 non-null   object \n 58  garage_type      2773 non-null   object \n 59  garage_yr_blt    2771 non-null   float64\n 60  garage_finish    2771 non-null   object \n 61  garage_cars      2929 non-null   float64\n 62  garage_area      2929 non-null   float64\n 63  garage_qual      2771 non-null   object \n 64  garage_cond      2771 non-null   object \n 65  paved_drive      2930 non-null   object \n 66  wood_deck_sf     2930 non-null   int64  \n 67  open_porch_sf    2930 non-null   int64  \n 68  enclosed_porch   2930 non-null   int64  \n 69  3ssn_porch       2930 non-null   int64  \n 70  screen_porch     2930 non-null   int64  \n 71  pool_area        2930 non-null   int64  \n 72  pool_qc          13 non-null     object \n 73  fence            572 non-null    object \n 74  misc_feature     106 non-null    object \n 75  misc_val         2930 non-null   int64  \n 76  mo_sold          2930 non-null   int64  \n 77  yr_sold          2930 non-null   int64  \n 78  sale_type        2930 non-null   object \n 79  sale_condition   2930 non-null   object \n 80  saleprice        2930 non-null   int64  \n 81  price_per_sqft   2930 non-null   float64\n 82  nonsense         2930 non-null   int64  \ndtypes: float64(12), int64(28), object(43)\nmemory usage: 1.9+ MB\n\n\nWe can use any() to identify which columns have missing values. We can use this information for various reasons such as subsetting for just those columns that have missing values.\n\nmissing = ames.isnull().any() # identify if missing values exist in each column\names[missing[missing].index]  # subset for just those columns that have missing values\n\n\n\n\n\n\n\n\nlot_frontage\nalley\nmas_vnr_type\nmas_vnr_area\nbsmt_qual\nbsmt_cond\nbsmt_exposure\nbsmtfin_type_1\nbsmtfin_sf_1\nbsmtfin_type_2\n...\ngarage_type\ngarage_yr_blt\ngarage_finish\ngarage_cars\ngarage_area\ngarage_qual\ngarage_cond\npool_qc\nfence\nmisc_feature\n\n\n\n\n0\n141.0\nNaN\nStone\n112.0\nTA\nGd\nGd\nBLQ\n639.0\nUnf\n...\nAttchd\n1960.0\nFin\n2.0\n528.0\nTA\nTA\nNaN\nNaN\nNaN\n\n\n1\n80.0\nNaN\nNaN\n0.0\nTA\nTA\nNo\nRec\n468.0\nLwQ\n...\nAttchd\n1961.0\nUnf\n1.0\n730.0\nTA\nTA\nNaN\nMnPrv\nNaN\n\n\n2\n81.0\nNaN\nBrkFace\n108.0\nTA\nTA\nNo\nALQ\n923.0\nUnf\n...\nAttchd\n1958.0\nUnf\n1.0\n312.0\nTA\nTA\nNaN\nNaN\nGar2\n\n\n3\n93.0\nNaN\nNaN\n0.0\nTA\nTA\nNo\nALQ\n1065.0\nUnf\n...\nAttchd\n1968.0\nFin\n2.0\n522.0\nTA\nTA\nNaN\nNaN\nNaN\n\n\n4\n74.0\nNaN\nNaN\n0.0\nGd\nTA\nNo\nGLQ\n791.0\nUnf\n...\nAttchd\n1997.0\nFin\n2.0\n482.0\nTA\nTA\nNaN\nMnPrv\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\n37.0\nNaN\nNaN\n0.0\nTA\nTA\nAv\nGLQ\n819.0\nUnf\n...\nDetchd\n1984.0\nUnf\n2.0\n588.0\nTA\nTA\nNaN\nGdPrv\nNaN\n\n\n2926\nNaN\nNaN\nNaN\n0.0\nGd\nTA\nAv\nBLQ\n301.0\nALQ\n...\nAttchd\n1983.0\nUnf\n2.0\n484.0\nTA\nTA\nNaN\nMnPrv\nNaN\n\n\n2927\n62.0\nNaN\nNaN\n0.0\nGd\nTA\nAv\nGLQ\n337.0\nUnf\n...\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nMnPrv\nShed\n\n\n2928\n77.0\nNaN\nNaN\n0.0\nGd\nTA\nAv\nALQ\n1071.0\nLwQ\n...\nAttchd\n1975.0\nRFn\n2.0\n418.0\nTA\nTA\nNaN\nNaN\nNaN\n\n\n2929\n74.0\nNaN\nBrkFace\n94.0\nGd\nTA\nAv\nLwQ\n758.0\nUnf\n...\nAttchd\n1993.0\nFin\n3.0\n650.0\nTA\nTA\nNaN\nNaN\nNaN\n\n\n\n\n2930 rows × 27 columns\n\n\n\n\nDropping Missing Values\nWhen you have missing values, we usually either drop them or impute them.You can drop missing values with .dropna():\n\names.dropna()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\nnonsense\n\n\n\n\n\n\n0 rows × 83 columns\n\n\n\nWhoa! What just happened? Well, this data set actually has a missing value in every single row. .dropna() drops every row that contains a missing value so we end up dropping all observations. Consequently, we probably want to figure out what’s going on with these missing values and isolate the column causing the problem and imputing the values if possible.\n\n\n\n\n\n\nAnother “drop” method is .drop_duplcates() which will drop duplicated rows in your DataFrame.\n\n\n\n\n\nVisualizing Missingness\nSometimes visualizations help identify patterns in missing values. One thing I often do is print a heatmap of my dataframe to get a feel for where my missing values are. We’ll get into data visualization in future lessons but for now here is an example using the searborn library. We can see that several variables have a lot of missing values (alley, fireplace_qu, pool_qc, fence, misc_feature).\n\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(12, 8)})\n\n\names_missing = ames[missing[missing].index]\nsns.heatmap(ames_missing.isnull(), cmap='viridis', cbar=False);\n\n\n\n\n\n\n\n\n\n\nFilling Missing Values (Imputation)\nSince we can’t drop all missing values in this data set (since it leaves us with no rows), we need to impute (“fill”) them in. There are several approaches we can use to do this; one of which uses the .fillna() method. This method has various options for filling, you can use a fixed value, the mean of the column, the previous non-nan value, etc:\n\nimport numpy as np\n\n# example DataFrame with missing values\ndf = pd.DataFrame([[np.nan, 2, np.nan, 0],\n                   [3, 4, np.nan, 1],\n                   [np.nan, np.nan, np.nan, 5],\n                   [np.nan, 3, np.nan, 4]],\n                  columns=list('ABCD'))\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\nNaN\n2.0\nNaN\n0\n\n\n1\n3.0\n4.0\nNaN\n1\n\n\n2\nNaN\nNaN\nNaN\n5\n\n\n3\nNaN\n3.0\nNaN\n4\n\n\n\n\n\n\n\n\ndf.fillna(0)  # fill with 0\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n0.0\n2.0\n0.0\n0\n\n\n1\n3.0\n4.0\n0.0\n1\n\n\n2\n0.0\n0.0\n0.0\n5\n\n\n3\n0.0\n3.0\n0.0\n4\n\n\n\n\n\n\n\n\ndf.fillna(df.mean())  # fill with the mean\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n3.0\n2.0\nNaN\n0\n\n\n1\n3.0\n4.0\nNaN\n1\n\n\n2\n3.0\n3.0\nNaN\n5\n\n\n3\n3.0\n3.0\nNaN\n4\n\n\n\n\n\n\n\n\ndf.bfill()  # backward (upwards) fill from non-nan values\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n3.0\n2.0\nNaN\n0\n\n\n1\n3.0\n4.0\nNaN\n1\n\n\n2\nNaN\n3.0\nNaN\n5\n\n\n3\nNaN\n3.0\nNaN\n4\n\n\n\n\n\n\n\n\ndf.ffill()  # forward (downward) fill from non-nan values\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\nNaN\n2.0\nNaN\n0\n\n\n1\n3.0\n4.0\nNaN\n1\n\n\n2\n3.0\n4.0\nNaN\n5\n\n\n3\n3.0\n3.0\nNaN\n4\n\n\n\n\n\n\n\n\n\n\nApplying custom functions\nThere will be times when you want to apply a function that is not built-in to Pandas. For this, we have methods:\n\ndf.apply(), applies a function column-wise or row-wise across a dataframe (the function must be able to accept/return an array)\ndf.applymap(), applies a function element-wise (for functions that accept/return single values at a time)\nseries.apply()/series.map(), same as above but for Pandas series\n\nFor example, say you had the following custom function that defines if a home is considered a luxery home simply based on the price sold.\n\n\n\n\n\n\nDon’t worry, you’ll learn more about writing your own functions in future lessons!\n\n\n\n\ndef is_luxery_home(x):\n    if x &gt; 500000:\n        return 'Luxery'\n    else:\n        return 'Non-luxery'\n\names['saleprice'].apply(is_luxery_home)\n\n0       Non-luxery\n1       Non-luxery\n2       Non-luxery\n3       Non-luxery\n4       Non-luxery\n           ...    \n2925    Non-luxery\n2926    Non-luxery\n2927    Non-luxery\n2928    Non-luxery\n2929    Non-luxery\nName: saleprice, Length: 2930, dtype: object\n\n\nThis may have been better as a lambda function, which is just a shorter approach to writing functions. This may be a bit confusing but we’ll talk more about lambda functions in the writing functions lesson. For now, just think of it as being able to write a function for single use application on the fly.\n\names['saleprice'].apply(lambda x: 'Luxery' if x &gt; 500000 else 'Non-luxery')\n\n0       Non-luxery\n1       Non-luxery\n2       Non-luxery\n3       Non-luxery\n4       Non-luxery\n           ...    \n2925    Non-luxery\n2926    Non-luxery\n2927    Non-luxery\n2928    Non-luxery\n2929    Non-luxery\nName: saleprice, Length: 2930, dtype: object\n\n\nYou can even use functions that require additional arguments. Just specify the arguments in .apply():\n\ndef is_luxery_home(x, price):\n    if x &gt; price:\n        return 'Luxery'\n    else:\n        return 'Non-luxery'\n\names['saleprice'].apply(is_luxery_home, price=200000)\n\n0           Luxery\n1       Non-luxery\n2       Non-luxery\n3           Luxery\n4       Non-luxery\n           ...    \n2925    Non-luxery\n2926    Non-luxery\n2927    Non-luxery\n2928    Non-luxery\n2929    Non-luxery\nName: saleprice, Length: 2930, dtype: object\n\n\nSometimes we may have a function that we want to apply to every element across multiple columns. For example, say we wanted to convert several of the square footage variables to be represented as square meters. For this we can use the .applymap() method.\n\ndef convert_to_sq_meters(x):\n    return x*0.092903\n\names[['gr_liv_area', 'garage_area', 'lot_area']].map(convert_to_sq_meters)\n\n\n\n\n\n\n\n\ngr_liv_area\ngarage_area\nlot_area\n\n\n\n\n0\n153.847368\n49.052784\n2946.883160\n\n\n1\n83.241088\n67.819190\n1075.073516\n\n\n2\n123.468087\n28.985736\n1320.801951\n\n\n3\n196.025330\n48.495366\n1032.152330\n\n\n4\n151.338987\n44.779246\n1280.203340\n\n\n...\n...\n...\n...\n\n\n2925\n93.181709\n54.626964\n732.725961\n\n\n2926\n83.798506\n44.965052\n820.798005\n\n\n2927\n90.115910\n0.000000\n965.355073\n\n\n2928\n129.042267\n38.833454\n925.313880\n\n\n2929\n185.806000\n60.386950\n889.732031\n\n\n\n\n2930 rows × 3 columns",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#chapter-summary",
    "href": "10-manipulating-data.html#chapter-summary",
    "title": "10  Manipulating Data",
    "section": "10.8 Chapter Summary",
    "text": "10.8 Chapter Summary\nIn this chapter, you learned how to manipulate columns in a pandas DataFrame — a foundational skill for any kind of data analysis or modeling. You practiced working with both numeric and non-numeric data and explored common data wrangling tasks that analysts use every day.\nHere’s a recap of what you learned:\n\nHow to rename columns using .rename() or string methods like .str.lower() and .str.replace()\nHow to create new columns by performing arithmetic with scalars or other columns\nHow to remove columns using .drop()\nHow to work with text data, including string concatenation and using the .str accessor\nHow to replace values using a mapping (via .replace())\nHow to detect and handle missing values using .isnull(), .dropna(), and .fillna()\nHow to apply custom functions to transform your data using .apply() and .applymap()\n\nThese skills form the building blocks of effective data cleaning and transformation.\nIn the next few chapters, we’ll build on this foundation and introduce even more essential data wrangling techniques, including:\n\nComputing summary statistics and descriptive analytics\nGrouping and aggregating data\nJoining multiple datasets\nReshaping data with pivot tables and the .melt() and .pivot() methods\n\nBy the end of these upcoming lessons, you’ll be well-equipped to clean, prepare, and explore real-world datasets using pandas.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#exercise-heart-disease-data",
    "href": "10-manipulating-data.html#exercise-heart-disease-data",
    "title": "10  Manipulating Data",
    "section": "10.9 Exercise: Heart Disease Data",
    "text": "10.9 Exercise: Heart Disease Data\nIn this exercise, you’ll apply what you’ve learned in this chapter to a new dataset on heart disease, which includes various patient health indicators that may be predictive of cardiovascular conditions.\nRead more about this dataset on Kaggle, and you can download copy of the heart.csv file here.\nYour task is to perform several data wrangling steps to start cleaning and transforming this dataset.\n\n\n\n\n\n\nNoneImport the Data\n\n\n\n\n\n\nLoad the dataset into a DataFrame using pd.read_csv().\nPreview the first few rows with .head().\n\n\n\n\n\n\n\n\n\n\nNoneClean the Column Names\n\n\n\n\n\nCheck out the column names and think how you would standardize these names. Use .columns and string methods to:\n\nConvert all column names to lowercase\nReplace any spaces or dashes with underscores\nRemove any trailing or leading whitespace\n\n\n\n\n\n\n\n\n\n\nNoneHandle Missing Values\n\n\n\n\n\n\nCheck for missing values using .isnull().sum().\nIf any columns contain missing values:\n\nIdentify the mode (most frequent value) for those columns\nFill the missing values using .fillna()\n\n\n\n\n\n\n\n\nThe mode can be accessed with .mode().iloc[0] to retrieve the most frequent value.\n\n\n\n\n\n\n\n\n\n\n\n\nNoneCreate a New Column\n\n\n\n\n\nCreate a new column called risk, calculated as:\n\\[\n     \\text{risk} = \\frac{\\text{age}}{\\text{rest\\_bp} + \\text{chol} + \\text{max\\_hr}}\n\\]\n\n\n\n\n\n\nBe sure to use parentheses in your formula to ensure proper order of operations.\n\n\n\n\n\n\n\n\n\n\n\n\nNoneReplace Values in a Categorical Column\n\n\n\n\n\nThe rest_ecg column contains several text categories. Recode the values using .replace() and the following mapping:\n\n\n\nOriginal Value\nNew Value\n\n\n\n\nnormal\nnormal\n\n\nleft ventricular hypertrophy\nlvh\n\n\nST-T wave abnormality\nstt_wav_abn\n\n\n\n\n\n\n\n\n\nMake sure to overwrite the existing column with the updated values.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html",
    "href": "11_aggregating_data.html",
    "title": "11  Summarizing Data",
    "section": "",
    "text": "11.1 Simple aggregation\nAs datasets grow in size and complexity, the ability to summarize information becomes an essential skill in any data scientist’s toolkit. Summary statistics—like averages, medians, and group-level comparisons—help us cut through the noise and uncover meaningful patterns. Whether you’re reporting results to stakeholders or exploring data to form new hypotheses, knowing how to aggregate data is fundamental.\nThese types of questions will guide your thinking as we explore tools in Pandas for summarizing and grouping data. And by the end of this lesson, you will be able to:\nIn the previous lesson, we learned how to manipulate data across columns, typically by performing operations on two or more Series within the same row. For example, we might calculate a total by adding two columns together:\nFor example, we can calculate a total by adding two columns together with code that looks like this:\nThis adds the values in column A to the corresponding values in column B, row by row. You could also use other operators (e.g., subtraction, multiplication) as long as the result returns one value per row.\nHowever, sometimes we want to shift our focus from individual rows to the entire column. Instead of computing values across columns within a row, we want to aggregate values across rows within a column. This is the foundation of many summary statistics—like computing the average, median, or maximum of a column.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html#simple-aggregation",
    "href": "11_aggregating_data.html#simple-aggregation",
    "title": "11  Summarizing Data",
    "section": "",
    "text": "These types of operations return the same number of rows as the original DataFrame. This is sometimes called a window function—but you can think of it as performing calculations at the row level.\n\n\n\n\nDataFrame['A'] + DataFrame['B']\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese types of operations return a single value that summarizes multiple values. This is often referred to as a summary function, but you can think of it as aggregating values across rows.\n\n\n\n\nSummarizing a Series\nOnce we’ve loaded a dataset into a Pandas DataFrame, one of the simplest ways to explore it is by summarizing individual columns—also known as Series. Pandas makes this easy with built-in methods like .sum(), .mean(), and .median().\nSuppose we want to compute the total of all home sale prices. We can do that by selecting the SalePrice column and using the .sum() method:\n\names['SalePrice'].sum()\n\nnp.int64(529732456)\n\n\nThis returns a single value, which makes this a summary operation—we’re aggregating values across all rows in the SalePrice column.\nPandas includes many other helpful summary methods for numerical data:\n\n# Average sale price\names['SalePrice'].mean()\n\nnp.float64(180796.0600682594)\n\n\n\n# Median sale price\names['SalePrice'].median()\n\nnp.float64(160000.0)\n\n\n\n# Standard deviation of sale prices\names['SalePrice'].std()   \n\nnp.float64(79886.692356665)\n\n\nThese methods are designed for quantitative variables and won’t work as expected on text-based columns.\n\n\n\n\n\n\nPandas will include the data type in the summary statistic output preceeding the actual summary stat (i.e. np.float64). If you want to not see that you can just wrap it with print():\n\nprint(ames['SalePrice'].sum())\n\n529732456\n\n\n\n\n\nHowever, Pandas also provides summary methods that are useful for categorical variables (like neighborhood names):\n\n# Number of unique neighborhoods\names['Neighborhood'].nunique()\n\n28\n\n\n\n# Most frequent neighborhood\names['Neighborhood'].mode()    \n\n0    NAmes\nName: Neighborhood, dtype: object\n\n\nThese allow us to summarize the structure and frequency of categorical data—just as we do with numbers.\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nWhat is the difference between a window operation and a summary operation?\nWhat are the mean, median, and standard deviation of the Gr Liv Area (above ground square footage)?\nHow many times does each neighborhood appear in the dataset? (Hint: Try using the GenAI code assistant to figure this out. Ask it how to “count value frequency in a Pandas Series.”). Then reflect: Would this count as a summary operation? Why or why not?\n\n\n\n\n\n\n\nThe describe() Method\nWhen you’re first getting familiar with a dataset, it’s helpful to quickly view a variety of summary statistics all at once. Pandas provides the .describe() method for exactly this purpose.\nFor numeric variables, .describe() returns common summary statistics such as the count, mean, standard deviation, min, and max values:\n\names['SalePrice'].describe()\n\ncount      2930.000000\nmean     180796.060068\nstd       79886.692357\nmin       12789.000000\n25%      129500.000000\n50%      160000.000000\n75%      213500.000000\nmax      755000.000000\nName: SalePrice, dtype: float64\n\n\nThis is especially useful during exploratory data analysis (EDA) when you’re trying to get a sense of a column’s range, distribution, and central tendency.\n\n\n\n\n\n\nThe behavior of .describe() changes based on the data type of the Series.\n\n\n\nFor categorical variables, .describe() provides a different summary, showing the number of non-null entries, number of unique values, most frequent value, and its frequency:\n\names['Neighborhood'].describe()\n\ncount      2930\nunique       28\ntop       NAmes\nfreq        443\nName: Neighborhood, dtype: object\n\n\nIn both cases, .describe() gives you a quick and informative overview—whether you’re working with numbers or categories.\n\n\nSummarizing a DataFrame\nSo far, we’ve focused on summarizing individual columns (Series). But in many cases, we want to explore multiple variables at once. Fortunately, Pandas allows us to apply the same summary methods to a subset of columns in a DataFrame.\nFirst, recall how to select multiple columns using double brackets and a list of column names:\n\names[['SalePrice', 'Gr Liv Area']]\n\n\n\n\n\n\n\n\nSalePrice\nGr Liv Area\n\n\n\n\n0\n215000\n1656\n\n\n1\n105000\n896\n\n\n2\n172000\n1329\n\n\n3\n244000\n2110\n\n\n4\n189900\n1629\n\n\n...\n...\n...\n\n\n2925\n142500\n1003\n\n\n2926\n131000\n902\n\n\n2927\n132000\n970\n\n\n2928\n170000\n1389\n\n\n2929\n188000\n2000\n\n\n\n\n2930 rows × 2 columns\n\n\n\nThis returns a new DataFrame with just the SalePrice and Gr Liv Area columns.\nWe can now apply summary methods—just like we did with a single Series:\n\names[['SalePrice', 'Gr Liv Area']].mean()\n\nSalePrice      180796.060068\nGr Liv Area      1499.690444\ndtype: float64\n\n\n\names[['SalePrice', 'Gr Liv Area']].median()\n\nSalePrice      160000.0\nGr Liv Area      1442.0\ndtype: float64\n\n\nThese methods return a Series object, where:\n\nThe index contains the column names, and\nThe values are the summary statistics (e.g., mean or median) for each column.\n\nThis approach is useful when you’re interested in comparing summary values across several numeric variables at once.\n\n\n\n\n\n\nEven though you’re summarizing multiple columns, each summary method still operates column-by-column under the hood.\n\n\n\n\n\nThe .agg() Method\nSo far, we’ve used built-in summary methods like .mean() and .median() to compute statistics on one or more columns. While this approach works well, it has a few important limitations when applied to DataFrames:\n\nYou can only apply one summary method at a time.\nThe same method gets applied to every selected column.\nThe result is returned as a Series, which can be harder to work with later.\n\nTo overcome these limitations, Pandas provides a more flexible tool: the .agg() method (short for .aggregate()).\nHere’s a basic example:\n\names.agg({\n    'SalePrice': ['mean']\n})\n\n\n\n\n\n\n\n\nSalePrice\n\n\n\n\nmean\n180796.060068\n\n\n\n\n\n\n\nThis returns a DataFrame rather than a Series—and gives us more control over how we summarize each column.\nLet’s break it down:\n\nWe pass a dictionary to .agg()\nThe keys are column names\nThe values are lists of summary functions we want to apply\n\n\n\n\n\n\n\nThe .agg() method is just shorthand for .aggregate(). You can use either version—they’re equivalent!\n\n\n\n\n# Verbose version\names.aggregate({\n    'SalePrice': ['mean']\n})\n\n# Concise version\names.agg({\n    'SalePrice': ['mean']\n})\n\n\n\n\n\n\n\n\nSalePrice\n\n\n\n\nmean\n180796.060068\n\n\n\n\n\n\n\nWe can easily extend this to include multiple columns:\n\names.agg({\n    'SalePrice': ['mean'],\n    'Gr Liv Area': ['mean']\n})\n\n\n\n\n\n\n\n\nSalePrice\nGr Liv Area\n\n\n\n\nmean\n180796.060068\n1499.690444\n\n\n\n\n\n\n\nAnd because the values in the dictionary are lists, we can apply multiple summary functions to each column:\n\names.agg({\n    'SalePrice': ['mean', 'median'],\n    'Gr Liv Area': ['mean', 'min']\n})\n\n\n\n\n\n\n\n\nSalePrice\nGr Liv Area\n\n\n\n\nmean\n180796.060068\n1499.690444\n\n\nmedian\n160000.000000\nNaN\n\n\nmin\nNaN\n334.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou don’t have to apply the same summary functions to every variable. If a summary function isn’t applied to a particular column, Pandas will return a NaN in that spot.\n\n\n\nThe .agg() method is especially useful when you want to apply different aggregations to different columns and get the results in a clean, tabular format.\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nFill in the blanks to compute the average number of rooms above ground (TotRms AbvGrd) and the average number of bedrooms above ground (Bedroom AbvGr). What type of object is returned?\names[['______', '______']].______()\nUse the .agg() method to complete the same computation as above. How does the output differ?\nFill in the blanks in the below code to calculate the minimum and maximum year built (Year Built) and the mean and median number of garage stalls (Garage Cars):\names.agg({\n    '_____': ['min', '_____'],\n    '_____': ['_____', 'median']\n})\n\n\n\n\n\n\n\nThe describe() Method\nWhile .agg() is a powerful and flexible tool for customized summaries, the .describe() method offers a quick and convenient overview of your entire DataFrame—making it especially useful during exploratory data analysis (EDA).\nTry running .describe() on the full Ames dataset:\n\names.describe()\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nLot Frontage\nLot Area\nOverall Qual\nOverall Cond\nYear Built\nYear Remod/Add\nMas Vnr Area\n...\nWood Deck SF\nOpen Porch SF\nEnclosed Porch\n3Ssn Porch\nScreen Porch\nPool Area\nMisc Val\nMo Sold\nYr Sold\nSalePrice\n\n\n\n\ncount\n2930.00000\n2.930000e+03\n2930.000000\n2440.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2907.000000\n...\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n\n\nmean\n1465.50000\n7.144645e+08\n57.387372\n69.224590\n10147.921843\n6.094881\n5.563140\n1971.356314\n1984.266553\n101.896801\n...\n93.751877\n47.533447\n23.011604\n2.592491\n16.002048\n2.243345\n50.635154\n6.216041\n2007.790444\n180796.060068\n\n\nstd\n845.96247\n1.887308e+08\n42.638025\n23.365335\n7880.017759\n1.411026\n1.111537\n30.245361\n20.860286\n179.112611\n...\n126.361562\n67.483400\n64.139059\n25.141331\n56.087370\n35.597181\n566.344288\n2.714492\n1.316613\n79886.692357\n\n\nmin\n1.00000\n5.263011e+08\n20.000000\n21.000000\n1300.000000\n1.000000\n1.000000\n1872.000000\n1950.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n2006.000000\n12789.000000\n\n\n25%\n733.25000\n5.284770e+08\n20.000000\n58.000000\n7440.250000\n5.000000\n5.000000\n1954.000000\n1965.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n4.000000\n2007.000000\n129500.000000\n\n\n50%\n1465.50000\n5.354536e+08\n50.000000\n68.000000\n9436.500000\n6.000000\n5.000000\n1973.000000\n1993.000000\n0.000000\n...\n0.000000\n27.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n6.000000\n2008.000000\n160000.000000\n\n\n75%\n2197.75000\n9.071811e+08\n70.000000\n80.000000\n11555.250000\n7.000000\n6.000000\n2001.000000\n2004.000000\n164.000000\n...\n168.000000\n70.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n8.000000\n2009.000000\n213500.000000\n\n\nmax\n2930.00000\n1.007100e+09\n190.000000\n313.000000\n215245.000000\n10.000000\n9.000000\n2010.000000\n2010.000000\n1600.000000\n...\n1424.000000\n742.000000\n1012.000000\n508.000000\n576.000000\n800.000000\n17000.000000\n12.000000\n2010.000000\n755000.000000\n\n\n\n\n8 rows × 39 columns\n\n\n\nBy default, Pandas will summarize only the numeric columns, returning statistics like count, mean, standard deviation, min, max, and quartiles.\n\n\n\n\n\n\nBut wait, what’s missing from the output?\nString (object) columns — like Neighborhood — are excluded!\n\n\n\nIf you want to include other variable types, you can use the include parameter to tell Pandas what to summarize. For example:\n\names.describe(include=['int', 'float', 'object'])\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\ncount\n2930.00000\n2.930000e+03\n2930.000000\n2930\n2440.000000\n2930.000000\n2930\n198\n2930\n2930\n...\n2930.000000\n13\n572\n106\n2930.000000\n2930.000000\n2930.000000\n2930\n2930\n2930.000000\n\n\nunique\nNaN\nNaN\nNaN\n7\nNaN\nNaN\n2\n2\n4\n4\n...\nNaN\n4\n4\n5\nNaN\nNaN\nNaN\n10\n6\nNaN\n\n\ntop\nNaN\nNaN\nNaN\nRL\nNaN\nNaN\nPave\nGrvl\nReg\nLvl\n...\nNaN\nEx\nMnPrv\nShed\nNaN\nNaN\nNaN\nWD\nNormal\nNaN\n\n\nfreq\nNaN\nNaN\nNaN\n2273\nNaN\nNaN\n2918\n120\n1859\n2633\n...\nNaN\n4\n330\n95\nNaN\nNaN\nNaN\n2536\n2413\nNaN\n\n\nmean\n1465.50000\n7.144645e+08\n57.387372\nNaN\n69.224590\n10147.921843\nNaN\nNaN\nNaN\nNaN\n...\n2.243345\nNaN\nNaN\nNaN\n50.635154\n6.216041\n2007.790444\nNaN\nNaN\n180796.060068\n\n\nstd\n845.96247\n1.887308e+08\n42.638025\nNaN\n23.365335\n7880.017759\nNaN\nNaN\nNaN\nNaN\n...\n35.597181\nNaN\nNaN\nNaN\n566.344288\n2.714492\n1.316613\nNaN\nNaN\n79886.692357\n\n\nmin\n1.00000\n5.263011e+08\n20.000000\nNaN\n21.000000\n1300.000000\nNaN\nNaN\nNaN\nNaN\n...\n0.000000\nNaN\nNaN\nNaN\n0.000000\n1.000000\n2006.000000\nNaN\nNaN\n12789.000000\n\n\n25%\n733.25000\n5.284770e+08\n20.000000\nNaN\n58.000000\n7440.250000\nNaN\nNaN\nNaN\nNaN\n...\n0.000000\nNaN\nNaN\nNaN\n0.000000\n4.000000\n2007.000000\nNaN\nNaN\n129500.000000\n\n\n50%\n1465.50000\n5.354536e+08\n50.000000\nNaN\n68.000000\n9436.500000\nNaN\nNaN\nNaN\nNaN\n...\n0.000000\nNaN\nNaN\nNaN\n0.000000\n6.000000\n2008.000000\nNaN\nNaN\n160000.000000\n\n\n75%\n2197.75000\n9.071811e+08\n70.000000\nNaN\n80.000000\n11555.250000\nNaN\nNaN\nNaN\nNaN\n...\n0.000000\nNaN\nNaN\nNaN\n0.000000\n8.000000\n2009.000000\nNaN\nNaN\n213500.000000\n\n\nmax\n2930.00000\n1.007100e+09\n190.000000\nNaN\n313.000000\n215245.000000\nNaN\nNaN\nNaN\nNaN\n...\n800.000000\nNaN\nNaN\nNaN\n17000.000000\n12.000000\n2010.000000\nNaN\nNaN\n755000.000000\n\n\n\n\n11 rows × 82 columns\n\n\n\nThis will generate descriptive statistics for all numeric and categorical variables, including counts, unique values, top categories, and their frequencies.\n\n\n\n\n\n\nYou can also use include='all' to summarize every column, regardless of type. Just be aware that this can result in a mix of numeric and non-numeric statistics in the same table.\n\n\n\nThe .describe() method is a fast and effective way to get a high-level snapshot of your dataset—perfect for early-stage data exploration.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html#grouped-aggregation",
    "href": "11_aggregating_data.html#grouped-aggregation",
    "title": "11  Summarizing Data",
    "section": "11.2 Grouped Aggregation",
    "text": "11.2 Grouped Aggregation\nSo far, we’ve focused on summary operations that collapse an entire column—or a subset of columns—into a single result. But in many real-world analyses, we’re interested in summarizing within groups rather than across the whole dataset.\nThis is called a grouped aggregation. Instead of collapsing a DataFrame into a single row, we collapse it into one row per group.\nFor example, we might want to compute:\n\nTotal home sales by neighborhood\nAverage square footage by number of bedrooms\nMedian sale price by year\nMaximum temperature by month\n\nHere’s a simple illustration: suppose we want to compute the sum of column B for each unique category in column A:\n\n\n\n\n\n\nThe Groupby Model\nGrouped aggregation in Pandas always follows the same three-step process:\n\nGroup the data using groupby()\nApply a summary method like .sum(), .agg(), or .describe()\nReturn a DataFrame of group-level summaries\n\n\n\n\nCreating a Grouped Object\nWe use the groupby() method to define how we want to group the data. For example, to group homes by Neighborhood:\n\names_grp = ames.groupby('Neighborhood')\n\nThis creates a GroupBy object—it doesn’t return a DataFrame yet, but rather an internal structure that maps each group to its corresponding rows.\n\ntype(ames_grp)\n\npandas.core.groupby.generic.DataFrameGroupBy\n\n\nYou can inspect the structure of the groups:\n\names_grp.groups\n\n{'Blmngtn': [52, 53, 468, 469, 470, 471, 472, 473, 1080, 1081, 1082, 1083, 1084, 1741, 1742, 1743, 1744, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429], 'Blueste': [298, 299, 932, 933, 934, 935, 1542, 1543, 2225, 2227], 'BrDale': [29, 30, 31, 402, 403, 404, 405, 406, 407, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1675, 1676, 1677, 1678, 1679, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372], 'BrkSide': [129, 130, 191, 192, 193, 194, 195, 196, 197, 198, 199, 614, 615, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 1219, 1220, 1221, 1222, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1901, 1902, 1903, 1904, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2022, 2023, 2024, 2025, 2554, 2555, 2556, 2557, 2670, 2671, 2672, 2673, 2675, 2676, 2677, ...], 'ClearCr': [208, 209, 210, 228, 229, 232, 233, 255, 779, 781, 782, 785, 833, 834, 1374, 1392, 1395, 1398, 1399, 1400, 1401, 1402, 1406, 1429, 1430, 2045, 2071, 2072, 2073, 2077, 2115, 2116, 2117, 2118, 2701, 2725, 2726, 2727, 2730, 2731, 2764, 2765, 2766, 2767], 'CollgCr': [249, 250, 251, 252, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 1423, 1424, 1425, 1426, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, ...], 'Crawfor': [293, 294, 295, 296, 297, 300, 308, 912, 915, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 936, 937, 938, 944, 1523, 1524, 1525, 1526, 1528, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1559, 1560, 1561, 1562, 2196, 2197, 2198, 2199, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2226, 2228, 2229, 2230, 2245, 2246, 2850, 2853, 2854, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, ...], 'Edwards': [234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 287, 762, 763, 764, 765, 766, 767, 783, 784, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 1375, 1403, 1404, 1405, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, ...], 'Gilbert': [4, 5, 9, 10, 11, 12, 13, 16, 18, 51, 54, 55, 56, 57, 58, 344, 345, 346, 347, 348, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 369, 464, 465, 466, 467, 474, 475, 476, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 992, 993, 994, 995, 996, 997, 998, 1003, 1004, 1005, 1006, 1007, 1013, 1015, 1085, 1086, 1087, 1088, 1089, 1090, 1092, 1093, 1094, 1095, 1096, 1097, 1615, 1616, 1617, 1618, 1619, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1638, 1727, 1728, ...], 'Greens': [106, 107, 575, 1857, 2518, 2519, 2520, 2521], 'GrnHill': [2256, 2892], 'IDOTRR': [205, 206, 301, 302, 303, 304, 305, 306, 307, 726, 727, 754, 755, 758, 759, 760, 939, 940, 941, 942, 943, 945, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1368, 1369, 1370, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1610, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2043, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2669, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882], 'Landmrk': [2788], 'MeadowV': [326, 327, 328, 329, 330, 331, 332, 973, 975, 977, 978, 979, 1593, 1594, 1595, 1596, 1597, 1599, 1600, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2908, 2909, 2910, 2913, 2914, 2916, 2917, 2918, 2919, 2920], 'Mitchel': [309, 310, 311, 312, 313, 322, 323, 324, 325, 333, 334, 335, 336, 337, 338, 339, 340, 946, 947, 948, 949, 950, 951, 952, 953, 954, 969, 970, 971, 972, 974, 976, 980, 981, 982, 983, 984, 985, 986, 987, 988, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1588, 1589, 1590, 1591, 1592, 1598, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2277, 2278, 2279, 2280, 2281, 2282, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2885, 2886, 2887, 2888, 2889, 2890, 2903, 2904, 2905, ...], 'NAmes': [0, 1, 2, 3, 23, 24, 25, 26, 27, 28, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 341, 342, 343, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 418, 419, 593, 594, 595, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 616, 617, 618, 619, 620, 621, 622, 623, 624, ...], 'NPkVill': [32, 33, 34, 35, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 1046, 1047, 1048, 1680, 1681, 1682, 2373, 2376, 2377], 'NWAmes': [19, 20, 21, 110, 111, 112, 113, 114, 115, 116, 118, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 387, 388, 389, 390, 391, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 596, 597, 598, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1197, 1198, 1199, 1200, 1201, 1203, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1863, 1864, 1865, 1866, 1867, ...], 'NoRidge': [59, 60, 61, 62, 63, 64, 65, 90, 91, 92, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 564, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1157, 1158, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1832, 1833, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2499, 2500, 2501, 2502, 2503], 'NridgHt': [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 482, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1091, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, ...], 'OldTown': [158, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 200, 201, 202, 203, 204, 207, 650, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 749, 750, 751, 752, 753, 756, 757, 1254, 1255, 1257, 1258, 1259, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, ...], 'SWISU': [211, 212, 213, 214, 285, 286, 288, 289, 290, 291, 292, 905, 906, 907, 908, 909, 910, 911, 913, 914, 916, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1527, 1529, 2047, 2080, 2194, 2195, 2200, 2703, 2704, 2842, 2845, 2846, 2847, 2848, 2849, 2851, 2852, 2855, 2856], 'Sawyer': [83, 84, 85, 86, 87, 88, 89, 108, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 231, 554, 555, 556, 557, 558, 559, 560, 561, 562, 578, 761, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 780, 1149, 1150, 1151, 1152, 1153, 1154, 1156, 1371, 1372, 1373, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1393, 1394, 1396, 1397, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1829, 1830, 1831, 1861, 2044, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, ...], 'SawyerW': [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 244, 245, 246, 247, 248, 253, 254, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 832, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1418, 1419, 1420, 1421, 1422, 1427, 1428, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 2089, 2090, 2091, 2092, ...], 'Somerst': [22, 66, 67, 68, 69, 70, 71, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 109, 383, 384, 385, 386, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 565, 566, 567, 568, 569, 570, 571, 572, 573, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, ...], 'StoneBr': [6, 7, 8, 14, 15, 17, 349, 350, 351, 352, 365, 366, 367, 368, 999, 1000, 1001, 1002, 1008, 1009, 1010, 1011, 1012, 1014, 1620, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1639, 1640, 1641, 1642, 2322, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2339, 2340, 2341], 'Timber': [314, 315, 316, 317, 318, 319, 320, 321, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 2255, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2891, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902], 'Veenker': [563, 574, 576, 577, 1155, 1178, 1179, 1180, 1181, 1182, 1183, 1827, 1828, 1853, 1854, 1855, 1856, 1858, 1859, 1860, 2498, 2516, 2517, 2522]}\n\n\nAnd access a specific group:\n\n# Get the Bloomington neighborhood group\names_grp.get_group('Blmngtn').head()\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n52\n53\n528228285\n120\nRL\n43.0\n3203\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n1\n2010\nWD\nNormal\n160000\n\n\n53\n54\n528228440\n120\nRL\n43.0\n3182\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n192000\n\n\n468\n469\n528228290\n120\nRL\n53.0\n3684\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n6\n2009\nWD\nNormal\n174000\n\n\n469\n470\n528228295\n120\nRL\n51.0\n3635\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2009\nWD\nNormal\n175900\n\n\n470\n471\n528228435\n120\nRL\n43.0\n3182\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2009\nWD\nNormal\n192500\n\n\n\n\n5 rows × 82 columns\n\n\n\n\n\nApplying Aggregations to Groups\nOnce a group is defined, you can apply the same aggregation methods we used earlier:\n\names.groupby('Neighborhood').agg({'SalePrice': ['mean', 'median']}).head()\n\n\n\n\n\n\n\n\nSalePrice\n\n\n\nmean\nmedian\n\n\nNeighborhood\n\n\n\n\n\n\nBlmngtn\n196661.678571\n191500.0\n\n\nBlueste\n143590.000000\n130500.0\n\n\nBrDale\n105608.333333\n106000.0\n\n\nBrkSide\n124756.250000\n126750.0\n\n\nClearCr\n208662.090909\n197500.0\n\n\n\n\n\n\n\nThis returns a DataFrame where each row corresponds to a different neighborhood, and the columns represent the aggregated values.\n\n\nGroups as Index vs. Variables\n\n\n\n\n\n\nBy default, the grouped variable becomes the index of the resulting DataFrame.\n\n\n\n\names.groupby('Neighborhood').agg({'SalePrice': ['mean', 'median']}).index\n\nIndex(['Blmngtn', 'Blueste', 'BrDale', 'BrkSide', 'ClearCr', 'CollgCr',\n       'Crawfor', 'Edwards', 'Gilbert', 'Greens', 'GrnHill', 'IDOTRR',\n       'Landmrk', 'MeadowV', 'Mitchel', 'NAmes', 'NPkVill', 'NWAmes',\n       'NoRidge', 'NridgHt', 'OldTown', 'SWISU', 'Sawyer', 'SawyerW',\n       'Somerst', 'StoneBr', 'Timber', 'Veenker'],\n      dtype='object', name='Neighborhood')\n\n\nThis is the most efficient default behavior in Pandas. However, if you’d prefer the group column to remain a regular column instead of becoming the index, you can set as_index=False:\n\names.groupby('Neighborhood', as_index=False).agg({'SalePrice': ['mean', 'median']}).head()\n\n\n\n\n\n\n\n\nNeighborhood\nSalePrice\n\n\n\n\nmean\nmedian\n\n\n\n\n0\nBlmngtn\n196661.678571\n191500.0\n\n\n1\nBlueste\n143590.000000\n130500.0\n\n\n2\nBrDale\n105608.333333\n106000.0\n\n\n3\nBrkSide\n124756.250000\n126750.0\n\n\n4\nClearCr\n208662.090909\n197500.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing as_index=False can make your results easier to merge with other DataFrames or write to CSV later.\n\n\n\n\n\nGrouping by Multiple Variables\nYou can also group by more than one variable. For example, to compute the average sale price by both Neighborhood and Yr Sold:\n\names.groupby(['Neighborhood', 'Yr Sold'], as_index=False).agg({'SalePrice': 'mean'})\n\n\n\n\n\n\n\n\nNeighborhood\nYr Sold\nSalePrice\n\n\n\n\n0\nBlmngtn\n2006\n214424.454545\n\n\n1\nBlmngtn\n2007\n194671.500000\n\n\n2\nBlmngtn\n2008\n190714.400000\n\n\n3\nBlmngtn\n2009\n177266.666667\n\n\n4\nBlmngtn\n2010\n176000.000000\n\n\n...\n...\n...\n...\n\n\n125\nTimber\n2010\n224947.625000\n\n\n126\nVeenker\n2006\n270000.000000\n\n\n127\nVeenker\n2007\n253577.777778\n\n\n128\nVeenker\n2008\n225928.571429\n\n\n129\nVeenker\n2009\n253962.500000\n\n\n\n\n130 rows × 3 columns\n\n\n\nThis returns one row for each combination of Neighborhood and Yr Sold, giving you more granular insights.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nReframe the following question using Pandas’ grouped aggregation syntax. Which Pandas functions will you need?\n\nWhat is the average above-ground square footage of homes, grouped by neighborhood and number of bedrooms?\n\nNow compute the result using the Ames Housing dataset. 🔍 Hint…\n\nGr Liv Area = above-ground square footage\n\nNeighborhood = neighborhood name\n\nBedroom AbvGr = number of bedrooms\n\nUsing your results from #2, identify any neighborhoods where 1-bedroom homes have an average of more than 1500 square feet above ground. How many neighborhoods meet this condition?",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html#summary",
    "href": "11_aggregating_data.html#summary",
    "title": "11  Summarizing Data",
    "section": "11.3 Summary",
    "text": "11.3 Summary\nIn this chapter, we explored how to summarize and aggregate data using Pandas—a foundational skill in any data analysis workflow. You learned how to compute summary statistics on individual columns (Series), multiple columns in a DataFrame, and across groups using the powerful groupby() method. We introduced tools like .mean(), .median(), .describe(), and .agg() to help extract key insights from both numerical and categorical variables. These techniques allow us to make sense of large datasets by reducing complexity and identifying trends, patterns, and outliers.\nIn the chapters ahead, we’ll continue building on these data wrangling skills. You’ll learn how to combine datasets using joins, reshape data through pivoting and tidying, and prepare data for modeling and visualization. Mastering these techniques will give you the ability to transform raw data into a clean, structured form ready for deeper analysis.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html#exercise-aggregating-covid-college-data",
    "href": "11_aggregating_data.html#exercise-aggregating-covid-college-data",
    "title": "11  Summarizing Data",
    "section": "11.4 Exercise: Aggregating COVID College Data",
    "text": "11.4 Exercise: Aggregating COVID College Data\nNow that you’ve practiced subsetting and filtering, let’s dig deeper by computing some summary statistics from the college COVID-19 dataset.\n\n\n\n\n\n\nNoneStep 1: Load the Data\n\n\n\n\n\n\nimport pandas as pd\n\ndata_url = \"https://raw.githubusercontent.com/nytimes/covid-19-data/refs/heads/master/colleges/colleges.csv\"\ncollege_df = pd.read_csv(data_url)\ncollege_df.head()\n\n\n\n\n\n\n\n\ndate\nstate\ncounty\ncity\nipeds_id\ncollege\ncases\ncases_2021\nnotes\n\n\n\n\n0\n2021-05-26\nAlabama\nMadison\nHuntsville\n100654\nAlabama A&M University\n41\nNaN\nNaN\n\n\n1\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100724\nAlabama State University\n2\nNaN\nNaN\n\n\n2\n2021-05-26\nAlabama\nLimestone\nAthens\n100812\nAthens State University\n45\n10.0\nNaN\n\n\n3\n2021-05-26\nAlabama\nLee\nAuburn\n100858\nAuburn University\n2742\n567.0\nNaN\n\n\n4\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100830\nAuburn University at Montgomery\n220\n80.0\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneStep 2: Compute the average number of cases reported across all colleges\n\n\n\n\n\n\nWhat function will help you find the average (mean) value of a column?\nWhat is the average value of cases?\n\ncollege_df['cases'].____()\n\n\n\n\n\n\n\n\n\nNoneStep 3: Compute the total number of cases and total number of cases_2021\n\n\n\n\n\n\nWhat is the average value of cases?\nHow do these two numbers compare? Why might they be different? Hint: Read the variable definitions in the NYT GitHub repo. Are these columns measuring the same thing?\n\ncollege_df[['cases', 'cases_2021']].____()\n\n\n\n\n\n\n\n\n\nNoneStep 4: Compute the total number of cases by state\n\n\n\n\n\n\nUse groupby() and sum() to find the total cases reported by colleges in each state.\nWhich state had the most cases?\nWhich had the fewest?\n\ncollege_df.groupby('state')[['cases']].____()\n\n\n\n\n\n\n\n\n\nNoneStep 5: Focus on colleges in Ohio\n\n\n\n\n\nFilter the data to show only colleges in Ohio. Then compute:\n\nThe total number of cases\nThe average (mean) number of cases across Ohio colleges\n\nohio_df = college_df[college_df['state'] == '____']\n\n# Your aggregations go here",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html",
    "href": "12-joining-data.html",
    "title": "12  Relational data",
    "section": "",
    "text": "12.1 Prerequisites\nIn many data projects, we work with more than one table. A product’s description might be in one table, customer demographics in another, and transaction records in yet another. To make sense of these interconnected datasets, we need to combine them in ways that preserve the relationships between them.\nThis is the world of relational data—data stored in multiple related tables. Each table gives us a piece of the story, but we only get a full picture when we connect the pieces together.\nTo do that, we use joins, which allow us to merge tables using key variables they have in common. In this chapter, we’ll build up your understanding of joins in pandas and give you hands-on experience combining data from multiple sources.\nBy the end of this lesson you’ll be able to:\nBefore we dive into code, watch this short video for a practical overview of how to join DataFrames in pandas. It walks through key concepts like different types of joins and when to use them. After watching, you’ll be ready to roll up your sleeves and apply these techniques in the hands-on examples that follow.\nBefore we dive into real-world data, let’s start by loading the pandas library. This will give us access to the join functions we’ll be using throughout the chapter.\nimport pandas as pd\nTo build your intuition around joins, we’ll begin with two very simple DataFrames, x and y. In these examples:\nHere’s how we’ll create them in code:\nx = pd.DataFrame({'id': [1, 2, 3], 'val_x': ['x1', 'x2', 'x3']})\ny = pd.DataFrame({'id': [1, 2, 4], 'val_y': ['y1', 'y2', 'y4']})\nThese examples will help you understand what joins do and how they behave before we move on to working with larger, more complex tables.\nHowever, we will also build upon the simple examples by using various data sets from the completejourney_py library. This library provides access to data sets characterizing household level transactions over one year from a group of 2,469 households who are frequent shoppers at a grocery store.\nThere are eight built-in data sets available in this library. The data sets include:\nThis is a Python equivalent of the R package completejourney. The R package has a full guide to get you acquainted with the various data set schemas, which you can read here.\nfrom completejourney_py import get_data\n\n# get_data() provides a dictionary of several DataFrames\ncj_data = get_data()\ncj_data.keys()\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/completejourney_py/get_data.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.\n  from pkg_resources import resource_filename\n\n\ndict_keys(['campaign_descriptions', 'coupons', 'promotions', 'campaigns', 'demographics', 'transactions', 'coupon_redemptions', 'products'])\n# We can check out the transactions data with the following\ncj_data['transactions'].head()\n\n\n\n\n\n\n\n\nhousehold_id\nstore_id\nbasket_id\nproduct_id\nquantity\nsales_value\nretail_disc\ncoupon_disc\ncoupon_match_disc\nweek\ntransaction_timestamp\n\n\n\n\n0\n900\n330\n31198570044\n1095275\n1\n0.50\n0.00\n0.0\n0.0\n1\n2017-01-01 11:53:26\n\n\n1\n900\n330\n31198570047\n9878513\n1\n0.99\n0.10\n0.0\n0.0\n1\n2017-01-01 12:10:28\n\n\n2\n1228\n406\n31198655051\n1041453\n1\n1.43\n0.15\n0.0\n0.0\n1\n2017-01-01 12:26:30\n\n\n3\n906\n319\n31198705046\n1020156\n1\n1.50\n0.29\n0.0\n0.0\n1\n2017-01-01 12:30:27\n\n\n4\n906\n319\n31198705046\n1053875\n2\n2.78\n0.80\n0.0\n0.0\n1\n2017-01-01 12:30:27",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#prerequisites",
    "href": "12-joining-data.html#prerequisites",
    "title": "12  Relational data",
    "section": "",
    "text": "The colored column represents the key variable used to match rows between tables.\nThe gray column represents the value column—this is the information that gets carried along in the merge.\n\n\n\n\n\n\n\nSimple Example DFs\n\n\n\n\nFigure 12.1: Two simple DataFrames with a shared key column.\n\n\n\n\n\n\n\n\n\ntransactions: item-level purchases made by households at a retail grocery store\ndemographics: household demographic data (age, income, family size, etc.)\nproducts: product metadata (brand, description, etc.)\ncampaigns: campaigns received by each household\ncampaign_descriptions: campaign metadata (length of time active)\ncoupons: coupon metadata (UPC code, campaign, etc.)\ncoupon_redemptions: coupon redemptions (household, day, UPC code, campaign)\n\n\n\n\n\n\n\n\n\n\nNoneTODO\n\n\n\nTake some time to read about the completejourney data set schema here.\n\nWhat different data sets are available and what do they represent?\nWhat are the common variables between each table?",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#understanding-keys",
    "href": "12-joining-data.html#understanding-keys",
    "title": "12  Relational data",
    "section": "12.2 🔑 Understanding Keys",
    "text": "12.2 🔑 Understanding Keys\nTo combine two tables, we need a way to tell pandas how rows in one table relate to rows in another. That’s where keys come in.\nA key is one or more columns used to match rows between two tables. These columns typically contain identifiers that link observations—like customer IDs, product codes, or dates.\nThere are two main types of keys you’ll encounter:\n\nA primary key uniquely identifies each row within its own table.\nA foreign key connects to a primary key in another table, creating a relationship between the two.\n\n\n\n\n\n\n\nFor example:\n\nIn the transactions table, household_id acts as a foreign key—it tells us which household made the purchase.\nIn the demographics table, household_id is a primary key—each household appears only once.\n\n\n\n\nTogether, these keys form a relation. In most cases, the relationship is one-to-many: one household can have many transactions, but each transaction belongs to only one household. Occasionally, you’ll encounter one-to-one relationships, where each row in one table maps to exactly one row in another.\nWhen data is cleaned appropriately the keys used to match two tables will be commonly named. For example, the variable that can link our x and y data sets is named id:\n\nx.columns.intersection(y.columns)\n\nIndex(['id'], dtype='object')\n\n\nWe can easily see this by looking at the x and y data but when working with larger data sets this becomes more appropriate than just viewing the data. For example, we can easily identify the common columns in the completejourney_py transactions and demographics data:\n\ntransactions = cj_data['transactions']\ndemographics = cj_data['demographics']\n\ntransactions.columns.intersection(demographics.columns)\n\nIndex(['household_id'], dtype='object')\n\n\n\n\n\n\n\n\nNoteA Note on Column Names\n\n\n\nWhile it’s common for keys to have the same name in both tables (like id, household_id, or product_id), that’s not always the case. For example, our household identifier could be named household_id in the transaction data but be hshd_id in the demographics table. Although the names differ, they represent the same information. When column names differ, you can still join the tables—you just need to tell pandas which columns to use, which we will discuss later.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#mutating-joins",
    "href": "12-joining-data.html#mutating-joins",
    "title": "12  Relational data",
    "section": "12.3 Mutating Joins",
    "text": "12.3 Mutating Joins\nWhen working with multiple DataFrames, we often want to combine information from different sources based on a shared key. A mutating join allows us to do exactly that—it matches rows across two tables based on a key and brings in additional columns from one table to the other.\nIn this section, you’ll learn how to use pandas to perform various types of joins that are essential for working with relational data.\n\nTypes of Joins\nThere are several types of joins, each serving a different purpose:\n\nInner join: Keeps only the rows with keys that match in both tables.\nLeft join: Keeps all rows from the left table and brings in matching rows from the right.\nRight join: Keeps all rows from the right table and brings in matching rows from the left.\nFull outer join: Keeps all rows from both tables.\n\n\n\n\n\n\n\nIn pandas, you can join DataFrames using either .join() or .merge(). While .join() is designed for joining on indexes, .merge() is more flexible and allows joining on one or more columns. We’ll use .merge() throughout this chapter.\n\n\n\n\n\nInner Join\nAn inner join returns only the rows where the key exists in both DataFrames. This is the most restrictive type of join.\n\n\n\n\n\n\nInner join\n\n\n\n\nFigure 12.2: Inner join (source).\n\n\n\n\nx.merge(y, on=\"id\", how=\"inner\")\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nx1\ny1\n\n\n1\n2\nx2\ny2\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnly rows with matching values in both x and y are retained. In our example, only id values 1 and 2 appear in both tables.\n\n\n\n\n\nOuter Joins\nAn inner join keeps observations that appear in both tables. However, we often want to retain all observations in at least one of the tables. Consequently, we can apply various outer joins to retain observations that appear in at least one of the tables. There are three main types of outer joins:\n\nA left join keeps all observations in x.\nA right join keeps all observations in y.\nA full join keeps all observations in x and y.\n\nThese joins work by adding NaN in rows where non-matching information exists:\n\n\n\n\n\n\nExamples of outer joins\n\n\n\n\nFigure 12.3: Difference in left join, right join, and outer join procedures (source).\n\n\n\n\nLeft Join\nA left join keeps all rows from the left DataFrame (x) and adds matching rows from the right DataFrame (y). If no match is found, the result will contain NaN for the missing values.\n\nx.merge(y, on=\"id\", how=\"left\")\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nx1\ny1\n\n\n1\n2\nx2\ny2\n\n\n2\n3\nx3\nNaN\n\n\n\n\n\n\n\n\n\nRight Join\nA right join is similar to a left join, but it retains all rows from the right DataFrame (y).\n\nx.merge(y, on=\"id\", how=\"right\")\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nx1\ny1\n\n\n1\n2\nx2\ny2\n\n\n2\n4\nNaN\ny4\n\n\n\n\n\n\n\n\n\n\n\n\n\nShould I use a right join, or a left join? To answer this, ask yourself “which DataFrame should retain all of its rows?” - and use this one as the baseline. A left join keeps all the rows in the first (leftside) DataFrame written in the command, whereas a right join keeps all the rows in the second (rightside) DataFrame.\n\n\n\n\n\nFull Outer Join\nA full outer join retains all rows from both DataFrames. Where there are no matches, it fills in NaN for missing values.\n\nx.merge(y, on=\"id\", how=\"outer\")\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nx1\ny1\n\n\n1\n2\nx2\ny2\n\n\n2\n3\nx3\nNaN\n\n\n3\n4\nNaN\ny4\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is the most inclusive join. It’s useful when you don’t want to lose any data.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#working-with-differently-named-keys",
    "href": "12-joining-data.html#working-with-differently-named-keys",
    "title": "12  Relational data",
    "section": "12.4 Working with Differently Named Keys",
    "text": "12.4 Working with Differently Named Keys\nSo far, the keys we’ve used to join two DataFrames have had the same name. This was encoded by using on='id'. However, having keys with the same name is not a requirement. But what happens we our common key variable is named differently in each DataFrame?\nFor example:\n\na = pd.DataFrame({'id_a': [1, 2, 3], 'val_a': ['x1', 'x2', 'x3']})\nb = pd.DataFrame({'id_b': [1, 2, 4], 'val_b': ['y1', 'y2', 'y4']})\n\nIn this case, since our common key variable has different names in each table (id_a in a and id_b in b), our inner join function doesn’t know how to join these two DataFrames and an error results.\n\n\n\n\n\n\nWarningExample merge error\n\n\n\n\n\n\na.merge(b)\n\n\n---------------------------------------------------------------------------\nMergeError                                Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 a.merge(b)\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/frame.py:10839, in DataFrame.merge(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\n  10820 @Substitution(\"\")\n  10821 @Appender(_merge_doc, indents=2)\n  10822 def merge(\n   (...)\n  10835     validate: MergeValidate | None = None,\n  10836 ) -&gt; DataFrame:\n  10837     from pandas.core.reshape.merge import merge\n&gt; 10839     return merge(\n  10840         self,\n  10841         right,\n  10842         how=how,\n  10843         on=on,\n  10844         left_on=left_on,\n  10845         right_on=right_on,\n  10846         left_index=left_index,\n  10847         right_index=right_index,\n  10848         sort=sort,\n  10849         suffixes=suffixes,\n  10850         copy=copy,\n  10851         indicator=indicator,\n  10852         validate=validate,\n  10853     )\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/reshape/merge.py:170, in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\n    155     return _cross_merge(\n    156         left_df,\n    157         right_df,\n   (...)\n    167         copy=copy,\n    168     )\n    169 else:\n--&gt; 170     op = _MergeOperation(\n    171         left_df,\n    172         right_df,\n    173         how=how,\n    174         on=on,\n    175         left_on=left_on,\n    176         right_on=right_on,\n    177         left_index=left_index,\n    178         right_index=right_index,\n    179         sort=sort,\n    180         suffixes=suffixes,\n    181         indicator=indicator,\n    182         validate=validate,\n    183     )\n    184     return op.get_result(copy=copy)\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/reshape/merge.py:786, in _MergeOperation.__init__(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\n    779     msg = (\n    780         \"Not allowed to merge between different levels. \"\n    781         f\"({_left.columns.nlevels} levels on the left, \"\n    782         f\"{_right.columns.nlevels} on the right)\"\n    783     )\n    784     raise MergeError(msg)\n--&gt; 786 self.left_on, self.right_on = self._validate_left_right_on(left_on, right_on)\n    788 (\n    789     self.left_join_keys,\n    790     self.right_join_keys,\n   (...)\n    793     right_drop,\n    794 ) = self._get_merge_keys()\n    796 if left_drop:\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/reshape/merge.py:1572, in _MergeOperation._validate_left_right_on(self, left_on, right_on)\n   1570 common_cols = left_cols.intersection(right_cols)\n   1571 if len(common_cols) == 0:\n-&gt; 1572     raise MergeError(\n   1573         \"No common columns to perform merge on. \"\n   1574         f\"Merge options: left_on={left_on}, \"\n   1575         f\"right_on={right_on}, \"\n   1576         f\"left_index={self.left_index}, \"\n   1577         f\"right_index={self.right_index}\"\n   1578     )\n   1579 if (\n   1580     not left_cols.join(common_cols, how=\"inner\").is_unique\n   1581     or not right_cols.join(common_cols, how=\"inner\").is_unique\n   1582 ):\n   1583     raise MergeError(f\"Data columns not unique: {repr(common_cols)}\")\n\nMergeError: No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False\n\n\n\n\n\n\nWhen this happens, we can explicitly tell our join function to use unique key names in each DataFrame as a common key with the left_on and right_on arguments:\n\na.merge(b, left_on=\"id_a\", right_on=\"id_b\")\n\n\n\n\n\n\n\n\nid_a\nval_a\nid_b\nval_b\n\n\n\n\n0\n1\nx1\n1\ny1\n\n\n1\n2\nx2\n2\ny2",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#a-larger-example-with-complete-journey-data",
    "href": "12-joining-data.html#a-larger-example-with-complete-journey-data",
    "title": "12  Relational data",
    "section": "12.5 A Larger Example with Complete Journey Data",
    "text": "12.5 A Larger Example with Complete Journey Data\nLet’s apply what we’ve learned to real data from the completejourney_py package.\nSuppose we want to add product details to each transaction. That means we’ll join the transactions and products DataFrames. Because we want to retain all transaction records, even if product details are missing, we’ll use a left join.\nFirst, check the column names:\n\ncj_data = get_data()\ntransactions = cj_data[\"transactions\"]\nproducts = cj_data[\"products\"]\n\nprint(f'transactions columns: {transactions.columns}')\nprint(f'products columns: {products.columns}')\n\ntransactions columns: Index(['household_id', 'store_id', 'basket_id', 'product_id', 'quantity',\n       'sales_value', 'retail_disc', 'coupon_disc', 'coupon_match_disc',\n       'week', 'transaction_timestamp'],\n      dtype='object')\nproducts columns: Index(['product_id', 'manufacturer_id', 'department', 'brand',\n       'product_category', 'product_type', 'package_size'],\n      dtype='object')\n\n\nAnd we can find if a common column name exists:\n\ntransactions.columns.intersection(products.columns)\n\nIndex(['product_id'], dtype='object')\n\n\nWe see that both DataFrames share the product_id column. This aligns to the data dictionary so we can trust this is the accurate common key. We can now perform a left join usingproduct_id as the common key.\n\n\n\n\n\n\nJoins add new variables to the far right of the resulting DataFrame. If you’re working in a wide table, you may need to scroll to see the added columns.\n\n\n\n\ntransactions.merge(products, on=\"product_id\", how=\"left\").head()\n\n\n\n\n\n\n\n\nhousehold_id\nstore_id\nbasket_id\nproduct_id\nquantity\nsales_value\nretail_disc\ncoupon_disc\ncoupon_match_disc\nweek\ntransaction_timestamp\nmanufacturer_id\ndepartment\nbrand\nproduct_category\nproduct_type\npackage_size\n\n\n\n\n0\n900\n330\n31198570044\n1095275\n1\n0.50\n0.00\n0.0\n0.0\n1\n2017-01-01 11:53:26\n2.0\nPASTRY\nNational\nROLLS\nROLLS: BAGELS\n4 OZ\n\n\n1\n900\n330\n31198570047\n9878513\n1\n0.99\n0.10\n0.0\n0.0\n1\n2017-01-01 12:10:28\n69.0\nGROCERY\nPrivate\nFACIAL TISS/DNR NAPKIN\nFACIAL TISSUE & PAPER HANDKE\n85 CT\n\n\n2\n1228\n406\n31198655051\n1041453\n1\n1.43\n0.15\n0.0\n0.0\n1\n2017-01-01 12:26:30\n69.0\nGROCERY\nPrivate\nBAG SNACKS\nPOTATO CHIPS\n11.5 OZ\n\n\n3\n906\n319\n31198705046\n1020156\n1\n1.50\n0.29\n0.0\n0.0\n1\n2017-01-01 12:30:27\n2142.0\nGROCERY\nNational\nREFRGRATD DOUGH PRODUCTS\nREFRIGERATED BAGELS\n17.1 OZ\n\n\n4\n906\n319\n31198705046\n1053875\n2\n2.78\n0.80\n0.0\n0.0\n1\n2017-01-01 12:30:27\n2326.0\nGROCERY\nNational\nSEAFOOD - SHELF STABLE\nTUNA\n5.0 OZ\n\n\n\n\n\n\n\nThis has now added product information to each transaction. Consequently, if we wanted to get the total sales across the meat department but summarized at the product_category level so that we can identify which products generate the greatest sales we could follow this joining procedure with additional skills we learned in previous lessons:\n\n(\n    transactions\n    .merge(products, how='left', on='product_id')\n    .query(\"department == 'MEAT'\")\n    .groupby('product_category', as_index=False)\n    .agg({'sales_value': 'sum'})\n    .sort_values(by='sales_value', ascending=False)\n)\n\n\n\n\n\n\n\n\nproduct_category\nsales_value\n\n\n\n\n1\nBEEF\n176614.54\n\n\n2\nCHICKEN\n52703.51\n\n\n10\nPORK\n50809.31\n\n\n12\nSMOKED MEATS\n15324.22\n\n\n13\nTURKEY\n11128.95\n\n\n4\nEXOTIC GAME/FOWL\n860.42\n\n\n5\nLAMB\n829.27\n\n\n14\nVEAL\n167.13\n\n\n8\nMEAT SUPPLIES\n57.03\n\n\n7\nMEAT - MISC\n39.66\n\n\n11\nRW FRESH PROCESSED MEAT\n30.84\n\n\n3\nCOUPON\n7.00\n\n\n6\nLUNCHMEAT\n2.20\n\n\n9\nMISCELLANEOUS\n0.95\n\n\n0\nBACON\n0.30\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nJoin the transactions and demographics data so that you have household demographics for each transaction. Now compute the total sales by age category to identify which age group generates the most sales.\nUse successive joins to join transactions with coupons and then with coupon_redemptions. Use the proper join that will only retain those transactions that have coupon and coupon redemption data.\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#merge-indicator",
    "href": "12-joining-data.html#merge-indicator",
    "title": "12  Relational data",
    "section": "12.6 Merge Indicator",
    "text": "12.6 Merge Indicator\nYou can use the indicator argument to add a special column called _merge that shows where each row in the joined table came from. The values in _merge will be:\n\n'left_only' — the row came only from the left table\n'right_only' — the row came only from the right table\n'both' — the row had a match in both tables\n\nThis is helpful when you’re trying to understand or debug the result of a join:\n\nx.merge(y, how='outer', indicator=True)\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n_merge\n\n\n\n\n0\n1\nx1\ny1\nboth\n\n\n1\n2\nx2\ny2\nboth\n\n\n2\n3\nx3\nNaN\nleft_only\n\n\n3\n4\nNaN\ny4\nright_only\n\n\n\n\n\n\n\nThis feature is also useful when filtering rows based on whether they had a match. For example:\n\n\n\n\n\n\nNoneScenario: Your manager asks,\n\n\n\n\n“Of all our transactions, how many are from households that we don’t have demographic information for?”\n\n\n\nYou can answer this by performing an outer join between transactions and demographics, then filtering for rows with _merge == 'left_only':\n\n# Total number of transactions\ntransactions.shape\n\n(1469307, 11)\n\n\n\n# Transactions without matching demographic info\n(\n    transactions\n    .merge(demographics, how='outer', indicator=True)\n    .query(\"_merge == 'left_only'\")\n).shape\n\n(640457, 19)\n\n\nIn this case, 640,457 transactions (about 43%) come from households without demographic information.\n\n🔍 Knowledge Check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nUsing the products and transactions tables, how many products have been sold?\nHow many products are in inventory but have not appeared in any transaction?\nUsing demographics and transactions, which income group buys the highest total quantity of goods?\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#summary",
    "href": "12-joining-data.html#summary",
    "title": "12  Relational data",
    "section": "12.7 Summary",
    "text": "12.7 Summary\nIn this chapter, you explored how to work with relational data—data that lives across multiple, related tables. You learned how to use joins in pandas to combine these tables using shared key variables, allowing you to answer more complex and meaningful questions.\nYou practiced:\n\nIdentifying and working with keys (primary and foreign).\nPerforming mutating joins including inner, left, right, and full outer joins.\nHandling differently named key columns using left_on and right_on.\nUsing the merge indicator to understand which rows matched or didn’t match between tables.\nApplying these techniques to real retail transaction data using the completejourney_py package.\n\nRelational data techniques give you the power to piece together a fuller picture of what’s happening in your data. But once you’ve wrangled the data into shape, the next challenge is making your findings clear and compelling.\nIn the next chapter, we’ll shift gears and explore data visualization—learning how to communicate insights effectively using Python plotting libraries like matplotlib and seaborn.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#exercise-understanding-shopping-behavior",
    "href": "12-joining-data.html#exercise-understanding-shopping-behavior",
    "title": "12  Relational data",
    "section": "12.8 Exercise: Understanding Shopping Behavior",
    "text": "12.8 Exercise: Understanding Shopping Behavior\nUse the datasets provided by the completejourney_py package to complete the following exercises. These tasks will help you practice joining tables, filtering data, and computing summary statistics. If you’re unfamiliar with the structure of these datasets, take a few minutes to review the Complete Journey data dictionary to understand how the tables relate to one another.\n\n\n\n\n\n\nNone1. Total Sales by Age Group (With a Join)\n\n\n\n\n\nJoin the transactions and demographics tables. Then compute the total sales_value by age group.\n\nWhich age group generates the most total sales?\nBonus: How does this change if you only consider transactions over $5?\n\n\n\n\n\n\n\n\n\n\nNone2. What Are Families Buying?\n\n\n\n\n\nUsing the transactions, products, and demographics tables:\n\nFocus only on households with 3 or more members (use household_size from demographics).\nIdentify the top 5 most frequently purchased product categories among these households.\nWhat percent of their purchases come from the MEAT department?\n\n\n\n\n\n\n\n\n\n\nNone3. Coupon-Driven Purchases\n\n\n\n\n\nJoin transactions, coupon_redemptions, and coupons together. Then:\n\nFind the total number of coupon redemptions by campaign.\nWhich campaign resulted in the highest total sales_value?\n\n\n\n\n\n\n\n\n\n\nNone4. High Spenders Without Demographics\n\n\n\n\n\nFrom your existing prompt:\n\nIdentify households with total sales ≥ $100.\nAmong them, how many do not appear in the demographics table?\nWhat percent of all $100+ spenders are missing demographic info?\n\nTip: Use the indicator=True argument to help with this.\n\n\n\n\n\n\n\n\n\nNone5. Front Display Effectiveness\n\n\n\n\n\nJoin promotions with transactions and products.\n\nFilter to products that were part of a front-of-store display (display_location == 1).\nFor those products, compute:\n\nTotal sales_value\nTotal quantity sold\nTop 3 departments (by sales)\n\n\nHow do these compare to products not on display?",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html",
    "href": "13-data-viz-pandas.html",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "",
    "text": "13.1 Prerequisites\nData visualization is a core skill in any data analyst’s toolkit. While numbers and statistics tell part of the story, charts and graphs help us understand patterns, uncover insights, and communicate findings more effectively. There are two primary use cases for visualization in data science:\nIn this chapter, we focus on exploratory visualization using Pandas, a quick and powerful way to visually explore data and uncover insights — all within the same environment where your analysis lives. In future chapters, we’ll dive deeper into refining visualizations for explanatory purposes, showing how libraries like Matplotlib* and Bokeh can help you tell compelling, professional-grade data stories.\nBy the end of this chapter, you will be able to:\nWe’ll use the completejourney_py data to analyze household shopping behavior. First, import the libraries and prepare your data:\nimport pandas as pd\nfrom completejourney_py import get_data\n\ncj_data = get_data()\ntransactions = cj_data['transactions']\nproducts = cj_data['products']\ndemographics = cj_data['demographics']\n\n# Merge datasets to enrich transaction data\ndf = (transactions\n      .merge(products, on='product_id', how='left')\n      .merge(demographics, on='household_id', how='left'))\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/completejourney_py/get_data.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.\n  from pkg_resources import resource_filename",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html#the-.plot-attribute",
    "href": "13-data-viz-pandas.html#the-.plot-attribute",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "13.2 The .plot Attribute",
    "text": "13.2 The .plot Attribute\nOne of the most convenient aspects of using Pandas for data visualization is that Series and DataFrame objects come equipped with a built-in .plot attribute. This attribute serves as a simple wrapper around the powerful Matplotlib library (which we’ll discuss in the next chapter), enabling you to generate a wide range of plots with minimal code.\nThis .plot attribute is a little unique as you can call it as a method and specify the plot of interest using an argument:\ndf.plot(kind='scatter', ...)\nOr you can use it to access sub-methods:\ndf.plot.scatter(...)\nThere are several plotting options via .plot, including but not limited to:\n\nline: (default) for time series or continuous data\nbar / barh: for categorical comparisons\nhist: for distributions of continuous variables\nbox: for spotting outliers and summary statistics\nscatter: for examining relationships between two numeric variables\n\nHere’s a simple example of a histogram created from a Series:\n\ndf['sales_value'].plot.hist(bins=20, log=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneVideo Primer\n\n\n\nHere’s a nice introductory video to watch before we dig into the details that follow.\n\n\n\n\n🔍 Knowledge Check\n\n\n\n\n\n\nNoteDo This!\n\n\n\nCheck out the .plot documentation for Series and DataFrames.\n\nWhat parameter would you use to control the figure size?\nWhat parameter would you use to add a title?\nWhat parameter(s) would you use to log scale an x and/or y axis?",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html#visualizing-single-variables-univariate-plots",
    "href": "13-data-viz-pandas.html#visualizing-single-variables-univariate-plots",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "13.3 Visualizing Single Variables (Univariate Plots)",
    "text": "13.3 Visualizing Single Variables (Univariate Plots)\nUnivariate plots help us understand the distribution, range, and key characteristics of a single variable. These visualizations are especially useful during exploratory data analysis, where you want to:\n\nSpot patterns, such as skewness or multimodality\nIdentify outliers or unusual values\nCompare distributions across different groups (later, with grouping)\n\nCommon types of univariate plots include:\n\nHistograms: Show the distribution of a numerical variable by dividing values into bins.\nBoxplots: Display the median, quartiles, and potential outliers in a compact format.\nBar plots (for categorical variables): Visualize frequency or total values across discrete categories.\n\nThese plots are quick to generate and provide immediate insight into the shape and spread of your data. Let’s look at a couple of examples using the sales_value variable. We’ve already looked at some summary stats of our sales_value distribution…\n\ndf['sales_value'].describe()\n\ncount    1.469307e+06\nmean     3.128032e+00\nstd      4.290385e+00\nmin      0.000000e+00\n25%      1.290000e+00\n50%      2.000000e+00\n75%      3.490000e+00\nmax      8.400000e+02\nName: sales_value, dtype: float64\n\n\nbut we can understand more by visualizing this variable. But if we look at a basic histogram it doesn’t tell us a whole lot because we have several zero dollar transactions (i.e. returns) in our data plus this feature is heavily skewed towards low value transactions (measured by cents or single dollar values).\n\ndf['sales_value'].plot.hist()\n\n\n\n\n\n\n\n\nWe can make some adjustments such as remove any zero dollar transactions, log transform our axis, and increase the number of bins. This helps to pull out additional insights in our sales_value distribution. For example, we see we have a lot of very low dollar transactions and the frequency decreases as the transaction dollar amount increases. However, we also see an increase in transactions right at the $200 mark but that decreases quickly. There are also a few outlier transaction values that are around the $600 and $800 value marks.\n\n(\n    df.loc[df['sales_value'] &gt; 0, 'sales_value']\n    .plot.hist(log=True, bins=30, title='Distribution of Sales Values')\n);\n\n\n\n\n\n\n\n\nBox plots and kernel density estimation (KDE) plots are an alternative way to view univariate distribtions. For example, let’s compute the total sales_value across all stores. The resulting sales_by_store object is a Series. A boxplot provides a lot of information (read about them here). We can see that the median (red line) is around \\(10^2 = 100\\) and the interquartile range is within the blue box. We also see we have some outliers on the upper end.\n\nsales_by_store = df.groupby('store_id')['sales_value'].sum()\n\n# boxplot\nsales_by_store.plot.box(logy=True, title='Distribution of total sales across all stores');\n\n\n\n\n\n\n\n\nWe can quickly compare our boxplot with our numeric distribution and we see our they are similar (median: 96, interquartile range: 25-2966).\n\nsales_by_store.describe()\n\ncount       457.000000\nmean      10056.979387\nstd       20671.239346\nmin           0.500000\n25%          25.390000\n50%          95.590000\n75%        2965.560000\nmax      148169.670000\nName: sales_value, dtype: float64\n\n\nThe KDE plot (which is also produced with .plot.density()) provides a smoothed histogram.\n\nsales_by_store.plot.kde(title='Distribution of total sales across all stores');\n\n\n\n\n\n\n\n\nThe .plot sub-methods work exceptionally well with time series data. To illustrate, let’s create a Series that contains the sales_value of each transaction with the transaction_timestamp as the index.\n\nsales = df.set_index('transaction_timestamp')['sales_value']\nsales.head()\n\ntransaction_timestamp\n2017-01-01 11:53:26    0.50\n2017-01-01 12:10:28    0.99\n2017-01-01 12:26:30    1.43\n2017-01-01 12:30:27    1.50\n2017-01-01 12:30:27    2.78\nName: sales_value, dtype: float64\n\n\nA handy method we have not talked about is resample() which allows us to easily convert time series data. For example, if we wanted to sum all sales_values by the hour we can use .resample('h') followed by .sum().\n\nsales.resample('h').sum()\n\ntransaction_timestamp\n2017-01-01 11:00:00      0.50\n2017-01-01 12:00:00     19.76\n2017-01-01 13:00:00     41.92\n2017-01-01 14:00:00     97.31\n2017-01-01 15:00:00    769.62\n                        ...  \n2018-01-01 00:00:00    607.66\n2018-01-01 01:00:00    524.89\n2018-01-01 02:00:00    493.45\n2018-01-01 03:00:00    256.74\n2018-01-01 04:00:00     10.52\nFreq: h, Name: sales_value, Length: 8754, dtype: float64\n\n\nIf we followed this sequence of code with plot.line() we get a line plot of the total sales values on the y-axis and the date-time on the x-axis.\n\n(\n    sales\n    .resample('h')\n    .sum()\n    .plot.line(figsize=(10,4))\n);\n\n\n\n\n\n\n\n\nThe above plot is a bit busy since we’re plotting values for every hour over the course of a year. Let’s reduce the frequency and, instead, sum the sales_values by day (.resample('D')). Now we see a bit more of a descriptive pattern. It looks like there is routinely higher sales transactions on particular days (probably certain days of the week such as weekends).\n\n(\n    sales\n    .resample('D')\n    .sum()\n    .plot.line(figsize=(10,4))\n);\n\n\n\n\n\n\n\n\nLet’s validate our assumption above regarding the weekly shopping pattern. The below code chunk performs the same as above where we compute total daily sales across all days of the year but then we follow that up by extracting the name of the weekday from the date-timestamp and then grouping by the day of week and computing the median and interquartile range (IQR) for all daily sales for the year.\n\n\n\n\n\n\nIf you have not yet seen code that looks like lambda idx: idx.day_name() do not worry. These are called lambda (anonymous) functions and we’ll discuss them more in a future chapter.\n\n\n\nWe definitely see that Saturday and Sunday are the weekdays with the heaviest sales value transactions.\n\nday_order = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\ntotal_sales_by_weekday = (\n    sales\n    .resample('D')                        # resample by day\n    .sum()                                # compute total daily sales\n    .rename(lambda idx: idx.day_name())   # extract week day name from date-timestamp\n    .groupby('transaction_timestamp')     # group by day of week\n    .quantile([.25, .5, .75])             # compute median and IQR of sales values\n    .unstack()                            # flatten output (results in a DataFrame)\n    .reindex(day_order)                   # force index to follow weekday order         \n)\n\ntotal_sales_by_weekday.plot.line(title='Median and IQR of total sales by weekday', figsize=(10,4));\n\n\n\n\n\n\n\n\nAnother common plot for Series data is the bar plot. Let’s look at the median values from the analysis above. If we peak at the result we see we have a Series that contains the median total sales values for each weekday.\n\nmedian_sales_by_weekday = total_sales_by_weekday[0.50]\nmedian_sales_by_weekday\n\ntransaction_timestamp\nMonday       12129.070\nTuesday      11191.945\nWednesday    10558.995\nThursday     10852.630\nFriday       11959.995\nSaturday     14741.720\nSunday       15745.450\nName: 0.5, dtype: float64\n\n\nRather than plot this as a line chart as we did above, we can use .plot.bar() to create a bar plot:\n\nmedian_sales_by_weekday.plot.bar(title='Median total sales by weekday', figsize=(8,4));\n\n\n\n\n\n\n\n\nA common pattern you’ll use is to follow a .value_counts() method call with a bar plot. For example, say we want to assess the number of transactions in our data by department. We could easily get this with the following:\n\n(\n    df['department']\n    .value_counts(ascending=True)\n    .plot.barh(title='Total transactions by department', figsize=(6,8))\n);\n\n\n\n\n\n\n\n\nUnfortunately, we see a lot of very small values that overcrowds the plot. We can make some small adjustments to our code to leave all department values for those departments in the top 10 as is but for all departments not in the top 10 we can condense them down to an ‘Other’ category.\n\n\n\n\n\n\nWe will discuss the .where() method in a later module. For now just realize its a way to apply an if-else condition to a Series.\n\n\n\n\ntop10 = df['department'].value_counts().index[:10]\nisin_top10 = df['department'].isin(top10)\n\n(\n    df['department']\n    .where(isin_top10, 'Other')\n    .value_counts(ascending=True)\n    .plot.barh(title='Total transaction sales by department', figsize=(6,6))\n);\n\n\n\n\n\n\n\n\n\n🔍 Summary: Exploring Univariate Distributions Through Iteration\nThis section demonstrates how we can iteratively filter, aggregate, and visualize data to better understand the distribution and characteristics of a single variable — in this case, sales_value. Starting with basic summary statistics, we build deeper insights by:\n\nFiltering out uninformative records (e.g., $0 transactions)\nAggregating data at relevant levels (e.g., by basket or store)\nApplying different types of univariate plots (histograms, boxplots, KDEs) to reveal patterns, outliers, and skew\n\nThrough this iterative approach, we move from basic descriptive stats to more nuanced visual understanding of sales behavior — helping us identify patterns like high-frequency low-dollar transactions or weekly sales cycles. The section also illustrates how to apply .plot sub-methods for time series and categorical visualizations, reinforcing how visual exploration and data transformation go hand-in-hand in exploratory data analysis.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This!\n\n\n\n\nCreate a histogram for the quantity column. Remove any zero quantities and/or adjust the axis to make the plot more informative.\nCompute the sum of quantity for each store_id. Now create density plot and box plot. Compare these plots to the summary statistics provided by .describe().\nUse .resample() to compute the sum of quantity for each day. Plot the results to assess if there is similar pattern as we saw with sales_value.\nUse a bar plot to plot the total transaction quantities by department. Are the results similar to what we saw with total sales_value by department?",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html#visualizing-relationships-between-variables-bivariate-plots",
    "href": "13-data-viz-pandas.html#visualizing-relationships-between-variables-bivariate-plots",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "13.4 Visualizing Relationships Between Variables (Bivariate Plots)",
    "text": "13.4 Visualizing Relationships Between Variables (Bivariate Plots)\nIn the previous section, we focused on understanding the distribution of a single variable — sales_value — using univariate plots. But as we explored patterns like how sales varied by day of the week or department, we were already stepping into the world of bivariate analysis: looking at how one variable (sales) changes in relation to another (like day or department).\nThis section builds on that by introducing formal approaches to bivariate visualization — using Pandas’ .plot() method with DataFrames to uncover relationships between two variables. These visualizations help answer questions like:\n\nHow do total sales compare across demographic groups?\nAre some product categories driving more revenue than others?\nHow does purchasing behavior change over time?\n\nLet’s look at some examples of bar plots, line plots, and scatter plots to explore bivariate comparisons and trends — reinforcing how visualizing relationships is key to discovering insights in your data.\n\nScatter Plots: Continuous Relationships\nTo visualize the relationship between two continuous variables, such as sales_value and quantity, we can use a scatter plot. This requires specifying the x and y arguments:\n\ndf.plot.scatter(x='quantity', y='sales_value', title='Sales versus quantity', figsize=(8,4))\n\n\n\n\n\n\n\n\nThis plot helps us see whether higher quantities tend to drive higher sales values (they do — but with a lot of low-spend noise). Scatter plots are ideal for spotting correlations, clusters, and outliers.\n\n\nBar Charts: Group Comparisons\nAlthough we previously used bar plots in a univariate context (e.g., frequency of departments), they’re just as valuable for bivariate visualizations — especially when summarizing a numeric variable across categories.\nLet’s say we want to view the top 10 departments by total sales:\n\ndept_sales = (\n    df\n    .groupby('department', as_index=False)\n    .agg({'sales_value': 'sum'})\n    .nlargest(10, 'sales_value')\n    .reset_index(drop=True)\n)\n\ndept_sales\n\n\n\n\n\n\n\n\ndepartment\nsales_value\n\n\n\n\n0\nGROCERY\n2316393.89\n\n\n1\nDRUG GM\n596827.45\n\n\n2\nFUEL\n329594.45\n\n\n3\nPRODUCE\n322858.82\n\n\n4\nMEAT\n308575.33\n\n\n5\nMEAT-PCKGD\n232282.53\n\n\n6\nDELI\n148344.06\n\n\n7\nMISCELLANEOUS\n78858.67\n\n\n8\nPASTRY\n69116.68\n\n\n9\nNUTRITION\n57261.22\n\n\n\n\n\n\n\nTo visualize this, we plot department on the x-axis and sales_value on the y-axis:\n\n(\n    dept_sales\n    .sort_values('sales_value')\n    .plot.barh(x='department', y='sales_value', color='red')\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s common to sort values before bar plotting to produce a more readable left-to-right visual.\n\n\n\n\n\nMulti-Series Plots\nOne benefit of plotting from a DataFrame is that we can visualize multiple numeric columns at once. Suppose we want to compare total sales_value and quantity per department:\n\ndept_totals = (\n    df\n    .query(\"department != 'FUEL' & department != 'MISCELLANEOUS'\")\n    .groupby('department', as_index=False)\n    .agg({'sales_value': 'sum', 'quantity': 'sum'})\n    .nlargest(10, 'sales_value')\n    .reset_index(drop=True)\n)\n\ndept_totals\n\n\n\n\n\n\n\n\ndepartment\nsales_value\nquantity\n\n\n\n\n0\nGROCERY\n2316393.89\n1242944\n\n\n1\nDRUG GM\n596827.45\n198635\n\n\n2\nPRODUCE\n322858.82\n185444\n\n\n3\nMEAT\n308575.33\n67526\n\n\n4\nMEAT-PCKGD\n232282.53\n83423\n\n\n5\nDELI\n148344.06\n37954\n\n\n6\nPASTRY\n69116.68\n28132\n\n\n7\nNUTRITION\n57261.22\n25024\n\n\n8\nSEAFOOD-PCKGD\n35977.46\n8028\n\n\n9\nFLORAL\n22303.18\n3160\n\n\n\n\n\n\n\nWe can pass both columns to the y argument to produce a grouped bar chart:\n\n(\n    dept_totals\n    .sort_values('sales_value')\n    .plot.barh(x='department', y=['sales_value', 'quantity'])\n    .legend(loc='lower right')\n)\n\n\n\n\n\n\n\n\nThis is especially useful when comparing two related metrics across the same category.\n\n\nTime Series with Multiple Columns\nWhen your DataFrame includes a datetime index, the .plot() methods become even more powerful. For example, let’s look at daily total discounts for the GROCERY department across three types of discounts:\n\ntotal_daily_discounts = (\n    df\n    .query(\"department == 'GROCERY'\")\n    .set_index('transaction_timestamp')\n    .loc[:, ['retail_disc', 'coupon_disc', 'coupon_match_disc']]\n    .resample('D')\n    .sum()\n)\n\ntotal_daily_discounts.head()\n\n\n\n\n\n\n\n\nretail_disc\ncoupon_disc\ncoupon_match_disc\n\n\ntransaction_timestamp\n\n\n\n\n\n\n\n2017-01-01\n925.84\n15.75\n1.90\n\n\n2017-01-02\n1158.67\n13.14\n4.55\n\n\n2017-01-03\n1153.42\n13.75\n5.84\n\n\n2017-01-04\n1266.71\n6.10\n2.10\n\n\n2017-01-05\n1359.38\n30.74\n5.64\n\n\n\n\n\n\n\nUsing .plot.line() will automatically generate a multi-line plot for all three columns:\n\ntotal_daily_discounts.plot.line(logy=True, figsize=(10, 4));\n\n\n\n\n\n\n\n\nYou can apply this same logic to other plot types like KDE, histogram, or box plots:\n\ntotal_daily_discounts.plot.kde(logx=True);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote📚 Want to See More Pandas Plot Examples?\n\n\n\nIf you’re looking to go beyond the examples you’ve seen thus far, check out these high-quality resources:\n\nPandas Official Visualization Docs The most authoritative guide for all things .plot(), with clear examples and customization options.\nPython Graph Gallery – Pandas Section A collection of simple, well-formatted Pandas plotting examples with explanations and visuals.\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This!\n\n\n\n\nCompute the average sales_value and quantity by household_id. Create a density plot that visualizes both columns together.\nUse a bar plot to assess whether married versus unmarried customers produce more transactions. Then do the same for age groups.\nUse .resample() to compute the monthly totals of quantity and sales_value. Plot the results to explore which months are the busiest.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html#under-the-hood---matplotlib",
    "href": "13-data-viz-pandas.html#under-the-hood---matplotlib",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "13.5 Under the hood - Matplotlib",
    "text": "13.5 Under the hood - Matplotlib\nUnderneath the hood Pandas is using Matplotlib to create the plots. Matplotlib is the most tried-and-true, mature plotting library in Python; however, its a bit more difficult to digest Matplotlib which is why I first introduce plotting with Pandas.\nIn the next lesson we will dig into Matplotlib because, with it being the most popular plotting library in the Python ecosystem, it is important for you to have a baseline understanding of its capabilities. But one thing I want to point out here is, since Pandas builds plots based on Matplotlib, we can actually use Matplotlib in conjunction with Pandas to advance our plots.\nFor example, Matplotlib provides many style options that can be used to beautify our plots. If you are familiar with fivethirtyeight.com you’ll know that most of their visualizations have a consistent theme. We can use Matplotlib to change the style of our plots to look like fivethirtyeight plots.\n\nimport matplotlib.pyplot as plt\n\nplt.style.use('fivethirtyeight')\n\nmedian_sales_by_weekday.plot.bar(title='Median total sales by weekday', figsize=(8,4))\n\n\n\n\n\n\n\n\nWe may also want to refine our tick marks so that they are formatted in the units of interest. For example, below we use Matplotlib’s ticker module to format our y-axis to be in dollar and comma formatted units:\n\nimport matplotlib.ticker as mtick\n\ntick_format = mtick.StrMethodFormatter('${x:,.0f}')\n\n(\n    median_sales_by_weekday\n    .plot.bar(title='Median total sales by weekday', xlabel='', figsize=(8,4))\n    .yaxis.set_major_formatter(tick_format)\n)\n\n\n\n\n\n\n\n\nWe’ll explore more Matplotlib capabilities in the next lesson but for now, happy Pandas plotting!",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html#summary",
    "href": "13-data-viz-pandas.html#summary",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "13.6 Summary",
    "text": "13.6 Summary\nIn this chapter, we explored how to use Pandas’ built-in .plot() functionality to quickly and effectively visualize data. Starting with univariate plots, we examined how to understand the distribution of a single variable — like sales_value — using histograms, boxplots, and KDE plots. We saw how filtering and transforming data before plotting helps surface key insights, such as frequent low-dollar transactions and outlier behavior.\nWe then moved into bivariate visualizations, where we analyzed how one variable changes in relation to another. We used scatter plots to assess continuous relationships, bar charts to compare grouped values (like total sales by department), and time series plots to understand trends over days, weeks, and months. Along the way, we learned how .plot() works with both Series and DataFrames, and how to create multi-series plots for deeper comparative insight.\nWhile Pandas plots are perfect for fast exploratory analysis, they also serve as a gentle introduction to the broader Python visualization ecosystem.\n\nWhat’s Next?\nIn the next chapter, we’ll dive deeper into the core engine behind Pandas plots — the Matplotlib library. You’ll learn how to build highly customized visualizations from scratch using Matplotlib’s object-oriented API. This will give you full control over plot styling, layout, and annotations — crucial for producing professional-grade charts.\nThen, in the final chapter of this visualization module, we’ll explore Bokeh, a more advanced plotting library for building interactive, web-ready charts. These tools are especially powerful when communicating results to stakeholders, allowing you to go beyond static images to create interactive dashboards and storytelling experiences.\nSo far, you’ve built a strong foundation in fast, exploratory plotting with Pandas. Up next: mastering the tools that help you refine and present your story with clarity and impact.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html#exercise-visualizing-pizza-shopping-behavior",
    "href": "13-data-viz-pandas.html#exercise-visualizing-pizza-shopping-behavior",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "13.7 Exercise: Visualizing Pizza Shopping Behavior",
    "text": "13.7 Exercise: Visualizing Pizza Shopping Behavior\nUse the datasets provided by the completejourney_py package to complete the following exercises. These tasks will help you practice filtering, grouping, and visualizing data using .plot() for both univariate and bivariate analysis.\n\n\n\n\n\n\nNone1. Identifying Pizza Products\n\n\n\n\n\nUsing the products table:\n\nIdentify all unique products where the product_type contains the word \"pizza\" (case-insensitive).\nHow many distinct pizza products are sold?\n\nTip: Use .str.contains() to search the column.\n\n\n\n\n\n\n\n\n\nNone2. Pizza Purchases by Marital Status\n\n\n\n\n\nJoin the transactions, products, and demographics tables.\n\nFilter to transactions where the product_type contains \"pizza\".\nUse a bar plot to compare the total quantity of pizza items purchased by married vs. unmarried households.\n\nBonus: Are there any differences in total sales_value between the groups?\n\n\n\n\n\n\n\n\n\nNone3. Quantity vs. Sales Value of Pizza\n\n\n\n\n\nUse a scatter plot to visualize the relationship between quantity and sales_value for pizza product transactions.\n\nWhat patterns do you observe?\nAre there any outliers in either dimension?\n\nTip: Use .plot.scatter() on the filtered DataFrame.\n\n\n\n\n\n\n\n\n\nNone4. Daily Pizza Sales Trends\n\n\n\n\n\nUse .resample() to analyze time-based patterns:\n\nFilter to pizza products as before.\nCompute the total quantity of pizza products purchased per day.\nUse a line plot to visualize the daily trend across the year.\n\nBonus: Do you notice any recurring spikes? Can you hypothesize what might cause them?",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html",
    "href": "14-data-viz-matplotlib.html",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "",
    "text": "14.1 Anatomy of a Figure\nMatplotlib is one of the most widely used libraries for creating visualizations in Python. While it’s more low-level than the plotting tools you used with Pandas, it gives you much more control over how your charts look and behave.\nIn fact, many other visualization libraries—like Pandas, Seaborn, and Altair—are built on top of Matplotlib. So, by learning the basics of Matplotlib, you’re building a strong foundation that will transfer to many other tools in the Python ecosystem.\nIn this chapter, you’ll get hands-on experience with Matplotlib and learn how to create and customize common plot types from scratch. By the end of this lesson you will be able to:\nThere is a hierarchy you must understand when plotting with Matplotlib. The highest and outermost part of a plot is the Figure. The Figure contains all the other plotting elements. Typically, you do not interact with it much. Inside the Figure is the Axes. This is the actual plotting surface that you normally would refer to as a ‘plot’.\nA Figure may contain any number of these Axes. The Axes is a container for all of the other physical pixels that get drawn onto your screen. This includes the x and y axis, lines, text, points, legends, images, etc.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#anatomy-of-a-figure",
    "href": "14-data-viz-matplotlib.html#anatomy-of-a-figure",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "",
    "text": "Figure 14.1: Anatomy of Matplotlib figures.\n\n\n\n\n\n\n\n\n\nWithin Matplotlib the term Axes is not actually plural and does not mean more than one axis. It literally stands for a single ‘plot’. It’s unfortunate that this fundamental element has a name that is so confusing.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#importing-the-pyplot-module",
    "href": "14-data-viz-matplotlib.html#importing-the-pyplot-module",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "14.2 Importing the pyplot module",
    "text": "14.2 Importing the pyplot module\nImporting matplotlib into your workspace is done a little differently than NumPy or Pandas. You rarely will import matplotlib itself directly like this:\n\nimport matplotlib\n\nThe above is perfectly valid code, but the matplotlib developers decided not to put all the main functionality in the top level module.\nWhen you import pandas as pd, you get access to nearly all of the available functions and classes of the Pandas library. This isn’t true with Matplotlib. Instead, much of the functionality for quickly plotting is found in the pyplot module. If you navigate to the matplotlib source directory, found in your site-packages directory, you will see a pyplot.py file. This is the module that you are importing into your workspace.\n\n\n\n\n\n\nFigure 14.2: pyplot submodule within matplotlib library.\n\n\n\n\n\n\n\n\n\nThere is some functionality in other matplotlib submodules; however, vast majority of what you will use starting out is isolated to the pyplot submodule.\n\n\n\nLet’s import the pyplot module now and alias it as plt, which is commonly done by convention:\n\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#figures-and-axes",
    "href": "14-data-viz-matplotlib.html#figures-and-axes",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "14.3 Figures and axes",
    "text": "14.3 Figures and axes\npyplot does provide lots of useful functions, one of which creates a Figure and any number of Axes that you desire. You can do this without pyplot, but it involves more syntax. It’s also quite standard to begin the object-oriented approach by laying out your Figure and Axes first with pyplot and then proceed by calling methods from these objects.\nThe pyplot subplots() function creates a single Figure and any number of Axes. If you call it with the default parameters it will create a single Axes within a Figure.\n\n\n\n\n\n\nThe subplots function returns a two-item tuple containing the Figure and the Axes. Recall from our earlier lesson where we learned about tuples that we can unpack each of these objects as their own variable.\n\n\n\n\nfig, ax = plt.subplots()\n\n\n\n\n\n\n\n\nLet’s verify that we indeed have a Figure and Axes.\n\ntype(fig)\n\nmatplotlib.figure.Figure\n\n\n\ntype(ax)\n\nmatplotlib.axes._axes.Axes\n\n\n\nDistinguishing the Figure from the Axes\nIt’s not obvious, from looking at the plot which part is the Figure and which is the Axes. We will call our first method, set_facecolor in an object-oriented fashion from both the Figure and Axes objects. We pass it a name of a color (more on colors later).\n\n# set figure and axes colors\nfig.set_facecolor('skyblue')\nax.set_facecolor('sandybrown')\n\n# show result\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice, that the two calls above to the set_facecolor method were made without an assignment statement. Both of these operations happened in-place. The calling Figure and Axes objects were updated without a new one getting created.\n\n\n\n\n\nSetting the size of the Figure upon creation\nThe default Figure is fairly small. We can change this when creating it with the figsize parameter. Pass a two-item tuple to configure the height and width of the figure as we did in the previous lesson with Pandas plotting. By default, these dimensions are 6 by 4. They represent inches and are literally the inches that your figure would be if you printed out on paper.\nBelow, we create a new Figure that is 8 inches in width by 4 inches in height. We also color the faces of the both the Figure and Axes again.\n\nfig, ax = plt.subplots(figsize=(8, 4))\nfig.set_facecolor('skyblue')\nax.set_facecolor('sandybrown')\n\n\n\n\n\n\n\n\nEverything on our plot is a separate object. Each of these objects may be explicitly referenced by a variable. Once we have a reference to a particular object, we can then modify it by calling methods on it.\nThus far we have two references, fig and ax. There are many other objects on our Axes that we can reference such as the x and y axis, tick marks, tick labels, and others. We do not yet have references to these objects.\n\n\nCalling Axes methods - get_ and set_ methods\nBefore we start referencing these other objects, let’s change some of the properties of our Axes by calling some methods on it. Many methods begin with either get_ or set_ followed by the part of the Axes that will get retrieved or modified. The following list shows several of the most common properties that can be set on our Axes. We will see examples of each one below.\n\ntitle\nxlabel/ylabel\nxlim/ylim\nxticks/yticks\nxticklabels/yticklabels\n\n\nGetting and setting the title of the Axes\nThe get_title method will return the title of the Axes as a string. There is no title at this moment so it returns an empty string.\n\nax.get_title()\n\n''\n\n\nThe set_title method will place a centered title on our Axes when passing it a string. Notice that a matplotlib Text object has been returned. More on this later.\n\nax.set_title('My First Matplotlib Graph')\n\nText(0.5, 1.0, 'My First Matplotlib Graph')\n\n\nAgain, we must place our Figure variable name as the last line in our cell to show it in our notebook.\n\nfig\n\n\n\n\n\n\n\n\nNow, if we run the get_title method again, we will get the string that was used as the title.\n\nax.get_title()\n\n'My First Matplotlib Graph'\n\n\n\n\nGetting and setting the x and y limits\nBy default, the limits of both the x and y axis are 0 to 1. We can change this with the set_xlim and set_ylim methods. Pass these methods a new left and right boundary to change the limits. These methods actually return a tuple of the limits.\n\nax.get_xlim()\n\n(np.float64(0.0), np.float64(1.0))\n\n\n\nax.get_ylim()\n\n(np.float64(0.0), np.float64(1.0))\n\n\n\nax.set_xlim(0, 5)\nax.set_ylim(-10, 50)\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice, that the size of the figure remains the same. Only the limits of the x and y axis have changed.\n\n\n\n\n\nGetting and setting the location of the x and y ticks\nIn the graph above, it has chosen to place ticks every 1 unit for the x and every 10 units for the y. Matplotlib chooses reasonable default values. Let’s see the location of these ticks with the get_xticks and get_yticks methods.\n\nax.get_xticks()\n\narray([0., 1., 2., 3., 4., 5.])\n\n\n\nax.get_yticks()\n\narray([-10.,   0.,  10.,  20.,  30.,  40.,  50.])\n\n\nWe can specify the exact location of the x and y ticks with the set_xticks and set_ticks methods. We pass them a list of numbers indicating where we want the ticks.\n\n\n\n\n\n\nIf we set the ticks outside of the current bounds of the axis, this forces matplotlib to change the limits. For example, below we set the lower bound ytick (-99) beyond the lower bound of the y-axis limit (-10).\n\n\n\n\nax.set_xticks([1.8, 3.99, 4.4])\nax.set_yticks([-99, -9, -1, 22, 44])\nfig\n\n\n\n\n\n\n\n\n\n\nGetting and setting the x and y tick labels\nThe current tick labels for the x-axis and y-axis are the same as the tick locations. Let’s first view the current tick labels. The output displays a list of Text objects that contain Text(xaxis_location, yaxis_location, tick_label).\n\nax.get_xticklabels()\n\n[Text(1.8, 0, '1.80'), Text(3.99, 0, '3.99'), Text(4.4, 0, '4.40')]\n\n\nWe can pass the set_xticklabels and set_yticklabels methods a list of strings to use as the new labels.\n\nax.set_xticklabels(['dog', 'cat', 'snake'])\nax.set_yticklabels(['Boehmke', 'D', 'A', 'R', 'B'])\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe tick locations are a completely separate concept than the tick labels. The tick locations are always numeric and determine where on the plot the tick mark will appear. The tick labels on the other hand are the strings that are used on the graph.\nThe tick labels are defaulted to be a string of the tick location, but you can set them to be any string you want, as we did above. But use this wisely otherwise you may lead viewers of your plots astray!\n\n\n\n\n\n\nSetting text styles\nAll the text we placed on our plot was plain. We can add styling to it by changing the text properties. See the documentation for a list of all the text properties.\nCommon text properties:\n\nsize - Number in “points” where 1 point is defaulted to 1/72nd of an inch\ncolor - One of the named colors. See the colors API for more.\nbackgroundcolor - Same as above\nfontname - Name of font as a string\nrotation - Degree of rotation\n\n\nax.set_title(\n    'Tests',\n    size=20, \n    color='firebrick',\n    backgroundcolor='steelblue',\n    fontname='Courier New',\n    rotation=70\n    )\nfig\n\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Courier New' not found.\n\n\n\n\n\n\n\n\n\nAny other text may be stylized with those same parameters. Below we do so with the x labels.\n\nax.set_xlabel(\n    'New and Imporved X-Axis Stylized Label',\n    size=15,\n    color='indigo', \n    fontname='Times New Roman', \n    rotation=15\n    )\nfig\n\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Times New Roman' not found.\nfindfont: Font family 'Times New Roman' not found.\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Times New Roman' not found.\nfindfont: Font family 'Times New Roman' not found.\nfindfont: Font family 'Times New Roman' not found.\nfindfont: Font family 'Courier New' not found.\n\n\n\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoteDo This!\n\n\n\nCreate a Figure with a single Axes. Modify the Axes by using all of the methods in this notebook.\n\ntitle: set as your name and change the font size, color, and name.\nxlabel/ylabel: set as your two favorite colors.\nxlim/ylim: set x-axis to be 10-100 and y-axis to be -25-25\nxticks/yticks: set the xticks to be in increments of 10 and yticks to be in increments of 5.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#plotting-data",
    "href": "14-data-viz-matplotlib.html#plotting-data",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "14.4 Plotting Data",
    "text": "14.4 Plotting Data\nIn previous section, we created Figure and Axes objects, and proceeded to change their properties without plotting any actual data. In this section, we will learn how to create some of the same plots that we created in the Plotting with Pandas lesson.\nThe matplotlib documentation has a nice layout of the Axes API. There are around 300 different calls you make with an Axes object. The API page categorizes and groups each method by its functionality. The first third (approximately) of the categories in the API are used to create plots.\nThe simplest and most common plots are found in the Basics category and include plot, scatter, bar, pie, and others.\nLet’s go ahead and import Pandas along with our Complete Journey data:\n\nimport pandas as pd\nfrom completejourney_py import get_data\n\ncj_data = get_data()\ndf = (\n    cj_data['transactions']\n    .merge(cj_data['products'], how='inner', on='product_id')\n    .merge(cj_data['demographics'], how='inner', on='household_id')\n)\n\ndf.head()\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/completejourney_py/get_data.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.\n  from pkg_resources import resource_filename\n\n\n\n\n\n\n\n\n\nhousehold_id\nstore_id\nbasket_id\nproduct_id\nquantity\nsales_value\nretail_disc\ncoupon_disc\ncoupon_match_disc\nweek\n...\nproduct_category\nproduct_type\npackage_size\nage\nincome\nhome_ownership\nmarital_status\nhousehold_size\nhousehold_comp\nkids_count\n\n\n\n\n0\n900\n330\n31198570044\n1095275\n1\n0.50\n0.00\n0.0\n0.0\n1\n...\nROLLS\nROLLS: BAGELS\n4 OZ\n35-44\n35-49K\nHomeowner\nMarried\n2\n2 Adults No Kids\n0\n\n\n1\n900\n330\n31198570047\n9878513\n1\n0.99\n0.10\n0.0\n0.0\n1\n...\nFACIAL TISS/DNR NAPKIN\nFACIAL TISSUE & PAPER HANDKE\n85 CT\n35-44\n35-49K\nHomeowner\nMarried\n2\n2 Adults No Kids\n0\n\n\n2\n1228\n406\n31198655051\n1041453\n1\n1.43\n0.15\n0.0\n0.0\n1\n...\nBAG SNACKS\nPOTATO CHIPS\n11.5 OZ\n45-54\n100-124K\nNone\nUnmarried\n1\n1 Adult No Kids\n0\n\n\n3\n906\n319\n31198705046\n1020156\n1\n1.50\n0.29\n0.0\n0.0\n1\n...\nREFRGRATD DOUGH PRODUCTS\nREFRIGERATED BAGELS\n17.1 OZ\n55-64\nUnder 15K\nHomeowner\nMarried\n2\n1 Adult Kids\n1\n\n\n4\n906\n319\n31198705046\n1053875\n2\n2.78\n0.80\n0.0\n0.0\n1\n...\nSEAFOOD - SHELF STABLE\nTUNA\n5.0 OZ\n55-64\nUnder 15K\nHomeowner\nMarried\n2\n1 Adult Kids\n1\n\n\n\n\n5 rows × 24 columns\n\n\n\n\nLine plots\nThe plot method’s primary purpose is to create line plots. It does have the ability to create scatter plots as well, but that task is best reserved for scatter. The plot method is very flexible and can take a variety of different inputs (i.e. lists, numpy arrays, Pandas Series) but our examples will focus on using Matplotlib with a DataFrame.\nLet’s compute the total daily sales and create a line plot:\n\ndaily_sales = (\n    df\n    .set_index('transaction_timestamp')['sales_value']\n    .resample('D')\n    .sum()\n    .to_frame()\n    .reset_index()\n)\ndaily_sales.head()\n\n\n\n\n\n\n\n\ntransaction_timestamp\nsales_value\n\n\n\n\n0\n2017-01-01\n4604.39\n\n\n1\n2017-01-02\n6488.94\n\n\n2\n2017-01-03\n6856.84\n\n\n3\n2017-01-04\n7087.92\n\n\n4\n2017-01-05\n6894.67\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 4))\nax.plot('transaction_timestamp', 'sales_value', data=daily_sales);\n\n\n\n\n\n\n\n\nWe can change several properties of this plot. For example, we can change the line style and color of our line directly in the ax.plot call along with adding plot a\n\n\n\n\n\n\nYou can find all possible parameters to modify the line plot here. You can also find all the different color options and ways to specify them here.\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 4))\n\n# create/modify line plot\nax.plot(\n    'transaction_timestamp', \n    'sales_value', \n    data=daily_sales, \n    linestyle=':', \n    color='gray',\n    linewidth=2\n    )\n\n# add additional context\nax.set_title('Total daily sales across all stores', size=20)\nax.set_ylabel('Total sales ($)');\n\n\n\n\n\n\n\n\nWe can even add additional features such as gridlines and text/arrows to call out certain parts of the plot.\n\n\n\n\n\n\nSince our x-axis is a date object we need to specify a date location within ax.annotate. The datetime module comes as part of the Standard Library and is the defacto approach to creating and manipulating dates and times outside of Pandas.\n\n\n\n\nfrom datetime import date as dt\n\nfig, ax = plt.subplots(figsize=(10, 4))\n\n# create/modify line plot\nax.plot(\n    'transaction_timestamp', \n    'sales_value', \n    data=daily_sales, \n    linestyle=':', \n    color='gray',\n    linewidth=2\n    )\n\n# add additional context\nax.set_title('Total daily sales across all stores', size=20)\nax.set_ylabel('Total sales ($)');\n\n# add gridlines, arrow, and text\nax.grid(linestyle='dashed')\nax.annotate(\n    'Christmas Eve', \n    xy=([dt(2017, 12, 20), 15900]), \n    xytext=([dt(2017, 9, 1), 15500]), \n    arrowprops={'color':'blue', 'width':2},\n    size=10\n    );\n\n\n\n\n\n\n\n\n\n\nOther plots\nWe can create many of the other forms of plots that we saw in the previous lesson. The following are just a few examples that recreate the histogram, boxplot, and scatter plots that we made in Chapter 13.\n\ntotals_by_store = df.groupby('store_id').agg({'sales_value': 'sum', 'quantity': 'sum'})\n\n# histogram\nfig, ax = plt.subplots(figsize=(8, 4))\nax.hist('sales_value', data=totals_by_store, bins=30);\n\n\n\n\n\n\n\n\n\n# boxplot\nfig, ax = plt.subplots(figsize=(8, 4))\nax.boxplot('sales_value', data=totals_by_store, vert=False)\n\n# adjust axes\nax.set_xscale('log')\nax.set_yticklabels('')\nax.set_yticks([]);\n\n\n\n\n\n\n\n\n\n# scatter plot\nfig, ax = plt.subplots(figsize=(8, 4))\nax.scatter('quantity', 'sales_value', data=totals_by_store, c='gray', s=5);\n\n\n\n\n\n\n\n\n\n\nAdding more dimensions\nIn some cases we can even add additional dimensions to our data. For example, say we have the following data set that has total quantity and sales_value plus a third variable that indicates the number of transactions for each store.\n\nstore_count = (\n    df\n    .groupby('store_id', as_index=False)\n    .size()\n    .rename(columns={'size': 'n'})\n)\n\ntotals_by_store = (\n    df\n    .groupby('store_id', as_index=False)\n    .agg({'sales_value': 'sum', 'quantity': 'sum'})\n    .merge(store_count)\n)\n\ntotals_by_store.head()\n\n\n\n\n\n\n\n\nstore_id\nsales_value\nquantity\nn\n\n\n\n\n0\n2\n13.99\n1\n1\n\n\n1\n27\n443.97\n186\n150\n\n\n2\n37\n7.27\n3\n3\n\n\n3\n42\n22.78\n7\n7\n\n\n4\n45\n13.01\n9\n8\n\n\n\n\n\n\n\nWe can actually have the color (c) and size (s) of our points based on data. For example, here we create a Series that is based on the number of store observations and another Series to indicate if the store count was greater than the 95 percentile.\n\n\n\n\n\n\nWe could literally use s='n' to reference our count column within the DataFrame; however, since there is such a wide dispersion of count values the size of the points explode. Here, we adjust the dispersion to be smaller with a fractional exponentiation.\n\n\n\n\nsize_adj = totals_by_store['n']**0.4\nn_outliers = totals_by_store['n'] &gt; totals_by_store['n'].quantile(0.95)\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.scatter('quantity', 'sales_value', data=totals_by_store, c=n_outliers, s=size_adj);\n\n\n\n\n\n\n\n\n\n\nMultiple plots\nWe can create multiple plots (“Axes”) within our figure. For example, the following will create 4 plots (2 rows x 2 colums).\n\nfig, ax_array = plt.subplots(2, 2, figsize=(8, 6), constrained_layout=True)\n\n\n\n\n\n\n\n\nWhenever you create multiple Axes on a figure with subplots, you will be returned a NumPy array of Axes objects. Let’s verify that the type and shape of this array.\n\ntype(ax_array)\n\nnumpy.ndarray\n\n\n\nax_array.shape\n\n(2, 2)\n\n\nIf we simply output the array, we will see 4 different Axes objects. Let’s extract each of these Axes into their own variable. We need to index for both the row and column to get the respective plot (remember that we need to use zero-based indexing!)\n\nax_array\n\narray([[&lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: &gt;, &lt;Axes: &gt;]], dtype=object)\n\n\n\nax1 = ax_array[0, 0]  # row 0, col 0\nax2 = ax_array[0, 1]  # row 0, col 1\nax3 = ax_array[1, 0]  # row 1, col 0\nax4 = ax_array[1, 1]  # row 1, col 1\n\nWe can now customize our individual plots:\n\n# plot 1\nax1.plot(\n    'transaction_timestamp', \n    'sales_value', \n    data=daily_sales, \n    linestyle=':', \n    color='gray',\n    linewidth=2\n    )\n\n# add additional context\nax1.set_title('Total daily sales across all stores', size=12)\nax1.set_ylabel('Total sales ($)');\n\n# add gridlines, arrow, and text\nax1.grid(linestyle='dashed')\nax1.tick_params(axis='x', which='major', labelsize=8, labelrotation=45)\nax1.annotate(\n    'Christmas Eve', \n    xy=([dt(2017, 12, 20), 15900]), \n    xytext=([dt(2017, 7, 1), 15500]), \n    arrowprops={'color':'blue', 'width':0.5},\n    size=8\n    )\n    \n# plot 2\nax2.scatter('quantity', 'sales_value', data=totals_by_store, c=n_outliers, s=size_adj)\nax2.set_title('Total store-level sales vs quantity.', size=12)\n\n# plot 3\nax3.hist('sales_value', data=totals_by_store, bins=30)\nax3.set_title('Histogram of total store-level sales.', size=12)\n\n# plot 4\nax4.boxplot('quantity', data=totals_by_store, vert=False)\nax4.set_xscale('log')\nax4.set_yticklabels('')\nax4.set_yticks([])\nax4.set_title('Histogram of total store-level quantity.', size=12);\n\n# final plot title\nfig.suptitle('Total store-level sales and quantities (2017)', fontsize=20)\nfig",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#video-tutorials",
    "href": "14-data-viz-matplotlib.html#video-tutorials",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "14.5 Video Tutorials",
    "text": "14.5 Video Tutorials\n\n\n\n\n\n\nNoneVideo 🎥: Anatomy of Matplotlib\n\n\n\n\n\nThe following video is from the SciPy 2018 conference. It is an extended (3 hours) introduction to Matplotlib and has been a very popular training offered at SciPy conferences over the years.\n\n\n\n\n\n\n\n\n\n\nNoneVideo 🎥: Corey Shafer series on Matplotlib\n\n\n\n\n\nYou can also check out this series of Matplotlib tutorials provided by Corey Schafer. They are excellent! Here’s the first video:",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#summary",
    "href": "14-data-viz-matplotlib.html#summary",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "14.6 Summary",
    "text": "14.6 Summary\nIn this chapter, you explored the fundamentals of Matplotlib, Python’s core plotting library. While it may feel more manual compared to Pandas’ built-in plotting, understanding how Matplotlib works gives you the power to fully customize your visualizations—from titles and labels to tick marks, color schemes, and layout. This level of control is key to creating polished, professional-looking plots suitable for presentations, publications, and stakeholder reports.\nBy building your skills with Matplotlib, you’ve laid the groundwork for working with many other Python visualization libraries that use it under the hood.\nIn the next chapter, we’ll explore Bokeh, a more advanced library designed for creating interactive, web-ready visualizations. These tools allow you to move beyond static charts and into the world of dynamic dashboards and visual storytelling—ideal for engaging stakeholders and sharing results in more impactful ways.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#exercise-customizing-pizza-visualizations-with-matplotlib",
    "href": "14-data-viz-matplotlib.html#exercise-customizing-pizza-visualizations-with-matplotlib",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "14.7 Exercise: Customizing Pizza Visualizations with Matplotlib",
    "text": "14.7 Exercise: Customizing Pizza Visualizations with Matplotlib\nIn this exercise set, you’ll revisit the pizza-related analyses from the previous chapter. However, this time you’ll build the plots using Matplotlib directly, allowing you to practice creating visualizations from scratch and customizing them for improved clarity and presentation quality.\nUse the datasets from the completejourney_py package and import Matplotlib as needed:\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom completejourney_py import get_data\n\ncj_data = get_data()\ndf = (\n    cj_data['transactions']\n    .merge(cj_data['products'], how='inner', on='product_id')\n    .merge(cj_data['demographics'], how='inner', on='household_id')\n)\n\n\n\n\n\n\nTip💡 Stuck or unsure how to do something?\n\n\n\nDon’t hesitate to use AI tools like ChatGPT or GitHub Copilot to help you troubleshoot, format plots, or discover new ways to customize your charts. Part of learning data visualization is knowing how to find and adapt solutions.\n\n\n\n\n\n\n\n\nNone1. Identifying Pizza Products\n\n\n\n\n\nAs before, use the products table to:\n\nIdentify all products where the product_type contains \"pizza\" (case-insensitive).\nCount how many distinct pizza products are sold.\n\nThere’s no plot needed here—but this filtered data will be reused in later tasks, so keep it handy!\n\n\n\n\n\n\n\n\n\nNone2. Pizza Purchases by Marital Status (Bar Chart)\n\n\n\n\n\nUsing transactions, products, and demographics, create a Matplotlib bar chart comparing the total quantity of pizza purchased by married vs. unmarried households.\n\nAdd a descriptive title and axis labels.\nCustomize the bar colors and axis tick labels to make the chart more readable.\nBonus: Add the value labels directly on top of the bars.\n\n\n\n\n\n\n\n\n\n\nNone3. Quantity vs. Sales Value (Scatter Plot)\n\n\n\n\n\nCreate a scatter plot using Matplotlib to visualize the relationship between quantity and sales_value for pizza purchases.\n\nUse transparency (alpha) to handle overplotting.\nAdd a title and axis labels.\nBonus: Use color or marker size to represent another dimension (e.g., household income level or day of the week).\n\nOptional: Try annotating or highlighting outliers if they seem interesting.\n\n\n\n\n\n\n\n\n\nNone4. Daily Pizza Sales Trends (Line Plot)\n\n\n\n\n\nUsing resample() on the pizza transactions:\n\nCompute total quantity of pizza products purchased per day.\nCreate a line plot with Matplotlib to visualize the daily trend.\nAdd titles, axis labels, and format the date axis so it doesn’t appear cluttered.\n\nBonus: Add gridlines, a rolling average, or vertical lines to mark major holidays.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html",
    "href": "15-data-viz-bokeh.html",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "",
    "text": "15.1 Prerequisites\nIn the previous two lessons, you learned how to visualize data using pandas’ high-level plotting tools for quick insights, and matplotlib for more detailed and customized charting.\nIn this lesson, you’ll take your visualization skills to the next level with Bokeh, a Python library designed for building interactive visualizations in modern web browsers. With Bokeh, you can create everything from simple scatterplots to fully-featured, dynamic dashboards — all in Python, no JavaScript required.\nTools like Bokeh allow you to move beyond static charts and into the world of interactive, responsive data experiences. This shift enables visual storytelling, where users can explore data themselves, and helps you deliver more engaging, stakeholder-friendly output. Whether you’re sharing insights with a non-technical audience or designing a data dashboard, Bokeh gives you the flexibility and power to communicate results in more impactful ways.\nAlthough Bokeh is considered a lower-level visualization API than pandas or seaborn, it strikes a great balance between ease of use and customizability. You’ll find it intuitive to get started with and incredibly powerful as you go deeper.\nIn this chapter, you’ll get hands-on experience with Bokeh and learn how to create and customize interactive visualizations for the web. By the end of this lesson you will be able to:\nMost of the functionality of Bokeh is accessed through submodules such as bokeh.plotting and bokeh.models. Also, when using Bokeh in a notebook we need to run bokeh.io.output_notebook() to make our plots viewable and interactive.\nimport pandas as pd\n\n# Our main plotting package (must have explicit import of submodules)\nimport bokeh.io\nimport bokeh.models\nimport bokeh.plotting\nimport bokeh.transform\n\n# Enable viewing Bokeh plots in the notebook\nbokeh.io.output_notebook()\n\n    \n    \n        \n        Loading BokehJS ...\nWe’ll use a cleaned up version of the Ames, IA housing data for illustration purposes:\ndf = pd.read_csv('../data/ames_clean.csv')\ndf.head()\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html#bokehs-grammar-and-our-first-plot-with-bokeh",
    "href": "15-data-viz-bokeh.html#bokehs-grammar-and-our-first-plot-with-bokeh",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "15.2 Bokeh’s grammar and our first plot with Bokeh",
    "text": "15.2 Bokeh’s grammar and our first plot with Bokeh\nConstructing a plot with Bokeh consists of four main steps.\n\nCreating a figure on which to populate glyphs (symbols that represent data, e.g., dots for a scatter plot). Think of this figure as a “canvas” which sets the space on which you will “paint” your glyphs.\nDefining a data source that is the reference used to place the glyphs.\nChoose the kind of glyph you would like.\nRefining the plot by adding titles, formatted axis labels, or even interactive components.\n\nAfter completing these steps, you need to render the graphic.\nLet’s go through these steps to generate an interactive scatter plot of home sales price and total living area. So you have the concrete example in mind, the final graphic will look like this:\n\n\n\n  \n\n\n\n\n\n1. Our first step is creating a figure, our “canvas.” In creating the figure, we are implicitly thinking about what kind of representation for our data we want. That is, we have to specify axes and their labels. We might also want to specify the title of the figure, whether or not to have grid lines, and all sorts of other customizations. Naturally, we also want to specify the size of the figure.\n(Almost) all of this is accomplished in Bokeh by making a call to bokeh.plotting.figure() with the appropriate keyword arguments.\n\n# Create the figure, stored in variable `p`\np = bokeh.plotting.figure(\n    frame_width=700,\n    frame_height=350,\n    title='Relationship between home sale price and living area \\nAmes, Iowa (2006-2010)',\n    x_axis_label='Living Area (Square feet)',\n    y_axis_label='Sale Price'\n)\n\nThere are many more keyword attributes you can assign, including all of those listed in the Bokeh Plot class and the additional ones listed in the Bokeh Figure class.\n2. Now that we have set up our canvas, we can decide on the data source. It is convenient to create a ColumnDataSource, a special Bokeh object that holds data to be displayed in a plot. (Later on we will see that we can change the data in a ColumnDataSource and the plot will automatically update!) Conveniently, we can instantiate a ColumnDataSource directly from a Pandas data frame.\n\nsource = bokeh.models.ColumnDataSource(df)\n\n\n\n\n\n\n\nWe could also instantiate a data source using a dictionary of arrays, like…\nsource = bokeh.models.ColumnDataSource(dict(x=[1, 2, 3, 4], y=[1, 4, 9, 16]))\n\n\n\n3. Since we are creating a scatter plot we will choose scatter as our glyph. This kind of glyph requires that we specify which column of the data source will serve to place the glyphs along the \\(x\\)-axis and which will serve to place the glyphs along the \\(y\\)-axis. We choose the 'GrLivArea' column to specify the \\(x\\)-coordinate of the glyph and the 'SalePrice' column to specify the \\(y\\)-coordinate. Since there are a lot of observations clustered together we can control overplotting by adjusting the transparency with alpha.\nWe accomplish step 3 by calling one of the glyph methods of the Bokeh Figure instance, p. Since we are choosing a scatter plot, the appropriate method is p.scatter(), and we use the source, x, and y kwargs to specify the positions of the glyphs.\n\np.scatter(\n    source=source,\n    x='GrLivArea',\n    y='SalePrice',\n    alpha=0.25\n);\n\n4. Lastly, we can refine the plot in various ways. In this example we make the x and y-axis labels comma and dollar formatted respectively. We can also add interactive components to our visuals. Here, I add a hover tool so that sale price and total living area is displayed when my mouse hovers over a point.\n\n\n\n\n\n\nWe can specify these features (axis configuration and tooltips) when we instantiate the figure or afterwards by assigning attribute values to an already instantiated figure.\n\n\n\nThe syntax for a tooltip is a list of 2-tuples, where each tuple represents the tooltip you want. The first entry in the tuple is the label and the second is the column from the data source that has the values. The second entry must be preceded with an @ symbol signifying that it is a field in the data source and not field that is intrinsic to the plot, which is preceded with a $ sign. If there are spaces in the column heading, enclose the column name in braces (i.e. {name with spaces}). (See the documentation for tooltip specification for more information.)\n\np.yaxis.formatter = bokeh.models.NumeralTickFormatter(format=\"$,\")\np.xaxis.formatter = bokeh.models.NumeralTickFormatter(format=\",\")\n\ntooltips = [(\"Sale Price\",\"@SalePrice\"),(\"SqFt\",\"@GrLivArea\")]\nhover = bokeh.models.HoverTool(tooltips=tooltips, mode='mouse')\np.add_tools(hover)\n\nNow that we have built the plot, we can render it in the notebook using bokeh.io.show().\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nIn looking at the plot, notice a toolbar to right of the plot that enables you to zoom and pan within the plot.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html#coloring-with-other-dimensions",
    "href": "15-data-viz-bokeh.html#coloring-with-other-dimensions",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "15.3 Coloring with other dimensions",
    "text": "15.3 Coloring with other dimensions\nLet’s say we wanted to make the same plot, but we wanted to color the points based on another feature such as whether the home has central air or not (CentralAir). To do this, we take advantage of two features of Bokeh.\n\nWe create a color mapping using factor_cmap() that assigns colors to the discrete levels of a given factor (CentralAir in this example). Here, we simply assign red and blue colors; however, Bokeh has many color palettes to choose from.\nWe can then use the scatter method to assign the glyph of choice and pass the color_mapper object to fill_color and/or fill_line. I also add the legend field so it shows up in the plot and we can format our legend as necessary (i.e. add title, change font).\n\n\n# Create the figure, stored in variable `p`\np = bokeh.plotting.figure(\n    frame_width=700,\n    frame_height=350,\n    title='Relationship between home sale price and living area \\nAmes, Iowa (2006-2010)',\n    x_axis_label='Living Area (Square feet)',\n    y_axis_label='Sale Price'\n)\n\nsource = bokeh.models.ColumnDataSource(df)\n\n# create color mapper\ncolor_mapper = bokeh.transform.factor_cmap(\n    'CentralAir',\n    palette=['red', 'blue'],\n    factors=df['CentralAir'].unique()\n    )\n\np.scatter(\n    source=source,\n    x='GrLivArea',\n    y='SalePrice',\n    marker='circle',\n    alpha=0.25,\n    fill_color=color_mapper,\n    line_color=color_mapper,\n    legend_field='CentralAir'\n)\n\np.legend.title = \"Has central air\"\n\np.yaxis.formatter = bokeh.models.NumeralTickFormatter(format=\"$,\")\np.xaxis.formatter = bokeh.models.NumeralTickFormatter(format=\",\")\n\ntooltips = [(\"Sale Price\",\"@SalePrice\"),(\"SqFt\",\"@GrLivArea\")]\nhover = bokeh.models.HoverTool(tooltips=tooltips, mode='mouse')\np.add_tools(hover)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html#saving-bokeh-plots",
    "href": "15-data-viz-bokeh.html#saving-bokeh-plots",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "15.4 Saving Bokeh plots",
    "text": "15.4 Saving Bokeh plots\nAfter you create your plot, you can save it to a variety of formats. Most commonly you would save them as PNG (for presentations), SVG (for publications in the paper of the past), and HTML (for the paper of the future or sharing with colleagues).\nTo save as a PNG for quick use, you can click the disk icon in the tool bar.\nTo save to SVG, you first change the output backend to 'svg' and then you can click the disk icon again, and you will get an SVG rendering of the plot. After saving the SVG, you should change the output backend back to 'canvas' because it has much better in-browser performance.\n\np.output_backend = 'svg'\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nNow, click the disk icon in the plot above to save it.\nAfter saving, we should switch back to canvas.\n\np.output_backend = 'canvas'\n\nYou can also save the figure programmatically using the bokeh.io.export_svgs() function. This requires additional installations, so we will not do it here, but show the code to do it. Again, this will only work if the output backed is 'svg'.\np.output_backend = 'svg'\nbokeh.io.export_svgs(p, filename='ames_sale_price_vs_living_area.svg')\np.output_backend = 'canvas'\nFinally, to save as HTML, you can use the bokeh.io.save() function. This saves your plot as a standalone HTML page. Note that the title kwarg is not the title of the plot, but the title of the web page that will appear on your Browser tab.\nbokeh.io.save(\n    p,\n    filename='ames_sale_price_vs_living_area.html',\n    title='Bokeh plot'\n);",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html#video-tutorial",
    "href": "15-data-viz-bokeh.html#video-tutorial",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "15.5 Video Tutorial",
    "text": "15.5 Video Tutorial\nThis lesson only scratches the surface of what Bokeh can do. From interactive widgets and streaming data to fully responsive dashboards, Bokeh offers a wide range of advanced capabilities. To see more examples and explore what’s possible, visit the official Bokeh gallery at https://demo.bokeh.org/.\n\n\n\n\n\n\nNoneVideo 🎥\n\n\n\nThe following video provides an overview of Bokeh and will also expose you to other types of plots you can create (i.e. line charts, histograms, area plots).",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html#summary",
    "href": "15-data-viz-bokeh.html#summary",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "15.6 Summary",
    "text": "15.6 Summary\nIn this chapter, you explored the fundamentals of creating interactive data visualizations using Bokeh, a powerful Python library designed for the web. You learned how to use Bokeh’s figure interface to build plots from scratch, enhance them with interactive tools like hovers and legends, and combine multiple charts into dashboard-style layouts.\nBokeh opens the door to interactive, engaging visual storytelling—ideal for exploratory analysis and sharing insights with stakeholders in a more dynamic format. While this chapter focused on the core building blocks, Bokeh supports much more, including advanced widgets, real-time data streaming, and fully customizable layouts.\nTo explore Bokeh’s full capabilities, check out the official demo gallery.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html#exercise-exploring-housing-trends-with-bokeh",
    "href": "15-data-viz-bokeh.html#exercise-exploring-housing-trends-with-bokeh",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "15.7 Exercise: Exploring Housing Trends with Bokeh",
    "text": "15.7 Exercise: Exploring Housing Trends with Bokeh\nIn this exercise set, you’ll revisit the Ames Housing dataset and apply what you’ve learned about Bokeh to build interactive plots from scratch. These visualizations will give you hands-on practice with building charts, customizing tooltips, applying categorical coloring, and creating simple layouts.\nUse the Ames Housing dataset we used in this chapter, and import the necessary Bokeh modules as needed:\nimport pandas as pd\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\nfrom bokeh.transform import factor_cmap\nfrom bokeh.layouts import row, column\nfrom bokeh.models import HoverTool\n\noutput_notebook()\n\ndf = pd.read_csv(\"data/ames_clean.csv\")  # adjust path as needed\n\n\n\n\n\n\nTip💡 Stuck or unsure how to do something?\n\n\n\nUse tools like ChatGPT, GitHub Copilot, or the Bokeh documentation to help troubleshoot, format your plots, or discover ways to customize interactivity. Learning how to adapt examples is part of becoming a confident data visualizer.\n\n\n\n\n\n\n\n\nNone1. Explore the Bokeh Gallery\n\n\n\n\n\nVisit https://demo.bokeh.org and spend a few minutes exploring different chart types.\n\nWhat kinds of visualizations stand out to you?\nCan you imagine how these might be used in a data science or business setting?\n\nThere’s no coding for this part—just use it for inspiration before diving into the next tasks.\n\n\n\n\n\n\n\n\n\nNone2. Bar Chart: Housing Count by Neighborhood\n\n\n\n\n\nCreate a Bokeh bar chart that shows the number of houses sold per Neighborhood.\n\nUse groupby() and value_counts() to prepare your data.\nCreate a vertical bar chart with vbar().\nAdd a title, axis labels, and hover tooltips that show the neighborhood name and house count.\nBonus: Sort the bars in descending order.\n\n\n\n\n\n\n\n\n\n\nNone3. Scatter Plot: Living Area vs. Sale Price\n\n\n\n\n\nVisualize the relationship between GrLivArea (above ground living area) and SalePrice.\n\nCreate a scatter plot using figure().circle().\nAdd a HoverTool that shows the address (or another interesting feature), living area, and sale price when you hover over points.\nBonus: Add color or marker size based on a third variable like OverallQual.\n\n\n\n\n\n\n\n\n\n\nNone4. Add Categorical Color to Your Scatter Plot\n\n\n\n\n\nEnhance your scatter plot by mapping color to a categorical feature such as CentralAir, BldgType, or a binned version of OverallQual.\n\nUse factor_cmap to map categories to colors.\nInclude a legend and adjust the plot size or layout if needed.\n\nBonus: Use ColumnDataSource to simplify your interactivity.\n\n\n\n\n\n\n\n\n\nNone5. Combine Multiple Plots into a Layout\n\n\n\n\n\nCreate a second plot of your choice (e.g., a histogram of SalePrice or another scatter plot).\n\nUse row() or column() to combine it with your earlier scatter plot.\nAdd a shared title or explanatory text.\nReflect: How might this layout be useful as a small dashboard for exploring housing data?",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html",
    "href": "16-control-statements.html",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "",
    "text": "16.1 Prerequisites\nAs your programs become more complex, they need to be able to respond to different situations — choosing different paths depending on the data. Conditional statements are how Python makes those decisions.\nIn this chapter, you’ll learn how to:\nLet’s explore how conditional logic helps you write smarter, more dynamic code.\nBy the end of this lesson you will be able to:\nMost of the examples in this lesson use base Python code without any modules; however, we will illustrate some examples of integrating control statements within Pandas. We also illustrate a couple examples that use Numpy.\nimport pandas as pd\nimport numpy as np\nAlso, most of the examples use toy data; however, when illustrating concepts integrated with Pandas we will use the Complete Journey transaction data:\nfrom completejourney_py import get_data\n\ndf = get_data()['transactions']\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/completejourney_py/get_data.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.\n  from pkg_resources import resource_filename",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html#if-statement",
    "href": "16-control-statements.html#if-statement",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "16.2 if statement",
    "text": "16.2 if statement\nThe conditional if statement is used to test an expression. Below is some psuedo code illustrating what this looks like. If the test_expression is True, the statement gets executed. But if it’s False, nothing happens.\nif test_expression:\n    statement\nThe following is an example that tests if a particular object is negative. Notice that there are no braces or “begin/end” delimiters around the block of code. Python uses the indentation to determine blocks of code. Consequently, you can include multiple lines in the if statement as long as they have the same indentation.\n\nx = -8\n\nif x &lt; 0:\n    print('x contains a negative number')\n\nx contains a negative number\n\n\n\n# multiple lines in the statement are fine as long as they have the\n# same indentation\nif x &lt; 0:\n    new_value = abs(x)\n    print(f'absolute value of x is {new_value}')\n\nabsolute value of x is 8\n\n\nIt is possible to write very short if statements on one line. This can be useful in limited situations but as soon as your resulting statement become more verbose it is best practice to switch to a multi-line approach.\n\n# single line approach\nif x &lt; 0: print('x contains a negative number')\n\nx contains a negative number\n\n\nIts helpful to remember that everything in Python has some form of truthiness. In fact, any nonzero number or nonempty object is True. This allows you to evaluate the object directly:\n\n# a conditional statement on an empty object is equivalent to False\nempty_list = []\nif empty_list:\n    print(\"since empty_list is False this won't exectute\")\n\n\n# a conditional statement on a non-empty object is equivalent to True\nnon_empty_list = ['not', 'empty']\nif non_empty_list:\n    print(\"This list is not empty\")\n\nThis list is not empty\n\n\nPython uses and and or operators to evaluate multiple expressions. They always return a single True or False. Moreover, Python will stop evaluating multiple expressions as soon as the result is known.\n\nx = -1\ny = 4\nif x &lt; 0 or y &lt; 0:\n    print('At least one of these objects are less than zero.')\n\nAt least one of these objects are less than zero.\n\n\n\nif x &lt; 0 and y &lt; 0:\n    print('Both x and y or less than zero')\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nFill in the following code chunk so that:\n\nIf month has value 1-9 the file name printed out will be “data/Month-0X.csv”\nWhat happens if the month value is 10-12?\nmonth = 4\n\nif month ________ :\n    print(f'data/Month-0{month}.csv')\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html#multiway-branching",
    "href": "16-control-statements.html#multiway-branching",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "16.3 Multiway branching",
    "text": "16.3 Multiway branching\nMultiway branching is when we want to have multiple return statement options based on the input conditions. The general form of multiway branch if statements is as follows.\n\n\n\n\n\n\nThe elif block is not always necessary. If you want only two output branches then just use if followed by else. However, if you have many branches, you can use as many elif statements as necessary.\n\n\n\nif test_1:\n  statement_1\nelif test2:\n  statement_2\nelse:\n  statement_3\nThe following illustrates with a simple example. Python will perform this code in sequence and execute the statements nested under the first test that is True or the else if all tests are False.\n\nx = 22.50\n\nif 0 &lt;= x &lt; 10:\n    print('low')\nelif 10 &lt;= x &lt; 20:\n    print('medium-low')\nelif 20 &lt;= x &lt; 30:\n    print('medium')\nelse:\n    print('preferred')\n\nmedium\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nFill in the following code chunk so that:\n\nif month has value 1-9 the file name printed out will be “data/month-0X.csv”\nif month has value 10-12 the file name printed out will be “data/month-1X.csv”\nif month is an invalid month number (not 1-12), the result printed out is “Invalid month”\ntest it out for when month equals 6, 10, & 13\nmonth = 4\n\nif month ________:\n    print(f'data/Month-0{month}.csv')\n_____:\n    print(f'data/Month-{month}.csv')\n_____:\n    print('________')\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html#switch-statements",
    "href": "16-control-statements.html#switch-statements",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "16.4 Switch statements",
    "text": "16.4 Switch statements\nMany other languages have a switch or case statement that allows you to evaluate an expression and return the statement that aligns with the value. For example, in R, the switch statement looks like the following:\nchoice &lt;- 'ham'\n\nswitch(choice,\n       'spam'  = 1.25,\n       'ham'   = 1.99,\n       'eggs'  = 0.99,\n       'bacon' = 1.10,\n)\n## [1] 1.99\nPython does not have switch statement but has some handy alternatives. In the most basic approach, you could just use a multiway branch if statement:\n\nchoice = 'ham'\n\nif choice == 'spam':\n    print(1.25)\nelif choice == 'ham':\n    print(1.99)\nelif choice == 'eggs':\n    print(0.99)\nelif choice == 'bacon':\n    print(1.10)\nelse:\n    print('Bad choice')\n\n1.99\n\n\nHowever, this approach is a bit verbose. An efficient alternative is to use a dictionary that provides the same key-value matching as a switch statement.\n\noptions = {'spam': 1.25, 'ham': 1.99, 'eggs': 0.99, 'bacon': 1.10}\n\nYou can either index this dictionary for the matching key:\n\noptions[choice]\n\n1.99\n\n\nOr, a more trustworthy approach is to use the get() method. This allows you to provide a default response in the case that the key you are looking for is not in the dictionary\n\n\n\n\n\n\nUsing the get() method allows you to supply a value to provide if there is no matching key (or as in if statements if there are no other conditions that equate to True).\n\n\n\n\noptions.get(choice, 'Bad choice')\n\n1.99\n\n\n\nchoice = 'broccoli'\noptions.get(choice, 'Bad choice')\n\n'Bad choice'\n\n\nDictionaries are good for associating values with keys, but what about the more complicated actions you can code in the statement blocks associated with if statements? Fortunately, dictionaries can also hold functions (both named functions and lambda functions) which can allow you to perform more sophisticated switch-like execution.\n\n\n\n\n\n\nDon’t worry, you will learn more about functions in an upcoming lesson.\n\n\n\n\ndef produce_revenue(sqft, visits, trend):\n    total = 9.91 * sqft * visits * trend\n    return round(total, 2)\n\ndef frozen_revenue(sqft, visits, trend):\n    prod = produce_revenue(sqft, visits, trend)\n    total = 3.28 * sqft * visits * trend - prod * .005\n    return round(total, 2)\n\nexpected_annual_revenue = {\n    'produce':    produce_revenue,\n    'frozen':     frozen_revenue,\n    'pharmacy':   lambda: 16.11 * visits * trend\n    }\n\nchoice = 'frozen'\nexpected_annual_revenue.get(choice, 'Bad choice')(sqft=937, visits=465, trend=0.98)\n\n1379372.75\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nConvert the following multi-branch if-else statement into a dict where you get the month path file with path_files.get(month). In this case, which approach seems more reasonable?\nmonth = 4\n\nif month &lt;= 9:\n    print(f'data/Month-0{month}.csv')\nelif month &gt;= 10 and month &lt;= 12:\n    print(f'data/Month-{month}.csv')\nelse:\n    print('Invalid month')\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html#applying-in-pandas",
    "href": "16-control-statements.html#applying-in-pandas",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "16.5 Applying in Pandas",
    "text": "16.5 Applying in Pandas\nWhen data mining, we often want to perform conditional statements to not only filter observations, but also to create new variables. For example, say we want to create a new variable that classifies transactions above $10 as “high value” otherwise they are “low value”. There are several methods we can use to perform this but a simple one is to use the apply method:\n\ndf['value'] = df['sales_value'].apply(lambda x: 'high value' if x &gt; 10 else 'low value')\ndf.head()\n\n\n\n\n\n\n\n\nhousehold_id\nstore_id\nbasket_id\nproduct_id\nquantity\nsales_value\nretail_disc\ncoupon_disc\ncoupon_match_disc\nweek\ntransaction_timestamp\nvalue\n\n\n\n\n0\n900\n330\n31198570044\n1095275\n1\n0.50\n0.00\n0.0\n0.0\n1\n2017-01-01 11:53:26\nlow value\n\n\n1\n900\n330\n31198570047\n9878513\n1\n0.99\n0.10\n0.0\n0.0\n1\n2017-01-01 12:10:28\nlow value\n\n\n2\n1228\n406\n31198655051\n1041453\n1\n1.43\n0.15\n0.0\n0.0\n1\n2017-01-01 12:26:30\nlow value\n\n\n3\n906\n319\n31198705046\n1020156\n1\n1.50\n0.29\n0.0\n0.0\n1\n2017-01-01 12:30:27\nlow value\n\n\n4\n906\n319\n31198705046\n1053875\n2\n2.78\n0.80\n0.0\n0.0\n1\n2017-01-01 12:30:27\nlow value\n\n\n\n\n\n\n\n\ndf.groupby('value').size()\n\nvalue\nhigh value      45265\nlow value     1424042\ndtype: int64\n\n\nAn alternative, and much faster approach is to use np.where(), which requires numpy to be loaded. np.where has been show to be over 2.5 times faster than apply():\n\ndf['value'] = np.where(df['sales_value'] &gt; 10, 'high value', 'low value')\ndf.head()\n\n\n\n\n\n\n\n\nhousehold_id\nstore_id\nbasket_id\nproduct_id\nquantity\nsales_value\nretail_disc\ncoupon_disc\ncoupon_match_disc\nweek\ntransaction_timestamp\nvalue\n\n\n\n\n0\n900\n330\n31198570044\n1095275\n1\n0.50\n0.00\n0.0\n0.0\n1\n2017-01-01 11:53:26\nlow value\n\n\n1\n900\n330\n31198570047\n9878513\n1\n0.99\n0.10\n0.0\n0.0\n1\n2017-01-01 12:10:28\nlow value\n\n\n2\n1228\n406\n31198655051\n1041453\n1\n1.43\n0.15\n0.0\n0.0\n1\n2017-01-01 12:26:30\nlow value\n\n\n3\n906\n319\n31198705046\n1020156\n1\n1.50\n0.29\n0.0\n0.0\n1\n2017-01-01 12:30:27\nlow value\n\n\n4\n906\n319\n31198705046\n1053875\n2\n2.78\n0.80\n0.0\n0.0\n1\n2017-01-01 12:30:27\nlow value\n\n\n\n\n\n\n\nAs our conditions get more complex, it often becomes useful to create a separate function and use apply. This approach is probably the most legible; however, not always the fastest approach if you are working with significantly large data.\n\ndef flag(df):\n    if (df['quantity'] &gt; 20) or (df['sales_value'] &gt; 10):\n        return 'Large purchase'\n    elif (df['quantity'] &gt; 10) or (df['sales_value'] &gt; 5):\n        return 'Medium purchase'\n    elif (df['quantity'] &gt; 0) or (df['sales_value'] &gt; 0):\n        return 'Small purchase'\n    else:\n        return 'Alternative transaction'\n\ndf['purchase_flag'] = df.apply(flag, axis = 1)\ndf.head()\n\n\n\n\n\n\n\n\nhousehold_id\nstore_id\nbasket_id\nproduct_id\nquantity\nsales_value\nretail_disc\ncoupon_disc\ncoupon_match_disc\nweek\ntransaction_timestamp\nvalue\npurchase_flag\n\n\n\n\n0\n900\n330\n31198570044\n1095275\n1\n0.50\n0.00\n0.0\n0.0\n1\n2017-01-01 11:53:26\nlow value\nSmall purchase\n\n\n1\n900\n330\n31198570047\n9878513\n1\n0.99\n0.10\n0.0\n0.0\n1\n2017-01-01 12:10:28\nlow value\nSmall purchase\n\n\n2\n1228\n406\n31198655051\n1041453\n1\n1.43\n0.15\n0.0\n0.0\n1\n2017-01-01 12:26:30\nlow value\nSmall purchase\n\n\n3\n906\n319\n31198705046\n1020156\n1\n1.50\n0.29\n0.0\n0.0\n1\n2017-01-01 12:30:27\nlow value\nSmall purchase\n\n\n4\n906\n319\n31198705046\n1053875\n2\n2.78\n0.80\n0.0\n0.0\n1\n2017-01-01 12:30:27\nlow value\nSmall purchase\n\n\n\n\n\n\n\n\ndf.groupby('purchase_flag').size()\n\npurchase_flag\nAlternative transaction       8820\nLarge purchase               46730\nMedium purchase             128361\nSmall purchase             1285396\ndtype: int64\n\n\n\n\n\n\n\n\nTipVideo 🎥:\n\n\n\nHere is a more thorough introduction to the apply method; plus, you’ll also be introduced to the map and applymap methods.\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nFill in the blanks below to assign each transaction to a power rating of 1, 2, 3, or 4 based on the sales_value variable:\n\npower_rating = 1: if sales_value &lt; 25th percentile\npower_rating = 2: if sales_value &lt; 50th percentile\npower_rating = 3: if sales_value &lt; 75th percentile\npower_rating = 4: if sales_value &gt;= 75th percentile\n\nHint: use the .quantile(perc_value)\nlow = df['sales_value'].quantile(____)\nmed = df['sales_value'].quantile(____) \nhig = df['sales_value'].quantile(____)\n\ndef power_rater(df):\n   if (df['sales_value'] &lt; _____):\n      return ___\n   elif (df['sales_value'] &lt; _____):\n      return ___\n   elif (df['sales_value'] &lt; _____):\n      return ___\n   else:\n      return ___\n\ndf['power_rating'] = df.apply(power_rater, axis = 1)\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html#summary",
    "href": "16-control-statements.html#summary",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "16.6 Summary",
    "text": "16.6 Summary\nConditional statements are the foundation for making decisions in Python programs. In this chapter, you learned how to control the flow of your code by selectively executing blocks of logic depending on whether certain conditions are met.\nYou began by exploring simple if statements and saw how Python uses indentation, not brackets, to define code blocks. From there, you learned how to extend decision logic using elif and else to handle multiple branching paths.\nYou also saw how to:\n\nUse logical operators like and and or to combine conditions\nSimulate switch-like behavior using dictionaries and the .get() method\nEmbed functions within dictionaries to build more dynamic logic maps\n\nImportantly, you practiced applying conditional logic within Pandas DataFrames using methods like .apply() and np.where()—a key skill when working with real-world data. These tools allow you to classify, flag, or transform rows based on business rules or data thresholds.\nWhether you’re categorizing transactions, building smart logic into a script, or adapting decisions based on user input, mastering conditional statements sets the stage for writing more intelligent, flexible programs.\nYou’re now ready to take on iteration—using loops to automate repeated tasks—covered in the next chapter.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html#exercise-classifying-transaction-discounts-with-conditionals",
    "href": "16-control-statements.html#exercise-classifying-transaction-discounts-with-conditionals",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "16.7 Exercise: Classifying Transaction Discounts with Conditionals",
    "text": "16.7 Exercise: Classifying Transaction Discounts with Conditionals\nIn this exercise set, you’ll work with the Complete Journey transactions dataset and apply what you’ve learned about conditional statements to classify and analyze retail discount patterns. These tasks will help you get comfortable with conditional logic in Python and how to apply it to Pandas DataFrames.\nUse the transactions dataset from the completejourney_py package, and import the necessary libraries as needed:\nimport pandas as pd\nimport numpy as np\nfrom completejourney_py import get_data\n\ndf = get_data()['transactions']\n\n\n\n\n\n\nTip💡 Need a refresher?\n\n\n\nReview how to use np.where(), the .apply() method, and conditional logic blocks with if-elif-else. Try using ChatGPT or Pandas documentation to explore options for vectorized logic and grouping.\n\n\n\n\n\n\n\n\nNone1. Create a total_disc Column\n\n\n\n\n\nStart by calculating the total discount applied to each transaction.\n\nCreate a new column called total_disc.\nThis should be the sum of retail_disc and coupon_disc.\n\n\n\n\n\n\n\n\n\n\nNone2. Classify Discount Levels\n\n\n\n\n\nNext, classify each transaction based on the level of discount received.\n\nCreate a new column disc_rating using the following logic:\n\n'none': if total_disc == 0\n'low': if total_disc &gt; 0 but less than the 25th percentile\n'medium': if total_disc ≥ 25th percentile and &lt; 75th percentile\n'high': if total_disc ≥ 75th percentile\n'other': for any row that doesn’t meet the above conditions\n\n\n💡 Use .quantile(0.25) and .quantile(0.75) to calculate the thresholds.\n\n\n\n\n\n\n\n\n\nNone3. Summarize Discount Ratings\n\n\n\n\n\n\nUse groupby() and size() (or .value_counts()) to determine how many transactions fall into each disc_rating category.\nAre most transactions high-discount or low-discount?\n\n\n\n\n\n\n\n\n\n\nNone4. (Bonus) Compare with sales_value\n\n\n\n\n\nAs an extension, explore whether high-discount transactions are associated with higher sales_value.\n\nUse groupby('disc_rating')['sales_value'].mean() to compare average spending by discount group.\nWhat patterns do you notice?",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html",
    "href": "17-iteration-statements.html",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "",
    "text": "17.1 Prerequisites\nRepetition is common in data science tasks—from looping through rows of a dataset to applying transformations across multiple features or files. Fortunately, programming languages like Python provide iteration statements to handle these repetitive tasks efficiently and clearly.\nIn this chapter, you’ll learn how to use for and while loops to perform repetition in your code. You’ll also learn how to control loop behavior using break and continue, explore the concept of iterables, and practice using list comprehensions—a powerful and Pythonic way to iterate and transform data collections.\nThese tools are foundational in data mining and data science work, where we often need to process large amounts of data, automate repetitive operations, and build reusable code structures.\nBy the end of this lesson you will be able to:\nimport glob\nimport os\nimport random\nimport pandas as pd",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#for-loop",
    "href": "17-iteration-statements.html#for-loop",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.2 for loop",
    "text": "17.2 for loop\nThe for loop is used to execute repetitive code statements for a particular number of times. The general syntax is provided below where i is the counter and as i assumes each sequential value the code in the body will be performed for that ith value.\n# syntax of for loop\n# !! this code won't run but, rather, gives you an idea of what the syntax looks like !!\nfor i in sequence:\n    &lt;do stuff here with i&gt;\nThere are three main components of a for loop to consider:\n\nSequence: The sequence represents each element in a list or tuple, each key-value pair in a dictionary, or each column in a DataFrame.\nBody: apply some function(s) to the object we are iterating over.\nOutput: You must specify what to do with the result. This may include printing out a result or modifying the object in place.\n\nFor example, say we want to iterate N times, we can perform a for loop using the range() function:\n\nfor number in range(10):\n    print(number)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nWe can add multiple lines to our for loop; we just need to ensure that each line follows the same indentation patter:\n\nfor number in range(10):\n    squared = number * number\n    print(f'{number} squared = {squared}')\n\n0 squared = 0\n1 squared = 1\n2 squared = 4\n3 squared = 9\n4 squared = 16\n5 squared = 25\n6 squared = 36\n7 squared = 49\n8 squared = 64\n9 squared = 81\n\n\nRather than just print out some result, we can also assign the computation to an object. For example, say we wanted to assign the squared result in the previous for loop to a dictionary where the key is the original number and the value is the squared value.\n\nsquared_values = {}\n\nfor number in range(10):\n    squared = number * number\n    squared_values[number] = squared\n\nsquared_values\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81}\n\n\n\nKnowledge check\nWe can see all data sets that we have in the “data/monthly_data” folder with glob.glob:\n\nmonthly_data_files = sorted(glob.glob(\"../data/monthly_data/*\"))\nmonthly_data_files\n\n['../data/monthly_data/Month-01.csv',\n '../data/monthly_data/Month-02.csv',\n '../data/monthly_data/Month-03.csv',\n '../data/monthly_data/Month-04.csv',\n '../data/monthly_data/Month-05.csv',\n '../data/monthly_data/Month-06.csv',\n '../data/monthly_data/Month-07.csv',\n '../data/monthly_data/Month-08.csv',\n '../data/monthly_data/Month-09.csv',\n '../data/monthly_data/Month-10.csv',\n '../data/monthly_data/Month-11.csv']\n\n\nIf you wanted to get just the file name from the string path we can use os.path.basename:\n\nfile_name = os.path.basename(monthly_data_files[0])\nfile_name\n\n'Month-01.csv'\n\n\nAnd if we wanted to just get the name minus the file extension we can apply some simple string indexing to remove the last four characters (.csv):\n\nfile_name[:-4]\n\n'Month-01'\n\n\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nUse this knowledge to:\n\nCreate an empty dictionary called monthly_data.\nLoop over monthly_data_files and assign the file name as the dictionary key and assign the file path as the value.\nLoop over monthly_data_files and assign the file name as the dictionary key, import the data with pd.read_csv() and assign the imported DataFrame as the value in the dictionary.\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#controlling-sequences",
    "href": "17-iteration-statements.html#controlling-sequences",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.3 Controlling sequences",
    "text": "17.3 Controlling sequences\nThere are two ways to control the progression of a loop:\n\ncontinue: terminates the current iteration and advances to the next.\nbreak: exits the entire for loop.\n\nBoth are used in conjunction with if statements. For example, this for loop will iterate for each element in year; however, when it gets to the element that equals the year of covid (2020) it will break out and end the for loop process.\n\n# range will produce numbers starting at 2018 and up to but not include 2023\nyears = range(2018, 2023)\nlist(years)\n\n[2018, 2019, 2020, 2021, 2022]\n\n\n\ncovid = 2020\n\nfor year in years:\n    if year == covid: break\n    print(year)\n\n2018\n2019\n\n\nThe continue argument is useful when we want to skip the current iteration of a loop without terminating it. On encountering continue, the Python parser skips further evaluation and starts the next iteration of the loop. In this example, the for loop will iterate for each element in year; however, when it gets to the element that equals covid it will skip the rest of the code execution simply jump to the next iteration.\n\nfor year in years:\n    if year == covid: continue\n    print(year)\n\n2018\n2019\n2021\n2022\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nModify the following for loop with a continue or break statement to:\n\nonly import Month-01 through Month-07\nonly import Month-08 through Month-10\n\nmonthly_data_files = glob.glob(\"../data/monthly_data/*\")\nmonthly_data = {}\n\nfor file in monthly_data_files:\n    file_name = os.path.basename(file)[:-4]\n    monthly_data[file_name] = pd.read_csv(file)\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#list-comprehensions",
    "href": "17-iteration-statements.html#list-comprehensions",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.4 List comprehensions",
    "text": "17.4 List comprehensions\nList comprehensions offer a shorthand syntax for for loops and are very common in the Python community. Although a little odd at first, the way to think of list comprehensions is as a backward for loop where we state the expression first, and then the sequence.\n# !! this code won't run but, rather, gives you an idea of what the syntax looks like !!\n\n# syntax of for loop\nfor i in sequence:\n    expression\n  \n# syntax for a list comprehension\n[expression for i in sequence]\nOften, we’ll see a pattern like the following where we:\n\ncreate an empty object (list in this example)\nloop over an object and perform some computation\nsave the result to the empty object\n\n\nsquared_values = []\nfor number in range(5):\n    squared = number * number\n    squared_values.append(squared)\n\nsquared_values\n\n[0, 1, 4, 9, 16]\n\n\nA list comprehension allows us to condense this pattern to a single line:\n\nsquared_values = [number * number for number in range(5)]\nsquared_values\n\n[0, 1, 4, 9, 16]\n\n\nList comprehensions even allow us to add conditional statements. For example, here we use a conditional statement to skip even numbers:\n\nsquared_odd_values = [number * number for number in range(10) if number % 2 != 0]\nsquared_odd_values\n\n[1, 9, 25, 49, 81]\n\n\nFor more complex conditional statements, or if the list comprehension gets a bit long, we can use multiple lines to make it easier to digest:\n\nsquared_certain_values = [\n    number * number for number in range(10)\n    if number % 2 != 0 and number != 5\n    ]\n\nsquared_certain_values\n\n[1, 9, 49, 81]\n\n\nThere are other forms of comprehensions as well. For example, we can perform a dictionary comprehension where we follow the same patter; however, we use dict brackets ({) instead of list brackets ([):\n\nsquared_values_dict = {number: number*number for number in range(10)}\nsquared_values_dict\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81}\n\n\n\n\n\n\n\n\nTipVideo 🎥:\n\n\n\nCheck out this video that provides more discussion and examples of using comprehensions.\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nRe-write the following for loop using a dictionary comprehension:\nmonthly_data_files = glob.glob(\"../data/monthly_data/*\")\nmonthly_data = {}\n\nfor file in monthly_data_files:\n    file_name = os.path.basename(file)[:-4]\n    monthly_data[file_name] = pd.read_csv(file)\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#while-loop",
    "href": "17-iteration-statements.html#while-loop",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.5 while loop",
    "text": "17.5 while loop\nWe may not always know how many iterations we need to make. Rather, we simply want to perform some task while a particular condition exists. This is the job of a while loop. A while loop follows the same logic as a for loop, except, rather than specify a sequence we want to specify a condition that will determine how many iterations.\n# syntax of for loop\nwhile condition_holds:\n    &lt;do stuff here with i&gt;\nFor example, the probability of flipping 10 coins and getting all heads or tails is \\((\\frac{1}{2})^{10} = 0.0009765625\\) (1 in 1024 tries). Let’s implement this and see how many times it’ll take to accomplish this feat.\nThe following while statement will check if the number of unique values for 10 flips are 1, which implies that we flipped all heads or tails. If it is not equal to 1 then we repeat the process of flipping 10 coins and incrementing the number of tries. When our condition statement ten_of_a_kind == True then our while loop will stop.\n\n# create a coin\ncoin = ['heads', 'tails']\n\n# we'll use this to track how many tries it takes to get 10 heads or 10 tails\nn_tries = 0\n\n# signals if we got 10 heads or 10 tails\nten_of_a_kind = False\n\nwhile not ten_of_a_kind:\n    # flip coin 10 times\n    ten_coin_flips = [random.choice(coin) for flip in range(11)]\n\n    # check if there\n    ten_of_a_kind = len(set(ten_coin_flips)) == 1\n\n    # add iteration to counter\n    n_tries += 1\n\n\nprint(f'After {n_tries} flips: {ten_coin_flips}')\n\nAfter 2288 flips: ['heads', 'heads', 'heads', 'heads', 'heads', 'heads', 'heads', 'heads', 'heads', 'heads', 'heads']\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nAn elementary example of a random walk is the random walk on the integer number line, \\(Z\\), which starts at 0 and at each step moves +1 or −1 with equal probability.\nFill in the incomplete code chunk below to perform a random walk starting at value 0, with each step either adding or subtracting 1. Have your random walk stop if the value it exceeds 100 or if the number of steps taken exceeds 10,000.\nvalue = 0\nn_tries = 0\nexceeds_100 = False\n\nwhile not exceeds_100 or _______:\n    # randomly add or subtract 1\n    random_value = random.choice([-1, 1])\n    value += _____\n\n    # check if value exceeds 100\n    exceeds_100 = ______\n\n    # add iteration to counter\n    n_tries += _____\n\n  \nprint(f'The final value was {value} after {n_tries} iterations.')\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#iterables",
    "href": "17-iteration-statements.html#iterables",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.6 Iterables",
    "text": "17.6 Iterables\nPython strongly leverages the concept of iterable objects. An object is considered iterable if it is either a physically stored sequence, or an object that produces one result at a time in the context of an interation tool like a for loop. Up to this point, our example looping structures have primarily iterated over a DataFrame or a list.\nWhen our for loop iterates over a DataFrame, underneath the hood it is first accessing the iterable object, and then iterating over each item. As the following illustrates, the default iterable components of a DataFrame are the columns:\n\ndf = pd.DataFrame({'col1': [1, 2, 3], 'col2': [3, 4, 5], 'col3': [6, 6, 6]})\n\nI = df.__iter__() # access iterable object\nprint(next(I))    # first iteration\nprint(next(I))    # second iteration\nprint(next(I))    # third iteration\n\ncol1\ncol2\ncol3\n\n\nWhen our for loop iterates over a list, the same procedure unfolds. Note that when no more items are available to iterate over, a StopIteration is thrown which signals to our for loop that no more itertions should be performed.\n\nnames = ['Robert', 'Sandy', 'John', 'Patrick']\n\nI = names.__iter__() # access iterable object\nprint(next(I))       # first iteration\nprint(next(I))       # second iteration\nprint(next(I))       # third iteration\nprint(next(I))       # fourth iteration\nprint(next(I))       # no more items\n\nRobert\nSandy\nJohn\nPatrick\n\n\n\n---------------------------------------------------------------------------\nStopIteration                             Traceback (most recent call last)\nCell In[18], line 8\n      6 print(next(I))       # third iteration\n      7 print(next(I))       # fourth iteration\n----&gt; 8 print(next(I))       # no more items\n\nStopIteration: \n\n\n\nDictionaries and tuples are also iterable objects. Iterating over dictionary automatically returns one key at a time, which allows us to have the key and index for that key at the same time:\n\nD = {'a':1, 'b':2, 'c':3}\n\nI = D.__iter__()  # access iterable object\nprint(next(I))    # first iteration\nprint(next(I))    # second iteration\nprint(next(I))    # third iteration\n\na\nb\nc\n\n\n\nfor key in D:\n    print(key, D[key])\n\na 1\nb 2\nc 3\n\n\nAlthough using these iterables in a for loop is quite common, you will often see two other approaches which include the iterables range() and enumerate(). range is often used to generate indexes in a for loop but you can use it anywhere you need a series of integers. However, range is an iterable that generates items on demand:\n\nvalues = range(5)\n\nI = values.__iter__()\nprint(next(I))\nprint(next(I))\nprint(next(I))\n\n0\n1\n2\n\n\nSo if you wanted to iterate over each column in our DataFrame, an alternative is to use range. In this example, range produces the numeric index for each column so we simply use that value to index for the column within the for loop:\n\nunique_values = []\nfor col in range(len(df.columns)):\n  value = df.iloc[:, col].nunique()\n  unique_values.append(value)\n\nunique_values\n\n[3, 3, 1]\n\n\nAnother common iterator you will see is enumerate. Actually, the enumerate function returns a generator object, which also supports this iterator concept. The benefit of enumerate is that it returns a (index, value) tuple each time through the loop:\n\nE = enumerate(df) # access iterable object\nprint(next(E))    # first iteration\nprint(next(E))    # second iteration\nprint(next(E))    # third iteration\n\n(0, 'col1')\n(1, 'col2')\n(2, 'col3')\n\n\nThe for loop steps through these tuples automatically and allows us to unpack their values with tuple assignment in the header of the for loop. In the following example, we unpack the tuples into the variables index and col and we can now use both of these values however necessary in a for loop.\n\nfor index, col in enumerate(df):\n    print(f'{index} - {col}')\n\n0 - col1\n1 - col2\n2 - col3\n\n\nThere are additional iterable objects that can be used in looping structures (i.e. zip, map); however, the ones discussed here are the most common you will come across and likely use.\n\n\n\n\n\n\nTipVideo 🎥:\n\n\n\nLearn more about iterables and a similar, yet different concept – ‘iterators’ with this video.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#summary",
    "href": "17-iteration-statements.html#summary",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.7 Summary",
    "text": "17.7 Summary\nIn this chapter, you learned how to use iteration statements to write more efficient and powerful Python code. These tools are essential for any data scientist or analyst, especially when working with large datasets or needing to automate repetitive tasks.\nYou explored how:\n\nThe for loop allows you to iterate over sequences like lists, dictionaries, and DataFrames.\nThe while loop executes code repeatedly until a specified condition is no longer true.\nbreak and continue give you more control over loop execution.\nList and dictionary comprehensions provide a compact and readable way to create new collections.\nIterables and iterator objects, such as range() and enumerate(), form the foundation of Python looping behavior and data traversal.\n\nUnderstanding these concepts sets you up for more advanced programming patterns, where repetition, transformation, and control flow are crucial.\nBut iteration isn’t the only way to make your code more concise and reusable. In the next chapter, you’ll take your skills a step further by learning how to write your own functions. Functions allow you to encapsulate logic into clean, modular blocks of code—another key capability for data scientists who want to write readable, efficient, and maintainable analysis pipelines.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#exercise-practicing-looping-and-iteration-patterns",
    "href": "17-iteration-statements.html#exercise-practicing-looping-and-iteration-patterns",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.8 Exercise: Practicing Looping and Iteration Patterns",
    "text": "17.8 Exercise: Practicing Looping and Iteration Patterns\nIn this exercise set, you’ll practice using for loops, while loops, conditional logic, and comprehensions. These tasks will help you build fluency with the iteration patterns that show up frequently in data wrangling and automation tasks.\nYou can run these exercises in your own Python editor or in the companion notebook.\n\n\n\n\n\n\nTip💡 Stuck or Unsure?\n\n\n\nDon’t hesitate to ask for help! Use ChatGPT, GitHub Copilot, or any other AI coding assistant to get guidance or debug your code. It’s a great way to reinforce learning and explore alternate solutions.\n\n\n\n\n\n\n\n\nNone1. Filter Capitalized Names with a Comprehension\n\n\n\n\n\nUse the list of names below to write a list comprehension that returns only the values that start with a capital letter (i.e. a “title case” word).\nnames = ['Steve Irwin', 'koala', 'kangaroo', 'Australia', 'Sydney', 'desert']\nHint: Try using the .istitle() method.\nWhich names are included in the result?\n\n\n\n\n\n\n\n\n\nNone2. Generate the Fibonacci Sequence\n\n\n\n\n\nThe Fibonacci Sequence starts with the numbers 0 and 1, and each subsequent number is the sum of the two previous numbers. For example: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...]\nWrite a for loop that generates the first 25 Fibonacci numbers and stores them in a list.\n\n\n\n\n\n\n\n\n\nNone3. Sum with Conditional Skip\n\n\n\n\n\nWrite a for loop that computes the sum of all numbers from 0 through 100, excluding the numbers in the list below:\nskip_these_numbers = [8, 29, 43, 68, 98]\nUse a continue statement to skip over those values. What is the resulting sum?",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "18-functions.html",
    "href": "18-functions.html",
    "title": "18  Writing Your Own Functions",
    "section": "",
    "text": "18.1 Prerequisites\nIn data science and data mining, writing your own functions is critical. As your analyses grow in complexity, so do your code bases. Functions allow you to:\nThis chapter will show you how to write your own functions—from the basics of syntax to more advanced ideas like type hints and anonymous functions. Whether you’re preparing data, building machine learning models, or conducting statistical analyses, writing custom functions will make you a more effective data scientist.\nBy the end of this lesson you will be able to:\nimport numpy as np\nimport pandas as pd\nfrom completejourney_py import get_data\n\ndf = get_data()['transactions']\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/completejourney_py/get_data.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.\n  from pkg_resources import resource_filename",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#when-to-write-functions",
    "href": "18-functions.html#when-to-write-functions",
    "title": "18  Writing Your Own Functions",
    "section": "18.2 When to write functions",
    "text": "18.2 When to write functions\nYou should consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code). For example, take a look at this code. What does it do?\n\n# array containing 4 sets of 10 random numbers\nx = np.random.random_sample((4, 10))\n\nx[0] = (x[0] - x[0].min()) / (x[0].max() - x[0].min())\nx[1] = (x[1] - x[1].min()) / (x[1].max() - x[1].min())\nx[2] = (x[2] - x[2].min()) / (x[1].max() - x[2].min())\nx[3] = (x[3] - x[3].min()) / (x[3].max() - x[3].min())\n\nYou might be able to puzzle out that this rescales each array to have a range from 0 to 1. But did you spot the mistake? I made an error when copying-and-pasting the code for the third array: I forgot to change an x[1] to an x[2]. Writing a function reduces the chance of this error and makes it more explicit regarding our intentions since we can name our action (rescale) that we want to perform:\n\nx = np.random.random_sample((4, 10))\n\ndef rescale(array):\n    for index, vector in enumerate(array):\n        array[index] = (vector - vector.min()) / (vector.max() - vector.min())\n\n    return(array)\n\nrescale(x)\n\narray([[0.        , 0.03841467, 0.03789933, 1.        , 0.85339659,\n        0.08065389, 0.32694126, 0.21989806, 0.21157039, 0.89959812],\n       [0.        , 1.        , 0.24512148, 0.24726478, 0.5181382 ,\n        0.75725332, 0.22376755, 0.06974714, 0.21287652, 0.15810726],\n       [0.86091631, 0.        , 0.92519151, 0.61407877, 0.78990797,\n        0.41973743, 1.        , 0.04778701, 0.18895006, 0.84726445],\n       [0.78307989, 0.17598957, 0.06630417, 0.73824984, 0.41616928,\n        0.        , 0.55599547, 1.        , 0.89319031, 0.67332148]])",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#functions-vs-methods",
    "href": "18-functions.html#functions-vs-methods",
    "title": "18  Writing Your Own Functions",
    "section": "18.3 Functions vs methods",
    "text": "18.3 Functions vs methods\nFunctions and methods are very similar, and beginners to object oriented programming languages such as Python often refer to them synonymously. Although they are very similar, it is always good to distinguish between the two as it will help you explain code situations more explicitly.\nA method refers to a function which is part of a class. You access it with an instance or object of the class. A function doesn’t have this restriction: it just refers to a standalone function. This means that all methods are functions, but not all functions are methods.\n\n# stand alone function\nsum(x)\n\narray([1.6439962 , 1.21440424, 1.27451648, 2.59959338, 2.57761205,\n       1.25764464, 2.10670428, 1.33743221, 1.50658728, 2.5782913 ])\n\n\n\n# method\nx.sum(axis = 0)\n\narray([1.6439962 , 1.21440424, 1.27451648, 2.59959338, 2.57761205,\n       1.25764464, 2.10670428, 1.33743221, 1.50658728, 2.5782913 ])\n\n\nAs illustrated above, there can be functions and methods that behave in a similar manner. However, often, methods have special parameters that allow them to behave uniquely to the object they are attached to. For example, the sum() method for the Numpy array allows you to get the overall sum, sum of each column, and sum of each row. This would take more effort to compute with the stand alone function.\n\n# overall sum\nx.sum()\n\nnp.float64(18.096782061312464)\n\n\n\n# sum of each column\nx.sum(axis = 0)\n\narray([1.6439962 , 1.21440424, 1.27451648, 2.59959338, 2.57761205,\n       1.25764464, 2.10670428, 1.33743221, 1.50658728, 2.5782913 ])\n\n\n\n# sum of each row\nx.sum(axis = 1)\n\narray([3.66837231, 3.43227625, 5.6938335 , 5.3023    ])\n\n\nNonetheless, being able to create stand alone functions is a critical task. And, if you delve into building your own object classes down the road, defining methods is a piece of cake if you know how to define stand alone functions.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#defining-functions",
    "href": "18-functions.html#defining-functions",
    "title": "18  Writing Your Own Functions",
    "section": "18.4 Defining functions",
    "text": "18.4 Defining functions\nThere are four main steps to defining a function:\n\nUse the keyword def to declare the function and follow this up with the function name.\nAdd parameters to the function: they should be within the parentheses of the function. End your line with a colon.\nAdd statements that the functions should execute.\nEnd your function with a return statement if the function should output something. Without the return statement, your function will return an object None.\n\n\ndef yell(text):\n    new_text = text.upper()\n    return new_text\n\nyell('hello world!')\n\n'HELLO WORLD!'\n\n\nOf course, your functions will get more complex as you go along: you can add for loops, flow control, and more to it to make it more finegrained. Let’s build a function that finds the total sales for a store, for a given day. The parameters required for the function are the data frame being analyzed, the store_id, and the specific week for which we need the sales.\n\ndef store_sales(data, store, week):\n    filt = (data['store_id'] == store) & (data['week'] == week)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\nstore_sales(data=df, store=309, week=48)\n\nnp.float64(395.6)\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nDefine a function titled ratio that takes arguments x and y and returns their ratio, \\(\\frac{x}{y}\\).\nNow add a third variable digits that allows you to round the output to a specified decimal.\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#parameters-vs-arguments",
    "href": "18-functions.html#parameters-vs-arguments",
    "title": "18  Writing Your Own Functions",
    "section": "18.5 Parameters vs arguments",
    "text": "18.5 Parameters vs arguments\nPeople often refer to function parameters and arguments synonymously but, technically, they represent two different things. Parameters are the names used when defining a function or a method, and into which arguments will be mapped. In other words, arguments are the things which are supplied to any function or method call, while the function or method code refers to the arguments by their parameter names.\n\n\n\n\n\n\nBeing specific about the term ‘parameters’ versus ‘arguments’ can be helpful when discussing specific details regarding a function or inputs, especially during debugging.\n\n\n\nPython accepts a variety of parameter-argument calls. The following discusses some of the more common implementations you’ll see.\n\nKeyword arguments\nKeyword arguments are simply named arguments in the function call. Consider our store_sales() function, we can specify parameter arguments in two different ways. The first is positional, in which we supply our arguments in the same position as the parameter. However, this is less explicit and if we happen to switch our store and week argument placement then we may get different results then anticipated.\n\n# implicitly computing store sales for store 46 during week 43\nstore_sales(df, 46, 43)\n\nnp.float64(60.39)\n\n\n\n# implicitly computing store sales for store 43 (does not exist) during week 46\nstore_sales(df, 43, 46)\n\nnp.float64(0.0)\n\n\nUsing keyword, rather than positional, arguments makes your function call more explicit and allows you to specify them in different orders.\n\n\n\n\n\n\nPer the Zen of Python, ‘explicit is better than implicit’.\n\n\n\n\n# explicitly computing store sales for store 46 during week 43\nstore_sales(data=df, week=43, store=46)\n\nnp.float64(60.39)\n\n\n\n\nDefault arguments\nDefault arguments are those that take a default value if no argument value is passed during the function call. You can assign this default value by with the assignment operator =, just like in the below example. For example, if your analysis routinely analyzes transactions for all quantities but sometimes you only focus on sales for when quantity purchased meets some threshold, you could add a new parameter with a default value.\n\n\n\n\n\n\nDefault arguments should be placed after parameters with no defaults assigned in the function call.\n\n\n\n\ndef store_sales(data, store, week, qty_greater_than=0):\n    filt = (data['store_id'] == store) & (data['week'] == week) & (data['quantity'] &gt; qty_greater_than)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\n# you do not need to specify an input for qty_greater_than\nstore_sales(data=df, store=309, week=48)\n\nnp.float64(395.6)\n\n\n\n# but you can if you want to change it from the default\nstore_sales(data=df, store=309, week=48, qty_greater_than=2)\n\nnp.float64(92.73)\n\n\n\n\n*args and **kwargs\n*args and **kwargs allow you to pass a variable number of arguments to a function. This can be usefule when you do not know before hand how many arguments will be passed to your function by the user or if you want to allow users to pass arguments through to an embedded function.\n\n\n\n\n\n\nJust an FYI that it is not necessary to write *args or **kwargs. Only the * (asterisk) is necessary. You could have also written *var and **vars. Writing *args and **kwargs is just a convention established in the Python community.\n\n\n\n*args is used to send a non-keyworded variable length argument list to the function. When the function is called, Python collects the *args values as a tuple, which can be used in a similar manner as any other tuple functionality (i.e. indexing, for looped) For example, we can let the user supply as many string inputs to the very useful yell() function by using *args.\n\ndef yell(*args):\n    new_text = ' '.join(args).upper()\n    return new_text\n\nyell('hello world!', 'I', 'love', 'Python!!')\n\n'HELLO WORLD! I LOVE PYTHON!!'\n\n\nThe special syntax **kwargs in function definitions in python is used to pass a keyworded, variable-length argument list. The reason is because the double star allows us to pass through keyword arguments (and any number of them). Python collects the **kwargs into a new dictionary, which can be used for any normal dictionary functionality.\n\n# **kwargs just creates a dictionary\ndef students(**kwargs):\n    print(kwargs)\n\nstudents(student1='John', student2='Robert', student3='Sally')\n\n{'student1': 'John', 'student2': 'Robert', 'student3': 'Sally'}\n\n\n\n# we can use this dictionary however necessary\ndef print_student_names(**kwargs):\n    for key, value in kwargs.items():\n        print(f'{key} = {value}')\n\nprint_student_names(student1='John', student2='Robert', student3='Sally')\n\nstudent1 = John\nstudent2 = Robert\nstudent3 = Sally\n\n\n\n\nType hints\nPython 3.5 added a new library called typing that adds type hinting to Python. Type hinting is part of a larger functionality called Type Checking (see this tutorial for a comprehensive Type Checking guide). Type hinting provides a way to help users better understand what type of arguments a function takes and the expected output.\nFor example, let’s look at a simple function:\n\ndef some_function(name, age):\n    return f'{name} is {age} years old'\n\nsome_function('Tom', 27)\n\n'Tom is 27 years old'\n\n\nIn the above function we simply expect the user to insert the proper type of arguments and to realize the output will be a string. Now, this isn’t that challenging because its a small and simple function but as our functions get more complex it is not always easy to decipher what type of arguments should be passed and what the output will be.\nWe can hint to the user this information with type hints by re-writing the function as below. Here, we are hinting that the name argument should be a string, age should be an integer, and the output will be a string (-&gt; str).\n\ndef some_function(name: str, age: int) -&gt; str:\n    return f'{name} is {age} years old'\n\nsome_function('Tom', 27)\n\n'Tom is 27 years old'\n\n\nNow, when you look at the function documentation, you can quickly see the type hints (we’ll see additional ways to improve function documentation in the Docstrings section).\n\nhelp(some_function)\n\nHelp on function some_function in module __main__:\n\nsome_function(name: str, age: int) -&gt; str\n\n\n\nEven better, most IDEs support type hints so that they can give you feedback as you are typing:\n\n\n\n\n\n\nFigure 18.1: Type hints often show up when you hover over the function in most IDEs.\n\n\n\nWhen declaring types, you can use any of the built-in Python data types. This includes, but is not limited to:\n\nint, float, str, bool\ntuple, list, set, dict, array\nfrozenset\nOptional\nIterable, Iterator\nand many more!\n\nYou can also declare non-built-in data types such as Pandas and Numpy object types. For example, we can re-write our store_sales() function with type hints where we hint that data should be a pd.DataFrame.\n\n\n\n\n\n\nNote that pd.DataFrame assumes that you imported the Pandas module with the pd alias.\n\n\n\n\ndef store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n    filt = (data['store_id'] == store) & (data['week'] == week)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\nstore_sales(data=df, store=309, week=48)\n\nnp.float64(395.6)\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nBuild on to your ratio() function by:\n\nSetting the digits parameter to default to 2.\nAdd type hints that specify x and y to be numeric, digits to be int and the output returned to be float.\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#docstrings",
    "href": "18-functions.html#docstrings",
    "title": "18  Writing Your Own Functions",
    "section": "18.6 Docstrings",
    "text": "18.6 Docstrings\nType hints are great for guiding users; however, they only provide small hints. Python docstrings are a much more robust way to document a Python function (docstrings can also be used to document modules, classes, and methods). Docstrings are used to provide users with descriptive information about the function, required inputs, expected outputs, example usage and more. Also, it is a common practice to generate online (html) documentation automatically from docstrings using tools such as Sphinx.\n\n\n\n\n\n\nSee the Pandas API reference for the various Pandas functions/methods/class for example Sphinx documentation that is built from docstrings.\n\n\n\nDocstrings typically consist of a multi-line character string description that follows the header line of a function definition. The following provides an example for a simple addition function. Note how the docstring provides a summary of what the function does, information regarding expected parameter inputs, what the returned output will be, a “see also” section that lets users know about related functions/methods (this can be skipped if not applicable), and some examples of implementing the function.\n\ndef store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n    \"\"\"\n    Compute total store sales.\n\n    This function computes the total sales for a given\n    store and week based on a user supplied DataFrame that\n    contains sales in a column named `sales_value`.\n\n    Parameters\n    ----------\n    data : DataFrame\n        Pandas DataFrame\n    store : int\n        Integer value representing store number\n    week : int\n        Integer value representing week of year\n\n    Returns\n    -------\n    float\n        A float object representing total store sales\n\n    See Also\n    --------\n    store_visits : Computes total store visits\n\n    Examples\n    --------\n    &gt;&gt;&gt; store_sales(data=df, store=309, week=48)\n    395.6\n    &gt;&gt;&gt; store_sales(data=df, store=46, week=43)\n    60.39\n    \"\"\"\n    filt = (data['store_id'] == store) & (data['week'] == week)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\n\n\n\n\n\n\nThere are some typical Pythonic conventions used when writing docstrings. Rather than reiterate them here, we highly suggest using the Pandas and Numpy docstring guide standards when writing your docstrings.\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nGo back to the ratio function you created in the last knowledge check and add docstrings. Be sure to include a general description of what the function does, documentation on the parameters and return output, along with including examples.\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#errors-and-exceptions",
    "href": "18-functions.html#errors-and-exceptions",
    "title": "18  Writing Your Own Functions",
    "section": "18.7 Errors and exceptions",
    "text": "18.7 Errors and exceptions\nFor functions that will be used over and over again, and especially for those used by someone other than the creator of the function, it is good to include procedures to check for errors that may derail function execution. This may include ensuring the user supplies proper argument types, the computation can perform with the values supplied, or even that execution can be performed in a reasonable amount of time.\nThis falls under the umbrella of exception handling and is actually far broader than what we can cover here. In this section, we’ll demonstrate some of the basics.\n\nValidating arguments\nA common problem is when the user supplies invalid argument types or values. Although we have seen how to use type hints and docstrings to inform users, we can also include useful exception calls for when users supply improper argument types and values. For example, the following will check if the arguments of the correct type by using isinstance and, if they are not, we raise an Exception and include an informative error.\n\ndef store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n     # argument validation\n    if not isinstance(data, pd.DataFrame): raise Exception('`data` should be a Pandas DataFrame')\n    if not isinstance(store, int): raise Exception('`store` should be an integer')\n    if not isinstance(week, int): raise Exception('`week` should be an integer')\n\n    # computation\n    filt = (data['store_id'] == store) & (data['week'] == week)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\nstore_sales(data=df, store='309', week=48)\n\n\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\nCell In[24], line 12\n      9     total_sales = data['sales_value'][filt].sum()\n     10     return total_sales\n---&gt; 12 store_sales(data=df, store='309', week=48)\n\nCell In[24], line 4, in store_sales(data, store, week)\n      1 def store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n      2      # argument validation\n      3     if not isinstance(data, pd.DataFrame): raise Exception('`data` should be a Pandas DataFrame')\n----&gt; 4     if not isinstance(store, int): raise Exception('`store` should be an integer')\n      5     if not isinstance(week, int): raise Exception('`week` should be an integer')\n      7     # computation\n\nException: `store` should be an integer\n\n\n\nNote that Exception() is used to create a generic exception object/output. Python has many built-in exception types that can be used to indicate a specific error has occured. For example, we could replace Exception() with TypeError() in the previous example to make it more specific that the error is due to an invalid argument type.\n\ndef store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n    # argument validation\n    if not isinstance(data, pd.DataFrame): raise TypeError('`data` should be a Pandas DataFrame')\n    if not isinstance(store, int): raise TypeError('`store` should be an integer')\n    if not isinstance(week, int): raise TypeError('`week` should be an integer')\n\n    # computation\n    filt = (data['store_id'] == store) & (data['week'] == week)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\nstore_sales(data=df, store='309', week=48)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[25], line 12\n      9     total_sales = data['sales_value'][filt].sum()\n     10     return total_sales\n---&gt; 12 store_sales(data=df, store='309', week=48)\n\nCell In[25], line 4, in store_sales(data, store, week)\n      1 def store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n      2     # argument validation\n      3     if not isinstance(data, pd.DataFrame): raise TypeError('`data` should be a Pandas DataFrame')\n----&gt; 4     if not isinstance(store, int): raise TypeError('`store` should be an integer')\n      5     if not isinstance(week, int): raise TypeError('`week` should be an integer')\n      7     # computation\n\nTypeError: `store` should be an integer\n\n\n\nWe can expand this as much as necessary. For example, say we want to ensure users only use a store value that exists in our data. Currently, if the user supplies a store value that does not exist (i.e. 35), they simply get a return value of 0.\n\nstore_sales(data=df, store=35, week=48)\n\nnp.float64(0.0)\n\n\nWe can add an if statement to check if the store number exists and supply an error message to the user if it does not:\n\ndef store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n    # argument validation\n    if not isinstance(data, pd.DataFrame): raise TypeError('`data` should be a Pandas DataFrame')\n    if not isinstance(store, int): raise TypeError('`store` should be an integer')\n    if not isinstance(week, int): raise TypeError('`week` should be an integer')\n    if store not in data.store_id.unique():\n        raise ValueError(f'`store` {store} does not exist in the supplied DataFrame')\n\n\n    # computation\n    filt = (data['store_id'] == store) & (data['week'] == week)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\nstore_sales(data=df, store=35, week=48)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[27], line 15\n     12     total_sales = data['sales_value'][filt].sum()\n     13     return total_sales\n---&gt; 15 store_sales(data=df, store=35, week=48)\n\nCell In[27], line 7, in store_sales(data, store, week)\n      5 if not isinstance(week, int): raise TypeError('`week` should be an integer')\n      6 if store not in data.store_id.unique():\n----&gt; 7     raise ValueError(f'`store` {store} does not exist in the supplied DataFrame')\n     10 # computation\n     11 filt = (data['store_id'] == store) & (data['week'] == week)\n\nValueError: `store` 35 does not exist in the supplied DataFrame\n\n\n\n\n\nAssert statements\nPython’s assert statement is a debugging aid that tests a condition. If the condition is true, it does nothing and your program just continues to execute. But if the assert condition evaluates to false, it raises an AssertionError exception with an optional error message. This seems very similar to the example in the last section where we checked if the supplied store ID exists. However, the proper use of assertions is to inform users about unrecoverable errors in a program. They’re not intended to signal expected error conditions, like “store ID not found”, where a user can take corrective action or just try again.\n\n\n\n\n\n\nAnother way to look at it is to say that assertions are internal self-checks for your program. They work by declaring some conditions as impossible in your code. If one of these conditions doesn’t hold that means there’s a bug in the program.\n\n\n\nFor example, say you have a program that automatically applies a discount to product and you eventually write the following apply_discount function:\n\ndef apply_discount(product, discount):\n    price = round(product['price'] * (1.0 - discount), 2)\n    assert 0 &lt;= price &lt;= product['price']\n    return price\n\nNotice the assert statement in there? It will guarantee that, no matter what, discounted prices cannot be lower than $0 and they cannot be higher than the original price of the product.\nLet’s make sure this actually works as intended. You can see that the second example throws an AssertionError\n\n# 25% off 3.50 should equal 2.62\nmilk = {'name': 'Chocolate Milk', 'price': 3.50}\napply_discount(milk, 0.25)\n\n2.62\n\n\n\n# 200% discount is not allowed\napply_discount(milk, 2.00)\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[30], line 2\n      1 # 200% discount is not allowed\n----&gt; 2 apply_discount(milk, 2.00)\n\nCell In[28], line 3, in apply_discount(product, discount)\n      1 def apply_discount(product, discount):\n      2     price = round(product['price'] * (1.0 - discount), 2)\n----&gt; 3     assert 0 &lt;= price &lt;= product['price']\n      4     return price\n\nAssertionError: \n\n\n\nIn the above example, we simply got an AssertionError but no message to help us understand what caused the error. The assert statement follows the syntax of assert condition, message so we can add an informative message at the end of our assert statement.\n\ndef apply_discount(product, discount):\n    price = round(product['price'] * (1.0 - discount), 2)\n    assert 0 &lt;= price &lt;= product['price'], 'Invalid discount applied'\n    return price\n\napply_discount(milk, 2.00)\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[31], line 6\n      3     assert 0 &lt;= price &lt;= product['price'], 'Invalid discount applied'\n      4     return price\n----&gt; 6 apply_discount(milk, 2.00)\n\nCell In[31], line 3, in apply_discount(product, discount)\n      1 def apply_discount(product, discount):\n      2     price = round(product['price'] * (1.0 - discount), 2)\n----&gt; 3     assert 0 &lt;= price &lt;= product['price'], 'Invalid discount applied'\n      4     return price\n\nAssertionError: Invalid discount applied\n\n\n\n\n\nTry and except\nIn the prior sections, we looked at ways to identify errors that may occur. The result of our exception handling is to signal an error occurred and stop the program. However, there are often times when we do not want to stop the program. For example, if the program has a database connection that should be closed after execution then we need our program to continue running even if an error occurs to ensure the connection is closed.\nThe try except procedure allows us to try execute code. If it works, great! If not, rather than just throw an exception error, we define what the program should continue to do. For example, the following tries the apply_discount() function with a pre-defined discount value. If it throws and exception then we adjust the discount value (if discount is greater than 100% of product price we adjust it to the largest acceptable value, if its less than 0 we just set it to 0%).\n\n# this discount is created somewhere else in the program\ndiscount = 2\n\n# if discount causes an error adjust it\ntry:\n    apply_discount(milk, discount)\nexcept Exception:\n    if discount &gt; 1: discount = 0.99\n    if discount &lt; 0: discount = 0\n    apply_discount(milk, discount)\n\nThe try except procedure allows us to include as many except statements as necessary for different types of errors (think of them like elifs). For example, the following illustrates how we could have different types of code execution for a TypeError, a ValueError, and then the final else statement captures all other errors.\n\n\n\n\n\n\nThis procedure will run in order, consequently you want to have the most specific exception handlers first followed by more general exception handlers later on.\n\n\n\n\ntry:\n    store_sales(data=df, store=35, week=48)\nexcept TypeError:\n    print('do something specific for a `TypeError`')\nexcept ValueError:\n    print('do something specific for a `ValueError`')\nelse:\n    print('do something specific for all other errors')\n\ndo something specific for a `ValueError`\n\n\nLastly, we may have a certain piece of code that we always want to ensure gets ran in a program. For example, we may need to ensure a database connection or file is closed before an error is thrown. The following illustrates how we can add a finally at the end of our try except procedure. This finally will always be ran. If the code in the try clause runs without error, the finally code chunk will run after the try block. If an error occurse, the finally code chunk will run before the relevant except code chunk.\n\ntry:\n    store_sales(data=df, store=35, week=48)\nexcept TypeError:\n    raise\nexcept ValueError:\n    raise\nfinally:\n    print('Code to close database connection')\n\nCode to close database connection\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[34], line 2\n      1 try:\n----&gt; 2     store_sales(data=df, store=35, week=48)\n      3 except TypeError:\n      4     raise\n\nCell In[27], line 7, in store_sales(data, store, week)\n      5 if not isinstance(week, int): raise TypeError('`week` should be an integer')\n      6 if store not in data.store_id.unique():\n----&gt; 7     raise ValueError(f'`store` {store} does not exist in the supplied DataFrame')\n     10 # computation\n     11 filt = (data['store_id'] == store) & (data['week'] == week)\n\nValueError: `store` 35 does not exist in the supplied DataFrame\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nGoing back to the ratio function:\n\nAdd a procdure to validate that x and y inputs are numeric and digits is an integer.\nAdd a try and except procedure where if a TypeError is thrown for digits because it is a float then the digits value is rounded to the nearest integer and applied.\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#scoping",
    "href": "18-functions.html#scoping",
    "title": "18  Writing Your Own Functions",
    "section": "18.8 Scoping",
    "text": "18.8 Scoping\nScoping refers to the set of rules a programming language uses to lookup the value to variables and/or symbols. The following illustrates the basic concept behind the lexical scoping rules that Python follows. In short, Python follows a nested environment structure and uses what is commonly referred to as the LEGB search rule:\n\nSearch local scope first,\nthen the local scopes of enclosing functions,\nthen the global scope,\nand finally the built-in scope\n\n\nSearching for variables\nWhat exactly does this mean? A function has its own environment and when you assign an argument in the def header of the function, the function creates a separate environment that keeps that variable separate from any variable in the global environment. This is why you can have an x variable in the global environment that won’t be confused with an x variable in your function:\n\nx = 84\n\ndef func(x):\n  return x + 1\n\nfunc(x = 50)\n\n51\n\n\nHowever, the function environment is only active when called and all function variables are removed from memory after being called. Consequently, you can continue using x that is contained in the global environment.\n\nx\n\n84\n\n\nHowever, if a variable does not exist within the function, Python will look one level up to see if the variable exists. In this case, since y is not supplied in the function header, Python will look in the next environment up (global environment in this case) for that variable:\n\ny = 'Boehmke'\n\ndef func(x):\n  return x + ' ' + y\n\nfunc(x = 'Brad')\n\n'Brad Boehmke'\n\n\nThe same will happen if we have nested functions. Python will search in enclosing functions in a hierarchical fashion until it finds the necessary variables. In this convoluted procedure…\n\ny is a global variable,\nx & sep are local variables to the my_name function,\nand my_paste has no local variables,\nthe my_name function gets y from the global environment,\nand the my_paste function gets all its inputs from the my_name environment.\n\n\n\n\n\n\n\nWe do not recommend that you write functions like this. This is primarily only for explaining how Python searches for information, which can be helpful for debugging.\n\n\n\n\ny = 'Boehmke'\n\ndef my_name(sep):\n    x = 'Brad'\n    def my_paste():\n        return x + sep + y\n    return my_paste()\n\nmy_name(sep=' ')\n\n'Brad Boehmke'\n\n\n\n\nChanging variables\nIt is possible to change variable values that are outside of the active environment. This is rarely necessary, and is usually not good practice, but it is good to know about. For example, the following changes the global variable y by including the keyword-variable statement global y prior to making the assignment of the new value for y.\n\n\n\n\n\n\nThe same can be done with changing values in nested functions; however, you would use the keyword nonlocal to change the value of an enclosing function’s local variable.\n\n\n\n\ny = 8451\n\ndef convert(x):\n    x = str(x)\n    firstpart, secondpart = x[:len(x)//2], x[len(x)//2:]\n    global y\n    y = firstpart + '.' + secondpart\n    return y\n\nconvert(8451)\n\n'84.51'\n\n\n\ny\n\n'84.51'",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#anonymous-functions",
    "href": "18-functions.html#anonymous-functions",
    "title": "18  Writing Your Own Functions",
    "section": "18.9 Anonymous functions",
    "text": "18.9 Anonymous functions\nSo far we have been discussing defined functions; however, Python allows you to generate anonymous functions on the fly. These are often referred to as lambdas. Lambdas allow for an alternative approach when creating short and simple functions that are only used once or twice.\nFor example, the following two functions are equivalent:\n# defined function\ndef func(x, y, z):\n    return x + y + z\n  \n# lambda function  \nlambda x, y, z: x + y + z  \nNote how the lambda’s body is a single expression and not a block statement. This is a requirement, which typically restricts lambdas to very short, concise function calls. The best use case for lambda functions are for when you want a simple function to be anonymously embedded within a larger expressions. For example, say we wanted to loop over each item in a list and apply a simple square function, we could accomplish this by supplying a lambda function to the map function. map just iterates over each item in an object and applies a given function (you could accomplish the exact same with a list comprehension).\n\nnums = [48, 6, 9, 21, 1]\n\nlist(map(lambda x: x ** 2, nums))\n\n[2304, 36, 81, 441, 1]\n\n\nAnother good example is the following one you already lesson 6a where we apply a lambda function to assign ‘high value’ for each transaction where sales_value is greater than 10 and ‘low value’ for all other transactions.\n\n(\n    df['sales_value']\n    .apply(lambda x: 'high value' if x &gt; 10 else 'low value')\n)\n\n0          low value\n1          low value\n2          low value\n3          low value\n4          low value\n             ...    \n1469302    low value\n1469303    low value\n1469304    low value\n1469305    low value\n1469306    low value\nName: sales_value, Length: 1469307, dtype: object\n\n\nHere is another example where we group by basket_id and then apply a lambda function to compute the average cost per item in each basket.\n\n(\n    df[['basket_id', 'sales_value', 'quantity']]\n    .groupby('basket_id')\n    .apply(lambda x: (x['sales_value'] / x['quantity']).mean())\n)\n\n/tmp/ipykernel_12896/1550772873.py:4: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda x: (x['sales_value'] / x['quantity']).mean())\n\n\nbasket_id\n31198437603    1.951207\n31198445400    1.250000\n31198445429    2.045000\n31198445465    1.680000\n31198452527    4.048333\n                 ...   \n41480008448    1.533810\n41480013048    1.206667\n41480018446    1.773210\n41481252623    1.285556\n41481282915    8.495000\nLength: 155848, dtype: float64\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nLet’s focus on the transaction timestamp for each transaction (df['transaction_timestamp']). Use the .apply() method along with a lambda function to assign each transaction to ‘weekday’ or ‘weekend’. To do this check out the .day_name() method (i.e. df['transaction_timestamp'][0].day_name())",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#summary",
    "href": "18-functions.html#summary",
    "title": "18  Writing Your Own Functions",
    "section": "18.10 Summary",
    "text": "18.10 Summary\nWriting your own functions is one of the most essential skills for any data scientist. In this chapter, you learned how to encapsulate your logic, reduce repetition, and build cleaner, more modular code through custom function definitions.\nWe began by discussing when and why to write functions, especially when working on complex data analysis tasks that benefit from reusability and clarity. You saw how functions differ from methods and how they support both simple and advanced programming constructs.\nYou learned how to:\n\nDefine functions using the def keyword, including parameters and return values.\nUse keyword arguments and default arguments to make your functions more flexible and explicit.\nLeverage *args and **kwargs to support variable-length argument lists for more general-purpose tools.\nAdd type hints to clarify what types of inputs a function expects and what it returns—great for documentation and collaboration.\nDocument your functions with docstrings, following common Python conventions that enhance readability and support automated documentation tools.\nApply error handling techniques using raise, try-except, assert, and custom error messages to make your functions more robust and user-friendly.\nUnderstand Python’s variable scoping rules, including the LEGB rule (Local, Enclosing, Global, Built-in), and how variables are resolved when functions are nested.\nCreate concise anonymous functions (a.k.a. lambda functions) and apply them within higher-order functions like map, apply, and groupby.\n\nThroughout the chapter, you saw realistic examples related to store sales and transaction data, reinforcing the importance of custom functions in real-world data mining and analysis workflows. Whether you’re cleaning data, transforming values, calculating metrics, or building more advanced analytics pipelines, your ability to write well-structured functions will directly improve the quality and maintainability of your work.\nIn short, functions are foundational to writing professional, efficient, and reusable Python code—skills that will serve you across any domain of data science.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#exercise-practicing-function-writing-and-application",
    "href": "18-functions.html#exercise-practicing-function-writing-and-application",
    "title": "18  Writing Your Own Functions",
    "section": "18.11 Exercise: Practicing Function Writing and Application",
    "text": "18.11 Exercise: Practicing Function Writing and Application\nIn this exercise set, you’ll practice defining and applying custom Python functions, using type hints and docstrings, and leveraging methods like .apply() to work with real-world data. These tasks will help solidify your understanding of functions and how to use them in data cleaning, feature engineering, and exploratory analysis workflows.\nYou can run these exercises in your own Python editor or in the companion notebook.\n\n\n\n\n\n\nTip💡 Stuck or Unsure?\n\n\n\nUse ChatGPT, GitHub Copilot, or any other AI coding assistant to debug your code or talk through your logic. It’s a great way to reinforce concepts and practice problem solving.\n\n\n\n\n\n\n\n\nNone1. Load and Inspect the Data\n\n\n\n\n\nDownload the companies.csv dataset and load it into a DataFrame. This dataset contains company names and financial attributes.\nInspect the first few rows. What columns are available?\n\n\n\n\n\n\n\n\n\nNone2. Define the is_incorporated() Function\n\n\n\n\n\nWrite a function is_incorporated(name) that checks whether the input string name contains the substring \"inc\" or \"Inc\". If either appears in the name, return True; otherwise return False.\nTest it using a few sample strings like:\nis_incorporated(\"Acme Inc.\")\nis_incorporated(\"Global Tech\")\n\n\n\n\n\n\n\n\n\nNone3. Add Type Hints and a Docstring\n\n\n\n\n\nNow update your is_incorporated() function to include:\n\nA type hint for the name parameter and the return type\nA docstring describing what the function does, the input parameter, and the return value\n\nUse the help() function or hover in your IDE to verify the documentation.\n\n\n\n\n\n\n\n\n\nNone4. Apply the Function with a Loop\n\n\n\n\n\nUse a for loop to iterate through the Name column of the companies DataFrame. For each value, call your is_incorporated() function and print the company name along with whether it’s incorporated.\nYour output might look like:\nAcme Inc. → True  \nGlobal Tech → False  \nBright Inc. → True\n\n\n\n\n\n\n\n\n\nNone5. Apply the Function with .apply()\n\n\n\n\n\nNow rewrite your logic using the .apply() method instead of a for loop.\n\nApply is_incorporated() to the Name column\nStore the result in a new column called \"is_incorporated\"\nPrint the updated DataFrame to verify the new column",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "19-intro-ml-ai.html",
    "href": "19-intro-ml-ai.html",
    "title": "19  Introduction to Machine Learning and Artificial Intelligence",
    "section": "",
    "text": "19.1 Learning Objectives\nThroughout this course, you’ve been building a solid foundation in Python and data science fundamentals. You’ve learned to import datasets, clean and manipulate data with pandas, create compelling visualizations, and write functions to automate your work. Much of what you’ve accomplished so far falls under exploratory data analysis — the essential practice of understanding your data through summary statistics, visualizations, and descriptive insights.\nBut data mining encompasses more than just exploration. It’s also about uncovering hidden patterns in your data that can help you make predictions, discover relationships, and automate decision-making. This is where machine learning enters the picture. While you’ve been using code to describe what happened in your data, machine learning allows you to predict what might happen next or discover patterns you never would have found manually.\nThis chapter introduces you to the fundamental concepts of artificial intelligence, machine learning, and how they fit into the broader data mining workflow you’ve been developing. In the chapters that follow, you’ll start applying these concepts hands-on, using ML techniques to uncover patterns in real datasets and make data-driven predictions. The Python skills you’ve built will serve as the perfect foundation for this next phase of your data science journey.\nBy the end of this chapter, you should be able to:",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to Machine Learning and Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "19-intro-ml-ai.html#learning-objectives",
    "href": "19-intro-ml-ai.html#learning-objectives",
    "title": "19  Introduction to Machine Learning and Artificial Intelligence",
    "section": "",
    "text": "Define Artificial Intelligence (AI) and Machine Learning (ML) and explain how they relate to data mining\nDistinguish between supervised learning and unsupervised learning approaches\nIdentify specialized ML/AI approaches including reinforcement learning and generative AI (GenAI)\nRecognize real-world ML applications in recommendation systems, fraud detection, and personalization\nMatch specific business problems to appropriate ML approaches",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to Machine Learning and Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "19-intro-ml-ai.html#what-is-ai-what-is-ml-where-does-data-mining-fit",
    "href": "19-intro-ml-ai.html#what-is-ai-what-is-ml-where-does-data-mining-fit",
    "title": "19  Introduction to Machine Learning and Artificial Intelligence",
    "section": "19.2 What is AI? What is ML? Where Does Data Mining Fit?",
    "text": "19.2 What is AI? What is ML? Where Does Data Mining Fit?\nWhen you hear terms like AI and machine learning in the news, they often sound like science fiction. In reality, these concepts are built on straightforward ideas about using data to make better decisions.\n\nArtificial Intelligence (AI) refers to the broad field of building computer systems that can mimic or approximate human-like intelligence. This might mean reasoning, problem solving, or adapting to new information.\nMachine Learning (ML) is a subset of AI focused on algorithms that learn patterns from data. Instead of being explicitly programmed with step-by-step rules, ML models improve their performance as they are exposed to more examples.\nData Mining is the process of discovering useful insights and patterns in data. ML and AI techniques are often used as advanced tools within data mining projects.\n\n\n\n\n\n\nThink of it this way:\n\nData mining is like digging into data to uncover hidden gems.\nML provides the machinery — the drills and excavators — to dig deeper and automate predictions.\nAI is the broader ambition: building systems that act intelligently using those insights.\n\n\n\n\n\n\n\nNoteA Soft Intro to Machine Learning\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneClassify AI, ML, and Data Mining:\n\n\n\nLet’s think about some data science techniques you’ve learned in this course so far and some that you will learn about in future weeks. Consider the following scenarios and classify each as primarily data mining, machine learning, or artificial intelligence:\n\nUsing Python to calculate the average price of houses in a dataset\nBuilding a system that automatically recommends movies to users based on their viewing history\nCreating a chatbot that can answer customer service questions in natural language\nUsing pandas to find patterns in customer purchase data\n\nWrite your answers and reasoning. Consider: What makes each scenario fit into its category? How do the definitions we discussed help you classify them?",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to Machine Learning and Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "19-intro-ml-ai.html#types-of-machine-learning",
    "href": "19-intro-ml-ai.html#types-of-machine-learning",
    "title": "19  Introduction to Machine Learning and Artificial Intelligence",
    "section": "19.3 Types of Machine Learning",
    "text": "19.3 Types of Machine Learning\nAlthough the ML field includes many different approaches, most methods fall into two main categories: supervised learning and unsupervised learning.\n\nSupervised Learning\nIn supervised learning, the model is trained on data where both the inputs (features) and the outputs (labels) are known. The goal is to learn a mapping from inputs → outputs that generalizes to new, unseen data.\nThink of supervised learning like learning with a teacher who provides both the questions and the correct answers. The algorithm studies these examples to understand the relationship between the input features and the desired output. Once trained, it can make predictions on new data where only the inputs are provided.\nWhy is it called “supervised”? Because we supervise the learning process by providing the correct answers (labels) during training. The algorithm learns by comparing its predictions to these known correct answers and adjusting accordingly.\n\n\n\n\n\nflowchart BT\n    subgraph Prediction [\"Making Predictions\"]\n        D[New Inputs&lt;br/&gt;1900 sq_ft, 3 bedrooms] --&gt; E[Trained Model]\n        E --&gt; F[Predictions&lt;br/&gt;$250,000]\n    end\n    subgraph Training [\"Model Training\"]\n        A[Inputs&lt;br/&gt;sq_feet, bedrooms] --&gt; C[Trained Model]\n        B[Outputs&lt;br/&gt;sale_price] --&gt; C\n    end\n    \n    style A fill:#e8f5e8\n    style B fill:#ffe6cc\n    style C fill:#fff2cc\n    style E fill:#fff2cc\n    style D fill:#e8f5e8\n    style F fill:#ffe6cc\n\n\n Supervised learning workflow: First, historical data with known inputs and outputs trains the model. Then, the trained model makes predictions on new input data. \n\n\n\nSupervised learning problems fall into two main categories based on what type of output we’re trying to predict:\n\n\n\nRegression vs Classification\n\n\n\nRegression Problems\nRegression predicts continuous numerical values — numbers that can take any value within a range.\nHow it works: The algorithm finds patterns between features (sq_feet, bedrooms) and the continuous output (sale_price). For example, it might learn that each additional square foot adds about $120 to the price, and each bedroom adds $15,000.\n\n\nRegression example - predicting house prices:\n   feature 1: sq_feet  feature 2: bedrooms  output: sale_price\n0                1200                    2              150000\n1                1500                    3              200000\n2                1800                    3              240000\n3                2100                    4              280000\n4                2400                    4              320000\n\n\nReal-world prediction: When a new home comes on the market with 1,900 sq_feet and 3 bedrooms, Zillow can use this learned pattern to predict an expected sale price of approximately $250,000.\n\n\nClassification Problems\nClassification predicts discrete categories or classes — specific labels from a predefined set.\nHow it works: The algorithm learns patterns that distinguish between categories. It might discover that emails with more than 5 links and more than 3 exclamation marks are usually spam.\n\n\nClassification example - predicting spam:\n   feature 1: num_links  feature 2: exclamation_marks output: spam_category\n0                     0                             1              Not Spam\n1                     8                            12                  Spam\n2                     2                             0              Not Spam\n3                    15                             8                  Spam\n4                     1                             2              Not Spam\n\n\nReal-world prediction: When a new email arrives with 10 links and 6 exclamation marks, the model can classify it as “Spam” and automatically move it to the spam folder.\n\n\n\nBusiness Applications\nSupervised learning is everywhere in the business world. From the moment you wake up and check your phone (spam filtering), to applying for a loan (credit approval), to watching Netflix recommendations (personalization), supervised learning algorithms are working behind the scenes. Companies across every industry use these techniques because they provide concrete, actionable predictions that directly support business decisions and automate complex processes.\nRegression Applications:\n\nFinance: Banks may use number of credit cards, current limits and balances, along with previous defaults, and job details (years employed, income, etc.) to predict the amount of money an applicant is approved for for a mortgage.\nRetail: E-commerce companies may use customer age, purchase history, browsing behavior, and seasonal trends to predict how much revenue a specific customer will generate over the next 12 months.\nReal Estate: Property platforms like Zillow may use square footage, number of bedrooms/bathrooms, neighborhood characteristics, and recent comparable sales to predict the market value of a home.\nMarketing: Digital marketing teams may use ad spend, audience demographics, campaign type, and historical performance data to predict the return on investment (ROI) for a new advertising campaign.\n\nClassification Applications:\n\nHealthcare: Medical systems may use patient symptoms, test results, medical history, and demographic information to classify whether a patient is likely to have a specific disease or condition.\nBanking: Financial institutions may use transaction amount, time of day, location, merchant type, and spending patterns to classify whether a credit card transaction is fraudulent or legitimate.\nTechnology: Social media platforms may use image pixels, metadata, user reports, and content analysis to classify whether a posted image contains inappropriate content that should be removed.\nManufacturing: Quality control systems may use sensor readings, temperature data, production line speed, and material specifications to classify whether a manufactured product meets quality standards or should be rejected.\n\nThe key insight is that supervised learning models identify mathematical patterns between input features and known outputs, allowing them to make accurate predictions on new, unseen data. This makes them incredibly valuable for business decision-making across virtually every industry.\n\n\nUnsupervised Learning\nIn unsupervised learning, the model is given inputs without labeled outputs. The goal is to discover patterns, structures, or groupings within the data.\nThink of unsupervised learning like exploring a new city without a map or tour guide. You have to discover the neighborhoods, landmarks, and patterns of organization on your own. The algorithm looks at the data and tries to find hidden structures or natural groupings that weren’t obvious before.\nWhy is it called “unsupervised”? Because there’s no teacher providing correct answers. The algorithm must find patterns and relationships in the data without any guidance about what the “right” groups or structures should be.\n\n\n\n\n\nflowchart TB\n    subgraph Data [\"Raw Data (No Labels)\"]\n        A[Feature 1&lt;br/&gt;Annual Spending] --&gt; D\n        B[Feature 2&lt;br/&gt;Visit Frequency] --&gt; D\n        C[Feature 3&lt;br/&gt;Average Purchase] --&gt; D\n    end\n    \n    subgraph Discovery [\"Pattern Discovery\"]\n        D[Trained Model] --&gt; E[Hidden Patterns&lt;br/&gt;Customer Segments]\n        E --&gt; F[\"Non Loyal Customers\"]\n        E --&gt; G[\"Moderatly Loyal Customers\"]\n        E --&gt; H[\"Highly Loyal Customers\"]\n    end\n    \n    style A fill:#fff0f5\n    style B fill:#fff0f5\n    style C fill:#fff0f5\n    style D fill:#f0f8ff\n    style E fill:#f5f5dc\n\n\n Unsupervised learning workflow: The algorithm analyzes unlabeled data to discover hidden patterns, groups, or structures that weren’t previously known. \n\n\n\nA very common type of unsupervised learning is clustering, which focuses on finding natural groups or segments in the data where similar items are grouped together.\nHow it works: The algorithm analyzes features (annual spending, visit frequency, purchase amounts) and identifies customers that behave similarly, grouping them into segments like “Budget Shoppers,” “Premium Customers,” and “Occasional Buyers.”\n\n\nClustering example - discovering customer segments:\n   feat1: annual_spending  feat2: visits_per_month  feat3: avg_purchase\n0                    2500                        2                  125\n1                   15000                        8                  400\n2                    3200                        3                  160\n3                   18000                       10                  450\n4                    2800                        2                  140\n5                   16500                        9                  380\n\n\nReal-world discovery: After analyzing thousands of customers, the algorithm might discover three distinct groups: Budget Shoppers (low spending, infrequent visits), Premium Customers (high spending, frequent visits), and Casual Browsers (moderate spending, moderate visits).\n\n\nBusiness Applications\nUnsupervised learning is the detective of the business world. When companies have lots of data but don’t know what insights might be hidden within it, unsupervised learning helps them discover unexpected patterns, customer segments, and market opportunities they never knew existed. It’s particularly valuable for exploratory analysis and uncovering new business strategies.\nClustering Applications:\n\nRetail: E-commerce companies may use purchase history, browsing patterns, time spent on site, and product preferences to discover natural customer segments for targeted marketing campaigns.\nMarketing: Digital platforms may use user demographics, content engagement, click patterns, and time spent to discover distinct audience segments for personalized advertising strategies.\nHealthcare: Medical researchers may use patient symptoms, test results, genetic markers, and treatment responses to discover new disease subtypes or patient groups.\nFinance: Investment firms may use trading patterns, risk preferences, portfolio compositions, and market behaviors to discover different investor personality types.\n\nOther Unsupervised Applications:\n\nMarket Research: Companies may use survey responses, purchasing data, and demographic information to discover unrecognized market segments and consumer preferences.\nOperations: Manufacturing companies may use sensor data, production metrics, and quality measurements to discover hidden operational inefficiencies or process improvements.\nTechnology: Social media platforms may use user interactions, content preferences, and network connections to discover communities and recommend new connections.\nSupply Chain: Logistics companies may use delivery patterns, route data, and timing information to discover optimal distribution strategies and warehouse locations.\n\nThe key insight is that unsupervised learning reveals hidden structures and relationships in data that weren’t previously known, enabling businesses to discover new opportunities, understand their customers better, and optimize operations in ways they never considered before.\n\n\n\n\n\n\nNoteSupervised vs. Unsupervised Learning\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneSupervised vs. Unsupervised Learning:\n\n\n\nYou’re working as a data analyst for different companies. For each scenario below, determine whether you would use supervised or unsupervised learning:\n\nEmail Company: You have 10,000 emails labeled as “spam” or “not spam” and want to build a system to automatically classify new emails.\nRetail Store: You have customer purchase data but no existing categories. You want to discover natural groupings of customers to create targeted marketing campaigns.\nInsurance Company: Using historical data of past claims (labeled as “fraudulent” or “legitimate”), you want to predict whether new claims are likely to be fraudulent.\nStreaming Service: You have viewing data for all users but no predefined customer segments. You want to identify different viewing behavior patterns.\n\nFor each scenario, explain your reasoning: What clues in the problem description helped you decide? What would your algorithm be trying to learn?",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to Machine Learning and Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "19-intro-ml-ai.html#specialized-topics",
    "href": "19-intro-ml-ai.html#specialized-topics",
    "title": "19  Introduction to Machine Learning and Artificial Intelligence",
    "section": "19.4 Specialized Topics",
    "text": "19.4 Specialized Topics\nWhile supervised and unsupervised learning are the main pillars, there are other types of learning that are frequently discussed in the ML/AI space. We won’t dive deeply into them in this course, but it’s important to know they exist.\n\n\n\n\n\n\nNoneReinforcement Learning (RL)\n\n\n\n\n\n\nHow it works: An agent learns by interacting with an environment and receiving feedback in the form of rewards or penalties.\nExamples:\n\nTraining a robot to walk.\n\nAlphaGo (the system that beat human champions at the game of Go).\n\n\nWant to Learn More? \n\n\n\n\n\n\n\n\n\n\nNoneGenerative AI (GenAI)\n\n\n\n\n\n\nHow it works: Models are trained on vast amounts of data and then generate new content based on the patterns they’ve learned.\nExamples:\n\nLarge language models (LLMs) like ChatGPT.\n\nImage generators like Stable Diffusion or DALL·E.\n\n\nWhy it matters: GenAI has opened new opportunities for creativity and productivity, from automated report writing to code generation.\nWant to Learn More? \n\n\n\n\n\n\n\n\n\n\nNoneSemi-Supervised Learning\n\n\n\n\n\n\nHow it works: Combines a small amount of labeled data with a large amount of unlabeled data during training. The model learns from both the explicit labels and the patterns in the unlabeled data.\nExamples:\n\nMedical image analysis where only some scans are labeled by doctors.\n\nDocument classification where manually labeling thousands of documents is expensive.\n\n\nWhy it matters: Addresses the common real-world problem where labeling data is costly or time-consuming, but unlabeled data is abundant.\nWant to Learn More? \n\n\n\n\n\n\n\n\n\n\nNoneTransfer Learning\n\n\n\n\n\n\nHow it works: Takes a model trained on one task and adapts it for a related but different task. Instead of starting from scratch, you leverage existing knowledge.\nExamples:\n\nUsing a model trained on general images to identify specific medical conditions.\n\nAdapting a language model trained on English to work with Spanish text.\n\n\nWhy it matters: Dramatically reduces the data and computational resources needed for new applications, making AI more accessible and practical.\nWant to Learn More? \n\n\n\n\n\n\n\n\n\n\nNoneEnsemble Methods\n\n\n\n\n\n\nHow it works: Combines predictions from multiple different models to make a final decision. The idea is that a group of models can be more accurate than any single model.\nExamples:\n\nRandom Forest (combines many decision trees).\n\nNetflix Prize winners used ensembles of hundreds of models.\n\n\nWhy it matters: Often achieves better performance than individual models and is widely used in competitive machine learning and high-stakes applications.\nWant to Learn More? \n\n\n\n\n\n\n\n\n\n\nNoneAnomaly Detection\n\n\n\n\n\n\nHow it works: Identifies unusual patterns or outliers that don’t conform to expected normal behavior. Unlike clustering, it specifically focuses on finding the “weird” cases.\nExamples:\n\nNetwork security systems detecting unusual login patterns.\n\nManufacturing quality control identifying defective products.\n\n\nWhy it matters: Critical for identifying problems, fraud, or rare events that could have significant consequences if missed.\nWant to Learn More?",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to Machine Learning and Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "19-intro-ml-ai.html#summary",
    "href": "19-intro-ml-ai.html#summary",
    "title": "19  Introduction to Machine Learning and Artificial Intelligence",
    "section": "19.5 Summary",
    "text": "19.5 Summary\nThis chapter has taken you from the exploratory data analysis techniques you’ve mastered to the exciting world of machine learning and artificial intelligence. You’ve discovered that ML is not science fiction, but rather a practical extension of the data mining skills you’ve been developing—a way to move from describing what happened in your data to predicting what might happen next or discovering hidden patterns automatically.\nUnderstanding the landscape of machine learning approaches gives you a roadmap for tackling different types of business problems. Supervised learning becomes your tool when you have clear examples of inputs and desired outputs—whether you’re predicting house prices (regression) or classifying emails as spam (classification). Unsupervised learning serves as your detective tool for exploring data when you don’t know what patterns might exist, helping you discover customer segments or identify natural groupings in your data.\nBeyond these foundational approaches, you’ve seen how specialized techniques like reinforcement learning, generative AI, transfer learning, and ensemble methods expand the ML toolkit for specific challenges. From the recommendation systems that suggest your next Netflix show to the fraud detection algorithms protecting your credit card, these techniques power the intelligent systems you interact with daily.\nKey takeaways from this chapter:\n\nAI, ML, and data mining work together: AI is the broad ambition, ML provides the algorithmic pattern-finding machinery, and data mining encompasses the entire process of extracting insights from data\nSupervised learning uses labeled examples to learn input-output mappings, supporting both regression (predicting numbers) and classification (predicting categories) problems\nUnsupervised learning discovers hidden structures in data without pre-defined answers, with clustering being a common approach for finding natural groups\nSpecialized approaches like semi-supervised learning, transfer learning, and ensemble methods address real-world challenges beyond the basic supervised/unsupervised dichotomy\nBusiness applications are everywhere—from personalized recommendations and fraud detection to customer segmentation and automated content generation\n\nWhat’s coming next: In the next chapter, we’ll step back from the excitement of ML techniques to focus on the critical considerations you need to address before building any model. Just as you wouldn’t start cooking without checking if you have the right ingredients, successful ML projects require careful planning around data quality, problem definition, and evaluation strategies. Following that foundation, the upcoming modules will dive deep into fundamental algorithms—starting with simple but powerful techniques and building toward more sophisticated approaches. You’ll learn not just the theory behind these algorithms, but how to implement them in Python and apply them to real business problems using the data science skills you’ve developed throughout this course.",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to Machine Learning and Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "19-intro-ml-ai.html#end-of-chapter-exercise",
    "href": "19-intro-ml-ai.html#end-of-chapter-exercise",
    "title": "19  Introduction to Machine Learning and Artificial Intelligence",
    "section": "19.6 End of Chapter Exercise",
    "text": "19.6 End of Chapter Exercise\nYou work as a data analyst for different organizations. For each business scenario below, determine the most appropriate approach to address the business need. Remember that not every business problem requires machine learning—sometimes simple data analysis techniques you’ve already learned are the best solution.\nFor each scenario: 1. Identify the approach: Is this a regression problem, classification problem, clustering problem, or can it be solved with data visualization/aggregation techniques you’ve already learned? 2. Explain your reasoning: Why is this approach most appropriate? What clues in the problem description helped you decide? 3. Describe the expected output: What would the final result look like?\n\n\n\n\n\n\nNoneScenario A: Regional Sales Performance\n\n\n\n\n\nBusiness Context: You work for a national retail chain with 500+ stores across the country. The executives are preparing for the quarterly board meeting and need to understand sales performance patterns.\nAvailable Data: Daily sales data for each store including: store location (state, city), sales revenue, number of transactions, store size, and demographics of surrounding area.\nBusiness Question: “Which regions are performing best this quarter, and are there any concerning trends we should address immediately?”\n\n\n\n\n\n\n\n\n\nNoneScenario B: Customer Lifetime Value Prediction\n\n\n\n\n\nBusiness Context: You work for a subscription-based software company. The marketing team wants to optimize their customer acquisition spending by focusing on customers who will generate the most revenue over time.\nAvailable Data: Historical customer data including: subscription start date, monthly subscription fees, customer demographics, usage patterns (logins per month, features used), support tickets created, and churn date (if applicable).\nBusiness Question: “For each new customer, predict how much total revenue they will generate over their entire relationship with our company.”\n\n\n\n\n\n\n\n\n\nNoneScenario C: Email Marketing Optimization\n\n\n\n\n\nBusiness Context: You work for an e-commerce company that sends promotional emails to customers. Recent complaints suggest customers are receiving irrelevant promotions, and email engagement rates are declining.\nAvailable Data: Customer transaction history, browsing behavior, email click-through rates, product categories purchased, demographic information, and email preferences.\nBusiness Question: “Determine whether each outgoing promotional email should be sent to a customer or not, based on their likelihood to engage with the specific promotion.”\n\n\n\n\n\n\n\n\n\nNoneScenario D: Market Expansion Analysis\n\n\n\n\n\nBusiness Context: You work for a coffee shop chain considering expansion into new cities. Leadership wants to understand what types of locations and customer bases have made existing stores successful.\nAvailable Data: Store performance data including: daily revenue, customer traffic, location characteristics (foot traffic, nearby businesses, rent costs), customer demographics, and local competition data.\nBusiness Question: “Identify natural groups among our existing successful stores to understand different types of profitable locations and customer bases.”\n\n\n\n\n\n\n\n\n\nNoneScenario E: Executive Dashboard Creation\n\n\n\n\n\nBusiness Context: You work for a manufacturing company. The CEO wants a monthly dashboard to quickly understand company performance across different product lines and regions without having to dig through detailed reports.\nAvailable Data: Manufacturing data including: production volumes, quality metrics, costs, sales by product line, regional performance, customer satisfaction scores, and employee productivity metrics.\nBusiness Question: “Create a visual summary that allows executives to quickly identify the top-performing and underperforming areas of the business each month.”",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to Machine Learning and Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "20-before-we-build.html",
    "href": "20-before-we-build.html",
    "title": "20  Before You Build: Key Considerations",
    "section": "",
    "text": "20.1 Learning Objectives\nThink of machine learning like building a house. You wouldn’t start hammering nails without first checking your foundation, reviewing your blueprints, and ensuring you have the right permits. Similarly, before you write a single line of modeling code, you need to establish a solid foundation: What exactly are you trying to solve? Is your data ready? How will you know if your model is working? And what ethical considerations should guide your decisions?\nThis chapter focuses on the critical thinking and planning that separates successful ML projects from costly failures. You won’t find complex algorithms or extensive coding here—instead, you’ll develop the judgment to ask the right questions, spot potential pitfalls, and set your projects up for success from the very beginning.\nThe goal isn’t to overwhelm you with theory, but to give you the practical wisdom that experienced data scientists use every day. By the end of this chapter, you’ll have a mental checklist that will guide you through the early stages of any machine learning project, helping you avoid common mistakes and build models that actually solve real business problems.\nBy the end of this chapter, you should be able to:",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Before You Build: Key Considerations</span>"
    ]
  },
  {
    "objectID": "20-before-we-build.html#learning-objectives",
    "href": "20-before-we-build.html#learning-objectives",
    "title": "20  Before You Build: Key Considerations",
    "section": "",
    "text": "Frame machine learning problems by defining clear business questions and success criteria\nAssess data readiness including data quality and the importance of proper train/test splits\nIdentify data leakage and understand why using “future” information invalidates model results\nRecognize ethical considerations around fairness, privacy, and interpretability in ML applications\nApply a systematic pre-modeling checklist to real business scenarios\n\n\n\n\n\n\n\nNoteA Foundation for Deeper Learning\n\n\n\nThis chapter’s goal is to get you thinking about these critical concepts and developing the right mindset for successful machine learning projects. Many of the topics we introduce here—from data splitting techniques to evaluation metrics to ethical considerations—will be explored in much greater detail in later chapters where you’ll learn the practical implementation skills and hands-on techniques to apply these concepts in real projects.",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Before You Build: Key Considerations</span>"
    ]
  },
  {
    "objectID": "20-before-we-build.html#framing-the-problem",
    "href": "20-before-we-build.html#framing-the-problem",
    "title": "20  Before You Build: Key Considerations",
    "section": "20.2 Framing the Problem",
    "text": "20.2 Framing the Problem\nThe most critical step in any machine learning project happens before you even touch your data: clearly defining what you’re trying to accomplish. This might sound obvious, but it’s where many projects go astray. Without a clear problem statement and definition of success, you’ll find yourself building technically impressive models that don’t actually solve business problems.\n\nStart with the Business Question\nMachine learning should always begin with a specific, answerable business question. Vague goals like “use AI to improve our business” or “build a model to predict customer behavior” are recipes for failure. Instead, successful ML projects start with questions like:\n\n“Can we predict which customers are likely to cancel their subscription in the next 30 days?”\n“What price should we set for this product to maximize profit while remaining competitive?”\n“Which marketing email should we send to each customer to maximize click-through rates?”\n\nNotice how each of these questions has specific, measurable outcomes. They define exactly what you’re trying to predict, over what time horizon, and for what business purpose.\n\n\nDefine Success Upfront\nBefore building any model, you need to answer: “How will I know if this model is successful?” This isn’t just about technical metrics—it’s about business impact.\nConsider these different perspectives on success:\nTechnical Success: “Our model achieves 85% accuracy on our test set”\nBusiness Success: “Using our model’s predictions, we reduced customer churn by 15% this quarter”\nBoth matter, but business success is what justifies the time and resources invested in your project. You should establish both technical benchmarks (maximum accuracy, minimum error rates) and business benchmarks (cost savings, revenue increase, time saved) before you begin modeling.\n\n\n\n\n\n\nflowchart LR\n  subgraph ML[ML System]\n    direction BT\n    subgraph p0[Overall Performance]\n    end\n    subgraph p1[Model performance metrics]\n    end\n    subgraph p2[Business performance metrics]\n    end\n\n   p1 --&gt; p0\n   p2 --&gt; p0\n  end\n\n\n\n\n\nFigure 20.1: Understanding an ML sytems performance requires understanding model performance metrics and business performance metrics.\n\n\n\n\n\nEnsuring Metric Alignment: The most critical consideration is ensuring your technical and business metrics are aligned and incentivize the same behavior. For example, if your business goal is to minimize customer complaints, optimizing purely for model accuracy might not be the right approach—a model that achieves high accuracy by being overly cautious might miss too many legitimate issues, leading to more complaints. Instead, you might prioritize recall (catching more potential problems) even if it means lower overall accuracy. The key is choosing technical metrics that, when optimized, naturally drive the business outcomes you care about.\nStakeholder Communication: It’s equally important to educate stakeholders about how models are measured technically and why those measurements matter for business outcomes. Business leaders often focus solely on bottom-line results, but understanding technical metrics helps them make informed decisions about model deployment, resource allocation, and acceptable trade-offs. When stakeholders understand that a 95% accurate fraud detection model still flags thousands of legitimate transactions for review, they can better plan operational processes and set realistic expectations for the model’s impact.\n\n\n\n\n\n\nTipModel Performance Metrics\n\n\n\n\n\nThese metrics evaluate the accuracy and reliability of the ML model in making predictions. These metrics include common metrics you may have heard before if you’ve done any ML tasks such as mean squared error, \\(R^2\\), and mean absolute error for regression problems; accuracy, precision, and recall for classification problems; or BLEU, BERTScore, and perplexity for large language models. The choice of metric depends on the type of ML task (e.g., classification, regression) and the consequences of different kinds of errors.\nFurther reading: We’ll discuss model performance metrics more in later weeks but here are some additional readings you can browse now regarding selecting the right metric for evaluating ML models - Part 1, Part 2\n\n\n\n\n\n\n\n\n\nTipBusiness Performance Metrics\n\n\n\n\n\nThese metrics measure the real-world impact of your ML system on business outcomes and organizational goals. Common business metrics for ML projects include:\n\nFinancial Metrics: Revenue increase, cost savings, profit margin improvement, return on investment (ROI), customer lifetime value changes\nOperational Metrics: Process automation percentage, time savings, error reduction, productivity improvements, resource utilization\nCustomer Metrics: Customer satisfaction scores, churn rate reduction, conversion rate improvement, engagement increases, retention rates\nRisk Metrics: Fraud detection rates, compliance improvements, risk exposure reduction, safety incident decreases\nEfficiency Metrics: Decision-making speed, manual review time reduction, processing capacity increases, workflow optimization\n\nThe key is selecting business metrics that directly connect to your organization’s strategic objectives and can be clearly attributed to your ML system’s performance. Remember that business impact often takes time to materialize and may require longer measurement periods than technical metrics.\n\n\n\n\n\nUnderstand the Decision Context\nEvery ML model exists to support human decision-making. Understanding exactly how your predictions will be used helps you frame the problem correctly.\nWhy this matters so much: The consequences of model failures can range from minor inconveniences to life-changing impacts. History is filled with examples of well-intentioned ML systems that caused significant harm because their creators didn’t fully consider the decision context. From biased hiring algorithms that discriminated against qualified candidates to criminal justice risk assessment tools that perpetuated racial inequities, the stakes of getting this wrong can be enormous.\n\n\n\n\n\n\nWarningReal-World Impact\n\n\n\nMachine learning models are increasingly used in high-stakes decisions affecting people’s lives—from loan approvals and job applications to medical diagnoses and criminal sentencing. For a sobering look at what can go wrong when models are deployed without careful consideration of their decision context, see Cathy O’Neil’s “Weapons of Math Destruction” which documents how algorithmic bias can perpetuate and amplify inequality.\n\n\nAsk yourself:\n\nWho will use these predictions? (Marketing team, customer service reps, automated system)\nWhat action will they take? (Send targeted offers, flag for manual review, adjust pricing)\nHow quickly do they need results? (Real-time, daily batch, weekly reports)\nWhat happens if the model is wrong? (Minor inconvenience, financial loss, safety risk)\nWho is affected by these decisions? (Internal teams, customers, broader society)\nWhat are the potential unintended consequences? (Bias amplification, privacy violations, safety risks)\n\nFor example, a model predicting whether a customer will purchase a product has very different requirements than a model predicting whether a medical device will fail. The first might tolerate some false positives if it leads to higher overall sales; the second requires extremely high precision to avoid potential safety hazards. Similarly, a model used for automated loan approvals carries legal and ethical responsibilities that don’t apply to a model suggesting Netflix recommendations.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneProblem Framing Practice:\n\n\n\n\nBackground: You work for TechFlow, a software company with 50,000+ customers. Your customer service team is drowning in support tickets and needs help managing their workload effectively.\nCurrent Situation:\n\nVolume: 2,000+ tickets per day across email, chat, and phone\nTeam: 15 customer service representatives working regular business hours\nResponse Goals: Respond to all tickets within 4 hours, resolve within 24 hours\nCurrent Problems: Missing response deadlines, customers complaining about slow service, team working overtime\n\nTicket Types Include:\n\nPassword resets (25% of tickets) - Usually quick, can be automated\nBilling questions (20% of tickets) - Require access to account details, moderate complexity\nTechnical bugs (30% of tickets) - Range from simple to complex, may need engineering team\nFeature requests (15% of tickets) - Need to be forwarded to product team\nAccount cancellations (10% of tickets) - High priority, need immediate attention to retain customers\n\nAvailable Data:\n\nHistorical ticket data including subject lines, descriptions, categories, resolution times\nCustomer information (subscription type, tenure, previous tickets)\nRepresentative performance data and specializations\n\nBusiness Context:\n\nCost of delay: Each hour of delay costs approximately $50 in customer satisfaction\nStaff constraints: Can’t hire more representatives immediately due to budget/training time\nCustomer retention: Quick resolution of cancellation requests can save 30% of departing customers\n\n\nThe Ask: The customer service manager says: “Use machine learning to help manage our workload—we’re drowning and need any help we can get!”\nYour Task:\n\nRewrite this request as a specific, measurable business question that a machine learning model could address.\nDefine what success would look like from both technical and business perspectives.\nIdentify the decision context: Who would use this model? What actions would they take based on the predictions?\n\nConsider the different approaches you could take and which would provide the most business value!",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Before You Build: Key Considerations</span>"
    ]
  },
  {
    "objectID": "20-before-we-build.html#data-readiness",
    "href": "20-before-we-build.html#data-readiness",
    "title": "20  Before You Build: Key Considerations",
    "section": "20.3 Data Readiness",
    "text": "20.3 Data Readiness\nOnce you’ve clearly defined your problem, the next step is ensuring your data is ready to support that goal. This isn’t just about having “enough” data—it’s about having the right data of sufficient quality, organized in a way that allows for valid model training and testing.\n\nData Quality: Garbage In, Garbage Out\nThe famous computer science principle “garbage in, garbage out” is especially true for machine learning. Even the most sophisticated algorithms can’t overcome fundamentally flawed data. Before you begin modeling, you need to honestly assess whether your data can support your goals.\nCommon data quality issues include:\n\nMissing values: Are there gaps in your data? Are they random, or do they follow patterns that could bias your model?\nInconsistent formats: Do you have dates recorded as “2023-01-15” in some places and “Jan 15, 2023” in others?\nDuplicate records: The same customer or transaction appearing multiple times can skew your results\nOutliers and errors: Unrealistic values like negative ages or sales dates in the future\nInconsistent definitions: What exactly counts as a “customer”? An “active user”? A “conversion”?\n\n\n\n\n\n\n\nTipPartner with Data Owners Early\n\n\n\nIn most organizations, specific teams own the data assets you’ll be using for your analysis—whether it’s the marketing team managing customer data, the finance team handling transaction records, or the engineering team maintaining system logs. It’s extremely important to work with these data owners upfront to fully understand what you’re working with. They can explain what each field represents, how the data is collected, what business rules affect the data, and any nuances or quirks you should be aware of. This partnership can save you from making costly assumptions and help you identify potential data quality issues before they derail your project.\n\n\n\n\nThe Critical Importance of Train/Test Splits\nOne of the most fundamental concepts in machine learning is the train/test split. This is so important that getting it wrong can invalidate your entire project, regardless of how sophisticated your model is.\nWhile later chapters will dive deep into the practical implementation of data splitting techniques and how to incorporate them into your Python workflow, for now it’s crucial to understand that how you prepare and split your data is a major factor in how well your model will perform on unseen data. The choices you make here directly impact whether your model will succeed or fail in real-world deployment.\n\n\n\n\n\n\nHere’s the basic principle: You cannot fairly evaluate a model using the same data you used to train it. Separating your data into training and testing sets is essential for building reliable models, which helps prevent overfitting. Check out the following video for a short, clear, beginner-friendly explanation.\n\n\n\n\nThink of it like studying for an exam. If you practice using the exact same questions that will appear on the test, your practice score will be unrealistically high—it doesn’t reflect how well you’ll perform on new, unseen questions. Similarly, a model’s performance on its training data is almost always overly optimistic.\n\n\nHow Train/Test Splits Work\nThe solution is to split your data into distinct portions:\n\nTraining Set (typically 70-80% of data): Used to build and tune your model\nTest Set (typically 20-30% of data): Used only to evaluate final model performance\nValidation Set (optional, for complex projects): Used for model selection and tuning\n\n\n\n\n\n\n\nflowchart TD\n    A[Complete Dataset] --&gt; B[Training Set&lt;br/&gt;70-80%]\n    A --&gt; C[Test Set&lt;br/&gt;20-30%]\n    \n    B --&gt; D[Train Model]\n    D --&gt; E[Trained Model]\n    \n    C --&gt; F[Evaluate Performance]\n    E --&gt; F\n    F --&gt; G[Unbiased Performance&lt;br/&gt;Estimate]\n    \n    style A fill:#f0f8ff\n    style B fill:#e8f5e8\n    style C fill:#ffe6e6\n    style G fill:#fff2cc\n\n\n\n\nFigure 20.2: Proper data splitting ensures unbiased evaluation by keeping test data completely separate from model training.\n\n\n\n\n\nThe golden rule: Once you set aside your test set, don’t touch it until you’re completely done with model development. The moment you use test data to make decisions about your model, it’s no longer a fair evaluation.\n\n\nRandom vs. Strategic Splitting\nThe central principle: How you split your data is crucial and depends entirely on the type of problem you’re solving. The wrong splitting strategy can lead to overly optimistic results that don’t translate to real-world performance, while the right strategy sets your model up for success from the start. We’ll explore the technical implementation of these strategies in later chapters, but understanding when and why to use different approaches is fundamental.\n\nRandom Splitting: The Default Approach\nHow it works: Random splitting assigns each row in your dataset to either training or testing using pure chance—like flipping a coin for each record. This ensures that your training and test sets are representative samples of your overall data, with similar distributions of features and outcomes.\nWhen it’s appropriate: Random splitting works well when:\n\nYour data represents a single time period or snapshot\nEach row is independent (no customer groupings, time sequences, or hierarchical relationships)\nYou’re not trying to predict future events\nYour outcome variable is reasonably balanced\n\nExample: Predicting whether customers will respond to a marketing email using data from a single campaign where each customer received one email.\n\n\nWhen Random Splitting Fails: Strategic Alternatives\nHowever, many real-world problems require more thoughtful splitting strategies:\n\n\n\n\n\n\nNoneTime-Based Data: Predicting the Future\n\n\n\n\n\nProblem: You’re building a model to predict which customers will churn next month using 2 years of customer data.\nWhy random splitting fails: If you randomly split this data, your model might use information from December 2023 to predict churn that happened in January 2023—essentially using the future to predict the past!\nStrategic solution: Split by time—train on data from January 2022 to December 2022, test on data from January 2023 to December 2023. This mirrors real-world deployment where you use historical data to predict future events.\n\n\n\n\n\n\n\n\n\nNoneGrouped Data: Avoiding Customer Leakage\n\n\n\n\n\nProblem: Predicting transaction fraud using a dataset where each customer has multiple transactions.\nWhy random splitting fails: If Customer A’s transactions appear in both training and test sets, your model might just learn to recognize Customer A rather than learning general fraud patterns.\nStrategic solution: Keep all of each customer’s transactions together—if Customer A is in training, ALL their transactions stay in training.\n\n\n\n\n\n\n\n\n\nNoneImbalanced Outcomes: Rare Events\n\n\n\n\n\nProblem: Detecting equipment failures where only 2% of your data represents actual failures.\nWhy random splitting fails: You might randomly end up with very few (or even zero) failure cases in your test set, making evaluation impossible.\nStrategic solution: Use stratified splitting to ensure both training and test sets maintain the same 2% failure rate as your original dataset.\n\n\n\n\n\n\n\n\n\nNoneHierarchical or Nested Data: Multiple Levels\n\n\n\n\n\nProblem: Predicting student performance using data from multiple schools and classrooms.\nWhy random splitting fails: Students from the same classroom might be more similar to each other than to students from other classrooms, creating hidden dependencies.\nStrategic solution: Split at the school or classroom level rather than at the student level to ensure true independence.\n\n\n\nThe key insight is that your splitting strategy should mirror how your model will be used in the real world. If you’ll use historical data to predict future events, split by time. If you’ll make predictions about new customers, split by customer. The goal is always to create test conditions that simulate actual deployment as closely as possible.\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneData Readiness Assessment:\n\n\n\n\nBusiness Context: You work for RetailMax, an e-commerce company that wants to launch a targeted marketing campaign. The marketing team plans to send personalized discount offers to customers who are likely to make a purchase in the next 30 days, hoping to convert them before they buy from competitors. They’ve allocated a budget for 10,000 targeted emails and want to maximize return on investment by selecting the customers most likely to purchase.\nYour Dataset: 100,000 customer records spanning the past 2 years, including purchase history, browsing behavior, demographic information, and customer service interactions. The marketing team wants to deploy this model next month to identify targets for their campaign.\nYour Task:\n\nIdentify potential data quality issues you should check for before modeling.\nDesign your train/test split strategy: How would you split this data? Why might a random split not be appropriate for this time-sensitive prediction?\nSpot the problem: A colleague tells you they achieved 99% accuracy by including “days_since_last_purchase” as a feature. What might be wrong with this approach?\n\n\nConsider these questions carefully—they represent some of the most common pitfalls in ML projects!",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Before You Build: Key Considerations</span>"
    ]
  },
  {
    "objectID": "20-before-we-build.html#data-leakage-the-silent-model-killer",
    "href": "20-before-we-build.html#data-leakage-the-silent-model-killer",
    "title": "20  Before You Build: Key Considerations",
    "section": "20.4 Data Leakage: The Silent Model Killer",
    "text": "20.4 Data Leakage: The Silent Model Killer\nData leakage is one of the most insidious problems in machine learning. It makes your model look incredibly successful during development, only to fail spectacularly when deployed in the real world. Understanding and preventing data leakage is crucial for building models that actually work.\n\nWhat Is Data Leakage?\nData leakage occurs when information that would not be available at prediction time somehow finds its way into your training data. Essentially, your model is “cheating” by using information from the future or information that contains the answer you’re trying to predict.\nThe tricky part is that leakage often leads to models with impressive performance metrics—99% accuracy, perfect predictions, results that seem too good to be true. And they are too good to be true.\n\n\nTypes of Data Leakage\nData leakage can creep into your models through several different pathways, each with its own characteristics and warning signs. Understanding these distinct types helps you systematically check for and prevent leakage in your own projects.\n\n\n\n\n\n\nNoneTemporal Leakage (Using Future Information)\n\n\n\n\n\nThis happens when you accidentally include information from after the time you’re trying to make predictions.\nExample: You’re building a model to predict which customers will cancel their subscription in January 2024. You accidentally include a feature called “customer_satisfaction_survey_february_2024” in your training data. Your model performs amazingly well—of course it does, because customers who cancelled in January probably gave poor satisfaction ratings in February!\n\n\n\n\n\n\n\n\n\nNoneTarget Leakage (Information That Contains the Answer)\n\n\n\n\n\nThis occurs when you include features that are directly caused by or contain information about the outcome you’re predicting.\nExample: You’re predicting whether someone will default on a loan, and you include a feature called “account_status” which has values like “current,” “late,” and “charged_off.” The “charged_off” status literally means the person defaulted—you’ve accidentally included the answer in your features!\n\n\n\n\n\n\n\n\n\nNoneFeature Leakage (Using Information Not Available at Prediction Time)\n\n\n\n\n\nThis happens when you include features that wouldn’t be available when you actually need to make predictions.\nExample: You’re building a model to approve credit applications instantly online. You include features from a detailed financial audit that takes 30 days to complete. Your model might be accurate, but it’s useless because you can’t wait 30 days to approve applications.\n\n\n\n\n\n\nHow to Spot Data Leakage\nWarning signs that might indicate leakage:\n\nPerformance that’s too good: If your model achieves near-perfect accuracy on a complex real-world problem, be suspicious\nOne feature dominates: If removing a single feature causes model performance to collapse dramatically\nPerfect correlation: If any feature correlates almost perfectly with your target variable\nTemporal inconsistencies: Features that would only be available after the event you’re predicting\n\n\n\nPreventing Data Leakage\nSuccessfully preventing data leakage requires developing a systematic mindset and establishing rigorous practices throughout your model development process. The most effective approach is to think like your model will actually be deployed—constantly asking yourself whether each piece of information would realistically be available at the moment you need to make a prediction. This simple question can prevent most leakage issues before they occur.\nUnderstanding your data timeline is equally crucial. Before you begin feature engineering, create a clear timeline showing when each piece of information becomes available in your business process. Your model can only use information that exists before the prediction moment, so mapping out these temporal relationships helps you identify potential temporal leakage early. For instance, if you’re predicting monthly churn, you should only use data that would be available at the beginning of that month, not data that accumulates during the month itself.\nBe especially suspicious of features that seem too good to be true. If a feature appears to be a perfect predictor—correlating almost perfectly with your target variable—investigate thoroughly. Often, these “perfect” predictors are actually just different ways of measuring the outcome you’re trying to predict, or they contain information that wouldn’t be available at prediction time. Features that dramatically improve your model’s performance deserve extra scrutiny rather than celebration.\nFinally, implement time-aware validation strategies for any problem involving temporal data. Instead of randomly splitting your data, use time-based validation where you train on earlier data and test on later data. This approach mirrors real-world deployment conditions and helps catch temporal leakage that might slip through random validation splits.\n\n\nA Real-World Example: Hospital Readmission Prediction\nConsider a common healthcare analytics challenge: building a model to predict which patients will be readmitted to the hospital within 30 days of discharge. This type of model is valuable for hospitals trying to improve patient care and reduce costs, but it’s also prone to subtle leakage issues that can make the model appear more accurate than it actually is.\nImagine you’re working with a feature called “length_of_initial_stay” measured in days. At first glance, this seems like a perfectly legitimate feature—longer hospital stays might indicate more severe conditions that increase readmission risk. However, the way you calculate this feature makes all the difference between a valid model and a leaky one.\nThe leaky approach might seem logical: calculate length_of_stay as the total number of days from initial admission to final discharge. If a patient is readmitted within your 30-day prediction window, this calculation would include the readmission dates, artificially inflating the stay length for patients who end up being readmitted. The model might then show a strong correlation between longer stays and readmission risk—but this is circular logic since the readmission itself is contributing to the “longer stay” measurement.\nThe correct approach calculates length_of_stay only from admission to the initial discharge date, completely ignoring any subsequent readmissions. This ensures you’re only using information that would have been available at the moment of initial discharge when you would actually need to make your prediction. While this version might show weaker correlations, it represents genuine predictive relationships that will hold up in real-world deployment.\nThis example illustrates how subtle definitional choices can introduce leakage that dramatically inflates apparent model performance while rendering the model useless for its intended purpose. The leaky version might achieve impressive accuracy in testing but would fail completely when deployed because the “future information” it relies on wouldn’t be available for new patients.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneSpot the Leakage:\n\n\n\nFor each scenario below, identify whether data leakage is present and explain why:\n\nEmail Spam Detection: You’re predicting whether emails are spam. One of your features is “email_moved_to_spam_folder” (yes/no). The model achieves 95% accuracy.\nCustomer Churn Prediction: You’re predicting which customers will cancel next month. Your features include last month’s purchase amount, customer age, and account creation date.\nStock Price Prediction: You’re predicting tomorrow’s stock price using today’s opening price, trading volume, and tomorrow’s closing volume.\nMedical Diagnosis: You’re predicting disease presence using patient symptoms, lab results from the day of diagnosis, and treatment prescribed by the doctor.\n\nWhich scenarios contain leakage? What makes them problematic?",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Before You Build: Key Considerations</span>"
    ]
  },
  {
    "objectID": "20-before-we-build.html#fairness-privacy-and-interpretability-the-human-impact",
    "href": "20-before-we-build.html#fairness-privacy-and-interpretability-the-human-impact",
    "title": "20  Before You Build: Key Considerations",
    "section": "20.5 Fairness, Privacy, and Interpretability: The Human Impact",
    "text": "20.5 Fairness, Privacy, and Interpretability: The Human Impact\nMachine learning models don’t exist in a vacuum—they make decisions that affect real people’s lives. Whether it’s determining who gets a loan, which job candidates get interviews, or what content people see on social media, your models have ethical implications that go far beyond technical performance metrics.\n\nFairness: Avoiding Discriminatory Outcomes\nAlgorithmic fairness means ensuring your model doesn’t systematically discriminate against protected groups based on characteristics like race, gender, age, or other sensitive attributes. The challenge is that bias can creep into models in subtle ways, even when you’re trying to be fair, often through the very data we use to train them.\nBias typically enters through several pathways: historical bias reflects past discrimination embedded in your training data (like historical hiring records that favor certain groups), representation bias occurs when some groups are underrepresented in your dataset, measurement bias stems from biased ways of measuring outcomes (such as credit scores that reflect historical lending discrimination), and proxy variables that seem neutral but correlate with protected characteristics (like zip codes that correlate with race).\nConsider a hiring model trained on historical data that learns certain universities predict success. If those universities had discriminatory admission practices, the model perpetuates that bias into future hiring decisions. Promoting fairness requires proactive steps: auditing your data to understand demographics and historical outcomes, testing for disparate impact across different groups, considering algorithms that can optimize for both accuracy and fairness simultaneously, and maintaining human oversight for high-stakes decisions, especially edge cases.\n\n\nPrivacy: Protecting Sensitive Information\nMachine learning models can inadvertently reveal sensitive information about individuals in your training data or make predictions that expose private details. Privacy concerns span multiple dimensions: data collection practices may gather more personal information than necessary, model inversion attacks can use outputs to infer sensitive information about training individuals, re-identification can occur when “anonymized” data is combined with other sources, and prediction privacy issues arise when models reveal sensitive information through their outputs (like predicting health conditions from seemingly unrelated data).\nProtecting privacy requires systematic practices: data minimization ensures you only collect and use necessary information, proper anonymization removes or encrypts personally identifiable information, differential privacy adds carefully calibrated noise to protect individual privacy while preserving statistical patterns, and secure storage ensures data and models are properly protected and accessed only by authorized personnel.\n\n\nInterpretability: Understanding Model Decisions\nAs models become more complex, understanding why they make specific decisions becomes increasingly difficult. This “black box” problem is particularly concerning for high-stakes applications where decisions significantly impact people’s lives. Interpretability matters for multiple reasons: it enables debugging by helping you understand and fix model mistakes, builds trust among users and stakeholders who need to understand decisions, ensures compliance with regulations that require explainable decisions (like certain credit approval requirements), and supports fairness efforts since you can’t fix bias without understanding how decisions are made.\nInterpretability operates at different levels. Global interpretability helps you understand how the model works overall (such as knowing a credit model primarily relies on credit score and income), while local interpretability explains specific decisions (like understanding a particular loan was denied primarily due to high debt-to-income ratio). Common approaches include using inherently interpretable models like linear regression and decision trees, analyzing feature importance to understand which variables matter most, applying techniques like SHAP values to explain individual predictions, and providing counterfactual explanations that show how different inputs would change outcomes.\n\n\nBalancing Competing Concerns\nYou’ll often face trade-offs between accuracy, fairness, privacy, and interpretability, and there’s no single “right” answer. The key is making these trade-offs consciously and transparently, guided by your specific application context and stakeholder needs. Critical questions to consider include: whether a small decrease in accuracy is justified by significant fairness improvements, how much model complexity is warranted by performance gains, what level of privacy protection is appropriate for your application, and who needs to understand model decisions and at what level of detail. These decisions should align with your organization’s values, regulatory requirements, and the real-world impact of your model’s deployment.\n\n\n\n\n\n\nNote🎥 Video Spotlight: Responsible AI – Model Interpretability and Fairness\n\n\n\nTake a few minutes to watch this video on Responsible AI from Microsoft Research. It provides a practical perspective on how data scientists and engineers are tackling issues of fairness, privacy, and interpretability in real-world systems.\nAs you watch, think about the following:\n\nHow do the concepts of fairness and transparency show up in the examples discussed?\nWhat trade-offs between accuracy, interpretability, and ethics are highlighted?\nHow might these lessons apply to your own future data science projects?\n\n\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneEthical Considerations in Practice:\n\n\n\nConsider a model being developed to screen job applications for a tech company.\n\nIdentify potential fairness issues: What sources of bias might affect this model? What groups might be unfairly disadvantaged?\nPrivacy concerns: What personal information might this model expose? How could the company protect applicant privacy?\nInterpretability requirements: Who would need to understand this model’s decisions? What level of explanation would be appropriate?\nDesign better practices: How would you modify the model development process to address these concerns?\n\nThink about how you would balance model performance with ethical considerations in this scenario.",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Before You Build: Key Considerations</span>"
    ]
  },
  {
    "objectID": "20-before-we-build.html#summary",
    "href": "20-before-we-build.html#summary",
    "title": "20  Before You Build: Key Considerations",
    "section": "20.6 Summary",
    "text": "20.6 Summary\nBuilding successful machine learning models requires much more than selecting the right algorithm and achieving high accuracy scores. The foundation of any ML project lies in careful planning, thoughtful problem framing, and systematic consideration of potential pitfalls before you write your first line of modeling code.\nThroughout this chapter, you’ve learned that effective machine learning begins with clearly defining your business problem and establishing concrete success criteria. You’ve seen how data quality and proper train/test splits form the bedrock of reliable model evaluation, and how data leakage can make models appear deceptively successful while actually being worthless in practice. Finally, you’ve considered the human impact of machine learning through the lenses of fairness, privacy, and interpretability—recognizing that technical excellence alone isn’t sufficient when models affect people’s lives.\nThe key insight is that good machine learning isn’t just about algorithms, it’s about problem framing, data discipline, appropriate evaluation, and responsible use. The time you invest in these foundational considerations will pay dividends throughout your project, helping you avoid costly mistakes and build models that actually solve real business problems.\n\n\n\n\n\n\nTipYour pre-modeling checklist should include:\n\n\n\n\nClear problem definition: Specific, measurable business questions with defined success criteria\nData quality assessment: Understanding your data’s limitations, biases, and gaps\nProper validation strategy: Time-aware train/test splits that reflect real-world deployment\nLeakage prevention: Ensuring only information available at prediction time is used for training\nAppropriate metrics: Evaluation criteria aligned with business objectives and model type\nEthical considerations: Proactive assessment of fairness, privacy, and interpretability requirements\n\n\n\nAs you move forward in your machine learning journey, remember that the most sophisticated algorithms in the world can’t compensate for poor foundational planning. The habits and mindset you develop now—asking the right questions, being suspicious of results that seem too good to be true, and always considering the human impact of your models—will serve you well regardless of which specific techniques you eventually master.",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Before You Build: Key Considerations</span>"
    ]
  },
  {
    "objectID": "20-before-we-build.html#end-of-chapter-exercise-ml-project-pitfall-analysis",
    "href": "20-before-we-build.html#end-of-chapter-exercise-ml-project-pitfall-analysis",
    "title": "20  Before You Build: Key Considerations",
    "section": "20.7 End of Chapter Exercise: ML Project Pitfall Analysis",
    "text": "20.7 End of Chapter Exercise: ML Project Pitfall Analysis\nYou work as a consultant helping companies identify potential problems in their machine learning projects before they invest significant resources. For each scenario below, identify the key issues that could derail the project and suggest what the team should address before building their models.\n\n\n\n\n\n\nNoneScenario 1: Predicting Customer Lifetime Value\n\n\n\n\n\nCompany: A subscription streaming service\nGoal: “Build a model to predict how much revenue each new customer will generate over their entire relationship with our company”\nProposed Approach: The data science team plans to use all available customer data, including viewing history, payment information, and customer service interactions. They want to achieve 90% accuracy to justify the project to executives.\nTimeline: “We need this model deployed in 2 weeks for the next marketing campaign”\nYour Analysis:\n\nProblem Framing Issues: What’s unclear or problematic about how they’ve defined their goal?\nData and Methodology Concerns: What potential issues do you see with their proposed approach?\nTimeline and Expectations: What’s unrealistic about their timeline and success metrics?\nRecommendations: What should they clarify or change before proceeding?\n\n\n\n\n\n\n\n\n\n\nNoneScenario 2: Loan Approval Automation\n\n\n\n\n\nCompany: A regional bank\nGoal: Automate loan approval decisions to reduce processing time and costs\nCurrent Approach: The team has 10 years of historical loan data including applicant demographics, credit scores, employment history, and loan outcomes. They plan to train a model that achieves 95% accuracy, then deploy it to make instant approval decisions.\nSpecial Note: “We included a feature called ‘loan_officer_final_decision’ because it correlates perfectly with whether loans were approved—this will make our model really accurate!”\nYour Analysis:\n\nData Leakage: What’s the obvious leakage problem in this scenario?\nFairness Concerns: What bias issues might this model have?\nEvaluation Strategy: How should they properly evaluate this model?\nEthical Considerations: What additional considerations should guide this project?\n\n\n\n\n\n\n\n\n\n\nNoneScenario 3: Medical Diagnosis Support\n\n\n\n\n\nCompany: A healthcare technology startup\nGoal: Build a model to help doctors diagnose skin conditions from photographs\nData: 50,000 images labeled by dermatologists, with 95% showing healthy skin and 5% showing various conditions\nApproach: Random train/test split with 80/20 division. They’re excited because their model achieves 95% accuracy!\nDeployment Plan: “The model will provide diagnostic suggestions directly to patients through our app”\nYour Analysis:\n\nData Issues: What problems do you see with their dataset composition?\nEvaluation Problems: Why might 95% accuracy be misleading here?\nDeployment Concerns: What’s risky about their deployment plan?\nRegulatory and Safety Issues: What additional considerations apply to medical applications?\n\n\n\n\n\n\n\n\n\n\nNoneScenario 4: Social Media Content Moderation\n\n\n\n\n\nCompany: A social media platform\nGoal: Automatically detect and remove hate speech from user posts\nData: 1 million posts from the last 6 months, labeled by content moderators\nApproach: The team wants to optimize for maximum recall (“we want to catch all hate speech”) and plans to use all available data including user profiles, posting history, and network connections.\nSuccess Metric: “If we can catch 99% of hate speech, we’ll have solved the problem”\nYour Analysis:\n\nProblem Framing: What’s oversimplified about their success metric?\nPrivacy Concerns: What privacy issues arise from their data usage?\nFairness Issues: How might this model create unfair outcomes?\nMetric Selection: What trade-offs are they ignoring by focusing only on recall?\n\n\n\n\n\n\n\n\n\n\nNoneScenario 5: Predictive Maintenance for Manufacturing\n\n\n\n\n\nCompany: A manufacturing plant\nGoal: Predict equipment failures to schedule maintenance before breakdowns occur\nData: 2 years of sensor readings (temperature, vibration, pressure) and maintenance records\nApproach: They plan to include a feature called “maintenance_scheduled_next_week” because it seems predictive of failures. Their validation shows perfect predictions!\nBusiness Case: “This will save millions by preventing unexpected downtime”\nYour Analysis:\n\nLeakage Detection: What’s the leakage issue here?\nTemporal Considerations: How should they handle the time-series nature of this data?\nCost-Benefit Analysis: What additional factors should they consider beyond prediction accuracy?\nPractical Deployment: What challenges might they face when actually using this model?\n\n\n\n\n\nReflection Questions\nAfter analyzing these scenarios, consider:\n\nCommon Patterns: What types of mistakes appear across multiple scenarios?\nDetection Skills: How can you develop the ability to spot these issues early in your own projects?\nPrevention Strategies: What processes or checklists could help teams avoid these pitfalls?\nStakeholder Communication: How would you explain these technical issues to non-technical business leaders?\n\nThe goal of this exercise isn’t to memorize specific problems, but to develop the critical thinking skills and systematic approach that will help you identify and address issues before they derail your machine learning projects.",
    "crumbs": [
      "Module 7",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Before You Build: Key Considerations</span>"
    ]
  },
  {
    "objectID": "21-correlation-regression.html",
    "href": "21-correlation-regression.html",
    "title": "21  Correlation and Linear Regression Foundations",
    "section": "",
    "text": "21.1 Correlation: Measuring the Strength of a Relationship\nIn business, we rarely care about a single number in isolation. Instead, leaders ask questions like:\nAnswering these questions requires analyzing relationships between variables. This chapter introduces two fundamental tools for that: correlation (a descriptive statistic) and linear regression (a predictive model).\nBy the end of this chapter, you will be able to:\nCorrelation measures how strongly two variables move together in a straight-line (linear) relationship.",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation and Linear Regression Foundations</span>"
    ]
  },
  {
    "objectID": "21-correlation-regression.html#correlation-measuring-the-strength-of-a-relationship",
    "href": "21-correlation-regression.html#correlation-measuring-the-strength-of-a-relationship",
    "title": "21  Correlation and Linear Regression Foundations",
    "section": "",
    "text": "Let’s See This in Action\nImagine you are working for a regional grocery chain. The marketing director has been steadily increasing the advertising budget over the last couple years and wants to know if these additional dollars are truly paying off. Do higher ad spends actually translate into higher weekly sales? This is a common business scenario where correlation can help us quickly explore whether a relationship exists between two key variables.\nBut before you even compute the correlation you would probably visualize the relationship as below.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Example dataset: advertising spend vs. weekly sales\ndata = pd.DataFrame({\n    \"ad_spend\": [400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300],\n    \"weekly_sales\": [4200, 4400, 4100, 4800, 5600, 5200, 4900, 5500, 5300, 5900, 5700, 6300, 6900, 6200, 5800, 6600, 7100, 6800, 7300, 7800]\n})\n\n# First look at the relationship visually\nplt.scatter(data[\"ad_spend\"], data[\"weekly_sales\"])\nplt.xlabel(\"Advertising Spend\")\nplt.ylabel(\"Weekly Sales\")\nplt.title(\"Ad Spend vs. Weekly Sales\")\nplt.show()\n\n\n\n\n\n\n\n\nIt appears from the scatterplot that there is some relationship between advertising spend and weekly sales. However, the natural next question is: how do we measure this relationship? Is there a way to quantify it? This is where correlation comes in. Correlation provides a single number to summarize the strength and direction of the association.\n\n# Now compute the correlation to quantify the relationship\ndata.corr()\n\n\n\n\n\n\n\n\nad_spend\nweekly_sales\n\n\n\n\nad_spend\n1.000000\n0.941372\n\n\nweekly_sales\n0.941372\n1.000000\n\n\n\n\n\n\n\nThe correlation table shows us that advertising spend and weekly sales have a correlation of approximately 0.94, which indicates a strong positive linear relationship. This means that as advertising spending increases, weekly sales tend to increase as well. The diagonal values of 1.0 in the table above simply show that each variable is perfectly correlated with itself, which is always true.\n\n\nWhat Does the Correlation Value Mean?\nCorrelation (often denoted \\(\\rho\\) or \\(r\\) ) ranges from −1 to +1:\n\n+1: perfect positive linear relationship\n\n0: no linear relationship\n\n−1: perfect negative linear relationship\n\nTo build intuition, let’s simulate five datasets and visualize what different correlation strengths look like: strong positive (~0.8), weak positive (~0.3), no linear relationship (~0), weak negative (~−0.3), and strong negative (~−0.8). The dotted diagonal line in each plot shows what perfect positive correlation would look like.\n\n\nShow code for correlation visualization\nimport numpy as np\n\ndef simulate_pair(rho, n=250, seed=123):\n    rng = np.random.default_rng(seed + int((rho + 1) * 1000))\n    cov = np.array([[1.0, rho],[rho, 1.0]])\n    xy = rng.multivariate_normal([0, 0], cov, size=n)\n    df = pd.DataFrame({\"x\": xy[:,0], \"y\": xy[:,1]})\n    df[\"rho_sample\"] = df[\"x\"].corr(df[\"y\"])\n    return df\n\nrhos = [0.8, 0.3, 0.0, -0.3, -0.8]\ndfs = [simulate_pair(r) for r in rhos]\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 8), constrained_layout=True)\naxes = axes.flatten()  # Convert 2D array to 1D for easier indexing\n\nfor i, (df, target_r) in enumerate(zip(dfs, rhos)):\n    ax = axes[i]\n    ax.scatter(df[\"x\"], df[\"y\"], alpha=0.7)\n    \n    # Add diagonal line showing perfect correlation\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    line_range = [max(xlim[0], ylim[0]), min(xlim[1], ylim[1])]\n    ax.plot(line_range, line_range, 'k--', alpha=0.5, linewidth=1, label='Perfect correlation')\n    \n    ax.set_title(f\"Target ρ ≈ {target_r}\\nSample ρ = {df['rho_sample'][0].round(2)}\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n\n# Hide the empty subplot\naxes[5].set_visible(False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipPractice Guessing Correlations\n\n\n\nSharpen your intuition at Guess the Correlation.\n\n\n\n\nCorrelation in Business: Association ≠ Causation\nCorrelation is descriptive—it tells you variables move together, but not why. Treating correlation as causation can lead to costly mistakes. For example, ice cream sales and drowning deaths are positively correlated, but ice cream doesn’t cause drownings—both increase during hot summer weather. Similarly, finding that stores with more staff have higher sales doesn’t mean hiring more staff will automatically increase sales; successful stores might simply need more employees to handle existing demand. When making business decisions, always ask: could there be a third variable explaining both trends? Is the relationship truly causal, or just coincidental? Strong correlation is a starting point for investigation, not a conclusion for action.\n\n\n\n\n\n\nNote🎥 Video Spotlight: Pearson’s Correlation – Clearly Explained!\n\n\n\nIn this video, StatQuest with Josh Starmer provides an excellent introduction to correlation—what it measures, how to interpret it, and why it’s such an important concept in data analysis. The video uses clear visuals and intuitive examples to show how two variables can move together in positive, negative, or no linear relationship.\nAs you watch, think about the following:\n\nHow does the correlation coefficient quantify the strength and direction of a relationship?\nWhy does correlation not necessarily imply causation?\nHow might correlation analysis help you make better data-driven business decisions?\n\n\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneExplore the Advertising Data\n\n\n\nThe Advertising dataset is a classic dataset that represents a fictional company’s advertising expenditures and corresponding sales data. Variables in this data set include:\n\nTV (continuous): Advertising budget spent on TV advertising (in thousands of dollars)\nradio (continuous): Advertising budget spent on radio advertising (in thousands of dollars)\nnewspaper (continuous): Advertising budget spent on newspaper advertising (in thousands of dollars)\nsales (continuous): Sales in thousands of units for the product in each market\n\n\n\n\n\n\n\nData Access: You can download the Advertising.csv dataset from the course GitHub repository here.\n\n\n\n\nadvertising = pd.read_csv(\"../data/Advertising.csv\")\nadvertising.head()\n\n\n\n\n\n\n\n\nTV\nradio\nnewspaper\nsales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n9.3\n\n\n3\n151.5\n41.3\n58.5\n18.5\n\n\n4\n180.8\n10.8\n58.4\n12.9\n\n\n\n\n\n\n\nTasks:\n\nCreate scatterplots between Sales and each channel.\n\nGuess which predictor is strongest.\n\nCompute correlations.\n\nWhich channels appear to have the strongest relationship to sales?",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation and Linear Regression Foundations</span>"
    ]
  },
  {
    "objectID": "21-correlation-regression.html#from-correlation-to-regression",
    "href": "21-correlation-regression.html#from-correlation-to-regression",
    "title": "21  Correlation and Linear Regression Foundations",
    "section": "21.2 From Correlation to Regression",
    "text": "21.2 From Correlation to Regression\nCorrelation tells us two variables move together. Regression provides an equation to predict one variable from another. While correlation simply measures the strength of association, linear regression goes further by finding the “best-fitting line” through your data points and expressing this relationship as a mathematical equation. This equation allows you to make concrete predictions: given a specific value for your predictor variable (like advertising spend), you can estimate the expected value of your outcome variable (like weekly sales).\n\n\n\n\n\n\nNoneVisualizing the Relationship Before Modeling\n\n\n\nGo back to the data we created earlier and plot ad_spend vs weekly_sales again. Imagine drawing a straight line to represent the relationship. How steep would it be? Would all points fall close to it?\n\n\nIf you completed the visualization activity above, you probably imagined a line that looks something like this:\n\n\nShow code for regression line visualization\n# Create scatter plot with best-fit line\nplt.figure(figsize=(10, 6))\nplt.scatter(data[\"ad_spend\"], data[\"weekly_sales\"], alpha=0.7, color='blue', label='Data points')\n\n# Calculate and plot the best-fit line\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Fit a simple linear regression\nX_simple = data[\"ad_spend\"].values.reshape(-1, 1)\ny_simple = data[\"weekly_sales\"].values\nreg = LinearRegression().fit(X_simple, y_simple)\n\n# Create line points for plotting\nline_x = np.linspace(data[\"ad_spend\"].min(), data[\"ad_spend\"].max(), 100)\nline_y = reg.predict(line_x.reshape(-1, 1))\n\nplt.plot(line_x, line_y, color='red', linewidth=2, label='Best-fit line')\nplt.xlabel(\"Advertising Spend ($)\")\nplt.ylabel(\"Weekly Sales\")\nplt.title(\"Linear Relationship: Ad Spend vs Weekly Sales\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\nThis visualization captures the essence of linear regression: finding the “best-fit line” that best represents the directional relationship between two variables. While you can imagine many possible lines through these points, regression uses mathematical techniques to determine the single line that minimizes the overall distance between the line and all data points. This optimal line becomes our predictive model, allowing us to estimate weekly sales for any given advertising spend amount.\nIn this chapter, we’ll use scikit-learn’s LinearRegression for building our models. Scikit-learn focuses on prediction and provides clean, easy-to-interpret output that emphasizes the practical business insights we can gain from regression coefficients and predictions.\n\nfrom sklearn.linear_model import LinearRegression\n\n# Prepare the data\nX = data[[\"ad_spend\"]]  # Feature matrix\ny = data[\"weekly_sales\"]  # Target variable\n\n# Fit the model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nWe use the .fit() method to train our regression model, where X represents our features (predictors) - the variables we use to make predictions - and y represents our target variable - the outcome we’re trying to predict.\nOnce our model is fitted, there are several outputs we can extract to understand our regression results. For now, we’ll focus on the two most important ones: the intercept and the coefficient. The intercept tells us the predicted value of our target variable when all predictors equal zero, while the coefficient tells us how much our target variable changes for each one-unit increase in our predictor variable.\n\n# Extract key model components\nintercept = model.intercept_\ncoefficient = model.coef_[0]\n\nprint(f\"Intercept: {intercept:.2f}\")\nprint(f\"Ad Spend Coefficient: {coefficient:.2f}\")\n\nIntercept: 3552.41\nAd Spend Coefficient: 1.68",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation and Linear Regression Foundations</span>"
    ]
  },
  {
    "objectID": "21-correlation-regression.html#interpreting-the-regression-line",
    "href": "21-correlation-regression.html#interpreting-the-regression-line",
    "title": "21  Correlation and Linear Regression Foundations",
    "section": "21.3 Interpreting the Regression Line",
    "text": "21.3 Interpreting the Regression Line\nThe regression model is simply an equation that represents the relationship between our variables. The general form of our equation is: \\[ \\text{Weekly Sales} = \\text{Intercept} + \\text{Slope} \\times \\text{Ad Spend} \\]\nAs we fit our model, this equation gets updated to incorporate our specific intercept and coefficient values. Based on our model results above, our fitted equation becomes: \\[ \\text{Weekly Sales} = 3552 + 1.68 \\times \\text{Ad Spend} \\]\nHere’s how to interpret this model:\n\nIntercept (3552): When ad spend is $0, we expect weekly sales of $3,552\nCoefficient (1.68): For every $1 increase in ad spend, weekly sales increase by $1.68\n\nThis means advertising appears to have a positive effect on sales, and we can quantify exactly how much impact each advertising dollar has. If we were to visualize this equation, it would look like the following plot. We can think of this red line as our model’s “best guess” about the relationship between advertising spend and sales—it represents the equation we just derived. For example, if we wanted to predict the expected weekly sales when advertising spend is $1,500.\n\n\nShow code for regression line visualization\n# Visualize the fitted regression line\nplt.figure(figsize=(10, 6))\nplt.scatter(data[\"ad_spend\"], data[\"weekly_sales\"], alpha=0.7, label=\"Data points\")\nplt.plot(data[\"ad_spend\"], model.predict(X), color=\"red\", linewidth=2, label=\"Fitted line\")\n\n# Add prediction point for $1,500 ad spend\nprediction_x = 1500\nprediction_df = pd.DataFrame({\"ad_spend\": [prediction_x]})\nprediction_y = model.predict(prediction_df)[0]\nplt.scatter(prediction_x, prediction_y, color=\"orange\", s=100, zorder=5, label=f\"Prediction: ${prediction_x} → ${prediction_y:.0f}\")\n\nplt.xlabel(\"Advertising Spend ($)\")\nplt.ylabel(\"Weekly Sales\")\nplt.title(\"Simple Linear Regression: Ad Spend vs Weekly Sales\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote🎥 Video Spotlight: Simple Linear Regression Explained\n\n\n\nThis video introduces the goals of regression, connects the model to the basic linear equation ( \\(y = \\beta_0 + \\beta_1x\\) ), and shows how to interpret the slope and intercept using a clear, real-world example.\n\n\n\n\nMaking Predictions with Our Model\nOne of the most powerful aspects of regression is that we can use our fitted model to make predictions for new scenarios. While we could manually calculate predictions using our equation (as shown below), scikit-learn provides a convenient .predict() method that does this calculation for us automatically.\n\\[ \\$6072 = 3552 + 1.68 \\times \\$1500 \\]\nInstead of manually calculating, we can use our fitted model’s .predict() method. This method takes new input data (in the same format as our training data) and applies our learned equation to generate predictions. This approach is especially useful when making predictions for multiple scenarios or when working with more complex models.\n\n# Make a prediction for $1,500 in advertising spend\nnew_ad_spend = pd.DataFrame({\"ad_spend\": [1500]})  # $1500 in advertising\npredicted_sales = model.predict(new_ad_spend)\nprint(f\"Prediction: If we spend $1,500 on advertising, we expect {predicted_sales[0]:.0f} in weekly sales\")\n\nPrediction: If we spend $1,500 on advertising, we expect 6072 in weekly sales\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneYour Turn!\n\n\n\nFit a simple regression using ISLP Advertising data to predict Sales from TV advertising spend.\nYour Tasks:\n\nLoad the data and fit a LinearRegression model\n\nExtract and interpret the intercept and coefficient\nMake a prediction for $50k in TV advertising\nHow might a marketing manager use these insights?",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation and Linear Regression Foundations</span>"
    ]
  },
  {
    "objectID": "21-correlation-regression.html#multiple-linear-regression",
    "href": "21-correlation-regression.html#multiple-linear-regression",
    "title": "21  Correlation and Linear Regression Foundations",
    "section": "21.4 Multiple Linear Regression",
    "text": "21.4 Multiple Linear Regression\nWhile simple linear regression uses a single predictor variable to make predictions, multiple linear regression extends this approach by incorporating multiple predictor variables simultaneously. This is much more realistic for business scenarios, where outcomes are rarely influenced by just one factor. Instead of asking “How does advertising spend affect sales?” we can ask more nuanced questions like “How do advertising spend AND number of stores AND seasonal factors together affect sales?”\nThe power of multiple regression lies in its ability to isolate the effect of each predictor while holding all other predictors constant. This allows us to answer questions like: “If we increase advertising spend by $100 while keeping the number of stores the same, how much will sales increase?” This type of insight is invaluable for business decision-making because it helps managers understand the independent contribution of each factor they control.\nExpanding Our Scenario: Let’s return to our grocery chain example, but now imagine the marketing director realizes that sales aren’t just influenced by advertising spend—the number of stores in their network also plays a crucial role. As the company has expanded over the past few years, they’ve opened new locations, and the director suspects that having more stores amplifies the effect of their advertising efforts. They want to understand both factors simultaneously: how does advertising spend affect sales, and how does the number of stores affect sales, when we account for both factors together?\n\ndata2 = pd.DataFrame({\n    \"ad_spend\": [400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300],\n    \"num_stores\": [3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9],\n    \"weekly_sales\": [4200, 4400, 4100, 4800, 5600, 5200, 4900, 5500, 5300, 5900, 5700, 6300, 6900, 6200, 5800, 6600, 7100, 6800, 7300, 7800]\n})\ndata2.head()\n\n\n\n\n\n\n\n\nad_spend\nnum_stores\nweekly_sales\n\n\n\n\n0\n400\n3\n4200\n\n\n1\n500\n3\n4400\n\n\n2\n600\n4\n4100\n\n\n3\n700\n4\n4800\n\n\n4\n800\n4\n5600\n\n\n\n\n\n\n\nTo model this scenario, we follow the same process as simple linear regression, but now we include multiple predictors in our feature matrix. All we’re really doing is expanding our simple equation to accommodate additional variables. Instead of predicting weekly sales using only advertising spend, we now predict it using both advertising spend AND number of stores:\n\\[ \\text{Weekly Sales} = \\text{Intercept} + \\beta_1 \\times \\text{Ad Spend} + \\beta_2 \\times \\text{Num Stores} \\]\nThis equation allows us to understand how each factor independently contributes to sales while accounting for the presence of the other factors.\n\n# Prepare the data for multiple regression\nX2 = data2[[\"ad_spend\", \"num_stores\"]]  # Multiple features\ny2 = data2[\"weekly_sales\"]\n\n# Fit the multiple regression model\nmodel2 = LinearRegression()\nmodel2.fit(X2, y2)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nOnce our multiple regression model is fitted, we can examine the three key parameters of interest for our equation: the intercept and the two coefficients (one for each predictor variable). These parameters define our specific predictive equation and tell us how each factor influences weekly sales.\n\n# Extract key model components\nintercept = model2.intercept_\nad_spend_coef = model2.coef_[0]\nnum_stores_coef = model2.coef_[1]\n\nprint(f\"Intercept: {intercept:.2f}\")\nprint(f\"Ad Spend Coefficient: {ad_spend_coef:.2f}\")\nprint(f\"Num Stores Coefficient: {num_stores_coef:.2f}\")\n\nIntercept: 3843.58\nAd Spend Coefficient: 2.12\nNum Stores Coefficient: -153.07\n\n\nBased on our model results, our fitted multiple regression equation becomes: \\[ \\text{Weekly Sales} = 3843.58 + 2.12 \\times \\text{Ad Spend} + (-153.07) \\times \\text{Num Stores} \\]\nHere’s how to interpret each parameter:\n\nIntercept (3843.58): When both ad spend and number of stores are 0, we expect weekly sales of $3,844\nAd Spend coefficient (2.12): For every $1 increase in ad spend, weekly sales increase by $2.12, holding number of stores constant\nNum Stores coefficient (-153.07): For each additional store, weekly sales decrease by $153, holding ad spend constant\n\nInterestingly, the negative coefficient for number of stores suggests that having more stores actually decreases weekly sales when advertising spend is held constant. This could indicate that sales are being spread across more locations, or that newer stores are still building their customer base.\nWith two predictor variables, this equation now becomes a 3-dimensional problem where our “best-fit line” becomes a “best-fit plane.” Just as we could follow a line to make predictions with simple regression, we can still make predictions by following this plane in 3D space. However, as we add more predictors (creating 4, 5, or even hundreds of dimensions), visualization becomes impossible, though the mathematical principles remain the same.\n\n\nShow code for 3D visualization of multiple regression plane\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Suppress matplotlib 3D plotting warnings\nwarnings.filterwarnings('ignore', category=RuntimeWarning, module='mpl_toolkits.mplot3d')\n\n# Create 3D plot\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the actual data points\nax.scatter(data2[\"ad_spend\"], data2[\"num_stores\"], data2[\"weekly_sales\"], \n           color='blue', s=50, alpha=0.7, label='Data points')\n\n# Create meshgrid for the prediction plane\nad_range = np.linspace(data2[\"ad_spend\"].min(), data2[\"ad_spend\"].max(), 20)\nstores_range = np.linspace(data2[\"num_stores\"].min(), data2[\"num_stores\"].max(), 20)\nad_mesh, stores_mesh = np.meshgrid(ad_range, stores_range)\n\n# Calculate predictions for the plane\nplane_predictions = (model2.intercept_ + \n                    model2.coef_[0] * ad_mesh + \n                    model2.coef_[1] * stores_mesh)\n\n# Plot the prediction plane\nax.plot_surface(ad_mesh, stores_mesh, plane_predictions, alpha=0.3, color='red')\n\n# Set labels and title\nax.set_xlabel('Advertising Spend ($)')\nax.set_ylabel('Number of Stores')\nax.set_zlabel('Weekly Sales')\nax.set_title('Multiple Linear Regression: 3D Visualization\\nBest-Fit Plane Through Data Points')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMaking Predictions with Multiple Regression\nJust as with simple linear regression, we can use our fitted multiple regression model to make predictions for new scenarios. The key advantage of multiple regression is that we can predict outcomes based on multiple input variables simultaneously, allowing us to explore different business scenarios and understand how changes in multiple factors affect our outcome of interest.\n\n# Make predictions with multiple features\nprint(\"Multiple Regression Predictions:\")\nprint(\"Scenario 1: $1500 ad spend, 5 stores\")\nscenario1 = pd.DataFrame({\"ad_spend\": [1500], \"num_stores\": [5]})\npred1 = model2.predict(scenario1)\nprint(f\"Predicted sales: {pred1[0]:.0f}\")\n\nprint(\"\\nScenario 2: $1500 ad spend, 7 stores\") \nscenario2 = pd.DataFrame({\"ad_spend\": [1500], \"num_stores\": [7]})\npred2 = model2.predict(scenario2)\nprint(f\"Predicted sales: {pred2[0]:.0f}\")\n\nprint(f\"\\nEffect of 2 additional stores: {pred2[0] - pred1[0]:.0f} change in weekly sales\")\n\nMultiple Regression Predictions:\nScenario 1: $1500 ad spend, 5 stores\nPredicted sales: 6261\n\nScenario 2: $1500 ad spend, 7 stores\nPredicted sales: 5955\n\nEffect of 2 additional stores: -306 change in weekly sales\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneYour Turn!\n\n\n\nUse ISLP Advertising data to predict Sales using TV, Radio, and Newspaper advertising spend.\nYour Tasks:\n\nFit a multiple regression model with all three advertising channels\nExtract and interpret each coefficient\nWrite out the complete prediction equation\nWhich advertising channels have the strongest relationship with sales?",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation and Linear Regression Foundations</span>"
    ]
  },
  {
    "objectID": "21-correlation-regression.html#categorical-predictors",
    "href": "21-correlation-regression.html#categorical-predictors",
    "title": "21  Correlation and Linear Regression Foundations",
    "section": "21.5 Categorical Predictors",
    "text": "21.5 Categorical Predictors\nSo far, we’ve worked with continuous numeric predictors like advertising spend (measured in dollars) and number of stores (measured in counts). However, many important business factors are categorical rather than numeric—like customer type (Premium vs. Standard), sales region (North vs. South), product category (Electronics vs. Clothing), or marketing channel (Email vs. Social Media). These categorical variables can be just as important for predicting outcomes, but they require special handling because regression models fundamentally work with numbers, not categories.\nThe solution is dummy encoding (also called one-hot encoding), which converts categorical variables into numeric 0/1 indicators. While this might seem like a technical detail, it’s actually a powerful tool that allows us to quantify the impact of categorical factors on our outcomes. For instance, we can determine exactly how much more (or less) customers in the West region spend compared to those in the East, or how much premium customers differ from standard customers in their purchasing behavior.\n\nUnderstanding Dummy Encoding\nSuppose we have a variable region with two categories: East and West. Regression models require numeric inputs, so we need to transform this categorical variable into numbers.\n\nCreate a new variable region_West:\n\nIf a row is West → region_West = 1\nIf a row is East → region_West = 0\n\n\nThe East category becomes the baseline (reference group), and the coefficient for region_West tells us how much the West differs from East, holding other variables constant.\nHere’s how dummy encoding changes the data:\n\n\n\nad_spend\nregion\nweekly_sales\nregion_West\n\n\n\n\n1000\nEast\n5000\n0\n\n\n1200\nWest\n5200\n1\n\n\n1500\nEast\n6000\n0\n\n\n\n\n\nExample: Region and Sales\nLet’s see this in action by creating a new simulated dataset that includes both advertising spend and regional information. For this example, we’ll generate 50 data points (25 for each region) where West region stores have consistently higher baseline sales than East region stores, but both regions respond to advertising in the same way.\n\n\nShow code for simulating categorical predictor data\nimport numpy as np\n\n# Create 50 data points - 25 for each region\nnp.random.seed(123)  # For reproducible results\n\n# East region: lower baseline sales\neast_ad_spend = np.linspace(500, 2000, 25)\neast_base_sales = 3000 + 1.8 * east_ad_spend  # Same slope as West\neast_noise = np.random.normal(0, 200, 25)\neast_sales = east_base_sales + east_noise\n\n# West region: higher baseline sales (about $800 higher)\nwest_ad_spend = np.linspace(500, 2000, 25)\nwest_base_sales = 3800 + 1.8 * west_ad_spend  # Same slope, higher baseline\nwest_noise = np.random.normal(0, 200, 25)\nwest_sales = west_base_sales + west_noise\n\n# Combine into single DataFrame\ndata3 = pd.DataFrame({\n    \"ad_spend\": np.concatenate([east_ad_spend, west_ad_spend]),\n    \"region\": [\"East\"] * 25 + [\"West\"] * 25,\n    \"weekly_sales\": np.concatenate([east_sales, west_sales])\n})\n\nprint(\"Simulated data with regional baseline differences:\")\nprint(f\"East region samples: {len(data3[data3['region'] == 'East'])}\")\nprint(f\"West region samples: {len(data3[data3['region'] == 'West'])}\")\ndata3.head()\n\n\nSimulated data with regional baseline differences:\nEast region samples: 25\nWest region samples: 25\n\n\n\n\n\n\n\n\n\nad_spend\nregion\nweekly_sales\n\n\n\n\n0\n500.0\nEast\n3682.873879\n\n\n1\n562.5\nEast\n4211.969089\n\n\n2\n625.0\nEast\n4181.595700\n\n\n3\n687.5\nEast\n3936.241057\n\n\n4\n750.0\nEast\n4234.279950\n\n\n\n\n\n\n\nIn practice, we can use pandas’ pd.get_dummies() function to automatically perform this dummy encoding for us. This function takes categorical variables and creates new columns with 0/1 indicators for each category. The resulting dummy encoded variable (region_West) will appear as boolean values (True/False), but keep in mind that boolean values are mathematically equivalent to 1s and 0s, which is exactly what our regression model needs.\n\n# Create dummy variables using pandas\nX3_encoded = pd.get_dummies(data3[[\"ad_spend\", \"region\"]], drop_first=True)\nprint(\"Dummy encoded data:\")\nX3_encoded.head()\n\nDummy encoded data:\n\n\n\n\n\n\n\n\n\nad_spend\nregion_West\n\n\n\n\n0\n500.0\nFalse\n\n\n1\n562.5\nFalse\n\n\n2\n625.0\nFalse\n\n\n3\n687.5\nFalse\n\n\n4\n750.0\nFalse\n\n\n\n\n\n\n\nNow that we have our categorical variable properly encoded as numeric indicators, we can fit our regression model using the exact same process we’ve used before. The beauty of dummy encoding is that once the categorical variables are converted to numeric form, the regression algorithm treats them just like any other predictor variable.\n\ny3 = data3[\"weekly_sales\"]\n\n# Fit the model with categorical predictor\nmodel3 = LinearRegression()\nmodel3.fit(X3_encoded, y3)\n\n# Extract key model components\nintercept = model3.intercept_\nad_spend_coef = model3.coef_[0]\nregion_west_coef = model3.coef_[1]\n\nprint(f\"Intercept: {intercept:.2f}\")\nprint(f\"Ad Spend Coefficient: {ad_spend_coef:.2f}\")\nprint(f\"Region West Coefficient: {region_west_coef:.2f}\")\n\nIntercept: 2781.30\nAd Spend Coefficient: 2.00\nRegion West Coefficient: 749.27\n\n\nBased on our model results, region_West is coded as 1 if the store is in the West, 0 otherwise. The positive coefficient tells us that, holding advertising spend constant, West region stores average about 749 more in weekly sales compared to East region stores. This suggests that the West region may have more favorable market conditions or customer demographics that lead to higher baseline sales performance.\n\n\nInterpreting in Equation Form\nThe regression equation with a categorical predictor takes the form:\n\\[\n\\text{Weekly Sales} = \\text{Intercept} + \\beta_1 \\times \\text{Ad Spend} + \\beta_2 \\times \\text{Region\\_West}\n\\]\nWhere:\n\nIntercept: Expected sales for East region (baseline) when ad spend = 0\nAd Spend coefficient (β₁): Sales change per $1 ad spend increase (same for both regions)\nRegion_West coefficient (β₂): Sales difference between West and East regions\n\nIf we substitute our model results into the equation, we get:\n\\[ \\text{Weekly Sales} = 3040.48 + 1.80 \\times \\text{Ad Spend} + 749.01 \\times \\text{Region\\_West} \\]\nThis creates parallel lines with the same slope but different intercepts—showing that advertising has the same effect in both regions, but the West region has consistently higher baseline sales.\nWe can visualize this model to see how the categorical predictor creates two parallel prediction lines—one for each region. The lines have the same slope (representing the consistent effect of advertising spend) but different intercepts (representing the regional baseline difference).\n\n\nShow code for categorical predictor visualization\n# Create visualization of categorical predictor model\nplt.figure(figsize=(10, 6))\n\n# Separate data by region for plotting\neast_data = data3[data3[\"region\"] == \"East\"]\nwest_data = data3[data3[\"region\"] == \"West\"]\n\n# Plot data points\nplt.scatter(east_data[\"ad_spend\"], east_data[\"weekly_sales\"], \n           color='blue', alpha=0.7, label='East Region', s=60)\nplt.scatter(west_data[\"ad_spend\"], west_data[\"weekly_sales\"], \n           color='red', alpha=0.7, label='West Region', s=60)\n\n# Create parallel prediction lines\nad_range = np.linspace(data3[\"ad_spend\"].min(), data3[\"ad_spend\"].max(), 100)\n\n# East region line (region_West = 0)\n# Formula: intercept + ad_spend_coef * ad_spend + region_coef * 0\neast_predictions = model3.intercept_ + model3.coef_[0] * ad_range\nplt.plot(ad_range, east_predictions, color='blue', linewidth=3, linestyle='-', \n         label=f'East Region')\n\n# West region line (region_West = 1)\n# Formula: intercept + ad_spend_coef * ad_spend + region_coef * 1\nwest_predictions = model3.intercept_ + model3.coef_[0] * ad_range + model3.coef_[1] * 1\nplt.plot(ad_range, west_predictions, color='red', linewidth=3, linestyle='-', \n         label=f'West Region')\n\nplt.xlabel('Advertising Spend ($)')\nplt.ylabel('Weekly Sales')\nplt.title('Regression with Categorical Predictor: Parallel Lines by Region')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\nAs before, we can make predictions with our model on new data. Let’s see how our categorical predictor model works by predicting sales for stores in both regions with the same advertising spend.\n\n# Demonstrate predictions for both regions using DataFrame format\nprint(\"Categorical Predictor Predictions:\")\n\nprint(\"\\nEast region store with $1500 ad spend:\")\neast_scenario = pd.DataFrame({\n    \"ad_spend\": [1500], \n    \"region_West\": [0]\n})\neast_pred = model3.predict(east_scenario)\nprint(f\"Predicted sales: {east_pred[0]:.0f}\")\n\nprint(\"\\nWest region store with $1500 ad spend:\")\nwest_scenario = pd.DataFrame({\n    \"ad_spend\": [1500], \n    \"region_West\": [1]\n})\nwest_pred = model3.predict(west_scenario)\nprint(f\"Predicted sales: {west_pred[0]:.0f}\")\n\nprint(f\"\\nRegional difference: {west_pred[0] - east_pred[0]:.0f}\")\nprint(\"(This equals the Region_West coefficient)\")\n\nCategorical Predictor Predictions:\n\nEast region store with $1500 ad spend:\nPredicted sales: 5777\n\nWest region store with $1500 ad spend:\nPredicted sales: 6527\n\nRegional difference: 749\n(This equals the Region_West coefficient)\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneYour Turn! Credit Data with Categorical Variables\n\n\n\nUsing the Credit dataset from ISLP, build a regression model that includes both numeric predictors and the categorical variable Gender.\n\nfrom ISLP import load_data\nCredit = load_data('Credit')\nCredit.head()\n\n\n\n\n\n\n\n\nID\nIncome\nLimit\nRating\nCards\nAge\nEducation\nGender\nStudent\nMarried\nEthnicity\nBalance\n\n\n\n\n0\n1\n14.891\n3606\n283\n2\n34\n11\nMale\nNo\nYes\nCaucasian\n333\n\n\n1\n2\n106.025\n6645\n483\n3\n82\n15\nFemale\nYes\nYes\nAsian\n903\n\n\n2\n3\n104.593\n7075\n514\n4\n71\n11\nMale\nNo\nNo\nAsian\n580\n\n\n3\n4\n148.924\n9504\n681\n3\n36\n11\nFemale\nNo\nNo\nAsian\n964\n\n\n4\n5\n55.882\n4897\n357\n2\n68\n16\nMale\nNo\nYes\nCaucasian\n331\n\n\n\n\n\n\n\nYour Tasks:\n\nDummy encode the Gender variable (e.g., Female = 1, Male = 0).\nFit a regression model with Balance as the outcome and include Income, Limit, Age, and Gender as predictors.\nWrite the regression equation in full.\nInterpret the Gender coefficient: how does being Male vs Female affect balance, holding all else constant?\nBusiness reflection: How might a bank use this information?\nEthical reflection: Are there fairness or ethical concerns with using demographic variables like Gender in credit models?",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation and Linear Regression Foundations</span>"
    ]
  },
  {
    "objectID": "21-correlation-regression.html#assumptions-of-linear-regression",
    "href": "21-correlation-regression.html#assumptions-of-linear-regression",
    "title": "21  Correlation and Linear Regression Foundations",
    "section": "21.6 Assumptions of Linear Regression",
    "text": "21.6 Assumptions of Linear Regression\nLinear regression is a powerful and widely used technique, but it relies on certain assumptions. Understanding these assumptions is important because when they are violated, your model’s predictions and interpretations may become unreliable. Here’s what each assumption means and why it matters in a business context:\n\nLinearity: The relationship between predictors and the outcome is assumed to be linear. If the true relationship is curved or more complex, a linear model will oversimplify and potentially mislead decisions. Example: Assuming sales increase linearly with ad spend might overlook diminishing returns at higher spending levels.\nIndependence of errors: The residuals (errors) should not be correlated with each other. When errors are dependent, the model can give a false sense of confidence. Example: In time-series sales data, yesterday’s error often relates to today’s error—ignoring this can lead to poor forecasts.\nConstant variance (homoscedasticity): The spread of residuals should be roughly the same across all levels of the predictor(s). If variance grows with the predictor, your model’s predictions will be more uncertain for certain groups. Example: Predicting customer spend may be reliable for average-income customers but wildly variable for high-income ones.\nNormality of errors: Residuals should follow a roughly normal distribution. This assumption matters most for statistical inference (like confidence intervals and hypothesis tests). Example: If errors are highly skewed, a business might underestimate the risk of extreme losses.\n\nWhile these assumptions provide a useful framework, real-world data often violates them. That doesn’t mean regression is useless—it simply means you need to interpret results cautiously and sometimes transform your data, add interaction terms, or explore alternative methods.\nImportantly, later in this course you will learn about algorithms (such as decision trees, random forests, and boosting methods) that do not rely on these strict assumptions. These approaches can model nonlinear patterns, handle complex interactions, and provide more flexibility when linear regression falls short.",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation and Linear Regression Foundations</span>"
    ]
  },
  {
    "objectID": "21-correlation-regression.html#summary",
    "href": "21-correlation-regression.html#summary",
    "title": "21  Correlation and Linear Regression Foundations",
    "section": "21.7 Summary",
    "text": "21.7 Summary\nThis chapter introduced you to two fundamental techniques that bridge exploratory data analysis and predictive modeling: correlation and linear regression. You’ve moved beyond describing individual variables to understanding how variables relate to and influence each other.\nCorrelation serves as a powerful descriptive tool for measuring the strength and direction of linear relationships. You learned that correlation coefficients range from -1 to +1, quickly revealing patterns in your data, while remembering that correlation describes association but never implies causation.\nLinear regression extends correlation by providing a mathematical framework for prediction and interpretation. You mastered the progression from simple regression (one predictor) to multiple regression (several predictors) to categorical predictors (using dummy encoding). Key skills you developed include:\n\nFitting models with scikit-learn’s LinearRegression using .fit() and .predict() methods\nExtracting and interpreting intercepts and coefficients in business terms\nConverting categorical variables with pd.get_dummies() for use in regression models\nCreating effective visualizations that communicate regression results\nMaking predictions for new scenarios and business planning\n\nThroughout realistic business scenarios involving advertising effectiveness and regional sales differences, you learned to translate statistical outputs into actionable business insights and maintain healthy skepticism about causal claims.\nLooking ahead: The regression foundations you’ve built here are essential for all machine learning techniques you’ll encounter. In the next chapter, you’ll learn how to measure how well your models are performing through various evaluation metrics—a critical skill for determining when your models are ready for real-world deployment. The concepts of feature-target relationships, model fitting, and prediction interpretation you’ve mastered will transfer directly to any modeling framework in your data science career.",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation and Linear Regression Foundations</span>"
    ]
  },
  {
    "objectID": "21-correlation-regression.html#end-of-chapter-exercise",
    "href": "21-correlation-regression.html#end-of-chapter-exercise",
    "title": "21  Correlation and Linear Regression Foundations",
    "section": "21.8 End of Chapter Exercise",
    "text": "21.8 End of Chapter Exercise\nFor these exercises, you’ll work with three different datasets from the ISLP package. Each scenario mirrors a real-world decision context where regression can guide insights.\n\n\n\n\n\n\nNoneScenario 1: Credit Risk Analysis\n\n\n\n\n\nCompany: A regional bank\nGoal: Understand what drives customers’ credit card balances to inform risk management and marketing strategies\nDataset: Credit dataset from ISLP package\n\nfrom ISLP import load_data\nCredit = load_data('Credit')\nCredit.head()\n\n\n\n\n\n\n\n\nID\nIncome\nLimit\nRating\nCards\nAge\nEducation\nGender\nStudent\nMarried\nEthnicity\nBalance\n\n\n\n\n0\n1\n14.891\n3606\n283\n2\n34\n11\nMale\nNo\nYes\nCaucasian\n333\n\n\n1\n2\n106.025\n6645\n483\n3\n82\n15\nFemale\nYes\nYes\nAsian\n903\n\n\n2\n3\n104.593\n7075\n514\n4\n71\n11\nMale\nNo\nNo\nAsian\n580\n\n\n3\n4\n148.924\n9504\n681\n3\n36\n11\nFemale\nNo\nNo\nAsian\n964\n\n\n4\n5\n55.882\n4897\n357\n2\n68\n16\nMale\nNo\nYes\nCaucasian\n331\n\n\n\n\n\n\n\nYour Tasks:\n\nFit a regression model predicting Balance using Income, Limit, Age, and Gender.\n\nInterpret the coefficients: What does each variable suggest about customer balances, holding the others constant?\n\nConvert the model into equation form.\n\nEthical reflection: Should Gender be used in credit risk models? What fairness concerns arise if it is?\n\nBased on your model, how might the bank adjust its marketing or risk strategies?\n\n\n\n\n\n\n\n\n\n\n\nNoneScenario 2: Baseball Salary Analysis\n\n\n\n\n\nCompany: A professional baseball team\nGoal: Better understand the drivers of player salaries to inform contract negotiations and player evaluation\nDataset: Hitters dataset from ISLP package\n\nHitters = load_data('Hitters')\nHitters.head()\n\n\n\n\n\n\n\n\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nLeague\nDivision\nPutOuts\nAssists\nErrors\nSalary\nNewLeague\n\n\n\n\n0\n293\n66\n1\n30\n29\n14\n1\n293\n66\n1\n30\n29\n14\nA\nE\n446\n33\n20\nNaN\nA\n\n\n1\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n69\n321\n414\n375\nN\nW\n632\n43\n10\n475.0\nN\n\n\n2\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n63\n224\n266\n263\nA\nW\n880\n82\n14\n480.0\nA\n\n\n3\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n225\n828\n838\n354\nN\nE\n200\n11\n3\n500.0\nN\n\n\n4\n321\n87\n10\n39\n42\n30\n2\n396\n101\n12\n48\n46\n33\nN\nE\n805\n40\n4\n91.5\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Cleaning Hint: The Hitters dataset contains some missing values in the Salary column. You may need to remove rows with missing salary data before fitting your regression model. Consider using dropna() or similar methods to clean the data first.\n\n\n\nYour Tasks:\n\nFit a regression model predicting Salary using Years (experience), Hits (recent batting performance), and League (categorical: American or National).\n\nInterpret the coefficients: How do performance and experience impact salaries? What does the league dummy variable suggest?\n\nConvert the regression results into equation form.\n\nBusiness reflection: If you were a player agent, how could you use this model to advocate for higher salaries for your clients?\n\n\n\n\n\n\n\n\n\n\n\nNoneScenario 3: College Application Drivers\n\n\n\n\n\nCompany: Higher education consulting firm\nGoal: Analyze what factors drive the number of applications a college receives to advise institutions on strategic positioning\nDataset: College dataset from ISLP package\n\nCollege = load_data('College')\nCollege.head()\n\n\n\n\n\n\n\n\nPrivate\nApps\nAccept\nEnroll\nTop10perc\nTop25perc\nF.Undergrad\nP.Undergrad\nOutstate\nRoom.Board\nBooks\nPersonal\nPhD\nTerminal\nS.F.Ratio\nperc.alumni\nExpend\nGrad.Rate\n\n\n\n\n0\nYes\n1660\n1232\n721\n23\n52\n2885\n537\n7440\n3300\n450\n2200\n70\n78\n18.1\n12\n7041\n60\n\n\n1\nYes\n2186\n1924\n512\n16\n29\n2683\n1227\n12280\n6450\n750\n1500\n29\n30\n12.2\n16\n10527\n56\n\n\n2\nYes\n1428\n1097\n336\n22\n50\n1036\n99\n11250\n3750\n400\n1165\n53\n66\n12.9\n30\n8735\n54\n\n\n3\nYes\n417\n349\n137\n60\n89\n510\n63\n12960\n5450\n450\n875\n92\n97\n7.7\n37\n19016\n59\n\n\n4\nYes\n193\n146\n55\n16\n44\n249\n869\n7560\n4120\n800\n1500\n76\n72\n11.9\n2\n10922\n15\n\n\n\n\n\n\n\nYour Tasks:\n\nFit a regression model predicting Apps (applications received) using Top10perc (percent of students from top 10% of high school class), Outstate (out-of-state tuition), and Private (categorical: private vs. public).\n\nInterpret the coefficients: How do academics, price, and private/public status influence applications?\n\nConvert the model into equation form.\n\nReflection: If you were advising a college president, what strategies might you recommend based on this model? Are there any risks of oversimplifying the decision with this model?",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation and Linear Regression Foundations</span>"
    ]
  },
  {
    "objectID": "22-regression-evaluation.html",
    "href": "22-regression-evaluation.html",
    "title": "22  Evaluating Regression Models",
    "section": "",
    "text": "22.1 Understanding the “Best-Fit” Line\nIn business, building a model is only half the battle. The real question is: How good is your model? Consider these scenarios:\nModel evaluation answers the fundamental question: “How well does our model perform?” Without proper evaluation, you might deploy a model that makes systematically poor predictions, leading to costly business mistakes. This chapter teaches you to measure model performance using various metrics and understand when each metric is most appropriate for business decision-making.\nBy the end of this chapter, you will be able to:\nWhen we build a regression model, the algorithm automatically finds the “best-fit line” through our data points. But what makes one line “better” than another? The answer lies in prediction errors—the differences between what our model predicts and what actually happens.",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "22-regression-evaluation.html#understanding-the-best-fit-line",
    "href": "22-regression-evaluation.html#understanding-the-best-fit-line",
    "title": "22  Evaluating Regression Models",
    "section": "",
    "text": "From Visual Intuition to Mathematical Precision\nLet’s return to our advertising example from the previous chapter. When you look at a scatterplot, you can imagine drawing many different lines through the data points. Some would fit the pattern well, others would miss it entirely. We use residuals to guide us on which line is best.\nThe dashed lines in the plot below represent residuals (or errors), the vertical distances between each actual data point and our prediction line. Linear regression finds the line that minimizes the Sum of Squared Errors (SSE), which is exactly what it sounds like: add up all the squared residuals.\n\\[ SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\nWhere \\(y_i\\) is the actual value, \\(\\hat{y}_i\\) is the predicted value, and \\(n\\) is the number of data points.\n\n\nShow code for regression line errors\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error\n\n# Recreate our advertising data\ndata = pd.DataFrame({\n    \"ad_spend\": [400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300],\n    \"weekly_sales\": [4200, 4400, 4100, 4800, 5600, 5200, 4900, 5500, 5300, 5900, 5700, 6300, 6900, 6200, 5800, 6600, 7100, 6800, 7300, 7800]\n})\n\n# Fit our regression model\nX = data[['ad_spend']]\ny = data['weekly_sales']\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Make predictions\npredictions = model.predict(X)\n\n# Visualize the fit\nplt.figure(figsize=(10, 6))\nplt.scatter(data['ad_spend'], data['weekly_sales'], alpha=0.7, label='Actual data')\nplt.plot(data['ad_spend'], predictions, color='red', linewidth=2, label='Best-fit line')\n\n# Show residuals for a few points\nfor i in range(0, len(data), 4):  # Show every 4th point to avoid clutter\n    plt.plot([data['ad_spend'].iloc[i], data['ad_spend'].iloc[i]], \n             [data['weekly_sales'].iloc[i], predictions[i]], \n             'k--', alpha=0.5, linewidth=1)\n\nplt.xlabel('Advertising Spend ($)')\nplt.ylabel('Weekly Sales')\nplt.title('Regression Line with Prediction Errors (Residuals)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWhy Square the Errors?\nYou might wonder: why square the errors instead of just adding them up directly? There are several important reasons:\n\nPositive and negative errors don’t cancel out: Without squaring, a prediction that’s $100 too high would cancel out one that’s $100 too low, making it look like we have zero error when we actually have significant prediction problems.\nLarger errors get penalized more: Squaring means that one prediction that’s off by $200 contributes more to our error measure than two predictions that are each off by $100. This reflects the business reality that big mistakes are often disproportionately costly.\nMathematical convenience: Squared errors have nice mathematical properties that make the optimization problem solvable with standard techniques.\n\n\n\nComputing SSE Manually\nLet’s calculate the Sum of Squared Errors step-by-step for our advertising model to see this concept in action:\n\n# Calculate SSE manually using our advertising data\n# (Using the same data and model from the visualization above)\n\n# Step 1: Calculate residuals (errors) for each prediction\nresiduals = y - predictions\n\n# Step 2: Square each residual\nsquared_residuals = residuals ** 2\n\n# Step 3: Sum all squared residuals to get SSE\nsse_manual = np.sum(squared_residuals)\n\nprint(f\"Sum of Squared Errors: {sse_manual:,.0f}\")\nprint(f\"Number of data points: {len(y)}\")\nprint(f\"Average squared error per point: {sse_manual/len(y):,.0f}\")\n\n# Show the calculation for the first 5 data points\nprint(f\"\\nBreaking down the first 5 predictions:\")\nprint(f\"{'Point':&lt;8} {'Actual':&lt;8} {'Predicted':&lt;10} {'Error':&lt;8} {'Squared Error':&lt;12}\")\nprint(f\"{'-'*50}\")\nfor i in range(5):\n    actual = y.iloc[i]\n    predicted = predictions[i]\n    error = actual - predicted\n    squared_error = error ** 2\n    print(f\"{i+1:&lt;8} ${actual:&lt;7.0f} ${predicted:&lt;9.0f} {error:&lt;+7.0f} {squared_error:&lt;11.0f}\")\n\nprint(f\"\\nSum of first 5 squared errors: {np.sum(squared_residuals[:5]):.0f}\")\nprint(f\"Total SSE for all {len(y)} points: {sse_manual:.0f}\")\n\nSum of Squared Errors: 2,409,759\nNumber of data points: 20\nAverage squared error per point: 120,488\n\nBreaking down the first 5 predictions:\nPoint    Actual   Predicted  Error    Squared Error\n--------------------------------------------------\n1        $4200    $4224      -24     590        \n2        $4400    $4392      +8      60         \n3        $4100    $4560      -460    211808     \n4        $4800    $4728      +72     5156       \n5        $5600    $4896      +704    495383     \n\nSum of first 5 squared errors: 712996\nTotal SSE for all 20 points: 2409759\n\n\n\n\n\n\n\n\nImportantWhy SSE Matters (But Isn’t Always Interpretable)\n\n\n\nAlthough SSE is crucial for our regression algorithm to converge on the optimal solution—it’s literally what the algorithm minimizes—it’s not very interpretable for business decision-making. An SSE of several hundred thousand represents our total prediction error across all 20 data points, but this number alone doesn’t tell us if our model is good or poor.\nConsequently, we tend to lean on other metrics that are more interpretable to the problem at hand, such as R² (which gives us a percentage), RMSE (which is in the same units as our target variable), or business-specific metrics that directly relate to costs and outcomes.\n\n\n\n\n\n\n\n\nNote🎥 Video Spotlight: Understanding SSE (and SST & SSR)\n\n\n\n\n\nThis video provides an excellent foundation for understanding Sum of Squares Error (SSE) along with two other terms – Sum of Squares Total (SST) and Sum of Squares Regression (SSR). The video explores how these three concepts measure variability in a dataset and their interconnectedness in evaluating regression line effectiveness.",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "22-regression-evaluation.html#goodness-of-fit-r²-r-squared",
    "href": "22-regression-evaluation.html#goodness-of-fit-r²-r-squared",
    "title": "22  Evaluating Regression Models",
    "section": "22.2 Goodness of Fit: R² (R-Squared)",
    "text": "22.2 Goodness of Fit: R² (R-Squared)\nWhile SSE tells us about total error, it’s hard to interpret on its own. Is an SSE of 500,000 good or bad? It depends on the scale of your data. R² (R-squared) solves this problem by converting error into a more interpretable percentage.\n\nUnderstanding the R² Formula\nR² measures the proportion of variation in your target variable that’s explained by your model. It’s calculated using this relationship:\n\\[ R^2 = 1 - \\frac{SSE}{TSS} = 1 - \\frac{\\text{Sum of Squared Errors}}{\\text{Total Sum of Squares}} \\]\nWhere:\n\nSSE = Sum of squared differences between actual and predicted values\nTSS = Sum of squared differences between actual values and the mean\n\nThink of it this way: TSS represents how much your data varies around its average (if you had no model at all), while SSE represents how much it varies around your model’s predictions. R² tells you what fraction of the original variation your model successfully “explains.”\n\n\n\n\n\n\nNoteManual vs Automated R² Calculation\n\n\n\nWe can calculate R² manually as shown below, but since this is a very common metric used in machine learning, scikit-learn provides the r2_score function to simplify this for us, which we also see in the code below.\n\n\n\n# Calculate R² step by step\ny_mean = np.mean(y)\ntss = np.sum((y - y_mean) ** 2)  # Total Sum of Squares\nsse = np.sum((y - predictions) ** 2)  # Sum of Squared Errors\nr_squared_manual = 1 - (sse / tss)\n\n# Compare with sklearn's calculation\nr_squared_sklearn = r2_score(y, predictions)\n\nprint(f\"Manual R² calculation: {r_squared_manual:.4f}\")\nprint(f\"Sklearn R² calculation: {r_squared_sklearn:.4f}\")\nprint(f\"Model R² (from .score() method): {model.score(X, y):.4f}\")\n\nprint(f\"\\nInterpretation: {r_squared_manual:.1%} of the variation in weekly sales\")\nprint(f\"is explained by advertising spend in our model.\")\n\nManual R² calculation: 0.8862\nSklearn R² calculation: 0.8862\nModel R² (from .score() method): 0.8862\n\nInterpretation: 88.6% of the variation in weekly sales\nis explained by advertising spend in our model.\n\n\n\n\nInterpreting R² Values\nR² ranges from 0 to 1 (and can be negative for very poor models):\n\nR² = 1.0: Perfect fit—your model explains 100% of the variation\nR² = 0.8: Strong fit—your model explains 80% of the variation\n\nR² = 0.5: Moderate fit—your model explains 50% of the variation\nR² = 0.0: No relationship—your model is no better than just predicting the average\n\nIn business contexts, what constitutes a “good” R² depends heavily on your domain:\n\nFinancial markets: R² of 0.1-0.3 might be excellent (markets are noisy!)\nManufacturing quality: R² of 0.9+ might be expected (controlled processes)\nMarketing response: R² of 0.5-0.8 is often realistic (human behavior varies)\n\n\n\n\n\n\n\nTipThe Bottom Line on “Good” R² Values\n\n\n\nThere is no universal threshold that determines whether an R² value is “good” or “bad.” What constitutes a strong R² is entirely dependent on the domain and problem at hand. A seemingly low R² of 0.2 might be groundbreaking in a noisy field like stock market prediction, while the same value would be concerning in a controlled manufacturing setting. Always evaluate R² in the context of your specific industry, the inherent variability of your data, and the standards established by previous research in your domain.\nCheck out this video to help make the \\(R^2\\) concept more concrete:",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "22-regression-evaluation.html#error-metrics-for-business-decisions",
    "href": "22-regression-evaluation.html#error-metrics-for-business-decisions",
    "title": "22  Evaluating Regression Models",
    "section": "22.3 Error Metrics for Business Decisions",
    "text": "22.3 Error Metrics for Business Decisions\nWhile R² gives us a general sense of model quality, specific business decisions often require more targeted metrics. Different error measures emphasize different aspects of prediction accuracy.\n\nMean Squared Error (MSE) and Root Mean Squared Error (RMSE)\nMSE is simply the average of our squared errors, while RMSE is the square root of MSE. RMSE has a crucial advantage: it’s in the same units as our target variable, making it much easier to interpret.\n\\[ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\n\\[ RMSE = \\sqrt{MSE} \\]\nFor our advertising example, the RMSE is approximately $347. This means that when we predict weekly sales based on advertising spend, our predictions are typically off by about $347. To put this in business context: if we predict weekly sales of $6,000, the actual sales could reasonably range from about $5,653 to $6,347. For a marketing manager planning inventory or staffing, this level of uncertainty might be quite acceptable for weekly planning.\n\n# Calculate MSE and RMSE\nmse = mean_squared_error(y, predictions)\nrmse = root_mean_squared_error(y, predictions)\n\nprint(f\"Mean Squared Error (MSE): {mse:,.0f}\")\nprint(f\"Root Mean Squared Error (RMSE): ${rmse:,.0f}\")\nprint(f\"\\nInterpretation: On average, our predictions are off by about ${rmse:,.0f}\")\nprint(f\"when predicting weekly sales.\")\n\nMean Squared Error (MSE): 120,488\nRoot Mean Squared Error (RMSE): $347\n\nInterpretation: On average, our predictions are off by about $347\nwhen predicting weekly sales.\n\n\n\n\n\n\n\n\nTipBusiness Context for RMSE\n\n\n\nIf you’re a marketing manager and your model has an RMSE of $347 (like our advertising example), you know that your weekly sales predictions are typically within about $347 of the actual values. This helps you set realistic expectations and plan appropriate safety margins. The key advantage of RMSE is that it speaks in the same units as your business outcome—dollars, units sold, customers served—making it immediately interpretable for operational planning and risk assessment.\n\n\n\n\nMean Absolute Error (MAE)\nMAE calculates the average absolute difference between predictions and actual values. Unlike RMSE, it doesn’t square the errors, so it treats all errors equally regardless of size.\n\\[ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\]\n\n# Calculate MAE\nmae = mean_absolute_error(y, predictions)\n\nprint(f\"Mean Absolute Error (MAE): ${mae:,.0f}\")\nprint(f\"Root Mean Squared Error (RMSE): ${rmse:,.0f}\")\nprint(f\"\\nNotice that RMSE &gt; MAE because RMSE penalizes large errors more heavily.\")\n\n# Demonstrate the difference with an extreme outlier\ny_with_outlier = y.copy()\npredictions_with_outlier = predictions.copy()\ny_with_outlier.iloc[0] = 10000  # Simulate one very bad prediction\n\nmae_outlier = mean_absolute_error(y_with_outlier, predictions_with_outlier)\nrmse_outlier = root_mean_squared_error(y_with_outlier, predictions_with_outlier)\n\nprint(f\"\\nWith one extreme outlier:\")\nprint(f\"MAE changes from ${mae:,.0f} to ${mae_outlier:,.0f}\")\nprint(f\"RMSE changes from ${rmse:,.0f} to ${rmse_outlier:,.0f}\")\nprint(f\"RMSE is much more sensitive to outliers!\")\n\nMean Absolute Error (MAE): $270\nRoot Mean Squared Error (RMSE): $347\n\nNotice that RMSE &gt; MAE because RMSE penalizes large errors more heavily.\n\nWith one extreme outlier:\nMAE changes from $270 to $557\nRMSE changes from $347 to $1,337\nRMSE is much more sensitive to outliers!\n\n\n\n\n\n\n\n\nImportantBusiness Takeaway: Choosing Between MAE and RMSE\n\n\n\nThe choice between MAE and RMSE should align with your business risk tolerance. If one large prediction error could cause significant business damage—like underestimating demand for a critical product launch or miscalculating loan default risk—use RMSE because it heavily penalizes these costly outliers. However, if prediction errors have roughly linear business costs—such as staffing customer service or managing routine inventory—MAE provides a clearer picture of typical performance without being skewed by occasional extreme cases.\n\n\nWhen to use MAE vs RMSE:\n\nMAE when all errors are equally costly (e.g., customer support response time)\nRMSE when large errors are disproportionately bad (e.g., financial risk models)\n\n\n\nMean Absolute Percentage Error (MAPE)\nMAPE expresses errors as percentages of the actual values, making it useful for comparing models across different scales or for relative performance evaluation.\n\\[ MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100\\% \\]\n\n# Calculate MAPE using scikit-learn\nfrom sklearn.metrics import mean_absolute_percentage_error\n\nmape = mean_absolute_percentage_error(y, predictions) * 100  # Convert to percentage\n\nprint(f\"Mean Absolute Percentage Error (MAPE): {mape:.1f}%\")\nprint(f\"\\nInterpretation: On average, our predictions are off by {mape:.1f}%\")\nprint(f\"of the actual weekly sales value.\")\n\n# Show example: if actual sales are $6,000\nexample_sales = 6000\nexample_error = example_sales * (mape / 100)\nprint(f\"\\nExample: For actual sales of ${example_sales:,}\")\nprint(f\"we'd expect our prediction to be off by about ${example_error:.0f}\")\n\nMean Absolute Percentage Error (MAPE): 4.7%\n\nInterpretation: On average, our predictions are off by 4.7%\nof the actual weekly sales value.\n\nExample: For actual sales of $6,000\nwe'd expect our prediction to be off by about $280\n\n\n\n\n\n\n\n\nTipWhen MAPE is Most Valuable\n\n\n\nMAPE shines in scenarios where relative performance matters more than absolute errors. Retail forecasting often uses MAPE because a 10% error means the same thing whether you’re predicting sales of $100 or $10,000—it represents the same proportional impact on inventory planning or revenue projections. MAPE is ideal when comparing models across different product categories, time periods, or business units with vastly different scales.\nImportant limitation: Be cautious with MAPE when actual values can be close to zero, as it can explode to infinity and provide misleading results.\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneHands-On: Build and Evaluate Your Own Model\n\n\n\nNow it’s your turn to build a regression model and compute all the evaluation metrics we’ve learned about. You’ll use the Advertising dataset from Chapter 21.\nDataset: Load the Advertising data and build a model to predict sales using all three advertising channels (TV, radio, newspaper).\n\n# Load the data (adjust path as needed for local files)\nadvertising = pd.read_csv(\"../data/Advertising.csv\")\n\n# Alternative: Load directly from GitHub if you don't have local access\n# advertising = pd.read_csv(\"https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/main/data/Advertising.csv\")\n\nprint(\"Advertising dataset shape:\", advertising.shape)\nprint(advertising.head())\n\nAdvertising dataset shape: (200, 4)\n      TV  radio  newspaper  sales\n0  230.1   37.8       69.2   22.1\n1   44.5   39.3       45.1   10.4\n2   17.2   45.9       69.3    9.3\n3  151.5   41.3       58.5   18.5\n4  180.8   10.8       58.4   12.9\n\n\nYour Tasks:\n\nBuild the model: Create a multiple regression model predicting sales from TV, radio, and newspaper advertising spend.\nCalculate all metrics: Compute and report:\n\nSSE (Sum of Squared Errors)\nR² (R-squared)\nMSE (Mean Squared Error)\nRMSE (Root Mean Squared Error)\nMAE (Mean Absolute Error)\nMAPE (Mean Absolute Percentage Error)\n\nInterpret the results:\n\nWhat does each metric tell you about model performance?\nIf you were a marketing manager, which metric would be most useful for budget planning?\nHow would you explain the model’s accuracy to a business stakeholder?\n\nBusiness context: Given your results, would you trust this model to guide a $50,000 advertising budget allocation? Why or why not?",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "22-regression-evaluation.html#the-critical-importance-of-generalization",
    "href": "22-regression-evaluation.html#the-critical-importance-of-generalization",
    "title": "22  Evaluating Regression Models",
    "section": "22.4 The Critical Importance of Generalization",
    "text": "22.4 The Critical Importance of Generalization\nAll the metrics we’ve calculated so far have a fundamental problem: we computed them on the same data we used to train our model. This is like a student grading their own homework—the results will be overly optimistic.\n\n\n\n\n\n\nImportantThe Business Reality of Model Performance\n\n\n\nIn business, what matters isn’t how well your model fits historical data, but how well it predicts future, unseen data. This is called generalization.\n\n\n\nTrain/Test Splits: Simulating the Future\nThe solution is to split our data into two parts:\n\nTraining set (~70-80%): Used to fit the model\nTest set (~20-30%): Used to evaluate performance (model never sees this during training)\n\nThis simulates the real-world scenario where you build a model on historical data and then use it to predict future outcomes.\n\n\n\n\n\n\n\nflowchart TD\n    A[Complete Dataset] --&gt; B[Training Set&lt;br/&gt;70-80%]\n    A --&gt; C[Test Set&lt;br/&gt;20-30%]\n    \n    B --&gt; D[Train Model]\n    D --&gt; E[Trained Model]\n    \n    C --&gt; F[Evaluate Performance]\n    E --&gt; F\n    F --&gt; G[Unbiased Performance&lt;br/&gt;Estimate]\n    \n    style A fill:#f0f8ff\n    style B fill:#e8f5e8\n    style C fill:#ffe6e6\n    style G fill:#fff2cc\n\n\n\n\nFigure 22.1: Proper data splitting ensures unbiased evaluation by keeping test data completely separate from model training.\n\n\n\n\n\n\nSplitting the Data\nLet’s first look at how to properly split our data using random sampling. The random_state parameter ensures our results are reproducible—anyone running this code will get the same split.\n\nfrom sklearn.model_selection import train_test_split\n\n# Split the data randomly with reproducible results\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=30\n)\n\nprint(f\"Total data points: {len(X)}\")\nprint(f\"Training set: {len(X_train)} points ({len(X_train)/len(X):.1%})\")\nprint(f\"Test set: {len(X_test)} points ({len(X_test)/len(X):.1%})\")\n\nTotal data points: 20\nTraining set: 14 points (70.0%)\nTest set: 6 points (30.0%)\n\n\n\n\n\n\n\n\nImportantReproducible Results with random_state\n\n\n\nWhat random_state=30 does: This parameter controls the randomness of the split. Setting it to a specific number (like 30) ensures that every time you run this code, you’ll get exactly the same train/test split. This is crucial for reproducible results—without it, your model performance might vary slightly each time you run your analysis simply due to different random splits.\n\n\nThe golden rule: Once you set aside your test set, don’t touch it until you’re completely done with model development. The moment you use test data to make decisions about your model (like choosing features or tuning parameters), it’s no longer a fair evaluation.\n\n\nTraining and Evaluating the Model\nNow that we have our data split, we follow a two-step process: first train the model only on the training data, then evaluate its performance on both the training and test sets.\n\n# Train model on training data only\nmodel_train = LinearRegression()\nmodel_train.fit(X_train, y_train)\n\n# Evaluate on both training and test sets\ntrain_predictions = model_train.predict(X_train)\ntest_predictions = model_train.predict(X_test)\n\n# Calculate metrics for both sets\nprint(f\"\\n{'Metric':&lt;20} {'Training Set':&lt;15} {'Test Set':&lt;15}\")\nprint(f\"{'-'*50}\")\nprint(f\"{'R²':&lt;20} {r2_score(y_train, train_predictions):&lt;15.3f} {r2_score(y_test, test_predictions):&lt;15.3f}\")\nprint(f\"{'RMSE':&lt;20} {root_mean_squared_error(y_train, train_predictions):&lt;15.0f} {root_mean_squared_error(y_test, test_predictions):&lt;15.0f}\")\nprint(f\"{'MAE':&lt;20} {mean_absolute_error(y_train, train_predictions):&lt;15.0f} {mean_absolute_error(y_test, test_predictions):&lt;15.0f}\")\n\n\nMetric               Training Set    Test Set       \n--------------------------------------------------\nR²                   0.909           0.760          \nRMSE                 330             403            \nMAE                  258             296            \n\n\nOur example uses random splitting, which works well when each data point is independent and we’re not dealing with time-series data. For many business problems involving temporal data (like predicting next month’s sales), you’d want to use time-based splitting instead—training on earlier periods and testing on later ones to better simulate real-world deployment.\n\n\n\n\n\n\nTipTime-Series Splitting in Scikit-Learn\n\n\n\nFor time-series data, scikit-learn provides TimeSeriesSplit from sklearn.model_selection. This class creates multiple train/test splits where each split respects the temporal order—you always train on earlier data and test on later data. This is essential for problems like sales forecasting, stock prediction, or any scenario where you’re predicting future events based on historical patterns.\n\n\nLooking at our results, we can see that our model performs better on the training set (RMSE ≈ $330) compared to the test set (RMSE ≈ $403). This pattern—where training performance exceeds test performance—is actually quite common and tells us something important about how well our model generalizes to new data.\n\n\n\nInterpreting Training vs Test Performance\nThe relationship between training and test performance is one of the most important diagnostic tools in machine learning. It tells you not just how accurate your model is, but how trustworthy those accuracy estimates are for real-world deployment. The key insight is comparing training and test performance:\n\nSimilar performance: Good sign—your model generalizes well\nTraining much better than test: Overfitting—your model memorized the training data\nTest much better than training: Unusual—might indicate data leakage or a lucky split\n\nOur example shows the second scenario, where training RMSE ($330) is notably better than test RMSE ($403). This 22% performance gap suggests our model may be slightly overfitting, but isn’t necessarily a major concern for a simple linear model like ours. Let’s visualize this difference to better understand what’s happening.\n\n\nShow code for training vs test performance visualization\n# Visualize training vs test performance\nplt.figure(figsize=(12, 5))\n\n# Training set\nplt.subplot(1, 2, 1)\nplt.scatter(X_train, y_train, alpha=0.7, color='blue')\nplt.plot(X_train, train_predictions, color='red', linewidth=2)\nplt.xlabel('Ad Spend ($)')\nplt.ylabel('Weekly Sales')\nplt.title(f'Training Set\\nR² = {r2_score(y_train, train_predictions):.3f}')\nplt.grid(True, alpha=0.3)\n\n# Test set\nplt.subplot(1, 2, 2)\nplt.scatter(X_test, y_test, alpha=0.7, color='green')\nplt.plot(X_test, test_predictions, color='red', linewidth=2)\nplt.xlabel('Ad Spend ($)')\nplt.ylabel('Weekly Sales')\nplt.title(f'Test Set\\nR² = {r2_score(y_test, test_predictions):.3f}')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe side-by-side plots above show the same model (red line) applied to two different datasets. Notice how the training set (left) shows a tighter fit with less scatter around the line, while the test set (right) shows more variability and a slightly lower R² value. This visual comparison makes it clear why we can’t rely solely on training performance—the test set reveals how our model actually performs on unseen data, which is what matters for business decision-making.\n\n\n\n\n\n\nNoteImpact of Dataset Size on Performance Variation\n\n\n\nKeep in mind that our example dataset is quite small (only 20 data points total). With small datasets, it’s not uncommon to see larger disparities between training and test performance simply due to random variation in how the data gets split. Smaller test sets are more susceptible to containing “unlucky” or particularly challenging examples that make the model appear worse than it actually is. As your datasets grow larger (thousands or tens of thousands of observations), the performance gap between training and test sets typically becomes more stable and meaningful.\n\n\n\n\nOverfitting vs Underfitting: Finding the Sweet Spot\nOur evaluation process is fundamentally about finding a model that generalizes well—one that performs consistently on both training and test data. This means avoiding two common pitfalls: underfitting (too simple) and overfitting (too complex). Understanding the balance between these extremes is crucial for building models that work in practice:\n\nUnderfitting: Model is too simple—misses important patterns (high training error, high test error)\nGood fit: Model captures real patterns—generalizes well (low training error, low test error)\n\nOverfitting: Model is too complex—memorizes noise (very low training error, high test error)\n\nLet’s create some examples to illustrate these concepts visually. So far in this course, we’ve focused on linear relationships—where the relationship between variables can be represented by a straight line. However, not all real-world scenarios are linear in nature. Sometimes there are curved, non-linear patterns in data where a straight line simply won’t capture the true relationship.\nWhen we encounter non-linear patterns, we can use more complex models to try to capture these curvatures and bends in the data. We’ll explore these advanced techniques in future chapters. For now, the plots below demonstrate how different model complexities handle non-linear data: we can create models that underfit (don’t capture the non-linear curvature), provide a good fit (capture the essential pattern), and overfit (add too much complexity and noise):\n\n\nShow code for underfitting, good fit, and overfitting examples\n# Import necessary libraries\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nimport warnings\n\n# Suppress numerical warnings for high-degree polynomial demonstration\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n# Create synthetic datasets to demonstrate different fitting scenarios\nnp.random.seed(42)\n\n# Generate underlying non-linear relationship\nX_demo = np.linspace(0, 10, 50).reshape(-1, 1)\ny_true = 2 * X_demo.ravel() + 0.5 * X_demo.ravel()**2 + np.random.normal(0, 8, 50)\n\n# Split the demonstration data\nX_demo_train, X_demo_test, y_demo_train, y_demo_test = train_test_split(\n    X_demo, y_true, test_size=0.3, random_state=42\n)\n\n# Create three models: underfitting, good fit, overfitting\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Model 1: Underfitting (linear model on non-linear data)\nlinear_model = LinearRegression()\nlinear_model.fit(X_demo_train, y_demo_train)\n\nX_smooth = np.linspace(0, 10, 200).reshape(-1, 1)\ny_linear = linear_model.predict(X_smooth)\n\ntrain_rmse_linear = root_mean_squared_error(y_demo_train, linear_model.predict(X_demo_train))\ntest_rmse_linear = root_mean_squared_error(y_demo_test, linear_model.predict(X_demo_test))\n\naxes[0].scatter(X_demo_train, y_demo_train, alpha=0.6, color='blue', label='Training', s=30)\naxes[0].scatter(X_demo_test, y_demo_test, alpha=0.6, color='green', label='Test', s=30)\naxes[0].plot(X_smooth, y_linear, color='red', linewidth=2, label='Linear Model')\naxes[0].set_title(f'Underfitting\\nTrain RMSE: {train_rmse_linear:.1f}, Test RMSE: {test_rmse_linear:.1f}')\naxes[0].set_xlabel('X')\naxes[0].set_ylabel('Y')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Model 2: Good fit (polynomial degree 2)\npoly_good = Pipeline([\n    ('poly', PolynomialFeatures(degree=2)),\n    ('linear', LinearRegression())\n])\npoly_good.fit(X_demo_train, y_demo_train)\n\ny_poly_good = poly_good.predict(X_smooth)\n\ntrain_rmse_good = root_mean_squared_error(y_demo_train, poly_good.predict(X_demo_train))\ntest_rmse_good = root_mean_squared_error(y_demo_test, poly_good.predict(X_demo_test))\n\naxes[1].scatter(X_demo_train, y_demo_train, alpha=0.6, color='blue', label='Training', s=30)\naxes[1].scatter(X_demo_test, y_demo_test, alpha=0.6, color='green', label='Test', s=30)\naxes[1].plot(X_smooth, y_poly_good, color='red', linewidth=2, label='Polynomial (degree 2)')\naxes[1].set_title(f'Good Fit\\nTrain RMSE: {train_rmse_good:.1f}, Test RMSE: {test_rmse_good:.1f}')\naxes[1].set_xlabel('X')\naxes[1].set_ylabel('Y')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Model 3: Overfitting (high-degree polynomial)\npoly_overfit = Pipeline([\n    ('poly', PolynomialFeatures(degree=15)),\n    ('linear', LinearRegression())\n])\npoly_overfit.fit(X_demo_train, y_demo_train)\n\ny_poly_overfit = poly_overfit.predict(X_smooth)\n\ntrain_rmse_overfit = root_mean_squared_error(y_demo_train, poly_overfit.predict(X_demo_train))\ntest_rmse_overfit = root_mean_squared_error(y_demo_test, poly_overfit.predict(X_demo_test))\n\naxes[2].scatter(X_demo_train, y_demo_train, alpha=0.6, color='blue', label='Training', s=30)\naxes[2].scatter(X_demo_test, y_demo_test, alpha=0.6, color='green', label='Test', s=30)\naxes[2].plot(X_smooth, y_poly_overfit, color='red', linewidth=2, label='Polynomial (degree 15)')\naxes[2].set_title(f'Overfitting\\nTrain RMSE: {train_rmse_overfit:.1f}, Test RMSE: {test_rmse_overfit:.1f}')\naxes[2].set_xlabel('X')\naxes[2].set_ylabel('Y')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAlthough we can visualize the models above and clearly see underfitting and overfitting happening, in most real-world cases we will not be able to visualize our models because we are working with many predictor variables. When you have 5, 10, or even hundreds of features, creating meaningful visualizations becomes impossible. Instead, we rely on the train and test evaluation metrics to point us to these problems—which is exactly what the RMSE comparison table below demonstrates.\n\n\nShow code for RMSE comparison table\n# Summary of RMSE comparison\nprint(\"RMSE Comparison:\")\nprint(f\"{'Model':&lt;25} {'Train RMSE':&lt;12} {'Test RMSE':&lt;12} {'Interpretation'}\")\nprint(f\"{'-'*70}\")\nprint(f\"{'Linear (Underfit)':&lt;25} {train_rmse_linear:&lt;12.1f} {test_rmse_linear:&lt;12.1f} {'Poor on both'}\")\nprint(f\"{'Polynomial-2 (Good)':&lt;25} {train_rmse_good:&lt;12.1f} {test_rmse_good:&lt;12.1f} {'Good on both'}\")\nprint(f\"{'Polynomial-15 (Overfit)':&lt;25} {train_rmse_overfit:&lt;12.1f} {test_rmse_overfit:&lt;12.1f} {'Great on train, poor on test'}\")\n\n\nRMSE Comparison:\nModel                     Train RMSE   Test RMSE    Interpretation\n----------------------------------------------------------------------\nLinear (Underfit)         8.5          9.6          Poor on both\nPolynomial-2 (Good)       7.2          7.2          Good on both\nPolynomial-15 (Overfit)   5.7          9.6          Great on train, poor on test\n\n\n\n\n\n\n\n\nImportantDiagnosing Model Fit with Train/Test Metrics\n\n\n\nThe key insight is that we can diagnose these scenarios by comparing training and test RMSE values. Underfitting shows poor performance on both sets because the model is too simple to capture the underlying pattern. Good fit shows similar, reasonably low error on both training and test sets. Overfitting shows excellent training performance but significantly worse test performance—the model has memorized the training data rather than learning generalizable patterns.\n\n\nTrain/test splits are just the first step in finding a well-fitting model. In future chapters, you’ll learn how to systematically iterate through different models, tune model parameters, and implement additional validation procedures like cross-validation to help identify the best-fitting model for your specific problem.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneHands-On: Train/Test Split Practice\n\n\n\nNow it’s your turn to apply proper train/test evaluation! You’ll use the full Advertising dataset you imported in the previous knowledge check to build and evaluate a multiple regression model.\nYour Tasks:\n\nSplit the data into train/test sets using a 70/30 split with random_state=42\nBuild a multiple regression model using all three predictor variables (TV, radio, newspaper) on the training data only\nEvaluate the model by calculating these metrics for both training and test sets:\n\nR² (R-squared)\nRMSE (Root Mean Squared Error)\nMAE (Mean Absolute Error)\n\nInterpret your results:\n\nIs your model overfitting, underfitting, or showing good generalization?\nWhat does the RMSE tell you about prediction accuracy in business terms?\nHow would you explain these results to a marketing manager planning next quarter’s advertising budget?\n\n\nBonus Challenge: Compare your multiple regression results with a simple model using only TV advertising. Which generalizes better to the test set?",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "22-regression-evaluation.html#business-alignment-connecting-model-and-business-performance",
    "href": "22-regression-evaluation.html#business-alignment-connecting-model-and-business-performance",
    "title": "22  Evaluating Regression Models",
    "section": "22.5 Business Alignment: Connecting Model and Business Performance",
    "text": "22.5 Business Alignment: Connecting Model and Business Performance\nAs we discussed in Chapter 20, successful machine learning requires aligning your model performance metrics (like RMSE, MAE, and R²) with your business performance metrics (like cost savings, revenue increase, and customer satisfaction). The goal isn’t just to build a model that generalizes well to new data—it’s to build a model that generalizes well to your specific business scenario and drives meaningful outcomes.\n\n\n\n\n\n\nflowchart LR\n  subgraph ML[Successful ML System]\n    direction BT\n    subgraph p0[Business Impact]\n    end\n    subgraph p1[Model Performance&lt;br/&gt;RMSE, MAE, R², MAPE]\n    end\n    subgraph p2[Business Performance&lt;br/&gt;Cost reduction, Revenue, Efficiency]\n    end\n\n   p1 --&gt; p0\n   p2 --&gt; p0\n  end\n\n\n\n\nFigure 22.2: Model evaluation success requires both technical performance and business impact alignment.\n\n\n\n\n\n\nThe Critical Connection: Why Metric Choice Matters\nThe evaluation metrics you choose directly influence how your model learns and what it optimizes for. This means your choice of evaluation metric can make or break your business outcomes. Consider these scenarios:\nScenario 1: Inventory Forecasting\n\nBusiness goal: Minimize total inventory costs (storage + stockouts)\nPoor metric choice: R² only—ignores the cost asymmetry between overstocking and understocking\nBetter alignment: MAPE or MAE—reflects proportional costs across different product values\nBusiness result: Model optimizes for error patterns that actually reduce operational costs\n\nScenario 2: Financial Risk Assessment\n\nBusiness goal: Prevent catastrophic losses while maintaining profitability\nPoor metric choice: MAE—treats small and large losses equally\nBetter alignment: RMSE—heavily penalizes the large errors that could bankrupt the business\nBusiness result: Model prioritizes avoiding devastating losses over minor improvements\n\nScenario 3: Customer Service Staffing\n\nBusiness goal: Match staffing levels to call volume for consistent service quality\nPoor metric choice: MAPE—percentage errors don’t reflect linear staffing costs\nBetter alignment: MAE—each additional call has roughly the same staffing cost\nBusiness result: Model predictions translate directly to operational planning\n\n\n\n\n\n\n\nTipA Framework for Metric-Business Alignment\n\n\n\nInstead of asking “What’s the best metric?” ask “What business outcomes am I trying to drive?” Then work backward:\n\nIdentify your business cost structure: Are errors linear, quadratic, or asymmetric in cost?\nMatch the mathematical properties: Choose metrics whose optimization aligns with your cost structure\nConsider stakeholder needs: Will decision-makers understand and trust the metric?\nTest the alignment: Verify that improving your chosen metric actually improves business outcomes\n\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneApplying Evaluation Metrics to Real Business Problems\n\n\n\nConsider these three business scenarios:\nScenario A: Restaurant Chain Food Cost Optimization A regional restaurant chain wants to predict daily food costs at each location to minimize waste while ensuring adequate supply. The business goal is to order the right amount of fresh ingredients each day—overordering leads to expensive food waste, while underordering results in menu items being unavailable and lost revenue. The cost of waste is roughly proportional to the amount ordered, and each dollar of prediction error translates directly to operational costs.\nScenario B: Insurance Premium Setting\nAn insurance company needs to predict individual claim amounts to set appropriate premiums while remaining competitive. The business objective is to avoid catastrophic underpricing—one severely underpriced high-risk customer could cost millions in claims, potentially wiping out profits from hundreds of correctly priced policies. Small pricing errors are manageable, but large errors can threaten the company’s financial stability.\nScenario C: Retail Sales Forecasting Across Categories A retail store wants to predict next month’s sales across different product categories (electronics, clothing, home goods) to optimize inventory purchasing and staffing. The business goal is to have consistent prediction accuracy across all categories—whether predicting $1,000 in accessory sales or $50,000 in electronics sales. The store needs to compare model performance across vastly different sales volumes to make unified business decisions.\nYour Tasks:\n\nFor each scenario, consider how each error metric (R², RMSE, MAE, MAPE) would be interpreted in the business context. What would each metric tell the decision-makers? Based on this analysis, do you think one metric would be more preferred over the others? Explain your reasoning.\nTraining vs Test Performance: If Scenario B showed these results, what would you conclude?\n\nTraining RMSE: $850\nTest RMSE: $1,200\nTraining R²: 0.78\nTest R²: 0.52",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "22-regression-evaluation.html#summary",
    "href": "22-regression-evaluation.html#summary",
    "title": "22  Evaluating Regression Models",
    "section": "22.6 Summary",
    "text": "22.6 Summary\nThis chapter equipped you with essential tools for evaluating regression model performance and understanding whether your models are ready for real-world deployment. You learned that building a model is only the beginning—proper evaluation determines whether that model will actually help or hurt your business decisions.\nKey evaluation concepts you mastered include:\n\nR² as a measure of overall model fit, representing the proportion of variation your model explains\nError metrics (MSE, RMSE, MAE, MAPE) that quantify prediction accuracy in different ways\nTrain/test splits for honest evaluation that simulates real-world model deployment\nOverfitting vs underfitting and why models must balance complexity with generalizability\nBusiness-aligned metric selection based on the relative costs of different types of prediction errors\n\nThe critical insight is that model evaluation must align with business context. A model that minimizes RMSE might be perfect for financial risk management but inappropriate for inventory planning. Understanding when and why to use different metrics ensures your models support rather than undermine business objectives.\nLooking ahead: The evaluation techniques you’ve learned here apply to every machine learning algorithm you’ll encounter. Whether you’re building decision trees, neural networks, or ensemble methods, the fundamental principles of train/test splits, metric selection, and generalization remain constant. In future chapters, we’ll also learn about additional evaluation metrics designed specifically for classification models (like accuracy, precision, and recall) and unsupervised models (like silhouette scores and inertia). In the next chapters, you’ll explore more sophisticated modeling techniques, but you’ll always return to these evaluation fundamentals to determine which models work best for your specific business challenges.\n\nQuick Reference: Regression Evaluation Metrics\n\n\n\n\n\n\n\n\n\nConcept\nDescription\nScikit-Learn Function\nWhen to Use\n\n\n\n\nSum of Squared Errors (SSE)\nTotal squared differences between actual and predicted values\nManual calculation: np.sum((y_actual - y_pred)**2)\nUnderstanding algorithm optimization\n\n\nR² (R-squared)\nProportion of variance in target variable explained by model\nr2_score(y_true, y_pred) or model.score(X, y)\nOverall model goodness-of-fit\n\n\nMean Squared Error (MSE)\nAverage of squared prediction errors\nmean_squared_error(y_true, y_pred)\nWhen large errors are costly\n\n\nRoot Mean Squared Error (RMSE)\nSquare root of MSE, in same units as target\nroot_mean_squared_error(y_true, y_pred)\nInterpretable error magnitude\n\n\nMean Absolute Error (MAE)\nAverage absolute difference between actual and predicted\nmean_absolute_error(y_true, y_pred)\nWhen all errors have equal cost\n\n\nMean Absolute Percentage Error (MAPE)\nAverage percentage error relative to actual values\nmean_absolute_percentage_error(y_true, y_pred)\nComparing across different scales\n\n\nTrain/Test Split\nDividing data for training and evaluation\ntrain_test_split(X, y, test_size=0.3, random_state=42)\nHonest model evaluation\n\n\nTime Series Split\nTemporal data splitting for time-based problems\nTimeSeriesSplit(n_splits=5)\nTime-dependent data",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "22-regression-evaluation.html#end-of-chapter-exercise",
    "href": "22-regression-evaluation.html#end-of-chapter-exercise",
    "title": "22  Evaluating Regression Models",
    "section": "22.7 End of Chapter Exercise",
    "text": "22.7 End of Chapter Exercise\nThese exercises align with the regression modeling from Chapter 21 but now focus on proper evaluation techniques. You’ll work with the same three datasets from ISLP, applying the train/test split methodology and error metrics you’ve learned to assess model performance and business readiness.\n\n\n\n\n\n\nNoneExercise 1: Credit Risk Analysis with Evaluation\n\n\n\n\n\nCompany: A regional bank\nGoal: Understand what drives customers’ credit card balances to inform risk management and marketing strategies, with proper model evaluation\nDataset: Credit dataset from ISLP package\n\nfrom ISLP import load_data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error\n\nCredit = load_data('Credit')\nprint(\"Credit dataset loaded\")\nprint(Credit.head())\n\nCredit dataset loaded\n   ID   Income  Limit  Rating  Cards  Age  Education  Gender Student Married  \\\n0   1   14.891   3606     283      2   34         11    Male      No     Yes   \n1   2  106.025   6645     483      3   82         15  Female     Yes     Yes   \n2   3  104.593   7075     514      4   71         11    Male      No      No   \n3   4  148.924   9504     681      3   36         11  Female      No      No   \n4   5   55.882   4897     357      2   68         16    Male      No     Yes   \n\n   Ethnicity  Balance  \n0  Caucasian      333  \n1      Asian      903  \n2      Asian      580  \n3      Asian      964  \n4  Caucasian      331  \n\n\nYour Tasks:\n\nSplit the data into training (70%) and test (30%) sets using random_state=42\nBuild a regression model predicting Balance using Income, Limit, Age, and Gender (remember to dummy encode Gender)\nCalculate all error metrics (R², RMSE, MAE, MAPE) for both training and test sets\nEvaluate generalization: Is your model overfitting, underfitting, or showing good generalization? Compare training vs test performance\nBusiness interpretation: What does the RMSE tell you about prediction accuracy in dollar terms? If the bank uses this model to set credit limits, what’s the practical meaning of your error metrics?\n\n\n\n\n\n\n\n\n\n\nNoneExercise 2: Baseball Salary Analysis with Evaluation\n\n\n\n\n\nCompany: A professional baseball team\nGoal: Better understand the drivers of player salaries to inform contract negotiations, with rigorous model evaluation\nDataset: Hitters dataset from ISLP package\n\nHitters = load_data('Hitters')\nprint(\"Hitters dataset loaded\")\nprint(Hitters.head())\n\n# Note: You'll need to handle missing values in the Salary column\n\nHitters dataset loaded\n   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n0    293    66      1    30   29     14      1     293     66       1     30   \n1    315    81      7    24   38     39     14    3449    835      69    321   \n2    479   130     18    66   72     76      3    1624    457      63    224   \n3    496   141     20    65   78     37     11    5628   1575     225    828   \n4    321    87     10    39   42     30      2     396    101      12     48   \n\n   CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague  \n0    29      14      A        E      446       33      20     NaN         A  \n1   414     375      N        W      632       43      10   475.0         N  \n2   266     263      A        W      880       82      14   480.0         A  \n3   838     354      N        E      200       11       3   500.0         N  \n4    46      33      N        E      805       40       4    91.5         N  \n\n\n\n\n\n\n\n\nData Cleaning Hint: The Hitters dataset contains some missing values in the Salary column. You may need to remove rows with missing salary data before fitting your regression model. Consider using dropna() or similar methods to clean the data first.\n\n\n\nYour Tasks:\n\nClean the data by removing rows with missing salary values\nSplit the data into training (70%) and test (30%) sets using random_state=42\nBuild a regression model predicting Salary using Years (experience), Hits (recent batting performance), and League (dummy encode this categorical variable)\nCalculate all error metrics (R², RMSE, MAE, MAPE) for both training and test sets\nAssess model reliability: How well does your model generalize? What does the RMSE mean in terms of salary prediction accuracy?\nBusiness application: If you were a player agent, how would you use these results to negotiate contracts? What are the limitations of your model’s predictions?\n\n\n\n\n\n\n\n\n\n\nNoneExercise 3: College Application Analysis with Evaluation\n\n\n\n\n\nCompany: Higher education consulting firm\nGoal: Analyze what factors drive the number of applications a college receives, with proper evaluation for advisory recommendations\nDataset: College dataset from ISLP package\n\nCollege = load_data('College')\nprint(\"College dataset loaded\")\nprint(College.head())\n\nCollege dataset loaded\n  Private  Apps  Accept  Enroll  Top10perc  Top25perc  F.Undergrad  \\\n0     Yes  1660    1232     721         23         52         2885   \n1     Yes  2186    1924     512         16         29         2683   \n2     Yes  1428    1097     336         22         50         1036   \n3     Yes   417     349     137         60         89          510   \n4     Yes   193     146      55         16         44          249   \n\n   P.Undergrad  Outstate  Room.Board  Books  Personal  PhD  Terminal  \\\n0          537      7440        3300    450      2200   70        78   \n1         1227     12280        6450    750      1500   29        30   \n2           99     11250        3750    400      1165   53        66   \n3           63     12960        5450    450       875   92        97   \n4          869      7560        4120    800      1500   76        72   \n\n   S.F.Ratio  perc.alumni  Expend  Grad.Rate  \n0       18.1           12    7041         60  \n1       12.2           16   10527         56  \n2       12.9           30    8735         54  \n3        7.7           37   19016         59  \n4       11.9            2   10922         15  \n\n\nYour Tasks:\n\nSplit the data into training (70%) and test (30%) sets using random_state=42\nBuild a regression model predicting Apps (applications received) using Top10perc (percent of students from top 10% of high school class), Outstate (out-of-state tuition), and Private (dummy encode this categorical variable)\nCalculate all error metrics (R², RMSE, MAE, MAPE) for both training and test sets\nEvaluate model performance: Does your model show good generalization? What do the error metrics tell you about prediction reliability?\nBusiness context interpretation: What does the RMSE mean in terms of application prediction accuracy? If you’re advising college presidents on strategic decisions, how confident should they be in your model’s predictions?\nStrategic recommendations: Based on your model’s coefficients and performance metrics, what strategies would you recommend to increase applications? What are the limitations and risks of using this model for decision-making?\nMetric selection: Which error metric (R², RMSE, MAE, or MAPE) would be most useful for college administrators? Why?",
    "crumbs": [
      "Module 8",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "23-logistic-regression.html",
    "href": "23-logistic-regression.html",
    "title": "23  Introduction to Logistic Regression for Classification",
    "section": "",
    "text": "23.1 From Regression to Classification\nIn business, not every question is about predicting numbers. While regression helps us answer questions like “How much revenue will we generate?” or “What price should we set?”, many critical business decisions involve predicting categories:\nThis chapter introduces logistic regression, the foundational algorithm for classification problems—predicting which category or class an observation belongs to. While linear regression predicts continuous values, logistic regression predicts probabilities and categories, making it perfect for yes/no, approve/deny, and other categorical business decisions.\nBy the end of this chapter, you will be able to:\nYou’ve mastered linear regression for predicting continuous outcomes like sales revenue, house prices, or customer lifetime value. But what happens when your business outcome is categorical rather than numeric? This is where we transition from regression problems to classification problems.",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to Logistic Regression for Classification</span>"
    ]
  },
  {
    "objectID": "23-logistic-regression.html#from-regression-to-classification",
    "href": "23-logistic-regression.html#from-regression-to-classification",
    "title": "23  Introduction to Logistic Regression for Classification",
    "section": "",
    "text": "The Fundamental Difference\nThe key distinction lies in what you’re trying to predict:\n\nRegression: Predicts continuous numerical values (any value within a range)\n\n“How much will this customer spend next month?” → $0 to $10,000+\n“What will our quarterly revenue be?” → Any positive dollar amount\n\nClassification: Predicts discrete categories or classes (specific labels from a predefined set)\n\n“Will this customer default on their loan?” → Yes or No\n“Is this transaction fraudulent?” → Fraudulent or Legitimate\n\n\n\n\nWhy Linear Regression Doesn’t Work for Classification\nLet’s explore this with a concrete business example. Imagine you work for a credit card company trying to predict customer default based on account balance. You have historical data showing whether customers defaulted (1 = Yes, 0 = No) based on their credit card balance.\n\n\nShow code for creating sample data\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Create sample credit default data\nnp.random.seed(42)\nbalances = np.linspace(0, 3000, 100)\n# Higher balances increase default probability\nprobabilities = 1 / (1 + np.exp(-(balances - 1500) / 300))\ndefaults = np.random.binomial(1, probabilities)\n\ndefault_data = pd.DataFrame({\n    'balance': balances,\n    'default': defaults\n})\n\nprint(\"Sample of credit default data:\")\nprint(default_data.head(10))\n\n\nSample of credit default data:\n      balance  default\n0    0.000000        0\n1   30.303030        0\n2   60.606061        0\n3   90.909091        0\n4  121.212121        0\n5  151.515152        0\n6  181.818182        0\n7  212.121212        0\n8  242.424242        0\n9  272.727273        0\n\n\nIf we naively apply linear regression to this classification problem, we run into several critical issues:\n\nPredictions outside valid range: Linear regression can predict values like -0.3 or 1.8, but probabilities must be between 0 and 1\nStraight line assumption: Real-world classification relationships often follow S-shaped curves, not straight lines\nNo probability interpretation: A prediction of 0.7 from linear regression doesn’t clearly represent a 70% probability\n\n\n\nShow code for demonstrating linear regression problems\n# Try linear regression on classification data\nX = default_data[['balance']]\ny = default_data['default']\n\nlinear_model = LinearRegression()\nlinear_model.fit(X, y)\nlinear_predictions = linear_model.predict(X)\n\n# Visualize the problem\nplt.figure(figsize=(10, 6))\nplt.scatter(default_data['balance'], default_data['default'], alpha=0.6, label='Actual data')\nplt.plot(default_data['balance'], linear_predictions, color='red', linewidth=2, label='Linear regression')\nplt.xlabel('Credit Card Balance ($)')\nplt.ylabel('Default (0=No, 1=Yes)')\nplt.title('Why Linear Regression Fails for Classification')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Highlight the problems\nplt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\nplt.axhline(y=1, color='black', linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nThese classification challenges are incredibly common across industries, affecting everything from financial services and healthcare to marketing and e-commerce.\nBinary Classification (2 categories):\n\nEmail: Spam vs. Not Spam\nLoan Applications: Approve vs. Deny\n\nMedical Diagnosis: Disease vs. No Disease\nMarketing Response: Will Respond vs. Won’t Respond\n\nMulti-class Classification (3+ categories):\n\nCustomer Satisfaction: Low, Medium, High\nProduct Categories: Electronics, Clothing, Home & Garden\nRisk Levels: Low Risk, Medium Risk, High Risk\n\nFortunately, machine learning offers sophisticated algorithms specifically designed to address the critical concerns we identified with linear regression—algorithms that naturally handle the 0-1 probability constraint, capture non-linear S-shaped relationships, and provide meaningful probability interpretations. This chapter introduces logistic regression, a fundamental algorithm engineered specifically for binary classification problems (predicting between two categories). While more advanced algorithms exist for multi-class classification scenarios (predicting among three or more categories), mastering binary classification with logistic regression provides the essential foundation for understanding all classification approaches in machine learning.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneRegression vs. Classification Practice\n\n\n\nFor each business scenario below, determine whether this is a regression or classification problem:\n\nPredicting how many units of a product will sell next month\nDetermining whether a customer will purchase a premium subscription (Yes/No)\nEstimating the dollar amount of insurance claims\nClassifying customer support tickets as “Technical”, “Billing”, or “General”\nForecasting next quarter’s revenue in dollars\nDeciding whether to show a customer a promotional offer (Show/Don’t Show)\n\nFor each scenario, explain your reasoning: What clues helped you identify the problem type? What would the model output look like?",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to Logistic Regression for Classification</span>"
    ]
  },
  {
    "objectID": "23-logistic-regression.html#understanding-logistic-regression",
    "href": "23-logistic-regression.html#understanding-logistic-regression",
    "title": "23  Introduction to Logistic Regression for Classification",
    "section": "23.2 Understanding Logistic Regression",
    "text": "23.2 Understanding Logistic Regression\nLogistic regression solves the classification problem by transforming the linear regression approach with a mathematical function that constrains predictions to valid probability ranges and creates the S-shaped curve that better represents real-world classification relationships.\n\nThe Logistic Function: Creating Probabilities\nThe key innovation of logistic regression is the logistic function (also called the sigmoid function), which transforms any real number into a value between 0 and 1—perfect for representing probabilities.\n\\[p = \\frac{1}{1 + e^{-z}}\\]\nWhere \\(z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k\\) (the linear combination of features)\nLet’s visualize how this function creates the characteristic S-shaped curve:\n\n\nShow code for demonstrating the logistic function\n# Demonstrate the logistic function\nz_values = np.linspace(-6, 6, 100)\nprobabilities = 1 / (1 + np.exp(-z_values))\n\nplt.figure(figsize=(10, 6))\nplt.plot(z_values, probabilities, linewidth=3, color='blue')\nplt.xlabel('z (linear combination: β₀ + β₁x₁ + β₂x₂ + ...)')\nplt.ylabel('Probability')\nplt.title('The Logistic Function: Transforming Linear Predictions to Probabilities')\nplt.grid(True, alpha=0.3)\n\n# Highlight key points\nplt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='50% probability threshold')\nplt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\nplt.axhline(y=1, color='black', linestyle='-', alpha=0.3)\nplt.legend()\n\n# Add annotations\nplt.annotate('Approaches 0\\n(Very Low Probability)', xy=(-4, 0.02), xytext=(-5, 0.2),\n            arrowprops=dict(arrowstyle='-&gt;', color='gray'), fontsize=10)\nplt.annotate('Approaches 1\\n(Very High Probability)', xy=(4, 0.98), xytext=(3, 0.8),\n            arrowprops=dict(arrowstyle='-&gt;', color='gray'), fontsize=10)\nplt.annotate('50% Decision\\nBoundary', xy=(0, 0.5), xytext=(1, 0.6),\n            arrowprops=dict(arrowstyle='-&gt;', color='red'), fontsize=10)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote🎥 Video Spotlight: Logistic Regression Explained\n\n\n\nA simple introduction to logistic regression to help reinforce the concepts you just read!\n\n\n\n\n\nKey Properties of the Logistic Function\nThe logistic function has several properties that make it perfect for classification:\n\nAlways between 0 and 1: No matter what the linear combination z equals, the output is always a valid probability\nS-shaped curve: Captures the realistic relationship where small changes have big effects near the decision boundary\nSmooth transitions: Provides gradual probability changes rather than abrupt jumps\nSymmetric around 0.5: When z = 0, the probability equals exactly 50%\n\n\n\nFrom Probabilities to Odds to Log-Odds\nTo fully understand logistic regression coefficients, we need to explore three related concepts: probabilities, odds, and log-odds.\nProbability is what we’re most familiar with—the chance something will happen, expressed as a value between 0 and 1:\n\nProbability of default = 0.2 means 20% chance of defaulting\nProbability of customer purchase = 0.75 means 75% chance of buying the product\n\nOdds express the ratio of the probability something happens to the probability it doesn’t happen: \\[\\text{Odds} = \\frac{p}{1-p}\\]\n\n\n\nProbability\nOdds\nInterpretation\n\n\n\n\n0.10\n0.11\n10% chance (1 in 10)\n\n\n0.20\n0.25\n20% chance (1 in 5)\n\n\n0.50\n1.00\n50% chance (1 in 2)\n\n\n0.80\n4.00\n80% chance (4 in 5)\n\n\n0.90\n9.00\n90% chance (9 in 10)\n\n\n\nLog-odds (also called logit) is the natural logarithm of the odds: \\[\\text{Log-odds} = \\ln\\left(\\frac{p}{1-p}\\right)\\]\nLog-odds might seem abstract at first, but they serve a crucial mathematical purpose in logistic regression. While probabilities are constrained between 0 and 1, and odds range from 0 to infinity, log-odds can take any value from negative infinity to positive infinity. This unbounded range makes log-odds perfect for the linear combination part of logistic regression (the \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\) portion).\nHere’s how all three concepts relate to each other:\n\n\n\nProbability\nOdds\nLog-odds\nBusiness Interpretation\n\n\n\n\n0.10\n0.11\n-2.20\nVery unlikely event (10% chance)\n\n\n0.20\n0.25\n-1.39\nUnlikely event (20% chance)\n\n\n0.50\n1.00\n0.00\nNeutral/uncertain (50-50 chance)\n\n\n0.80\n4.00\n1.39\nLikely event (80% chance)\n\n\n0.90\n9.00\n2.20\nVery likely event (90% chance)\n\n\n\nNotice how:\n\nNegative log-odds indicate events are less likely than not (probability &lt; 0.5)\nZero log-odds indicates equal likelihood (probability = 0.5, odds = 1:1)\nPositive log-odds indicate events are more likely than not (probability &gt; 0.5)\n\n\n\nInterpreting Logistic Regression Coefficients\nIn logistic regression, coefficients represent the change in log-odds for a one-unit increase in the predictor variable. While this sounds complex, there’s a practical interpretation:\n\nPositive coefficient: Increases the odds of the outcome (makes it more likely)\nNegative coefficient: Decreases the odds of the outcome (makes it less likely)\nMagnitude: Larger absolute values indicate stronger effects\n\nWe’ll see concrete examples of coefficient interpretation in the next section when we build our first logistic regression model.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneUnderstanding Probabilities and Odds\n\n\n\nLet’s practice converting between probabilities and odds:\n\nIf a customer has a 30% probability of purchasing a product:\n\nWhat are the odds of purchasing?\nHow would you explain this to a business manager?\n\nIf the odds of loan default are 1:4 (1 to 4):\n\nWhat’s the probability of default?\nWhat’s the probability of no default?\n\nBusiness interpretation: If a marketing campaign has a 75% success probability:\n\nExpress this as odds\nIf you ran this campaign 100 times, how many successes would you expect?",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to Logistic Regression for Classification</span>"
    ]
  },
  {
    "objectID": "23-logistic-regression.html#implementing-logistic-regression-with-scikit-learn",
    "href": "23-logistic-regression.html#implementing-logistic-regression-with-scikit-learn",
    "title": "23  Introduction to Logistic Regression for Classification",
    "section": "23.3 Implementing Logistic Regression with Scikit-learn",
    "text": "23.3 Implementing Logistic Regression with Scikit-learn\nNow let’s apply logistic regression to a real business problem using the Default dataset from ISLP. This dataset contains information about credit card customers and whether they defaulted on their payments—a classic binary classification problem that banks face every day.\n\nLoading and Exploring the Default Dataset\nThe Default dataset provides the perfect introduction to logistic regression because it’s realistic, interpretable, and demonstrates clear business value. Let’s start by loading and understanding our data:\n\n# Load the Default dataset from ISLP\nfrom ISLP import load_data\n\nDefault = load_data('Default')\nprint(\"Default dataset shape:\", Default.shape)\nprint(\"\\nFirst few rows:\")\nprint(Default.head())\n\nDefault dataset shape: (10000, 4)\n\nFirst few rows:\n  default student      balance        income\n0      No      No   729.526495  44361.625074\n1      No     Yes   817.180407  12106.134700\n2      No      No  1073.549164  31767.138947\n3      No      No   529.250605  35704.493935\n4      No      No   785.655883  38463.495879\n\n\n\n# Explore the target variable\nprint(\"Default distribution:\")\nprint(Default['default'].value_counts())\nprint(f\"\\nDefault rate: {Default['default'].value_counts(normalize=True)['Yes']:.1%}\")\n\n# Summary statistics by default status\nprint(\"\\nSummary by default status:\")\nprint(Default.groupby('default', observed=False)[['balance', 'income']].mean().round(0))\n\nDefault distribution:\ndefault\nNo     9667\nYes     333\nName: count, dtype: int64\n\nDefault rate: 3.3%\n\nSummary by default status:\n         balance   income\ndefault                  \nNo         804.0  33566.0\nYes       1748.0  32089.0\n\n\nThe dataset contains 10,000 credit card customers with four key variables:\n\ndefault: Whether the customer defaulted (Yes/No) - our target variable\nbalance: Average credit card balance in dollars\nincome: Annual income in dollars\n\nstudent: Whether the customer is a student (Yes/No)\n\n\n\nPreparing Data for Logistic Regression\nBefore building our model, we need to prepare the data by encoding categorical variables and setting up our features and target. Machine learning algorithms like logistic regression work with numbers, not text categories. We have two categorical variables that need conversion:\n\nstudent variable: Currently “Yes”/“No” text values need to become 0/1 numbers. We use dummy encoding for the student variable, which creates a new binary column (student_Yes) where 1 means “is a student” and 0 means “not a student”.\ndefault variable: Currently “Yes”/“No” text values need to become 0/1 for our target variable. Logistic regression specifically requires the target variable to be binary integers where 0 represents the “negative” class (No default) and 1 represents the “positive” class (Yes default). The .astype(int) conversion ensures we get clean integer values rather than boolean True/False, which is important for consistent model training and prediction interpretation.\n\n\n# Prepare the data for modeling\n# Convert categorical variables to numeric\nDefault_encoded = pd.get_dummies(Default, columns=['student'], drop_first=True)\nDefault_encoded['default_binary'] = (Default_encoded['default'] == 'Yes').astype(int)\n\nprint(\"Encoded dataset:\")\nprint(Default_encoded.head())\n\n# Define features and target\nX = Default_encoded[['balance', 'income', 'student_Yes']]\ny = Default_encoded['default_binary']\n\nprint(f\"\\nFeature matrix shape: {X.shape}\")\nprint(f\"Target variable shape: {y.shape}\")\nprint(f\"Default rate in our target: {y.mean():.1%}\")\n\nEncoded dataset:\n  default      balance        income  student_Yes  default_binary\n0      No   729.526495  44361.625074        False               0\n1      No   817.180407  12106.134700         True               0\n2      No  1073.549164  31767.138947        False               0\n3      No   529.250605  35704.493935        False               0\n4      No   785.655883  38463.495879        False               0\n\nFeature matrix shape: (10000, 3)\nTarget variable shape: (10000,)\nDefault rate in our target: 3.3%\n\n\n\n\nBuilding Our First Logistic Regression Model\nLet’s start with a simple model using just the balance variable to predict default, then expand to include additional features.\nSimilar to linear regression, building a logistic regression model in scikit-learn follows the same familiar pattern: we use the LogisticRegression() class and call .fit() to train our model on the features and target variable.\n\n# Simple logistic regression with balance only\nX_simple = Default_encoded[['balance']]\n\n# Fit the logistic regression model\nlog_reg_simple = LogisticRegression(random_state=42)\nlog_reg_simple.fit(X_simple, y)\n\nLogisticRegression(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \n42\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n100\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\n\nNoteAbout Runtime Warnings\n\n\n\n\n\nWhen you run the logistic regression code, you may see some RuntimeWarnings about “divide by zero” or “overflow encountered” during the mathematical computations. This is normal and doesn’t affect your results!\nThese warnings occur because logistic regression uses iterative optimization algorithms that sometimes encounter numerical precision issues during the fitting process. While there are parameters you can adjust to minimize these warnings, we’re not concerned about them for learning purposes—the model still trains correctly and produces valid results.\nWe’ve included random_state=42 to ensure everyone gets the same reproducible results, which is helpful when following along with the examples.\n\n\n\nNow we’ll extract the intercept and coefficient parameters from our trained model to understand the mathematical relationship it has learned.\n\n# Extract model components\nintercept = log_reg_simple.intercept_[0]\nbalance_coef = log_reg_simple.coef_[0][0]\n\nprint(\"Simple Logistic Regression Results:\")\nprint(f\"Intercept: {intercept:.4f}\")\nprint(f\"Balance coefficient: {balance_coef:.6f}\")\n\n# Interpretation\nprint(f\"\\nModel interpretation:\")\nprint(f\"Log-odds equation: log-odds = {intercept:.4f} + {balance_coef:.6f} × balance\")\n\nSimple Logistic Regression Results:\nIntercept: -10.6513\nBalance coefficient: 0.005499\n\nModel interpretation:\nLog-odds equation: log-odds = -10.6513 + 0.005499 × balance\n\n\n\n\n\n\n\n\nCautionDon’t Worry About the Numbers Yet!\n\n\n\nThe intercept and coefficient values shown above might look confusing right now—that’s completely normal! We’ll explain exactly what these numbers mean and how to interpret them in the next section.\n\n\n\n\nUnderstanding the Model Equation\nOur logistic regression model creates the following equation:\n\\[\\text{Probability of Default} = \\frac{1}{1 + e^{-(-10.6513 + 0.005499 \\times \\text{Balance})}}\\]\nThe coefficients tell us:\n\nIntercept (-10.6513): When balance = $0, the log-odds of default are very negative (which means a very low probability of defaulting)\nBalance coefficient (0.005499): For each $1 increase in credit card balance, the log-odds of default increase by 0.005499. While this seems like a small number, think of it cumulatively—a $1,000 increase in balance would increase the log-odds by 5.499, which translates to a substantial increase in default probability. The positive coefficient confirms our business intuition: higher balances are associated with higher default risk.\n\nLet’s visualize how this creates the S-shaped probability curve and understand what it means for business decision-making:\nThe S-shaped curve is crucial for understanding customer risk profiles. At low balances, the probability stays near zero (safe customers), then transitions through a “danger zone” where small balance increases create large probability jumps (customers moving from low-risk to high-risk), and finally levels off at high balances where most customers are likely to default. This shape helps credit managers identify which balance ranges require the most attention and where interventions might be most effective.\n\n\nShow code for visualizing our model’s S-shaped probability curve\n# Create visualization of logistic regression fit\nbalance_range = np.linspace(0, 3000, 100)\nX_viz = pd.DataFrame({'balance': balance_range})\n\n# Get probability predictions\nprobabilities = log_reg_simple.predict_proba(X_viz)[:, 1]  # Probability of default (class 1)\n\nplt.figure(figsize=(8, 8))\n\n# Top plot: Actual data with fitted curve\nplt.subplot(2, 1, 1)\ndefault_yes = Default[Default['default'] == 'Yes']\ndefault_no = Default[Default['default'] == 'No']\n\nplt.scatter(default_no['balance'], [0]*len(default_no), alpha=0.4, label='No Default', color='blue')\nplt.scatter(default_yes['balance'], [1]*len(default_yes), alpha=0.4, label='Default', color='red')\nplt.plot(balance_range, probabilities, color='green', linewidth=3, label='Logistic Regression')\nplt.xlabel('Credit Card Balance ($)')\nplt.ylabel('Probability of Default')\nplt.title('Logistic Regression: Balance vs. Default Probability')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Bottom plot: Zoomed view of probability curve\nplt.subplot(2, 1, 2)\nplt.plot(balance_range, probabilities, color='green', linewidth=3)\nplt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='50% threshold')\nplt.xlabel('Credit Card Balance ($)')\nplt.ylabel('Probability of Default')\nplt.title('Probability Curve Detail')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMaking Predictions with Logistic Regression\nOne of the key advantages of logistic regression is that it provides both probability estimates and binary classifications. Let’s demonstrate this with a range of balance values to see how probabilities increase with balance and how the 0.5 threshold determines the final classification:\nFirst, let’s create some example balance amounts and see what different prediction methods return:\n\n# Make predictions for specific balance amounts to show the progression\nexample_balances = pd.DataFrame({'balance': [500, 1000, 1500, 2000, 2500, 3000]})\n\n# Get probability predictions - returns probabilities for both classes\nprobabilities = log_reg_simple.predict_proba(example_balances)\nprint(\"predict_proba() output (columns: [No Default, Default]):\")\nprint(probabilities.round(4))\n\n# Get binary classifications - returns 0 or 1 based on 50% threshold\nclassifications = log_reg_simple.predict(example_balances)\nprint(f\"\\npredict() output (0=No Default, 1=Default):\")\nprint(classifications)\n\npredict_proba() output (columns: [No Default, Default]):\n[[9.996e-01 4.000e-04]\n [9.942e-01 5.800e-03]\n [9.171e-01 8.290e-02]\n [4.142e-01 5.858e-01]\n [4.330e-02 9.567e-01]\n [2.900e-03 9.971e-01]]\n\npredict() output (0=No Default, 1=Default):\n[0 0 0 1 1 1]\n\n\nNow let’s organize this information into a clear table that puts these predictions into business context:\n\n# Extract just the default probabilities (column 1) for our table\nprob_default = probabilities[:, 1]\n\n# Create a comprehensive results table\nprediction_results = pd.DataFrame({\n    'Balance': example_balances['balance'],\n    'Probability_of_Default': prob_default,\n    'Predicted_Class': classifications,\n    'Business_Interpretation': [\n        'Very low risk - safe customer',\n        'Low risk - monitor balance growth',\n        'Moderate risk - consider credit limit review',\n        'High risk - proactive intervention recommended',\n        'Very high risk - immediate attention required',\n        'Extremely high risk - consider account restrictions'\n    ]\n})\n\nprint(\"Prediction Examples:\")\nprediction_results.round(4)\n\nPrediction Examples:\n\n\n\n\n\n\n\n\n\nBalance\nProbability_of_Default\nPredicted_Class\nBusiness_Interpretation\n\n\n\n\n0\n500\n0.0004\n0\nVery low risk - safe customer\n\n\n1\n1000\n0.0058\n0\nLow risk - monitor balance growth\n\n\n2\n1500\n0.0829\n0\nModerate risk - consider credit limit review\n\n\n3\n2000\n0.5858\n1\nHigh risk - proactive intervention recommended\n\n\n4\n2500\n0.9567\n1\nVery high risk - immediate attention required\n\n\n5\n3000\n0.9971\n1\nExtremely high risk - consider account restric...\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteManual Probability Calculation Verification\n\n\n\n\n\nLet’s verify our scikit-learn predictions by calculating probabilities manually using the logistic regression equation:\n\n# Manual probability calculation for $1,500 balance using our equation\nmanual_calc = 1 / (1 + np.exp(-(intercept + balance_coef * 1500)))\nprint(f\"Manual calculation for $1,500 balance: {manual_calc:.4f}\")\nprint(f\"Scikit-learn prediction for $1,500 balance: {prob_default[2]:.4f}\")\n\n# The values match, confirming our understanding of the logistic function!\n\nManual calculation for $1,500 balance: 0.0829\nScikit-learn prediction for $1,500 balance: 0.0829\n\n\n\n\n\n\n\nMultiple Predictor Logistic Regression\nNow let’s build a more comprehensive model using all available features. First, let’s remind ourselves what features we have in our feature matrix X:\n\n# Show the features we'll use in our multiple regression model\nprint(\"Features in our model:\")\nprint(X.columns.tolist())\nprint(f\"\\nFeature matrix shape: {X.shape}\")\nprint(f\"Sample of feature data:\")\nprint(X.head())\n\nFeatures in our model:\n['balance', 'income', 'student_Yes']\n\nFeature matrix shape: (10000, 3)\nSample of feature data:\n       balance        income  student_Yes\n0   729.526495  44361.625074        False\n1   817.180407  12106.134700         True\n2  1073.549164  31767.138947        False\n3   529.250605  35704.493935        False\n4   785.655883  38463.495879        False\n\n\nNow we’ll build and train our multiple logistic regression model:\n\n# Multiple logistic regression with all features\nlog_reg_multiple = LogisticRegression(random_state=42)\nlog_reg_multiple.fit(X, y)\n\n# Extract coefficients\nintercept_multi = log_reg_multiple.intercept_[0]\ncoefficients = log_reg_multiple.coef_[0]\n\nprint(\"Multiple Logistic Regression Results:\")\nprint(f\"Intercept: {intercept_multi:.6f}\")\nprint(f\"Balance coefficient: {coefficients[0]:.6f}\")\nprint(f\"Income coefficient: {coefficients[1]:.6f}\")\nprint(f\"Student coefficient: {coefficients[2]:.6f}\")\n\nMultiple Logistic Regression Results:\nIntercept: -10.901812\nBalance coefficient: 0.005731\nIncome coefficient: 0.000004\nStudent coefficient: -0.612565\n\n\nInterpreting the Coefficients in Business Context:\nEach coefficient tells us how a one-unit increase in that feature affects the log-odds of default, holding all other features constant. Here’s how to interpret them:\n\nBalance coefficient (positive): Higher credit card balances increase default risk, which aligns with our business intuition—customers with higher outstanding debt are more likely to struggle with payments.\nIncome coefficient (very small positive): Higher income has a negligible effect on default risk—the coefficient is essentially zero (0.000004). This suggests that once we account for balance, income doesn’t add much predictive power, which makes sense since balance already captures much of the financial stress that income would indicate.\nStudent coefficient (negative): Being a student decreases default risk (-0.612565). This might seem surprising, but it suggests that students, holding balance and income constant, are actually less likely to default. This could be because students may have family support, are more careful with debt, or have better future income prospects that make them more reliable borrowers.\n\nThe intercept represents the baseline log-odds of default when all features equal zero (which isn’t practically meaningful since we can’t have zero income, but it’s mathematically necessary for the model).\n\n\nComparing Model Performance\nNow that we have both a simple model (balance only) and a multiple regression model (all features), let’s compare their performance properly. To get an accurate assessment, we’ll split our data into training and testing sets—this ensures we evaluate the models on data they haven’t seen during training. For this introduction, we’ll use accuracy—the percentage of correct predictions—as our primary evaluation metric. The next chapter will introduce additional classification metrics like precision, recall, and F1-score that provide deeper insights into model performance.\n\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_simple_train, X_simple_test, X_train, X_test, y_train, y_test = train_test_split(\n    X_simple, X, y, test_size=0.3, random_state=42\n)\n\nprint(f\"Training set size: {len(X_train)} observations\")\nprint(f\"Test set size: {len(X_test)} observations\")\n\nTraining set size: 7000 observations\nTest set size: 3000 observations\n\n\nNow let’s retrain our models on the training data and evaluate them on the test data:\n\n# Retrain both models on training data only\nlog_reg_simple_new = LogisticRegression(random_state=42)\nlog_reg_multiple_new = LogisticRegression(random_state=42)\n\nlog_reg_simple_new.fit(X_simple_train, y_train)\nlog_reg_multiple_new.fit(X_train, y_train)\n\n# Make predictions on test data\npred_simple_test = log_reg_simple_new.predict(X_simple_test)\npred_multiple_test = log_reg_multiple_new.predict(X_test)\n\n# Calculate test accuracy\naccuracy_simple_test = accuracy_score(y_test, pred_simple_test)\naccuracy_multiple_test = accuracy_score(y_test, pred_multiple_test)\n\nprint(f\"Model Performance on Test Data:\")\nprint(f\"Simple model (balance only): {accuracy_simple_test:.1%} accuracy\")\nprint(f\"Multiple model (all features): {accuracy_multiple_test:.1%} accuracy\")\n\nModel Performance on Test Data:\nSimple model (balance only): 97.3% accuracy\nMultiple model (all features): 97.3% accuracy\n\n\nResults Interpretation: Surprisingly, both models achieve identical 97.3% accuracy on the test data! This might seem to suggest that adding income and student status doesn’t improve our model’s performance. However, accuracy can be misleading, especially when dealing with imbalanced datasets where one class (like “no default”) is much more common than the other.\nBoth models are performing very well at predicting the majority class but may differ in how they handle the minority class (defaults). The next chapter on classification model evaluation will introduce additional metrics like precision, recall, and F1-score that provide deeper insights into model performance and help us understand these nuanced differences that accuracy alone misses.\nComparing Individual Predictions: Beyond overall accuracy, we can also examine how the two models differ in their probability estimates for individual customers. This helps us understand when and why adding more features matters:\n\n# Show prediction probabilities for a few examples\nsample_customers = X.head(5)\nprob_simple = log_reg_simple.predict_proba(sample_customers[['balance']])[:, 1]\nprob_multiple = log_reg_multiple.predict_proba(sample_customers)[:, 1]\n\ncomparison_df = pd.DataFrame({\n    'Balance': sample_customers['balance'].values,\n    'Income': sample_customers['income'].values,\n    'Student': sample_customers['student_Yes'].values,\n    'Actual_Default': y.head(5).values,\n    'Simple_Model_Prob': prob_simple,\n    'Multiple_Model_Prob': prob_multiple\n})\n\nprint(f\"Sample Predictions:\")\ncomparison_df.round(4)\n\nSample Predictions:\n\n\n\n\n\n\n\n\n\nBalance\nIncome\nStudent\nActual_Default\nSimple_Model_Prob\nMultiple_Model_Prob\n\n\n\n\n0\n729.5265\n44361.6251\nFalse\n0\n0.0013\n0.0014\n\n\n1\n817.1804\n12106.1347\nTrue\n0\n0.0021\n0.0011\n\n\n2\n1073.5492\n31767.1389\nFalse\n0\n0.0086\n0.0097\n\n\n3\n529.2506\n35704.4939\nFalse\n0\n0.0004\n0.0004\n\n\n4\n785.6559\n38463.4959\nFalse\n0\n0.0018\n0.0019\n\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneHands-On: Build Your Own Logistic Regression Model\n\n\n\nNow it’s your turn to apply logistic regression! Using the Default dataset:\nYour Tasks:\n\nSimple Model: Build a logistic regression model predicting default using only income as a predictor\nExtract coefficients: What does the income coefficient tell you about the relationship between income and default risk?\nMake predictions: What’s the predicted probability of default for customers with incomes of $20,000, $50,000, and $100,000?\nBusiness insight: Based on your model, how would you advise the bank’s risk management team about income as a factor in loan decisions?\nCompare with balance: Which is a stronger predictor of default—balance or income? How can you tell?\n\nBonus Challenge: Create a model using only the student variable. What does this tell you about student default risk?",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to Logistic Regression for Classification</span>"
    ]
  },
  {
    "objectID": "23-logistic-regression.html#practical-considerations",
    "href": "23-logistic-regression.html#practical-considerations",
    "title": "23  Introduction to Logistic Regression for Classification",
    "section": "23.4 Practical Considerations",
    "text": "23.4 Practical Considerations\nAs you implement logistic regression in real business scenarios, there are several important practical considerations that can significantly impact your model’s performance and reliability.\n\nClass Imbalance\nWhen one class is much more common than the other, logistic regression may bias toward the majority class. Our Default dataset is a perfect example of this issue—with only a 3% default rate, the dataset is highly imbalanced. This means the model can achieve high accuracy simply by predicting “no default” for almost everyone, without actually learning to identify customers who are at risk of defaulting.\nClass imbalance can make logistic regression particularly problematic for business problems where correctly identifying the minority class is crucial (like fraud detection, disease diagnosis, or identifying high-value customers). The next chapter will introduce evaluation metrics beyond accuracy—such as precision, recall, and F1-score—that help identify when and how class imbalance is impacting model performance.\nIn cases of severe class imbalance like our Default dataset, logistic regression may not be the most appropriate model for the problem. Later chapters will discuss alternative algorithms like random forests, gradient boosting, and specialized techniques that are more robust to imbalanced data.\n\n\nModel Convergence\nLogistic regression uses iterative optimization algorithms that sometimes fail to converge, especially with complex data or numerical instability. When this happens, you’ll typically see runtime warnings about “divide by zero,” “overflow,” or “invalid values” during the fitting process.\n\n# Example of setting convergence parameters\nlog_reg_robust = LogisticRegression(\n    max_iter=1000,  # Increase iterations if needed\n    solver='liblinear',  # Try different solvers for stability\n    random_state=42\n)\n\nlog_reg_robust.fit(X, y)\nprint(\"Robust logistic regression fitted successfully\")\nprint(f\"Coefficients: {log_reg_robust.coef_[0].round(6)}\")\n\nRobust logistic regression fitted successfully\nCoefficients: [ 4.08e-04 -1.26e-04 -3.00e-06]\n\n\n\n\nAssumptions and Limitations\nLogistic regression, like all statistical methods, relies on certain assumptions. Understanding these assumptions is important because when they are violated, your model’s predictions and interpretations may become unreliable:\n\nIndependence: Observations should be independent of each other. When errors are dependent, the model can give a false sense of confidence. Example: In time-series customer data, yesterday’s behavior often relates to today’s behavior—ignoring this can lead to poor predictions.\nLinearity in log-odds: The relationship between predictors and log-odds should be linear. If the true relationship is curved or more complex, a linear model will oversimplify and potentially mislead decisions. Example: Assuming income has a linear effect on default risk might overlook that very high earners behave differently than expected.\nNo multicollinearity: Predictor variables shouldn’t be highly correlated with each other. When predictors are highly correlated, coefficient interpretations become unreliable. Example: Including both “total_debt” and “monthly_payment” might create interpretation issues since they’re likely highly correlated.\n\nWhile these assumptions provide a useful framework, real-world data often violates them. That doesn’t mean logistic regression is useless—it simply means you need to interpret results cautiously and sometimes consider alternative methods.\nImportantly, later in this course you will learn about algorithms (such as decision trees, random forests, and boosting methods) that do not rely on these strict assumptions and can handle more complex patterns when logistic regression falls short.",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to Logistic Regression for Classification</span>"
    ]
  },
  {
    "objectID": "23-logistic-regression.html#summary",
    "href": "23-logistic-regression.html#summary",
    "title": "23  Introduction to Logistic Regression for Classification",
    "section": "23.5 Summary",
    "text": "23.5 Summary\nThis chapter introduced you to logistic regression, the fundamental algorithm for classification problems in business. You learned how to move beyond predicting continuous numbers to predicting categories and probabilities—a critical skill for many real-world business decisions.\nKey concepts you mastered include:\n\nClassification vs. regression: Understanding when to predict categories versus continuous values, and why linear regression fails for classification problems\nThe logistic function: How the S-shaped curve transforms linear combinations into valid probabilities between 0 and 1\nProbability, odds, and log-odds: Understanding these three related concepts and how they connect to logistic regression coefficients\nModel building: Creating both simple (single predictor) and multiple (multiple predictors) logistic regression models using scikit-learn\nCoefficient interpretation: Reading logistic regression coefficients in terms of log-odds and translating them into practical business impact\nMaking predictions: Using both predict_proba() for probability estimates and predict() for binary classifications with the 0.5 threshold\nModel evaluation: Using proper train/test splits to assess model performance with accuracy as an introductory metric\n\nThe business value of logistic regression lies in its interpretability and reliability. Unlike black-box algorithms, logistic regression provides clear, explainable relationships between features and outcomes. This makes it particularly valuable for regulated industries, risk management, and any scenario where you need to justify your model’s decisions to stakeholders.\nReal-world application through the Default dataset demonstrated how logistic regression can help financial institutions make data-driven lending decisions while understanding the factors that drive risk. You learned to work with categorical variables through dummy encoding and saw how balance, income, and student status each contribute to default probability.\nPractical considerations you learned include recognizing class imbalance issues (like our 3% default rate), understanding model convergence problems and runtime warnings, and being aware of key assumptions like independence, linearity in log-odds, and avoiding multicollinearity.\nLooking ahead: In the next chapter, you’ll learn how to properly evaluate classification models like logistic regression. While we used accuracy in this chapter, classification evaluation requires specialized metrics like precision, recall, F1-score, and ROC curves. Understanding these metrics is crucial for determining whether your logistic regression models are ready for real-world deployment and how they align with specific business objectives and costs—especially important for imbalanced datasets like ours.\nThe foundation you’ve built with logistic regression will also transfer to more advanced classification algorithms you’ll encounter later. Whether you’re using decision trees, random forests, or neural networks, the core concepts of classification, probability interpretation, and feature relationships remain constant.\n\n\n\n\n\n\nNote🎥 Video Spotlight: End-to-End Logistic Regression with Scikit-Learn\n\n\n\nA nice short video that walks through an end-to-end example of training a logistic regression model, making predictions, and evaluating the accuracy of the model.",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to Logistic Regression for Classification</span>"
    ]
  },
  {
    "objectID": "23-logistic-regression.html#end-of-chapter-exercise",
    "href": "23-logistic-regression.html#end-of-chapter-exercise",
    "title": "23  Introduction to Logistic Regression for Classification",
    "section": "23.6 End of Chapter Exercise",
    "text": "23.6 End of Chapter Exercise\nFor these exercises, you’ll apply logistic regression to real business scenarios using datasets from the ISLP package. Each scenario mirrors decision contexts where classification helps drive business outcomes.\n\n\n\n\n\n\nNoneExercise 1: Stock Market Direction Prediction\n\n\n\n\n\nCompany: Investment management firm\nGoal: Predict whether the S&P 500 will go up or down based on previous market performance\nDataset: Weekly dataset from ISLP package\n\nWeekly = load_data('Weekly')\nprint(\"Weekly dataset loaded\")\nprint(Weekly.head())\nprint(Weekly.info())\n\nWeekly dataset loaded\n   Year   Lag1   Lag2   Lag3   Lag4   Lag5    Volume  Today Direction\n0  1990  0.816  1.572 -3.936 -0.229 -3.484  0.154976 -0.270      Down\n1  1990 -0.270  0.816  1.572 -3.936 -0.229  0.148574 -2.576      Down\n2  1990 -2.576 -0.270  0.816  1.572 -3.936  0.159837  3.514        Up\n3  1990  3.514 -2.576 -0.270  0.816  1.572  0.161630  0.712        Up\n4  1990  0.712  3.514 -2.576 -0.270  0.816  0.153728  1.178        Up\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1089 entries, 0 to 1088\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype   \n---  ------     --------------  -----   \n 0   Year       1089 non-null   int64   \n 1   Lag1       1089 non-null   float64 \n 2   Lag2       1089 non-null   float64 \n 3   Lag3       1089 non-null   float64 \n 4   Lag4       1089 non-null   float64 \n 5   Lag5       1089 non-null   float64 \n 6   Volume     1089 non-null   float64 \n 7   Today      1089 non-null   float64 \n 8   Direction  1089 non-null   category\ndtypes: category(1), float64(7), int64(1)\nmemory usage: 69.4 KB\nNone\n\n\nYour Tasks:\n\nExplore the target variable: Examine the Direction variable (Up/Down). What’s the distribution of up vs. down weeks?\nPrepare the data: Convert Direction to binary (1 for Up, 0 for Down) and select relevant lag features (Lag1, Lag2, Lag3, Lag4, Lag5)\nBuild a logistic regression model: Use the lag variables to predict market direction\nInterpret coefficients: Which lag periods are most influential? Do any show surprising relationships?\nMake predictions: What’s the predicted probability of an up week if Lag1=-0.5, Lag2=0.2, Lag3=-0.1?\nBusiness application: How might a portfolio manager use this model? What are its limitations for investment decisions?\n\n\n\n\n\n\n\n\n\n\nNoneExercise 2: Consumer Purchase Behavior\n\n\n\n\n\nCompany: Orange juice manufacturer\nGoal: Predict customer brand choice to optimize pricing and promotional strategies\nDataset: OJ dataset from ISLP package\n\nOJ = load_data('OJ')\nprint(\"OJ dataset loaded\")  \nprint(OJ.head())\nprint(OJ.info())\n\nOJ dataset loaded\n  Purchase  WeekofPurchase  StoreID  PriceCH  PriceMM  DiscCH  DiscMM  \\\n0       CH             237        1     1.75     1.99    0.00     0.0   \n1       CH             239        1     1.75     1.99    0.00     0.3   \n2       CH             245        1     1.86     2.09    0.17     0.0   \n3       MM             227        1     1.69     1.69    0.00     0.0   \n4       CH             228        7     1.69     1.69    0.00     0.0   \n\n   SpecialCH  SpecialMM   LoyalCH  SalePriceMM  SalePriceCH  PriceDiff Store7  \\\n0          0          0  0.500000         1.99         1.75       0.24     No   \n1          0          1  0.600000         1.69         1.75      -0.06     No   \n2          0          0  0.680000         2.09         1.69       0.40     No   \n3          0          0  0.400000         1.69         1.69       0.00     No   \n4          0          0  0.956535         1.69         1.69       0.00    Yes   \n\n   PctDiscMM  PctDiscCH  ListPriceDiff  STORE  \n0   0.000000   0.000000           0.24      1  \n1   0.150754   0.000000           0.24      1  \n2   0.000000   0.091398           0.23      1  \n3   0.000000   0.000000           0.00      1  \n4   0.000000   0.000000           0.00      0  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1070 entries, 0 to 1069\nData columns (total 18 columns):\n #   Column          Non-Null Count  Dtype   \n---  ------          --------------  -----   \n 0   Purchase        1070 non-null   category\n 1   WeekofPurchase  1070 non-null   int64   \n 2   StoreID         1070 non-null   int64   \n 3   PriceCH         1070 non-null   float64 \n 4   PriceMM         1070 non-null   float64 \n 5   DiscCH          1070 non-null   float64 \n 6   DiscMM          1070 non-null   float64 \n 7   SpecialCH       1070 non-null   int64   \n 8   SpecialMM       1070 non-null   int64   \n 9   LoyalCH         1070 non-null   float64 \n 10  SalePriceMM     1070 non-null   float64 \n 11  SalePriceCH     1070 non-null   float64 \n 12  PriceDiff       1070 non-null   float64 \n 13  Store7          1070 non-null   category\n 14  PctDiscMM       1070 non-null   float64 \n 15  PctDiscCH       1070 non-null   float64 \n 16  ListPriceDiff   1070 non-null   float64 \n 17  STORE           1070 non-null   int64   \ndtypes: category(2), float64(11), int64(5)\nmemory usage: 136.2 KB\nNone\n\n\nYour Tasks:\n\nUnderstand the business context: Examine the Purchase variable (CH = Citrus Hill, MM = Minute Maid). What’s the market share split?\nFeature selection: Choose relevant predictors like PriceCH, PriceMM, DiscCH, DiscMM, and any others you think matter\nBuild the model: Create a logistic regression model predicting brand choice\nCoefficient interpretation: How do price differences affect brand choice? What about discounts?\nBusiness scenarios:\n\nIf Citrus Hill costs $1.50 and Minute Maid costs $1.75 (no discounts), what’s the predicted purchase probability?\nHow would a $0.20 discount on Citrus Hill change this prediction?\n\nStrategic insights: Based on your model, what pricing strategy would you recommend to each brand?\n\n\n\n\n\n\n\n\n\n\nNoneExercise 3: Medical Risk Assessment\n\n\n\n\n\nCompany: Healthcare analytics firm\nGoal: Predict heart disease risk to help doctors prioritize patient care\nDataset: Heart dataset from ISLP package (if available, otherwise we’ll simulate)\n\n# Note: If Heart dataset isn't available in ISLP, we'll create a similar medical scenario\ntry:\n    Heart = load_data('Heart')\n    print(\"Heart dataset loaded\")\n    print(Heart.head())\nexcept:\n    print(\"Heart dataset not available - creating simulated medical data\")\n    np.random.seed(42)\n    \n    # Simulate realistic medical data\n    n = 1000\n    age = np.random.normal(55, 15, n)\n    age = np.clip(age, 20, 85)\n    \n    cholesterol = np.random.normal(220, 40, n)\n    cholesterol = np.clip(cholesterol, 150, 350)\n    \n    blood_pressure = np.random.normal(130, 20, n)\n    blood_pressure = np.clip(blood_pressure, 90, 200)\n    \n    # Heart disease probability increases with age, cholesterol, BP\n    risk_score = -8 + 0.05*age + 0.01*cholesterol + 0.02*blood_pressure\n    heart_disease = np.random.binomial(1, 1/(1 + np.exp(-risk_score)))\n    \n    Heart = pd.DataFrame({\n        'Age': age,\n        'Cholesterol': cholesterol,\n        'Blood_Pressure': blood_pressure,\n        'Heart_Disease': heart_disease\n    })\n    \n    print(\"Simulated heart disease data:\")\n    print(Heart.head())\n\nHeart dataset not available - creating simulated medical data\nSimulated heart disease data:\n         Age  Cholesterol  Blood_Pressure  Heart_Disease\n0  62.450712   275.974217      116.496435              0\n1  52.926035   256.985347      127.109627              1\n2  64.715328   222.385215      114.151602              0\n3  77.845448   194.122529      123.840769              0\n4  51.487699   247.928933       92.127707              0\n\n\nYour Tasks:\n\nMedical context: Examine the relationship between risk factors and heart disease. What patterns do you observe?\nBuild risk model: Create a logistic regression model using available risk factors\nClinical interpretation: Which factors have the strongest association with heart disease risk?\nRisk assessment: Calculate predicted risk for these patient profiles:\n\n45-year-old with cholesterol 200, BP 120\n65-year-old with cholesterol 280, BP 160\n\nMedical decision support: How would doctors use probability estimates vs. binary classifications? What threshold would you recommend for “high risk”?\nEthical considerations: What are the implications of false positives and false negatives in medical prediction models?",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to Logistic Regression for Classification</span>"
    ]
  },
  {
    "objectID": "24-classification-evaluation.html",
    "href": "24-classification-evaluation.html",
    "title": "24  Evaluating Classification Models",
    "section": "",
    "text": "24.1 Beyond Accuracy: Why We Need Better Metrics\nIn the previous chapter, you learned to build logistic regression models using the Default dataset from ISLP, successfully creating both simple (balance-only) and multiple regression models that achieved 97.3% accuracy in predicting customer default. But building a model is only the beginning. The critical question that follows is: How good is your classification model?\nWhile 97.3% accuracy sounds impressive, you discovered that the Default dataset has a severe class imbalance problem—only 3% of customers actually default. This means your logistic regression model could achieve high accuracy simply by predicting “no default” for almost everyone, without actually learning to identify customers who are at risk of defaulting.\nConsider these high-stakes scenarios:\nClassification evaluation goes far beyond simple accuracy. In business contexts, different types of errors often have vastly different costs, and understanding these trade-offs is crucial for building models that truly serve business objectives. Using the Default dataset context from the previous chapter, this chapter teaches you to evaluate classification models using metrics that align with business reality.\nBy the end of this chapter, you will be able to:\nWhen most people think about evaluating a classification model, they naturally gravitate toward accuracy—the percentage of predictions that are correct. While accuracy seems intuitive and straightforward, it can be deeply misleading in real business scenarios.",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "24-classification-evaluation.html#beyond-accuracy-why-we-need-better-metrics",
    "href": "24-classification-evaluation.html#beyond-accuracy-why-we-need-better-metrics",
    "title": "24  Evaluating Classification Models",
    "section": "",
    "text": "The Accuracy Trap: When 95% Accuracy is Actually Terrible\nLet’s explore this with a concrete business example. Imagine you work for a credit card company building a fraud detection system. You have 100,000 transactions, and historically, only 1% are fraudulent:\n\nFraudulent transactions: 1,000 (1%)\nLegitimate transactions: 99,000 (99%)\n\nNow consider two possible fraud detection models:\n\nModel A (Lazy): Always predicts “legitimate” for every transaction\nModel B (Smart): Uses sophisticated algorithms to identify 80% of fraud while incorrectly flagging 2% of legitimate transactions\n\nWe’ve trained both models on our dataset, and at first glance, they appear to perform quite similarly:\n\nModel A (Lazy): 99.0% accuracy\nModel B (Smart): 97.8% accuracy\n\n\n\nShow code for model comparison simulation\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Simulate fraud detection scenario\nnp.random.seed(42)\nn_transactions = 100000\nfraud_rate = 0.01\n\n# True labels: 1% fraud, 99% legitimate\ny_true = np.random.binomial(1, fraud_rate, n_transactions)\n\n# Model A: \"Lazy\" model that always predicts \"legitimate\" \ny_pred_lazy = np.zeros(n_transactions)  # Always predicts 0 (legitimate)\n\n# Model B: \"Smart\" model that catches some fraud but makes some mistakes\n# Let's say it correctly identifies 80% of fraud and incorrectly flags 2% of legitimate transactions\ny_pred_smart = y_true.copy()\n# Miss 20% of fraud (false negatives)\nfraud_indices = np.where(y_true == 1)[0]\nmissed_fraud = np.random.choice(fraud_indices, int(0.2 * len(fraud_indices)), replace=False)\ny_pred_smart[missed_fraud] = 0\n\n# Flag 2% of legitimate transactions as fraud (false positives)  \nlegit_indices = np.where(y_true == 0)[0]\nfalse_flags = np.random.choice(legit_indices, int(0.02 * len(legit_indices)), replace=False)\ny_pred_smart[false_flags] = 1\n\n# Calculate accuracies\naccuracy_lazy = accuracy_score(y_true, y_pred_lazy)\naccuracy_smart = accuracy_score(y_true, y_pred_smart)\n\nprint(\"Fraud Detection Model Comparison:\")\nprint(f\"Dataset: {n_transactions:,} transactions, {fraud_rate:.1%} fraud rate\")\nprint(f\"\\nModel A (Lazy): {accuracy_lazy:.1%} accuracy\")\nprint(f\"Model B (Smart): {accuracy_smart:.1%} accuracy\")\nprint(f\"\\nWhich model would you choose for your business?\")\n\n\nFraud Detection Model Comparison:\nDataset: 100,000 transactions, 1.0% fraud rate\n\nModel A (Lazy): 99.0% accuracy\nModel B (Smart): 97.8% accuracy\n\nWhich model would you choose for your business?\n\n\nThe Shocking Reality Behind These Numbers:\nThe results reveal a counterintuitive and deeply problematic outcome: the “lazy” model achieves 99.0% accuracy by never detecting fraud, while the “smart” model only achieves 97.8% accuracy despite actually catching 80% of fraudulent transactions!\nThis demonstrates the fundamental problem with accuracy in imbalanced datasets—it can make completely useless models appear excellent. The lazy model provides zero business value (catches 0% of fraud) yet appears superior by accuracy metrics. Meanwhile, the smart model that actually protects the business from financial losses appears inferior by the same metric.\nThis is exactly the trap that the 97.3% accuracy from chapter 23’s Default dataset model could represent—high accuracy that masks the model’s inability to identify the minority class (defaults) that the business actually cares about detecting.\n\n\nClass Imbalance: When the Obvious Choice is Wrong\nClass imbalance occurs when one category significantly outnumbers the others. This is extremely common in business:\n\nFraud detection: &lt;1% of transactions are fraudulent\nMedical screening: &lt;5% of patients have rare diseases\n\nCustomer churn: &lt;10% of customers leave per month\nEmail spam: ~15% of emails are spam\nQuality control: &lt;2% of products are defective\n\nIn these scenarios, a model can achieve high accuracy by simply predicting the majority class, but this provides zero business value.\nThe Accuracy Paradox Gets Worse with Extreme Imbalance:\nTo understand just how misleading accuracy becomes, let’s examine what happens when a “lazy model” (that always predicts the majority class) encounters different levels of class imbalance. The results are striking and counterintuitive:\n\n\n\nFraud Rate\nLazy Model Accuracy\nBusiness Value\n\n\n\n\n50.0%\n50.0%\nNone - catches 0% of fraud\n\n\n10.0%\n90.0%\nNone - catches 0% of fraud\n\n\n5.0%\n95.0%\nNone - catches 0% of fraud\n\n\n1.0%\n99.0%\nNone - catches 0% of fraud\n\n\n0.1%\n99.9%\nNone - catches 0% of fraud\n\n\n\nThe paradox is clear: The lazy model gets better accuracy as fraud becomes rarer, but provides ZERO business value by never catching fraud! This is exactly what happened with our Default dataset from chapter 23—the rarer the default events (3% rate), the easier it becomes for a useless model to achieve impressive accuracy scores.\n\n\nReal Business Costs Matter More Than Accuracy\nIn business, different prediction errors have different costs. While accuracy treats all errors equally, understanding the specific types of errors—False Positives and False Negatives—in relation to your business problem is crucial for making informed decisions about model performance and thresholds.\nUnderstanding the Two Types of Classification Errors:\n\nFalse Positive (FP): Your model predicts the positive class (fraud, disease, spam) when it’s actually negative (legitimate, healthy, normal email)\nFalse Negative (FN): Your model predicts the negative class when it’s actually positive (missing the thing you’re trying to detect)\n\nThe key insight is that these errors rarely have equal business impact. Understanding which type of error is more costly for your specific business context helps guide model evaluation, threshold selection, and deployment decisions.\n\n\n\n\n\n\n\n\nBusiness Example\nFalse Positive (FP)\nFalse Negative (FN)\n\n\n\n\nCredit Card Fraud Detection\nFlagging legitimate transaction as fraud: Customer tries to make a purchase but card is declined. Creates customer frustration, potential embarrassment at checkout, and may lead to customers switching to competitors. Bank loses transaction fees and risks customer churn (~$50 cost per incident)\nMissing actual fraud: Fraudulent transactions go undetected, resulting in direct financial losses to the bank, potential legal liability, and costs associated with identity theft resolution for customers. Often involves multiple fraudulent transactions before detection (~$500-5,000 cost per incident)\n\n\nMedical Cancer Screening\nIncorrectly diagnosing healthy patient with cancer: Patient experiences severe psychological distress, undergoes unnecessary and potentially harmful treatments, faces insurance complications, and incurs substantial medical costs for follow-up tests and procedures (~$1,000-10,000 cost)\nMissing actual cancer: Early-stage cancer goes undetected, leading to delayed treatment when disease has progressed to advanced stages. Dramatically reduces treatment success rates, increases treatment complexity and costs, and can be life-threatening (~$100,000+ cost, plus immeasurable human cost)\n\n\nEmail Spam Filter\nImportant business email sent to spam folder: Critical business communications are missed, potentially leading to lost deals, missed meetings, delayed responses to urgent matters, and damaged professional relationships (~$500 cost per important missed email)\nSpam reaching inbox: Users experience minor inconvenience from deleting unwanted emails, potential exposure to phishing attempts, and slight productivity loss from processing irrelevant content (~$1 cost per spam email)\n\n\n\nThese cost differences mean that accuracy—which treats all errors equally—provides little guidance for business decision-making.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneUnderstanding Business Costs\n\n\n\nConsider these business scenarios and identify which type of error would be more costly:\n\nAirport Security Screening: Flagging safe passengers vs. missing dangerous items\n\nWhich error is more costly? Why?\nHow might this influence the screening threshold?\n\nJob Application Screening: Rejecting qualified candidates vs. interviewing unqualified candidates\n\nWhat are the business costs of each error type?\nHow might company hiring needs affect this trade-off?\n\nProduct Quality Control: Rejecting good products vs. shipping defective products\n\nConsider both immediate costs and long-term reputation effects\nHow would the costs differ for luxury vs. budget products?",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "24-classification-evaluation.html#the-confusion-matrix-foundation-for-classification-evaluation",
    "href": "24-classification-evaluation.html#the-confusion-matrix-foundation-for-classification-evaluation",
    "title": "24  Evaluating Classification Models",
    "section": "24.2 The Confusion Matrix: Foundation for Classification Evaluation",
    "text": "24.2 The Confusion Matrix: Foundation for Classification Evaluation\nThe confusion matrix provides the foundation for understanding classification model performance by breaking down predictions into four categories. Rather than just telling you the percentage of correct predictions, it shows you exactly how your model is making mistakes.\n\nUnderstanding What a Confusion Matrix Shows\nBefore diving into real examples, let’s understand the conceptual framework of a confusion matrix. Think of it as a 2×2 table that organizes all possible prediction outcomes:\n\n\n\n\n\n\n\n\n\nUnderstanding the Four Quadrants:\n\nTrue Positives (TP): Model correctly identifies positive cases (e.g., correctly flagged default risk)\nTrue Negatives (TN): Model correctly identifies negative cases (e.g., correctly identified safe customers)\nFalse Positives (FP): Model incorrectly predicts positive (e.g., safe customer flagged as high risk) - Type I Error\nFalse Negatives (FN): Model incorrectly predicts negative (e.g., risky customer marked as safe) - Type II Error\n\nThe key insight is that correct predictions (TP and TN) lie on the diagonal, while errors (FP and FN) lie off the diagonal.\n\n\n\n\n\n\nNote🎥 Video Spotlight: The Confusion Matrix\n\n\n\nA great introduction to the confusion matrix.\n\n\n\n\n\nApplying the Confusion Matrix to Our Default Prediction Model\nNow let’s see how this framework applies to the same logistic regression model from chapter 23. We’ll use the exact same dataset preparation and model to ensure consistency:\n\n# Use the Default dataset from chapter 23 with identical preparation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom ISLP import load_data\n\n# Load and prepare data exactly as in chapter 23\nDefault = load_data('Default')\nDefault_encoded = pd.get_dummies(Default, columns=['student'], drop_first=True)\nDefault_encoded['default_binary'] = (Default_encoded['default'] == 'Yes').astype(int)\n\n# Use the same feature matrix and target as chapter 23\nX = Default_encoded[['balance', 'income', 'student_Yes']]\ny = Default_encoded['default_binary']\n\n# Split the data using the same approach as chapter 23 for consistency\nX_simple = Default_encoded[['balance']]\nX_simple_train, X_simple_test, X_train, X_test, y_train, y_test = train_test_split(\n    X_simple, X, y, test_size=0.3, random_state=42\n)\n\n# Fit the same logistic regression model from chapter 23\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions on test set\ny_pred = model.predict(X_test)\ny_pred_proba = model.predict_proba(X_test)[:, 1]\n\n# Create confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix for Default Prediction Model:\")\nprint(cm)\n\nprint(f\"\\nDataset context (matching chapter 23 results):\")\nprint(f\"Total test examples: {len(y_test):,}\")\nprint(f\"Actual default cases: {y_test.sum():,} ({y_test.mean():.1%})\")\nprint(f\"Actual non-default cases: {len(y_test) - y_test.sum():,} ({1-y_test.mean():.1%})\")\n\nConfusion Matrix for Default Prediction Model:\n[[2895   11]\n [  69   25]]\n\nDataset context (matching chapter 23 results):\nTotal test examples: 3,000\nActual default cases: 94 (3.1%)\nActual non-default cases: 2,906 (96.9%)\n\n\n\n\nInterpreting the Confusion Matrix Results\nThe matrix [[2895, 11], [69, 25]] represents our model’s performance in a 2×2 format where:\n\nPosition [0,0]: 2,895 = True Negatives (correctly identified non-default customers)\nPosition [0,1]: 11 = False Positives (safe customers incorrectly flagged as default risk)\n\nPosition [1,0]: 69 = False Negatives (risky customers that were missed)\nPosition [1,1]: 25 = True Positives (correctly identified default customers)\n\nModel Strengths:\n\nSuccessfully identifies the vast majority of non-default customers\n\nAchieves the same 97.3% accuracy we saw in chapter 23\nShows very few false alarms (only 11 safe customers incorrectly flagged)\nDemonstrates consistency with the logistic regression results from the previous chapter\n\nModel Limitations:\n\nCatches only 25 out of 94 actual default cases\nMisses 69 customers who will actually default (73.4% of defaults missed)\nThe severe class imbalance (3.1% default rate) makes detecting the minority class challenging\n\n\n\n\n\n\n\nImportantBusiness Impact Analysis\n\n\n\nFor a credit card company, the confusion matrix components translate directly to business costs:\n\nFalse Positives (11 customers): Good customers denied credit or charged higher rates → Lost revenue, customer churn\nFalse Negatives (69 customers): Bad customers approved for credit → Direct financial losses from defaults\n\nThis analysis demonstrates why accuracy alone can be misleading—while our model achieves high overall accuracy, it fails to identify most actual default cases, which represents the primary business value we’re seeking. Understanding these trade-offs helps determine whether the model’s performance aligns with business objectives and risk tolerance.\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneReading Confusion Matrices\n\n\n\nGiven this confusion matrix for a customer churn prediction model:\n                 Predicted\n                Stay  Churn\nActual  Stay   1850    150\n        Churn   200    100\nCalculate and interpret:\n\nBasic metrics: What’s the accuracy of this model?\nBusiness interpretation:\n\nHow many customers who churned were correctly identified?\nHow many “churn risk” alerts were false alarms?\nWhat’s the cost if each missed churn loses $500 and each false alarm costs $50 in intervention efforts?\n\nModel assessment: To improve business value with this model, would you rather focus on reducing the number of false positives or false negatives?",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "24-classification-evaluation.html#essential-classification-metrics-precision-recall-and-f1-score",
    "href": "24-classification-evaluation.html#essential-classification-metrics-precision-recall-and-f1-score",
    "title": "24  Evaluating Classification Models",
    "section": "24.3 Essential Classification Metrics: Precision, Recall, and F1-Score",
    "text": "24.3 Essential Classification Metrics: Precision, Recall, and F1-Score\nWhile accuracy treats all errors equally, business decisions require understanding the specific types of errors your model makes. This section builds on our confusion matrix foundation to introduce precision, recall, and F1-score—metrics that help align model evaluation with business priorities.\n\nStep 1: Quick Refresh - Extracting Key Values from the Confusion Matrix\nBefore diving into advanced metrics, let’s quickly review how we extract the fundamental building blocks from our confusion matrix:\n\n# Extract the four core values from our confusion matrix\ntn, fp, fn, tp = cm.ravel()\ntotal = tn + fp + fn + tp\n\n# Manually calculate basic accuracy\naccuracy = (tp + tn) / total\n\n\n\nShow code for printing results\nprint(\"Confusion Matrix Components for Default Prediction:\")\nprint(\"=\" * 60)\nprint(f\"True Negatives (TN):  {tn:,} - Correctly identified non-default customers\")\nprint(f\"False Positives (FP): {fp:,} - Safe customers incorrectly flagged as high risk\")\nprint(f\"False Negatives (FN): {fn:,} - Risky customers that were missed\")\nprint(f\"True Positives (TP):  {tp:,} - Correctly identified default customers\")\nprint(f\"Total customers:      {total:,}\")\n\nprint(f\"\\nAccuracy = (TP + TN) / Total = ({tp} + {tn}) / {total} = {accuracy:.3f} or {accuracy:.1%}\")\n\n\nConfusion Matrix Components for Default Prediction:\n============================================================\nTrue Negatives (TN):  2,895 - Correctly identified non-default customers\nFalse Positives (FP): 11 - Safe customers incorrectly flagged as high risk\nFalse Negatives (FN): 69 - Risky customers that were missed\nTrue Positives (TP):  25 - Correctly identified default customers\nTotal customers:      3,000\n\nAccuracy = (TP + TN) / Total = (25 + 2895) / 3000 = 0.973 or 97.3%\n\n\nThese four values (TP, TN, FP, FN) are the foundation for all classification metrics. Think of them as the raw ingredients that we’ll use to cook up more sophisticated measures.\n\n\nStep 2: Precision and Recall - The Core Business Metrics\nNow let’s use these building blocks to calculate precision and recall, two metrics that directly address business concerns about model performance. But before you read on, watch this short video as a simple primer on precision and recall with a clear example:\n\n\nUnderstanding Precision: “When I Act on a Prediction, How Often Am I Right?”\nPrecision answers the question: “Of all the times my model predicts the positive class (default, fraud, spam), what percentage are actually correct?”\n\\[\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}} = \\frac{TP}{TP + FP}\\]\n\n\n\n\n\n\nTipBusiness Translation: When Precision Matters Most\n\n\n\nPrecision is critical when acting on a prediction is expensive or disruptive. In business contexts, this means:\n\nHigh precision = Few false alarms = Lower operational costs and better customer experience\nLow precision = Many false alarms = Wasted resources, frustrated customers, and damaged trust\n\nThink of precision as answering: “When I decide to take action based on my model’s prediction, how confident can I be that I’m making the right decision?”\nReal-world impact: A credit approval model with low precision might deny loans to many qualified applicants, leading to lost revenue and competitor advantage.\nBusiness Examples Where Precision is Critical:\n\nCredit Card Fraud Detection: False positives block legitimate purchases → customer frustration\nEmail Spam Filtering: False positives send important emails to spam → missed opportunities\nMedical Diagnosis: False positives cause unnecessary anxiety and expensive follow-up tests\n\n\n\nNow let’s calculate precision for our Default prediction model and see what it tells us about our model’s performance:\n\n# Calculate precision manually and verify with sklearn\nprecision = tp / (tp + fp)\n\n# Verify with sklearn's precision_score function\nfrom sklearn.metrics import precision_score\nsklearn_precision = precision_score(y_test, y_pred)\n\n\n\nShow code for detailed precision analysis\nprint(\"PRECISION ANALYSIS\")\nprint(\"=\" * 30)\nprint(f\"Precision = TP / (TP + FP) = {tp} / ({tp} + {fp}) = {precision:.3f} or {precision:.1%}\")\nprint(f\"\\nBusiness Interpretation:\")\nprint(f\"• When our model flags a customer as 'high default risk', it's correct {precision:.1%} of the time\")\nprint(f\"• Out of {tp + fp} customers flagged as high risk, {tp} actually defaulted\")\nprint(f\"• {fp} customers were incorrectly flagged (false alarms)\")\n\nprint(f\"\\nSklearn verification:\")\nprint(f\"• Manual calculation: {precision:.3f}\")\nprint(f\"• sklearn precision_score: {sklearn_precision:.3f}\")\nprint(f\"• Results match: {'✓' if abs(precision - sklearn_precision) &lt; 0.001 else '✗'}\")\n\n\nPRECISION ANALYSIS\n==============================\nPrecision = TP / (TP + FP) = 25 / (25 + 11) = 0.694 or 69.4%\n\nBusiness Interpretation:\n• When our model flags a customer as 'high default risk', it's correct 69.4% of the time\n• Out of 36 customers flagged as high risk, 25 actually defaulted\n• 11 customers were incorrectly flagged (false alarms)\n\nSklearn verification:\n• Manual calculation: 0.694\n• sklearn precision_score: 0.694\n• Results match: ✓\n\n\n\n\nUnderstanding Recall: “Of All the Cases I Should Catch, How Many Do I Actually Find?”\nRecall (also called sensitivity) answers: “Of all the actual positive cases that exist, what percentage does my model successfully identify?”\n\\[\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}} = \\frac{TP}{TP + FN}\\]\n\n\n\n\n\n\nTipBusiness Translation: When Recall Matters Most\n\n\n\nRecall is critical when missing a positive case is costly or dangerous. In business contexts, this means:\n\nHigh recall = Catch most/all important cases = Minimize catastrophic misses\nLow recall = Miss many important cases = Risk serious consequences and liability\n\nThink of recall as answering: “Of all the critical situations that actually exist, am I catching enough of them to protect my business and stakeholders?”\nReal-world impact: A medical screening test with low recall might miss cancer cases, leading to delayed treatment when early detection could be life-saving. The cost of missing these cases far outweighs the inconvenience of false alarms.\nBusiness Examples Where Recall is Critical:\n\nDisease Screening: Missing cancer cases delays treatment → life-threatening\nSecurity Systems: Missing threats allows dangerous situations → safety risks\n\nQuality Control: Missing defective products damages brand reputation → long-term losses\n\n\n\nNow let’s calculate recall for our Default prediction model and see what it tells us about our model’s performance:\n\n# Calculate recall manually and verify with sklearn\nrecall = tp / (tp + fn)\n\n# Verify with sklearn's recall_score function\nfrom sklearn.metrics import recall_score\nsklearn_recall = recall_score(y_test, y_pred)\n\n\n\nShow code for detailed recall analysis\nprint(\"RECALL ANALYSIS\")\nprint(\"=\" * 30)\nprint(f\"Recall = TP / (TP + FN) = {tp} / ({tp} + {fn}) = {recall:.3f} or {recall:.1%}\")\nprint(f\"\\nBusiness Interpretation:\")\nprint(f\"• Our model catches {recall:.1%} of all customers who actually default\")\nprint(f\"• Out of {tp + fn} customers who actually defaulted, we caught {tp}\")\nprint(f\"• We missed {fn} customers who defaulted (this could be costly!)\")\n\nprint(f\"\\nSklearn verification:\")\nprint(f\"• Manual calculation: {recall:.3f}\")\nprint(f\"• sklearn recall_score: {sklearn_recall:.3f}\")\nprint(f\"• Results match: {'✓' if abs(recall - sklearn_recall) &lt; 0.001 else '✗'}\")\n\n\nRECALL ANALYSIS\n==============================\nRecall = TP / (TP + FN) = 25 / (25 + 69) = 0.266 or 26.6%\n\nBusiness Interpretation:\n• Our model catches 26.6% of all customers who actually default\n• Out of 94 customers who actually defaulted, we caught 25\n• We missed 69 customers who defaulted (this could be costly!)\n\nSklearn verification:\n• Manual calculation: 0.266\n• sklearn recall_score: 0.266\n• Results match: ✓\n\n\n\n\nPutting this Together for Our Default Prediction Model\nNow that we’ve calculated both precision and recall, let’s understand what they tell us about our Default prediction model’s performance and how they address different business concerns:\nKey Difference Reminder:\n\nPrecision focuses on the accuracy of our positive predictions: “When we flag a customer as high-risk, how often are we correct?”\nRecall focuses on completeness of detection: “Of all customers who will actually default, what percentage do we catch?”\n\nFor credit risk management, this creates a classic business trade-off:\n\nHigh precision keeps customers happy (fewer false alarms) but may miss some defaults\nHigh recall catches more defaults but may frustrate good customers with false flags\n\nLet’s see how our model performs on both dimensions:\n\n\nShow code for comprehensive Default model evaluation\nprint(\"DEFAULT PREDICTION MODEL EVALUATION\")\nprint(\"=\" * 40)\nprint(f\"Precision: {precision:.1%} - When we flag someone as high risk, we're right {precision:.1%} of the time\")\nprint(f\"Recall: {recall:.1%} - We catch {recall:.1%} of all customers who actually default\")\n\n# Business cost implications\nprint(f\"\\nBusiness Impact:\")\nprint(f\"• High precision ({precision:.1%}) = Few false alarms = Happy customers\")\nprint(f\"• Low recall ({recall:.1%}) = Miss many defaults = Financial losses\")\nprint(f\"\\nThis suggests our model is conservative - it makes fewer false accusations,\")\nprint(f\"but it misses many customers who will actually default.\")\n\n\nDEFAULT PREDICTION MODEL EVALUATION\n========================================\nPrecision: 69.4% - When we flag someone as high risk, we're right 69.4% of the time\nRecall: 26.6% - We catch 26.6% of all customers who actually default\n\nBusiness Impact:\n• High precision (69.4%) = Few false alarms = Happy customers\n• Low recall (26.6%) = Miss many defaults = Financial losses\n\nThis suggests our model is conservative - it makes fewer false accusations,\nbut it misses many customers who will actually default.\n\n\n\n\n\nStep 3: The Precision-Recall Trade-off and F1-Score\nIn most real-world scenarios, there’s a fundamental tension between precision and recall. Improving one often hurts the other. Understanding this trade-off leads us to a metric know as the F1-score.\n\nWhy the Trade-off Exists\nThe precision-recall trade-off stems from how classification models make decisions. Most models (including logistic regression) output probabilities rather than direct classifications. To make final predictions, we apply a decision threshold (typically 0.5) where:\n\nProbabilities ≥ 0.5 → Predict “Default”\nProbabilities &lt; 0.5 → Predict “No Default”\n\nThe Business Reality: Adjusting this threshold changes how many customers we flag as risky, creating the trade-off:\n\nLower threshold (e.g., 0.3): Flag more customers as risky\n\nEffect: Catch more actual defaults (higher recall) but also flag more safe customers (lower precision)\nBusiness impact: Better default detection but more customer complaints\n\nHigher threshold (e.g., 0.7): Flag fewer customers as risky\n\nEffect: When we do flag someone, we’re usually right (higher precision) but miss more defaults (lower recall)\nBusiness impact: Happier customers but more financial losses\n\n\nThink of it like airport security: Stricter screening catches more threats but inconveniences more innocent travelers. Looser screening is faster but might miss dangerous items.\nLet’s demonstrate this trade-off with our Default dataset:\n\n\nShow code for precision-recall trade-off demonstration\n# Demonstrate the precision-recall trade-off\nprint(\"PRECISION-RECALL TRADE-OFF DEMONSTRATION\")\nprint(\"=\" * 45)\n\n# Test different thresholds\nthresholds = [0.1, 0.3, 0.5, 0.7, 0.9]\nprint(f\"{'Threshold':&lt;12} {'Precision':&lt;12} {'Recall':&lt;12} {'Business Impact'}\")\nprint(\"-\" * 70)\n\nfor threshold in thresholds:\n    # Make predictions at this threshold\n    y_pred_thresh = (y_pred_proba &gt; threshold).astype(int)\n    \n    if y_pred_thresh.sum() &gt; 0:  # Avoid division by zero\n        prec = precision_score(y_test, y_pred_thresh)\n        rec = recall_score(y_test, y_pred_thresh)\n        \n        # Interpret the business impact\n        if threshold &lt;= 0.3:\n            impact = \"Flag many as risky - catch more defaults but annoy customers\"\n        elif threshold &gt;= 0.7:\n            impact = \"Flag few as risky - happy customers but miss defaults\"\n        else:\n            impact = \"Balanced approach\"\n            \n        print(f\"{threshold:&lt;12.1f} {prec:&lt;12.3f} {rec:&lt;12.3f} {impact}\")\n    else:\n        print(f\"{threshold:&lt;12.1f} {'N/A':&lt;12} {'0.000':&lt;12} No customers flagged as risky\")\n\n\nPRECISION-RECALL TRADE-OFF DEMONSTRATION\n=============================================\nThreshold    Precision    Recall       Business Impact\n----------------------------------------------------------------------\n0.1          0.273        0.691        Flag many as risky - catch more defaults but annoy customers\n0.3          0.487        0.415        Flag many as risky - catch more defaults but annoy customers\n0.5          0.694        0.266        Balanced approach\n0.7          0.750        0.128        Flag few as risky - happy customers but miss defaults\n0.9          0.500        0.011        Flag few as risky - happy customers but miss defaults\n\n\n\n\nF1-Score: Balancing Precision and Recall\nIn many business scenarios, you can’t simply choose to optimize only precision or only recall. You need a single metric that captures both dimensions of model performance. Not only is it easier to explain one number than two separate metrics to business leaders, but often its because the business problem requires us to balance the performance of both precision and reall.\n\n\n\n\n\n\nTipExamples when a single metric is helpful\n\n\n\n\nMarketing campaigns: You need both precise targeting (don’t waste budget) AND good coverage (reach enough prospects)\nQuality control: You need to catch defects (recall) while avoiding shutdowns for false alarms (precision)\nModel comparison: When comparing multiple models, you need a single metric rather than separate precision and recall scores\n\n\n\nThe F1-score provides a single metric that combines precision and recall using the harmonic mean:\n\\[\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\]\n\n\n\n\n\n\nNoteWhy harmonic mean instead of simple average?\n\n\n\nThe harmonic mean is much more sensitive to low values than arithmetic mean. This means:\n\nIf either precision OR recall is poor, F1-score will be low\nYou can’t “game” the system by making one metric extremely high while ignoring the other\nF1-score only rewards models that perform reasonably well on BOTH dimensions\n\n\n\nIn practice, we often use F1-score when:\n\nBalanced priorities: Both precision and recall are important to your business objectives\nModel selection: You need a single metric to compare multiple models fairly\nImbalanced datasets: F1-score handles class imbalance better than accuracy\nEqual error costs: The business impact of false positives and false negatives is roughly equivalent\nComprehensive evaluation: You want to avoid models that excel at one metric while failing at the other\n\nLet’s calculate the F1-score for our Default prediction model:\n\n# Calculate F1-score manually and verify with sklearn\nf1 = 2 * (precision * recall) / (precision + recall)\n\n# Compare with sklearn\nfrom sklearn.metrics import f1_score\nsklearn_f1 = f1_score(y_test, y_pred)\n\n\n\nShow code for F1-score calculation and verification\nprint(\"F1-SCORE CALCULATION\")\nprint(\"=\" * 25)\nprint(f\"F1-Score = 2 × (Precision × Recall) / (Precision + Recall)\")\nprint(f\"F1-Score = 2 × ({precision:.3f} × {recall:.3f}) / ({precision:.3f} + {recall:.3f})\")\nprint(f\"F1-Score = {f1:.3f} or {f1:.1%}\")\n\nprint(f\"\\nSklearn verification: F1-Score = {sklearn_f1:.3f}\")\nprint(f\"Results match: {'✓' if abs(f1 - sklearn_f1) &lt; 0.001 else '✗'}\")\n\n\nF1-SCORE CALCULATION\n=========================\nF1-Score = 2 × (Precision × Recall) / (Precision + Recall)\nF1-Score = 2 × (0.694 × 0.266) / (0.694 + 0.266)\nF1-Score = 0.385 or 38.5%\n\nSklearn verification: F1-Score = 0.385\nResults match: ✓\n\n\nInterpreting Our F1-Score Results:\nOur Default prediction model achieves an F1-score of 0.385 (38.5%), which reveals important insights about its performance and business implications. This relatively low F1-score reflects the fundamental challenge of predicting rare events in highly imbalanced datasets—while our model demonstrates good precision (69.4%), meaning that when it flags a customer as high-risk it’s usually correct, it suffers from poor recall (26.6%), missing nearly three-quarters of customers who will actually default.\nFor a credit card company, this represents a critical business trade-off. The model’s conservative approach minimizes customer complaints from false alarms (maintaining good customer relationships), but it comes at the cost of substantial financial losses from the 69 defaults that go undetected. The low F1-score suggests that if the business objective requires balanced performance—catching more defaults while maintaining reasonable precision—the current 0.5 probability threshold may be too conservative. Lowering the threshold to capture more defaults would improve recall but reduce precision, highlighting the fundamental tension between these metrics that F1-score helps quantify in a single measure.\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNonePrecision vs. Recall Business Decisions\n\n\n\nFor each scenario, determine whether you would prioritize precision, recall, or balanced F1-score:\n\nAirport Security: TSA screening for dangerous items\n\nWhich metric should be prioritized? Why?\nWhat are the consequences of optimizing for the wrong metric?\n\nJob Resume Screening: Initial filter for qualified candidates\n\nHow do you balance missing good candidates vs. interviewing unqualified ones?\nHow might this change if you’re hiring for a critical, hard-to-fill position?\n\nProduct Recommendation System: Suggesting items customers might buy\n\nWhat happens if precision is too low? If recall is too low?\nHow does the business model (advertising revenue vs. direct sales) affect this?\n\nQuality Control: Detecting defective products before shipping\n\nConsider both immediate costs and long-term brand reputation\nHow might this differ for safety-critical vs. cosmetic defects?",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "24-classification-evaluation.html#roc-curves-and-auc-when-you-need-to-rank-customers",
    "href": "24-classification-evaluation.html#roc-curves-and-auc-when-you-need-to-rank-customers",
    "title": "24  Evaluating Classification Models",
    "section": "24.4 ROC Curves and AUC: When You Need to Rank Customers",
    "text": "24.4 ROC Curves and AUC: When You Need to Rank Customers\nSo far we’ve focused on making binary decisions—default or no default. But many business scenarios need something different: ranking customers from lowest risk to highest risk. This is where ROC curves and AUC become essential.\n\n\n\n\n\n\nNoteWhen Rankings Matter More Than Binary Decisions:\n\n\n\n\nInsurance pricing: You need to charge different rates based on risk levels, not just approve/deny\nLoan approval workflows: Create different approval tiers with varying terms and rates\n\nMarketing prioritization: Rank prospects from most likely to least likely to respond\nInvestment analysis: Score opportunities from highest to lowest potential return\n\n\n\n\nWhat is AUC? The Simple Explanation\nAUC (Area Under the Curve) answers this question: “If I randomly pick one high-risk customer and one low-risk customer, what’s the chance my model will correctly rank the high-risk customer as more risky?”\n\n# Calculate ROC curve and AUC score\nfpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\nauc_score = roc_auc_score(y_test, y_pred_proba)\n\n\n\nShow code for AUC explanation\nprint(f\"Our model's AUC: {auc_score:.3f}\")\nprint(f\"\\nSimple interpretation: {auc_score:.1%} chance our model correctly ranks\")\nprint(f\"a defaulting customer as higher risk than a non-defaulting customer.\")\n\n\nOur model's AUC: 0.947\n\nSimple interpretation: 94.7% chance our model correctly ranks\na defaulting customer as higher risk than a non-defaulting customer.\n\n\nInterpreting AUC Scores for Business Decisions:\nNow that we know our model’s AUC score, how do we interpret whether this is good enough for business use? AUC scores range from 0.5 (random guessing) to 1.0 (perfect ranking), but what constitutes “good enough” depends on your business context and risk tolerance. Here’s a practical guide for interpreting AUC scores and making deployment decisions:\n\n\n\nAUC Range\nQuality Rating\nBusiness Recommendation\n\n\n\n\n0.9 - 1.0\nOutstanding\nDeploy with confidence\n\n\n0.8 - 0.9\nExcellent\nStrong business value\n\n\n0.7 - 0.8\nGood\nUseful with monitoring\n\n\n0.6 - 0.7\nFair\nLimited value\n\n\n0.5 - 0.6\nPoor\nBarely better than random\n\n\nBelow 0.5\nProblematic\nModel has serious issues\n\n\n\n\n\nROC Curves: Visualizing Ranking Performance\nWhile the AUC gives us a single number to evaluate ranking quality, the ROC (Receiver Operating Characteristic) curve provides a visual representation of how our model performs across all possible decision thresholds. Think of it as a graph that shows the trade-off between catching defaults (True Positive Rate) and incorrectly flagging good customers (False Positive Rate).\nThe ROC curve plots:\n\nY-axis (True Positive Rate): How well we catch actual defaults = Recall\nX-axis (False Positive Rate): How often we incorrectly flag good customers\n\nThe closer the curve is to the top-left corner, the better the ranking ability—this represents high recall with low false positive rates.\n\n\n\n\n\n\n\n\n\n\n\nUsing Our Default Model for Risk-Based Pricing\nInstead of just approving or denying credit applications, what if our bank could offer different interest rates based on each customer’s predicted risk? This is where our model’s ranking ability (AUC) becomes valuable for business strategy.\nWhy Risk-Based Pricing Makes Business Sense:\nRather than using a single “yes/no” decision threshold, banks can use the probability scores to create pricing tiers. Low-risk customers get better rates (attracting good business), while high-risk customers pay premiums that reflect their actual default risk. This approach maximizes both profitability and market coverage.\n\n# Create risk tiers using our model's probability predictions\nrisk_buckets = pd.qcut(y_pred_proba, q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\nrisk_analysis = pd.DataFrame({\n    'Risk_Bucket': risk_buckets,\n    'Actual_Default': y_test\n}).groupby('Risk_Bucket', observed=True)['Actual_Default'].mean()\n\nprint(\"Default Rates by Risk Tier:\")\nfor bucket, default_rate in risk_analysis.items():\n    print(f\"{bucket:&gt;10}: {default_rate:.4%} default rate\")\n\nDefault Rates by Risk Tier:\n  Very Low: 0.0000% default rate\n       Low: 0.0000% default rate\n    Medium: 0.0000% default rate\n      High: 1.3333% default rate\n Very High: 14.3333% default rate\n\n\nOur model creates an excellent risk gradient from 0.0% (Very Low/Low/Medium tiers) to 14.3% (Very High) default rates. This demonstrates that our AUC of 0.947 translates into exceptional business value—the model creates such clear separation that the three lowest risk tiers have zero defaults, while the highest tier shows substantial risk. This enables confident risk-based pricing decisions.\n\n\n\n\n\n\nImportantThe Bottom Line\n\n\n\nROC/AUC is excellent for ranking and risk assessment, but be cautious with highly imbalanced datasets like ours. The curves can make performance look better than it actually is for the minority class you care about most.\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneROC vs. Precision-Recall: Choosing the Right Evaluation\n\n\n\nFor each business scenario, determine whether ROC/AUC or Precision-Recall curves would be more appropriate:\n\nCredit Scoring: Bank needs to rank loan applicants by default risk for pricing decisions\n\nDataset: 100,000 applications, 5% default rate\nBusiness goal: Risk-based pricing across risk spectrum\n\nRare Disease Detection: Medical test for disease affecting 0.1% of population\n\nDataset: 1,000,000 patients, 0.1% disease rate\nBusiness goal: Minimize missed cases while controlling false alarms\n\nCustomer Churn Prediction: Identify customers likely to cancel subscriptions\n\nDataset: 50,000 customers, 15% churn rate\n\nBusiness goal: Target retention campaigns effectively\n\nQuality Control: Detect defective products in manufacturing\n\nDataset: 100,000 products, 2% defect rate\nBusiness goal: Prevent defective products from shipping\n\nInsurance Premium Pricing: Auto insurance company setting rates based on accident risk\n\nDataset: 500,000 drivers, 8% accident rate\nBusiness goal: Create tiered pricing structure from low-risk to high-risk drivers\n\n\nFor each scenario, explain your reasoning and what the chosen metric tells you about model performance.",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "24-classification-evaluation.html#choosing-the-right-metric-for-your-business-context",
    "href": "24-classification-evaluation.html#choosing-the-right-metric-for-your-business-context",
    "title": "24  Evaluating Classification Models",
    "section": "24.5 Choosing the Right Metric for Your Business Context",
    "text": "24.5 Choosing the Right Metric for Your Business Context\nThroughout this chapter, we’ve explored multiple classification metrics—each serving different business purposes. The most sophisticated aspect of classification evaluation is aligning your choice of metrics with your specific business context and cost structure. Rather than asking “What’s the best metric?” the right question is “What business outcomes am I trying to optimize?”\n\nA Framework for Metric Selection\nThe decision process starts with understanding your primary business concern:\n\n\n\n\n\n\n\n\n\nThis framework guides you from your primary business concern to the most appropriate metric. Let’s translate this into practical guidance:\n\n\nComplete Metric Selection Reference\n\n\n\n\n\n\n\n\n\nBusiness Priority\nRecommended Metric\nWhen to Use\nExample Scenarios\n\n\n\n\nMinimize False Alarms\nPrecision\nFalse positives are expensive or damaging\n• Credit card fraud detection• Email spam filtering• Medical diagnosis confirmation\n\n\nCatch All Important Cases\nRecall\nMissing positives is dangerous or costly\n• Disease screening• Safety system alerts• Security threat detection\n\n\nBalance Both Concerns\nF1-Score\nBoth error types matter equally\n• Marketing campaign targeting• Quality control systems• Model comparison studies\n\n\nRank by Risk Level\nROC-AUC\nNeed to stratify customers/cases\n• Insurance pricing• Loan approval workflows• Investment risk assessment\n\n\nSimple Communication\nAccuracy\nBalanced classes, equal error costs\n• Simple classification with balanced data• Initial model exploration\n\n\n\n\n\nQuick Decision Rules\nFor rapid metric selection in common scenarios:\n\nUse PRECISION when: False positives cost more than false negatives (customer experience focus)\nUse RECALL when: False negatives cost more than false positives (safety/compliance focus)\n\nUse F1-SCORE when: You need balanced performance or want to compare models with a single metric\nUse ROC-AUC when: You need ranking quality across all thresholds (pricing/stratification focus)\nAvoid ACCURACY when: You have imbalanced classes (like our 3% default rate)\n\n\n\n\n\n\n\nImportantRemember:\n\n\n\nThe “best” metric is the one that aligns with your business objectives and cost structure. Our Default dataset example showed how different metrics (precision = 69%, recall = 27%, F1 = 39%, AUC = 95%) tell different stories about the same model’s performance.",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "24-classification-evaluation.html#summary",
    "href": "24-classification-evaluation.html#summary",
    "title": "24  Evaluating Classification Models",
    "section": "24.6 Summary",
    "text": "24.6 Summary\nThis chapter transformed your understanding of classification model evaluation from the simple but misleading concept of “accuracy” to a comprehensive toolkit that aligns with real business needs. You discovered that effective classification evaluation requires understanding not just whether your model is correct, but how it makes mistakes and what those mistakes cost your business.\nKey evaluation concepts you mastered include:\n\nThe accuracy trap: Why 97.3% accuracy can be misleading when only 3% of customers default—high accuracy doesn’t guarantee business value\nConfusion matrices: The 2×2 foundation showing exactly where your model succeeds and fails, enabling business impact analysis\nPrecision and recall: Understanding when to prioritize accuracy of positive predictions vs. completeness of detection\nF1-score: Combining precision and recall into a single metric when both matter equally to your business\nROC curves and AUC: Evaluating ranking quality for risk-based pricing and customer stratification\nBusiness-aligned frameworks: A systematic approach to selecting metrics based on error costs and business priorities\nProper evaluation methodology: Using train/test splits to ensure reliable model assessment\n\nThe critical business insight is that the “best” metric depends entirely on your business objectives and cost structure. Our Default dataset example demonstrated how the same model can appear excellent (95% AUC) or concerning (27% recall) depending on which business lens you apply. This chapter equipped you with the framework to make these trade-offs intelligently.\nReal-world impact of this knowledge includes correctly evaluating credit risk models, fraud detection systems, medical screening tools, and marketing campaign algorithms. You learned to create risk-based pricing tiers, understand precision-recall trade-offs in customer experience vs. loss prevention, and design evaluation strategies that align with specific business costs.\nFoundation for future learning: These evaluation principles apply to every classification algorithm you’ll encounter—decision trees, random forests, neural networks, and beyond. The framework for connecting model performance to business outcomes remains constant, regardless of algorithmic complexity. You now have the foundation to evaluate any classification model through the lens of business value rather than just technical metrics.\n\nQuick Reference: Classification Evaluation Metrics\n\n\n\n\n\n\n\n\n\nMetric\nFormula\nBusiness Use Case\nWhen to Prioritize\n\n\n\n\nAccuracy\n(TP + TN) / Total\nOverall correctness\nBalanced classes, equal error costs\n\n\nPrecision\nTP / (TP + FP)\nQuality of positive predictions\nHigh cost of false positives\n\n\nRecall (Sensitivity)\nTP / (TP + FN)\nCompleteness of positive detection\nHigh cost of false negatives\n\n\nF1-Score\n2 × (Precision × Recall) / (Precision + Recall)\nBalanced precision-recall\nNeed single metric, balanced priorities\n\n\nSpecificity\nTN / (TN + FP)\nQuality of negative predictions\nImportant to correctly identify negatives\n\n\nROC-AUC\nArea under ROC curve\nRanking/probability quality\nRisk stratification, pricing models\n\n\nConfusion Matrix\n2×2 table of predictions\nError pattern analysis\nUnderstanding specific mistake types",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "24-classification-evaluation.html#end-of-chapter-exercises",
    "href": "24-classification-evaluation.html#end-of-chapter-exercises",
    "title": "24  Evaluating Classification Models",
    "section": "24.7 End of Chapter Exercises",
    "text": "24.7 End of Chapter Exercises\nThese exercises build directly on the logistic regression exercises from Chapter 23, extending them to include the classification evaluation metrics and business-aligned thinking you’ve learned in this chapter. You’ll apply the same datasets and scenarios but now evaluate model performance using precision, recall, F1-score, ROC/AUC, and business cost analysis.\n\n\n\n\n\n\nNoneExercise 1: Stock Market Direction Prediction with Trading Strategy Evaluation\n\n\n\n\n\nCompany: Investment management firm\nGoal: Build on Chapter 23’s market direction prediction but now evaluate trading strategy performance using classification metrics\nDataset: Weekly dataset from ISLP package\nBusiness Context: The firm wants to implement an automated trading strategy. False positives (predicting “Up” when market goes down) lead to losses from bad trades (~$10,000 cost per mistake). False negatives (predicting “Down” when market goes up) represent missed profitable opportunities (~$5,000 opportunity cost per mistake).\n\nfrom ISLP import load_data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\n\nWeekly = load_data('Weekly')\nprint(\"Weekly dataset loaded for trading strategy evaluation\")\n\nWeekly dataset loaded for trading strategy evaluation\n\n\nYour Tasks:\n\nReproduce Chapter 23 model: Build the logistic regression model predicting market direction using lag variables, but now split data properly into train/test sets\nBusiness cost analysis:\n\nGiven the trading costs above, which type of error is more expensive?\nShould the trading firm prioritize precision or recall? Why?\nCalculate the total business cost of false positives vs. false negatives\n\nClassification metrics evaluation:\n\nCreate and interpret the confusion matrix for trading decisions\nCalculate precision, recall, and F1-score\nCompute ROC-AUC for the model’s ranking ability\n\nTrading strategy optimization:\n\nTest different probability thresholds (0.3, 0.5, 0.7) for making “buy” decisions\nFor each threshold, calculate precision, recall, and total business cost\nWhich threshold minimizes total expected losses?\n\nBusiness insights:\n\nUsing ROC-AUC, assess whether the model can effectively rank weeks by “up” probability\nHow would you recommend the portfolio manager use this model?\nWhat are the limitations of this approach for real trading decisions?\n\nAdvanced analysis: Create a precision-recall curve and identify the threshold that maximizes profit given the business costs\n\n\n\n\n\n\n\n\n\n\nNoneExercise 2: Consumer Purchase Behavior with Marketing Campaign Optimization\n\n\n\n\n\nCompany: Orange juice manufacturer\nGoal: Extend Chapter 23’s brand choice prediction to optimize targeted marketing campaigns using classification evaluation\nDataset: OJ dataset from ISLP package\nBusiness Context: The company wants to send targeted coupons to customers likely to purchase their brand (Citrus Hill). Each coupon costs $2 to send and process. Customers who receive coupons and purchase generate $8 profit. Customers who receive coupons but don’t purchase result in $2 loss. Missing customers who would have purchased (no coupon sent) represents $5 opportunity cost.\n\nOJ = load_data('OJ')\nprint(\"OJ dataset loaded for marketing campaign optimization\")\n\nOJ dataset loaded for marketing campaign optimization\n\n\nYour Tasks:\n\nReproduce Chapter 23 model: Build the logistic regression model predicting brand choice (focusing on Citrus Hill as positive class), including proper train/test evaluation\nMarketing cost framework:\n\nWhich error type is more costly: sending coupons to non-buyers or missing potential buyers?\nShould the marketing team prioritize precision (coupon efficiency) or recall (market coverage)?\nCalculate expected ROI for different precision/recall combinations\n\nCampaign targeting evaluation:\n\nCreate confusion matrix for coupon targeting decisions\nCalculate precision (% of coupon recipients who buy), recall (% of buyers who received coupons)\nCompute F1-score as a balanced campaign effectiveness measure\n\nThreshold optimization for profitability:\n\nTest probability thresholds from 0.1 to 0.9 in 0.1 increments\nFor each threshold, calculate: customers targeted, expected profit, campaign ROI\nIdentify the threshold that maximizes total profit\n\nSegment analysis:\n\nCompare model performance (precision, recall, AUC) for different customer segments\nUse the demographic variables in the dataset to identify high-value targeting opportunities\n\nStrategic recommendations:\n\nBased on your analysis, what targeting strategy would you recommend?\nHow does the optimal strategy change if coupon costs increase to $3?\nWhat additional data would improve targeting effectiveness?\n\n\n\n\n\n\n\n\n\n\n\nNoneExercise 3: Medical Risk Assessment with Clinical Decision Support\n\n\n\n\n\nCompany: Healthcare analytics firm\nGoal: Extend Chapter 23’s heart disease prediction to support clinical decision-making using appropriate evaluation metrics\nDataset: Heart dataset from ISLP package (or simulated medical data)\nBusiness Context: Doctors use the model to decide whether to order additional cardiac tests for patients. False positives lead to unnecessary tests (~$1,500 cost per patient) and patient anxiety. False negatives result in missed diagnoses, leading to delayed treatment and potentially serious health consequences (~$25,000 cost including treatment and liability).\n\n# Use simulated data as in Chapter 23 exercise\nimport numpy as np\nimport pandas as pd\n\nprint(\"Creating simulated medical data for clinical evaluation\")\nnp.random.seed(42)\n\n# Simulate realistic medical data\nn = 1000\nage = np.random.normal(55, 15, n)\nage = np.clip(age, 20, 85)\n\ncholesterol = np.random.normal(220, 40, n)\ncholesterol = np.clip(cholesterol, 150, 350)\n\nblood_pressure = np.random.normal(130, 20, n)\nblood_pressure = np.clip(blood_pressure, 90, 200)\n\n# Heart disease probability increases with age, cholesterol, BP\nrisk_score = -8 + 0.05*age + 0.01*cholesterol + 0.02*blood_pressure\nheart_disease = np.random.binomial(1, 1/(1 + np.exp(-risk_score)))\n\nHeart = pd.DataFrame({\n    'Age': age,\n    'Cholesterol': cholesterol,\n    'Blood_Pressure': blood_pressure,\n    'Heart_Disease': heart_disease\n})\n\nprint(f\"Heart disease rate: {Heart['Heart_Disease'].mean():.1%}\")\n\nCreating simulated medical data for clinical evaluation\nHeart disease rate: 38.4%\n\n\nYour Tasks:\n\nReproduce Chapter 23 model: Build the logistic regression model for heart disease prediction with proper train/test methodology\nClinical cost analysis:\n\nGiven the costs above, which error type has higher consequences?\nFor patient safety, should the model prioritize precision or recall?\nCalculate expected cost per patient for false positives vs. false negatives\n\nMedical decision support evaluation:\n\nCreate confusion matrix for test ordering decisions\nCalculate precision (% of positive predictions that are true cases)\nCalculate recall (% of actual cases detected) - critical for patient safety\nAssess ROC-AUC for the model’s ability to rank patients by risk\n\nClinical threshold analysis:\n\nTest different probability thresholds for ordering additional tests\nFor each threshold, calculate: sensitivity (recall), specificity, total expected cost\nIdentify threshold that minimizes total healthcare costs while maintaining patient safety\n\nRisk stratification:\n\nUse probability scores to create risk tiers (low, medium, high, very high)\nAnalyze heart disease rates in each tier\nRecommend different clinical actions for each risk level\n\nEthical considerations:\n\nHow do you balance healthcare costs with patient safety?\nWhat are the implications of false negatives in medical AI?\nHow would you communicate model limitations to doctors?\nWhat additional validation would be needed before clinical deployment?",
    "crumbs": [
      "Module 9",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "25-decision-trees.html",
    "href": "25-decision-trees.html",
    "title": "25  Decision Trees: Foundations and Interpretability",
    "section": "",
    "text": "25.1 When Linear Methods Hit Their Limits\nAfter mastering linear and logistic regression, you might think these methods can handle any business problem. However, many real-world relationships don’t follow the straight-line assumptions that linear methods require. Consider these challenging business scenarios:\nThis chapter introduces decision trees, algorithms designed specifically to handle the limitations of linear methods. Unlike regression models that assume relationships are linear and consistent across all observations, decision trees automatically discover non-linear patterns, complex interactions, and context-dependent rules that mirror how humans actually make decisions.\nBy the end of this chapter, you will be able to:\nYou’ve built solid foundations with linear and logistic regression, but these methods make strong assumptions that don’t always match business reality. Understanding these limitations helps you recognize when decision trees (and later, more advanced methods) become necessary.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Decision Trees: Foundations and Interpretability</span>"
    ]
  },
  {
    "objectID": "25-decision-trees.html#when-linear-methods-hit-their-limits",
    "href": "25-decision-trees.html#when-linear-methods-hit-their-limits",
    "title": "25  Decision Trees: Foundations and Interpretability",
    "section": "",
    "text": "Where Linear Methods Struggle\nUnderstanding why linear methods fall short helps you recognize when more sophisticated approaches like decision trees become necessary. Let’s explore these limitations with concrete examples that demonstrate how real business relationships often violate linear assumptions.\n1. Non-linear Relationships: Linear models assume that changes in predictors have consistent effects across their entire range. But real business relationships often involve complex curves, thresholds, and saturation points.\n\n\nShow code for non-linear relationship examples\n# Demonstrate non-linear relationships that linear models can't capture\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nplt.figure(figsize=(9, 4))\n\n# Example 1: Diminishing Returns - Marketing Spend vs Sales\nplt.subplot(1, 2, 1)\nmarketing_spend = np.linspace(0, 100, 150)\n# Sales show diminishing returns - big gains early, then plateau\nsales = 50 + 40 * np.sqrt(marketing_spend) + np.random.normal(0, 8, len(marketing_spend))\nsales = np.clip(sales, 0, None)\n\n# Fit linear model\nlinear_model1 = LinearRegression()\nX_marketing = marketing_spend.reshape(-1, 1)\nlinear_model1.fit(X_marketing, sales)\nlinear_pred1 = linear_model1.predict(X_marketing)\n\nplt.scatter(marketing_spend, sales, alpha=0.6, color='steelblue', s=25, label='Actual Sales')\nplt.plot(marketing_spend, linear_pred1, color='red', linewidth=3, label='Linear Model')\nplt.xlabel('Marketing Spend ($000s)')\nplt.ylabel('Sales ($000s)')\nplt.title('Diminishing Returns Pattern')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Example 2: Threshold Effect - Experience vs Performance\nplt.subplot(1, 2, 2)\nexperience_years = np.linspace(0, 20, 150)\n# Performance jumps after certain experience levels\nperformance = np.where(\n    experience_years &lt; 2, 60 + experience_years * 5,  # Slow start\n    np.where(\n        experience_years &lt; 8, 70 + (experience_years - 2) * 15,  # Rapid improvement\n        160 + (experience_years - 8) * 2  # Gradual improvement\n    )\n) + np.random.normal(0, 5, len(experience_years))\n\n# Fit linear model\nlinear_model2 = LinearRegression()\nX_experience = experience_years.reshape(-1, 1)\nlinear_model2.fit(X_experience, performance)\nlinear_pred2 = linear_model2.predict(X_experience)\n\nplt.scatter(experience_years, performance, alpha=0.6, color='darkgreen', s=25, label='Actual Performance')\nplt.plot(experience_years, linear_pred2, color='red', linewidth=3, label='Linear Model')\nplt.xlabel('Years of Experience')\nplt.ylabel('Performance Score')\nplt.title('Threshold Effects Pattern')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThese examples reveal critical business patterns that linear models consistently miss:\n\nDiminishing returns (left plot): Marketing spend shows dramatic early gains that plateau over time, but linear models assume constant returns throughout\nThreshold effects (right plot): Employee performance jumps occur at specific experience milestones—junior employees improve slowly, then accelerate rapidly between years 2-8, then plateau again\nNon-linear optimization: Linear models would suggest unlimited marketing spend or that all experience years are equally valuable, leading to poor resource allocation decisions\n\n2. Complex Interactions: Linear models require you to manually specify interactions (like creating age × income features). But in real business data, the most important interactions often involve multiple variables and aren’t obvious upfront.\n\n\nShow code for complex interaction example\n# Demonstrate complex interactions: Product pricing strategy\nnp.random.seed(123)\nn_products = 400\n\n# Generate product data\nproduct_quality = np.random.uniform(1, 10, n_products)\nbrand_reputation = np.random.choice([1, 2, 3], n_products, p=[0.4, 0.4, 0.2])  # 1=unknown, 2=known, 3=premium\nmarket_competition = np.random.uniform(1, 5, n_products)  # 1=low competition, 5=high competition\n\n# Complex interaction: pricing strategy depends on quality AND brand AND competition\n# High quality + premium brand + low competition = premium pricing\n# High quality + unknown brand + high competition = competitive pricing\n# The effect of quality depends entirely on brand and competition context\npricing_multiplier = np.where(\n    (product_quality &gt; 7) & (brand_reputation == 3) & (market_competition &lt; 2.5),\n    2.5 + product_quality * 0.3,  # Premium pricing strategy\n    np.where(\n        (product_quality &gt; 7) & (brand_reputation == 1) & (market_competition &gt; 3.5),\n        0.8 + product_quality * 0.1,  # Competitive pricing strategy\n        1.0 + product_quality * 0.15  # Standard pricing\n    )\n)\n\n# Calculate final prices with some noise\nbase_cost = 50\nprice = base_cost * pricing_multiplier + np.random.normal(0, 10, n_products)\nprice = np.clip(price, 30, 300)\n\n# Create visualization showing the interaction\nplt.figure(figsize=(9, 4))\n\n# Plot 1: Quality vs Price by Brand (shows interaction)\nplt.subplot(1, 2, 1)\ncolors = ['red', 'orange', 'blue']\nbrand_names = ['Unknown', 'Known', 'Premium']\nfor i, brand in enumerate([1, 2, 3]):\n    mask = brand_reputation == brand\n    plt.scatter(product_quality[mask], price[mask],\n               alpha=0.7, color=colors[i], s=30, label=f'{brand_names[i]} Brand')\n\nplt.xlabel('Product Quality (1-10)')\nplt.ylabel('Price ($)')\nplt.title('Quality-Price Relationship by Brand')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Same quality, different contexts = different prices\nplt.subplot(1, 2, 2)\n# Focus on high-quality products (quality &gt; 7) to show interaction effect\nhigh_quality = product_quality &gt; 7\npremium_brand_low_comp = (brand_reputation == 3) & (market_competition &lt; 2.5) & high_quality\nunknown_brand_high_comp = (brand_reputation == 1) & (market_competition &gt; 3.5) & high_quality\n\nplt.scatter(market_competition[premium_brand_low_comp], price[premium_brand_low_comp],\n           alpha=0.8, color='blue', s=40, label='Premium Brand + High Quality')\nplt.scatter(market_competition[unknown_brand_high_comp], price[unknown_brand_high_comp],\n           alpha=0.8, color='red', s=40, label='Unknown Brand + High Quality')\n\nplt.xlabel('Market Competition (1-5)')\nplt.ylabel('Price ($)')\nplt.title('Same Quality, Different Pricing Strategy')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe pricing example reveals how business interactions work in practice. In the left plot, notice how the relationship between quality and price completely changes depending on brand reputation - premium brands can charge much more for the same quality level. The right plot shows an even more striking interaction: products with identical high quality end up with dramatically different prices depending on the competitive context and brand positioning.\nThis demonstrates why linear models often miss the mark in business contexts:\n\nContext-dependent relationships: The value of “high quality” depends entirely on brand reputation and competitive environment\nSegment-specific strategies: Premium brands follow completely different pricing rules than unknown brands\nMulti-way interactions: Success requires understanding how quality AND brand AND competition work together, not just their individual effects and this is very difficult for linear models to capture.\n\n3. Mixed Data Types: Linear models require extensive preprocessing for categorical variables, often losing important information in the process:\n\nDummy encoding explosion: A categorical variable with 20 categories becomes 19 binary columns with sparse data\nLost ordinal relationships: Converting “Low/Medium/High” to binary variables loses the natural ordering\nProduction brittleness: New categories in live data can break existing dummy encoding schemes\n\n4. Rule Generation: Linear coefficients don’t easily translate to actionable business rules that stakeholders can understand and implement:\n\nMathematical abstractions: “Increase marketing coefficient by 0.003” isn’t as useful as “If customer age &gt; 45 AND income &gt; $75k, then offer premium products”\nStakeholder communication: Business leaders want clear decision criteria, not mathematical equations\nImplementation challenges: Complex linear combinations are harder to operationalize than simple if-then rules\n\n\n\n\n\n\n\nNoteWhy Linear Models Struggle in Business\n\n\n\nLinear models make strong assumptions that rarely hold in real business environments: they assume relationships are straight lines, effects are consistent across all data ranges, and interactions must be manually specified. Business data typically exhibits non-linear patterns like diminishing returns, threshold effects, and complex multi-way interactions that change based on context.\nNon-linear models like decision trees offer a more flexible approach by automatically discovering these complex patterns without requiring you to specify them upfront. Instead of forcing data into linear relationships, trees adapt to the natural structure of business data—making them particularly valuable when relationships are complex and stakeholder interpretability matters.\n\n\n\n\nWhere Decision Trees Excel\nWhile linear methods struggle with the complexities we’ve just described, decision trees are designed specifically to handle these challenges. Think of decision trees as the business world’s natural problem-solving approach—breaking complex decisions into a series of simple yes/no questions. Here are the key advantages that make trees particularly powerful for business applications:\n\nAutomatic Threshold Detection: Trees find meaningful cut-points in your data without you having to specify them.\nNatural Interaction Modeling: Trees automatically create different rules for different subgroups, capturing complex interactions.\nMixed Data Handling: Trees can conceptually handle numeric, categorical, and ordinal data (though sklearn requires encoding).\nBusiness-Friendly Output: Trees generate interpretable “if-then” rules that translate directly into business processes.\nNo Distribution Assumptions: Trees don’t assume your data follows any particular statistical distribution.\n\nBeyond these individual strengths, decision trees excel because they mirror how humans naturally make complex decisions in business contexts. When evaluating loan applications, hiring candidates, or diagnosing problems, we instinctively break down complex situations into a series of simpler questions. Decision trees formalize this intuitive approach, making them both powerful and understandable.\nConsider how a seasoned sales manager evaluates leads: they might first ask “Is the company budget above $100K?” Then, depending on the answer, ask different follow-up questions. High-budget prospects get questions about decision timeline and authority, while lower-budget prospects get questions about growth potential and pain points. This context-dependent questioning is exactly how decision trees operate—automatically learning the most informative questions and when to ask them.\n\n\n\n\n\nflowchart TD\n    A[Sales Lead] --&gt; B{\"Budget above 100K?\"}\n    B --&gt;|Yes| C{\"Timeline under 6 months?\"}\n    B --&gt;|No| D{\"High Growth Potential?\"}\n    C --&gt;|Yes| E{\"Decision Maker Access?\"}\n    C --&gt;|No| F[Nurture Lead]\n    D --&gt;|Yes| G{\"Significant Pain Points?\"}\n    D --&gt;|No| H[Low Priority]\n    E --&gt;|Yes| I[High Priority Prospect]\n    E --&gt;|No| J[Identify Decision Maker]\n    G --&gt;|Yes| K[Medium Priority Prospect]\n    G --&gt;|No| L[Monitor for Changes]\n\n    style A fill:#e1f5fe\n    style I fill:#c8e6c9\n    style K fill:#fff3c4\n    style F fill:#ffcdd2\n    style H fill:#ffcdd2\n    style J fill:#fff3c4\n    style L fill:#ffcdd2\n\n\n\n\n\n\nThe combination of these advantages makes decision trees particularly valuable in business environments where both accuracy and interpretability matter. Unlike black-box algorithms that provide predictions without explanations, trees offer a clear audit trail from input features to final decisions.\n\n\n\n\n\n\nNoteDecision Tree Advantages Summary\n\n\n\n\nFlexibility: Automatically handle non-linear relationships, complex interactions, and mixed data types without manual specification\nInterpretability: Generate clear if-then rules that stakeholders can understand, verify, and implement in business processes\nRobustness: Work with imperfect data (missing values, outliers) and make no assumptions about underlying distributions\nBusiness Alignment: Mirror natural human decision-making processes, making them intuitive for domain experts to evaluate and trust",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Decision Trees: Foundations and Interpretability</span>"
    ]
  },
  {
    "objectID": "25-decision-trees.html#how-decision-trees-think",
    "href": "25-decision-trees.html#how-decision-trees-think",
    "title": "25  Decision Trees: Foundations and Interpretability",
    "section": "25.2 How Decision Trees Think",
    "text": "25.2 How Decision Trees Think\nDecision trees work by learning a series of yes/no questions that best separate the data into meaningful groups. Each question splits the data based on a single feature, creating a tree-like structure where each internal node represents a question, each branch represents an answer, and each leaf represents a final prediction.\n\nThe Decision-Making Process\nImagine you’re a loan officer deciding whether to approve credit applications. You might naturally think through questions like:\n\n“Is the applicant’s income above $50,000?”\nIf yes: “Is their credit score above 700?”\nIf no: “Do they have a co-signer?”\n\n\n\n\n\n\nflowchart TD\n    A[Loan Application] --&gt; B{\"Income above 50K?\"}\n    B --&gt;|Yes| C{\"Credit Score above 700?\"}\n    B --&gt;|No| D{\"Has Co-signer?\"}\n    C --&gt;|Yes| E[Approve Loan]\n    C --&gt;|No| F[Review Additional Factors]\n    D --&gt;|Yes| G[Consider Approval]\n    D --&gt;|No| H[Deny Loan]\n\n    style A fill:#e1f5fe\n    style E fill:#c8e6c9\n    style G fill:#fff3c4\n    style F fill:#fff3c4\n    style H fill:#ffcdd2\n\n\n\n\n\n\nThis sequential questioning process is exactly how decision trees operate, but they learn the optimal questions and thresholds automatically from data.\n\n\nTree Anatomy: Nodes, Splits, and Leaves\nBefore we build our first tree, let’s understand the key components:\n\nRoot Node: The starting point where all data begins\nInternal Nodes: Decision points that ask yes/no questions\nBranches: Paths representing answers to questions\nLeaf Nodes: Final destinations that provide predictions\nDepth: How many questions deep the tree goes\n\n\n\n\n\n\nflowchart TD\n    A[\"🏠 ROOT NODE&lt;br/&gt;All Data Starts Here\"] --&gt; B{\"🔀 INTERNAL NODE&lt;br/&gt;Question 1\"}\n    B --&gt;|\"📈 BRANCH&lt;br/&gt;(Yes)\"| C{\"🔀 INTERNAL NODE&lt;br/&gt;Question 2A\"}\n    B --&gt;|\"📉 BRANCH&lt;br/&gt;(No)\"| D{\"🔀 INTERNAL NODE&lt;br/&gt;Question 2B\"}\n    C --&gt;|\"📈 BRANCH&lt;br/&gt;(Yes)\"| E[\"🎯 LEAF NODE&lt;br/&gt;Prediction A\"]\n    C --&gt;|\"📉 BRANCH&lt;br/&gt;(No)\"| F[\"🎯 LEAF NODE&lt;br/&gt;Prediction B\"]\n    D --&gt;|\"📈 BRANCH&lt;br/&gt;(Yes)\"| G[\"🎯 LEAF NODE&lt;br/&gt;Prediction C\"]\n    D --&gt;|\"📉 BRANCH&lt;br/&gt;(No)\"| H[\"🎯 LEAF NODE&lt;br/&gt;Prediction D\"]\n\n    %% Depth indicators\n    I[\"📏 DEPTH = 3&lt;br/&gt;(3 levels of questions)\"]\n\n    %% Styling\n    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px\n    style B fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style C fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style D fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style E fill:#e8f5e8,stroke:#388e3c,stroke-width:2px\n    style F fill:#e8f5e8,stroke:#388e3c,stroke-width:2px\n    style G fill:#e8f5e8,stroke:#388e3c,stroke-width:2px\n    style H fill:#e8f5e8,stroke:#388e3c,stroke-width:2px\n    style I fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n\n\n\n\n\n\n\n\nLet’s See This in Action\nTo understand how decision trees work, let’s start simple and build up complexity. We’ll begin with a tree that uses just one variable, then expand to show how multiple variables work together.\n\nExample 1: Single-Variable Decision Tree\nLet’s start with the simplest possible case—predicting loan approval based solely on income. We’ll create a synthetic dataset that mimics realistic loan approval patterns, where higher incomes generally lead to higher approval rates, but with some natural variation to reflect real-world complexity:\n\n\nShow code for simple one-variable example\n# Create simple loan approval dataset with one variable\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate simple income data\nn_samples = 200\nincome = np.random.uniform(30000, 120000, n_samples)\n\n# Create simple approval logic based on income thresholds\n# Higher income = higher approval probability, but with some variation\napproval_prob = np.where(income &lt; 50000, 0.2,\n                        np.where(income &lt; 80000, 0.7, 0.9))\n# Add some randomness\napproval_prob += np.random.normal(0, 0.1, n_samples)\napproved = (approval_prob &gt; 0.5).astype(int)\n\n# Create DataFrame\nsimple_data = pd.DataFrame({\n    'income': income,\n    'approved': approved\n})\n\nprint(f\"Sample size: {len(simple_data)}\")\nprint(f\"Overall approval rate: {simple_data['approved'].mean():.1%}\")\nprint(\"\\nFirst few rows of our simulated loan data:\")\nprint(simple_data.head())\n\n# Build simple decision tree with one variable\nX_simple = simple_data[['income']]\ny_simple = simple_data['approved']\n\n# Create a very simple tree (max_depth=2) to see clear splits\nsimple_tree = DecisionTreeClassifier(max_depth=2, min_samples_split=20, random_state=42)\nsimple_tree.fit(X_simple, y_simple)\n\nprint(f\"Single-variable tree accuracy: {simple_tree.score(X_simple, y_simple):.3f}\")\n\n\nSample size: 200\nOverall approval rate: 75.5%\n\nFirst few rows of our simulated loan data:\n          income  approved\n0   63708.610696         1\n1  115564.287577         1\n2   95879.454763         1\n3   83879.263578         1\n4   44041.677640         0\nSingle-variable tree accuracy: 0.985\n\n\n\n\nShow code for visualizing single-variable tree\n# Visualize the simple tree\nplt.figure(figsize=(8, 5))\ntree.plot_tree(\n    simple_tree,\n    feature_names=['Income'],\n    class_names=['Denied', 'Approved'],\n    filled=True,\n    rounded=True,\n    fontsize=10\n)\nplt.title(\"Simple Decision Tree: Loan Approval Based on Income Only\", fontsize=14, pad=20)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteReading Tree Node Information\n\n\n\nEach box (node) in the tree diagram contains valuable information:\n\nSplitting Condition (top line): The yes/no question being asked (e.g., “Income ≤ 50,187.5”)\ngini: Impurity measure ranging from 0.0 to 0.5 (more on this below)\nsamples: Number of data points that reach this node\nvalue: Count for each class as [denied_count, approved_count]\nclass: The predicted class for observations at this node (shown by the majority class)\nColor: Node color indicates the dominant class - darker colors mean more pure (confident) predictions\n\nPath Navigation: Follow the left branch when the condition is TRUE, right branch when FALSE.\n\n\n\n\n\nUnderstanding How CART Trees Make Decisions\nThe tree you see above was built using the CART (Classification and Regression Trees) algorithm—the foundation of scikit-learn’s DecisionTreeClassifier. Understanding how CART chooses where to split helps you appreciate why trees are so powerful for business applications.\nHow CART Finds the Best Split:\n\nExhaustive Search: At each node, CART considers every possible split for every feature. For income, it tests thresholds like “income ≤ $45,000”, “income ≤ $46,000”, etc.\nPurity Measurement: Each potential split is evaluated using Gini impurity, which measures how “mixed” the resulting groups are:\n\nGini = 0: Perfect purity (all approved or all denied)\nGini = 0.5: Maximum impurity (50/50 mix)\n\nBest Split Selection: CART chooses the split that creates the largest reduction in impurity—effectively asking “Which question best separates our data into distinct groups?”\nRecursive Splitting: The process repeats for each resulting branch until stopping criteria are met.\n\nWhy This Matters for Business: This systematic approach means trees automatically discover the income thresholds that matter most for loan decisions. In our example, the tree found that income around $50,187 is a critical decision point—not because we told it to look there, but because that’s where the data naturally splits between approvals and denials. Notice how the tree then makes additional splits at $47,215 (for lower incomes) and $58,021 (for higher incomes), creating distinct income bands with different approval patterns.\n\n\n\n\n\n\nImportantUnderstanding Gini Impurity\n\n\n\nGini impurity is CART’s way of measuring how “mixed up” or “messy” a group is in terms of the classes we’re trying to predict. Think of it as a messiness meter:\n\nPerfect Organization (Gini = 0.0): All loans in the group have the same outcome—either all approved or all denied. This is what we want! A perfectly pure group means we can make confident predictions.\nMaximum Mess (Gini = 0.5): The group is a 50/50 mix of approved and denied loans. This is the worst case—we’re essentially flipping a coin to make predictions.\nIn Between (Gini = 0.1 to 0.4): Most groups fall somewhere in the middle, with one outcome being more common than the other.\n\nWhy CART Loves Lower Gini: The algorithm always seeks splits that minimize Gini impurity because more organized groups lead to more confident, accurate predictions. When CART compares potential splits, it calculates: “If I split here, how much will the overall messiness decrease?” The split that creates the biggest reduction in messiness wins.\nBusiness Translation: Lower Gini impurity means clearer business rules. A node with Gini = 0.1 represents a very reliable business segment, while Gini = 0.4 suggests you need more information to make confident decisions about that group.\n\n\n\nExample 2: Two-Variable Decision Tree\nNow let’s see how the tree handles two variables—income and credit score:\n\n\nShow code for two-variable example\n# Create dataset with two variables\nnp.random.seed(123)\nn_samples = 300\n\n# Generate income and credit score\nincome = np.random.uniform(30000, 120000, n_samples)\ncredit_score = np.random.uniform(500, 800, n_samples)\n\n# Create approval logic based on both variables\n# Good income OR good credit = likely approval\n# Both good = very likely approval\napproval_prob = (\n    0.3 * (income &gt; 60000) +\n    0.4 * (credit_score &gt; 650) +\n    0.2 * ((income &gt; 60000) & (credit_score &gt; 650)) +  # Bonus for both\n    np.random.normal(0, 0.1, n_samples)  # Add noise\n)\n\napproved = (approval_prob &gt; 0.5).astype(int)\n\n# Create DataFrame\ntwo_var_data = pd.DataFrame({\n    'income': income,\n    'credit_score': credit_score,\n    'approved': approved\n})\n\nprint(f\"Two-variable dataset size: {len(two_var_data)}\")\nprint(f\"Approval rate: {two_var_data['approved'].mean():.1%}\")\nprint(\"\\nFirst few rows of our two-variable loan data:\")\nprint(two_var_data.head())\n\n# Build two-variable tree\nX_two = two_var_data[['income', 'credit_score']]\ny_two = two_var_data['approved']\n\ntwo_var_tree = DecisionTreeClassifier(max_depth=3, min_samples_split=30, random_state=42)\ntwo_var_tree.fit(X_two, y_two)\n\nprint(f\"Two-variable tree accuracy: {two_var_tree.score(X_two, y_two):.3f}\")\n\n\nTwo-variable dataset size: 300\nApproval rate: 37.3%\n\nFirst few rows of our two-variable loan data:\n         income  credit_score  approved\n0  92682.226704    504.917744         0\n1  55752.540146    716.355310         1\n2  50416.630821    502.321254         0\n3  79618.329217    525.446683         0\n4  94752.207281    567.649523         0\nTwo-variable tree accuracy: 0.960\n\n\n\n\nShow code for visualizing two-variable tree\n# Visualize the two-variable tree\nplt.figure(figsize=(10, 6))\ntree.plot_tree(\n    two_var_tree,\n    feature_names=['Income', 'Credit Score'],\n    class_names=['Denied', 'Approved'],\n    filled=True,\n    rounded=True,\n    fontsize=10\n)\nplt.title(\"Decision Tree: Loan Approval Based on Income and Credit Score\", fontsize=16, pad=20)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow CART Handles Multiple Variables\nNow with two variables (income and credit score), CART faces a more complex decision: at each split, it must choose not only the best threshold but also the best variable to split on. This showcases the algorithm’s ability to automatically discover variable importance and interactions.\nKey Insights from the Two-Variable Tree:\n\nVariable Selection: CART automatically chose which variable to split on first by testing all possible splits for both income and credit score, then selecting the one that provides the greatest reduction in Gini impurity. Notice that different branches may prioritize different variables based on what matters most for that subset of applicants.\nAutomatic Interaction Discovery: The tree naturally captures interactions between income and credit score without requiring us to manually create interaction terms. For example, a moderate income might be sufficient for approval if paired with an excellent credit score, but insufficient if paired with a poor credit score.\nContext-Dependent Rules: Each path through the tree represents a different business rule. Some paths might rely primarily on income thresholds, while others focus on credit score, depending on which combination best separates approved from denied applications in that segment.\nFeature Hierarchy: The tree structure reveals which factors matter most at different decision points, providing insights into the natural hierarchy of lending criteria that emerges from the data.\n\n\n\nReading Through the Two-Variable Tree\nLet’s walk through the actual tree above to understand how CART made its decisions and what business insights we can extract:\n\n\n\n\n\n\nNoteClick to view decision tree\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot Decision (Credit Score ≤ 649.56): CART chose credit score, not income, as the most important first question. This tells us that credit score provides the best initial separation of approved vs. denied loans. Notice the Gini = 0.468, indicating this starting group is quite mixed.\nLeft Branch - Low Credit Scores: If credit score ≤ 649.56, the decision is simple: automatic denial (Gini = 0.0, all 155 customers denied). This represents a clear business rule: “Below 650 credit score = automatic denial, regardless of income.”\nRight Branch - Higher Credit Scores: For credit scores &gt; 649.56, the algorithm now considers income (≤ $58,948). This shows context-dependent decision making: income only matters after passing the credit score threshold.\nIncome-Based Refinement:\n\nLower income + decent credit (left sub-branch): Mixed outcomes (Gini = 0.391), requiring further credit score refinement at 719.86\nHigher income + decent credit (right sub-branch): automatic approval (Gini = 0.0, all 100 customers approved)\n\nBusiness Translation: The tree discovered a natural lending hierarchy:\n\nCredit score &lt; 650: Deny (no exceptions)\nCredit score 650-719 + income ≤ $58,948: Needs case-by-case evaluation\nCredit score &gt; 650 + income &gt; $58,948: Approve automatically\nCredit score &gt; 719: Generally approve (even with lower income)\n\nKey Insight: This demonstrates how CART automatically finds the business logic that human loan officers might use, but derived purely from data patterns.\n\n\n\n\n\n\n\nTip🎥 Video: Decision Trees Explained\n\n\n\nWatch this comprehensive video on decision trees that covers:\n\nBasic decision tree concepts\nBuilding a tree with Gini Impurity\nNumeric and continuous variables\nAnd more",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Decision Trees: Foundations and Interpretability</span>"
    ]
  },
  {
    "objectID": "25-decision-trees.html#building-your-first-decision-tree",
    "href": "25-decision-trees.html#building-your-first-decision-tree",
    "title": "25  Decision Trees: Foundations and Interpretability",
    "section": "25.3 Building Your First Decision Tree",
    "text": "25.3 Building Your First Decision Tree\n\nClassification Trees\nNow let’s apply decision trees to a real medical problem: predicting heart disease based on patient health metrics. This classic dataset contains 303 patients with 13 clinical features including age, sex, chest pain type, resting blood pressure, cholesterol levels, and various cardiac measurements. The target variable indicates the presence (1) or absence (0) of heart disease, with approximately 46% of patients diagnosed with the condition.\nThe dataset includes both numerical features (age, blood pressure, cholesterol) and categorical features (sex, chest pain type, ECG results) that we’ll encode numerically for scikit-learn compatibility. This dataset demonstrates how decision trees excel at medical diagnosis tasks where interpretability is crucial for clinical decision-making.\nDataset Source: Heart Disease Dataset on GitHub\n\n\nShow code for loading heart disease dataset\n# Load heart disease dataset\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the data\nheart_data = pd.read_csv('../data/heart.csv')\n\n# Encode categorical variables to numeric\n# NOTE: scikit-learn decision trees require all features to be numeric.\n# We're using LabelEncoder here to convert categorical strings to integers.\n# This is a simple approach that works well for tree-based models, though\n# it does imply an ordinal relationship between categories. We'll explore\n# this topic and other feature engineering approaches in more depth in the\n# next module on feature engineering and preprocessing.\nheart_data_encoded = heart_data.copy()\n\n# Identify categorical columns (object dtype)\ncategorical_cols = heart_data_encoded.select_dtypes(include=['object']).columns\n\n# Encode each categorical column\nlabel_encoders = {}\nfor col in categorical_cols:\n    le = LabelEncoder()\n    heart_data_encoded[col] = le.fit_transform(heart_data_encoded[col])\n    label_encoders[col] = le\n\n# Display first few rows\nheart_data_encoded.head()\n\n\n\n\n\n\n\n\n\nAge\nsex\nChest pain\nrest BP\nChol\nfbs\nrest-ecg\nmax-hr\nexang\nold_peak\nslope\nca\nthal\ndisease\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n1\n150\n0\n2.3\n3\n0.0\n0\n0\n\n\n1\n67\n1\n0\n160\n286\n0\n1\n108\n1\n1.5\n2\n3.0\n1\n1\n\n\n2\n67\n1\n0\n120\n229\n0\n1\n129\n1\n2.6\n2\n2.0\n2\n1\n\n\n3\n37\n1\n1\n130\n250\n0\n2\n187\n0\n3.5\n3\n0.0\n1\n0\n\n\n4\n41\n0\n2\n130\n204\n0\n1\n172\n0\n1.4\n1\n0.0\n1\n0\n\n\n\n\n\n\n\nThe process of building a decision tree follows the same workflow you’ve seen with linear and logistic regression: prepare your features and target, split into training and test sets, fit the model, and evaluate performance. The main difference is that instead of LinearRegression() or LogisticRegression(), we use DecisionTreeClassifier() for classification tasks.\n\n# Build classification tree for heart disease prediction\n\n# Prepare features and target using the encoded data\nX_heart = heart_data_encoded.drop('disease', axis=1)\ny_heart = heart_data_encoded['disease']\n\n# Split data\nX_train_heart, X_test_heart, y_train_heart, y_test_heart = train_test_split(\n    X_heart, y_heart, test_size=0.3, random_state=42, stratify=y_heart\n)\n\n# Build decision tree with default settings\nheart_tree = DecisionTreeClassifier(random_state=42)\n\nheart_tree.fit(X_train_heart, y_train_heart)\n\n# Evaluate performance\ny_pred_heart = heart_tree.predict(X_test_heart)\n\nprint(\"\\nDetailed Classification Report:\")\nprint(classification_report(y_test_heart, y_pred_heart, target_names=['No Disease', 'Disease']))\n\n\nDetailed Classification Report:\n              precision    recall  f1-score   support\n\n  No Disease       0.73      0.84      0.78        49\n     Disease       0.77      0.64      0.70        42\n\n    accuracy                           0.75        91\n   macro avg       0.75      0.74      0.74        91\nweighted avg       0.75      0.75      0.74        91\n\n\n\n\n\n\n\n\n\nNoteClick here to see this decision tree plot\n\n\n\n\n\nThe tree below shows the full structure learned from the training data. Notice how deep it grows and how many leaf nodes it creates—this complexity is why we’re seeing perfect training accuracy but lower test accuracy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneKnowledge Check: Reading the Decision Tree\n\n\n\nUsing the tree visualization above, practice tracing through the decision-making process:\n\nUnderstanding the Root Split:\n\nWhat is the first (root) question the tree asks?\nWhat does this tell you about which feature the algorithm found most important for the initial split?\nHow many samples went left vs. right from the root?\n\nTracing a Simple Path:\n\nFind the path: Root → ca &lt;= 0.5 (left) → thal &lt;= 1.5 (left)\nWhat is the final prediction at this leaf node?\nHow many training samples reached this leaf?\nWhat is the gini impurity? What does this tell you about the purity of this prediction?\n\nFinding Evidence of Overfitting:\n\nExplore the tree and find at least two leaf nodes with very few samples (samples &lt; 5)\nLook at the gini values for these sparse leaves. Are they pure (gini ≈ 0) or mixed?\nWhat does it mean when a leaf has only 1-2 samples but gini = 0.0?\nWhy might these highly specific rules fail on new patients?\n\nComparing Leaf Depths:\n\nNotice how some paths are much longer (deeper) than others\nFind the shortest path from root to leaf (fewest splits)\nFind one of the longest paths (most splits)\nWhich type of path (short vs. long) is more likely to generalize well? Why?\n\n\nReflection: After exploring this tree, what problems do you anticipate with using it in a real clinical setting? Consider both the complexity and the reliance on very specific feature combinations.\n\n\n\n\nRegression Trees\nDecision trees aren’t limited to classification—they also work excellently for predicting continuous outcomes. Let’s predict house prices using the famous Ames Housing dataset, which contains detailed information about residential properties in Ames, Iowa. This dataset includes 2,930 homes with 80+ features describing various aspects of residential properties.\nFor our regression tree, we’ll focus on 8 key features that are intuitive and commonly used in real estate valuation: living area, overall quality, basement size, garage area, year built, lot size, bathrooms, and bedrooms. This demonstrates how regression trees handle numeric target variables while maintaining the same interpretable structure as classification trees.\nDataset Source: Ames Housing Dataset on GitHub\n\n\nShow code for loading Ames housing dataset\n# Load Ames housing dataset\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\n# Load the cleaned Ames data\names_data = pd.read_csv('../data/ames_clean.csv')\n\n# Select a subset of interpretable features for our tree\nfeatures_to_use = [\n    'GrLivArea',        # Above ground living area\n    'OverallQual',      # Overall material and finish quality\n    'TotalBsmtSF',      # Total basement square feet\n    'GarageArea',       # Size of garage in square feet\n    'YearBuilt',        # Original construction date\n    'LotArea',          # Lot size in square feet\n    'FullBath',         # Full bathrooms above grade\n    'BedroomAbvGr'      # Bedrooms above grade\n]\n\n# Create subset with selected features\nhouse_data = ames_data[features_to_use + ['SalePrice']].copy()\n\n# Remove any rows with missing values\nhouse_data = house_data.dropna()\n\n# Display first few rows\nhouse_data.head()\n\n\n\n\n\n\n\n\n\nGrLivArea\nOverallQual\nTotalBsmtSF\nGarageArea\nYearBuilt\nLotArea\nFullBath\nBedroomAbvGr\nSalePrice\n\n\n\n\n0\n1710\n7\n856\n548\n2003\n8450\n2\n3\n208500\n\n\n1\n1262\n6\n1262\n460\n1976\n9600\n2\n3\n181500\n\n\n2\n1786\n7\n920\n608\n2001\n11250\n2\n3\n223500\n\n\n3\n1717\n7\n756\n642\n1915\n9550\n1\n3\n140000\n\n\n4\n2198\n8\n1145\n836\n2000\n14260\n2\n4\n250000\n\n\n\n\n\n\n\nBuilding a regression tree follows the same familiar workflow, but we use DecisionTreeRegressor() instead of DecisionTreeClassifier(). The tree will predict continuous sale prices rather than discrete classes.\n\n\n\n\n\n\nImportantRegression Trees: SSE Instead of Gini\n\n\n\nWhile the tree structure and decision-making process are the same as classification trees, regression trees use a different splitting criterion:\n\nClassification trees minimize Gini impurity (or entropy) to create pure class groupings\nRegression trees minimize Sum of Squared Errors (SSE) to create groups with similar numeric values\n\nHow SSE works: At each potential split, the algorithm calculates the sum of squared differences between each house’s actual price and the average price in that group. The split that produces the lowest total SSE across both resulting groups is chosen.\nWhy this matters: Just like classification trees seek purity (all same class), regression trees seek homogeneity (all similar values). A leaf with houses priced at $180k, $182k, and $181k has low SSE. A leaf with houses priced at $100k, $200k, and $300k has high SSE and would benefit from further splitting.\nThe core principle remains the same: find splits that create the most homogeneous groups possible.\n\n\n\n# Build regression tree for house prices\n\n# Prepare features and target\nX_house = house_data.drop('SalePrice', axis=1)\ny_house = house_data['SalePrice']\n\n# Split data\nX_train_house, X_test_house, y_train_house, y_test_house = train_test_split(\n    X_house, y_house, test_size=0.3, random_state=42\n)\n\n# Build regression tree with default settings\nprice_tree = DecisionTreeRegressor(random_state=42)\n\nprice_tree.fit(X_train_house, y_train_house)\n\n# Make predictions\ny_pred_train = price_tree.predict(X_train_house)\ny_pred_test = price_tree.predict(X_test_house)\n\n# Evaluate performance\ntest_r2 = r2_score(y_test_house, y_pred_test)\ntest_mae = mean_absolute_error(y_test_house, y_pred_test)\ntest_rmse = np.sqrt(mean_squared_error(y_test_house, y_pred_test))\n\nprint(\"House Price Prediction Results:\")\nprint(f\"Test R² Score: {test_r2:.3f}\")\nprint(f\"Mean Absolute Error: ${test_mae:,.0f}\")\nprint(f\"Root Mean Squared Error: ${test_rmse:,.0f}\")\n\nHouse Price Prediction Results:\nTest R² Score: 0.768\nMean Absolute Error: $27,112\nRoot Mean Squared Error: $40,215\n\n\n\n\n\n\n\n\nNoteClick here to see this decision tree plot\n\n\n\n\n\nThe visualization below shows only the first 3 levels of the regression tree. The actual tree is far too deep and complex to display in its entirety—it would be impossible to read. Unlike classification trees that predict discrete classes, each leaf node in a regression tree predicts a specific dollar amount (the average price of houses in that leaf).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneKnowledge Check: Understanding Regression Trees\n\n\n\nUsing the tree visualization above, explore how regression trees differ from classification trees:\n\nLeaf Node Predictions:\n\nLook at any leaf node. What does the “value” represent in a regression tree?\nHow is this different from classification trees where value showed class counts?\n\nSplitting Decisions:\n\nWhat feature did the tree choose for the root split?\nWhy might this feature be most important for predicting house prices?\nFollow the left branch (lower values). What type of houses end up here?\n\nPrice Ranges:\n\nFind a leaf node on the left side of the tree (typically lower-priced homes)\nFind a leaf node on the right side (typically higher-priced homes)\nWhat’s the price difference between these segments?\nHow many training samples reached each leaf?\n\n\nReflection: Real estate agents price homes using “comps”—finding 3-5 similar recently sold properties and averaging their prices. How is this regression tree’s approach similar to and different from the comps method? Consider: How many houses does the tree use for each prediction? Does it always use the most relevant comparables? What makes a good comp vs. what makes the tree split?\n\n\n\n\n\n\n\n\nTip🎥 Video: Regression Trees Explained\n\n\n\nWatch this excellent video that compares regression trees to classification trees and clearly explains key concepts:\n\nHow regression trees differ from classification trees\nSSE (Sum of Squared Errors) as the splitting criterion\nBuilding and interpreting regression trees\nPractical applications and examples\n\n\n\n\n\n\nTree Parameters and Overfitting\nDecision trees have a natural tendency to overfit—they can keep splitting until each leaf contains just one data point, perfectly memorizing the training data but failing to generalize. Understanding and controlling tree complexity is crucial for business applications.\nWhat does “default” tree complexity look like? When we built our classification and regression trees earlier using default settings, the results were dramatic. Let’s examine exactly how complex these trees became:\n\n\nShow code for extracting tree complexity\n# Check the complexity of our default trees\nprint(\"Classification Tree (Heart Disease) Complexity:\")\nprint(f\"  Maximum depth: {heart_tree.get_depth()}\")\nprint(f\"  Total number of nodes: {heart_tree.tree_.node_count}\")\nprint(f\"  Number of leaves: {heart_tree.get_n_leaves()}\")\nprint(f\"  Training accuracy: {heart_tree.score(X_train_heart, y_train_heart):.3f}\")\nprint(f\"  Test accuracy: {heart_tree.score(X_test_heart, y_test_heart):.3f}\")\n\nprint(\"\\nRegression Tree (House Prices) Complexity:\")\nprint(f\"  Maximum depth: {price_tree.get_depth()}\")\nprint(f\"  Total number of nodes: {price_tree.tree_.node_count}\")\nprint(f\"  Number of leaves: {price_tree.get_n_leaves()}\")\nprint(f\"  Training R²: {price_tree.score(X_train_house, y_train_house):.3f}\")\nprint(f\"  Test R²: {price_tree.score(X_test_house, y_test_house):.3f}\")\n\n\nClassification Tree (Heart Disease) Complexity:\n  Maximum depth: 9\n  Total number of nodes: 73\n  Number of leaves: 37\n  Training accuracy: 1.000\n  Test accuracy: 0.747\n\nRegression Tree (House Prices) Complexity:\n  Maximum depth: 24\n  Total number of nodes: 1989\n  Number of leaves: 995\n  Training R²: 1.000\n  Test R²: 0.768\n\n\nNotice the striking patterns:\n\nHeart disease classification tree: With a depth of 9 levels, 73 total nodes, and 37 leaf nodes, this tree achieved perfect 1.000 training accuracy but only 0.747 test accuracy. The gap reveals classic overfitting—the tree learned highly specific rules that don’t generalize.\nHouse price regression tree: Even more extreme—depth of 24 levels with nearly 2,000 nodes and almost 1,000 leaf nodes! Perfect training R² of 1.000 but test R² dropped to 0.768. This tree literally created a separate leaf for almost every few training examples, memorizing individual houses rather than learning general pricing patterns.\n\nThis illustrates a critical point: without parameter constraints, decision trees will grow until they perfectly fit (memorize) your training data. The algorithm has no inherent preference for simplicity—it will keep asking questions until it achieves perfect purity or runs out of data to split. The 1.000 training scores on both trees prove this memorization happened, while the lower test scores reveal the cost of overfitting.\n\n\n\n\n\n\nTip🎥 Video: Overfitting in Decision Trees\n\n\n\nWatch this video that visualizes the overfitting problem as decision trees become more complex:\n\nHow overfitting occurs with increasing tree depth\nTraining accuracy improves while test accuracy deteriorates\nVisual demonstration of the bias-variance tradeoff\nWhy simpler trees often generalize better\n\n\n\n\nThe key to successful tree implementation lies in understanding and controlling the parameters that govern tree complexity. Without proper tuning, trees will default to memorizing every detail of your training data rather than learning generalizable patterns. Here are the most important parameters you can control:\n\nmax_depth: Maximum number of questions in any path\n\nBusiness impact: Deeper trees = more complex rules, harder to explain\nTypical values: 3-5 for interpretable models, 6-10 for better performance\n\nmin_samples_split: Minimum samples required to split a node\n\nBusiness impact: Prevents rules based on tiny groups\nTypical values: 20-100 depending on dataset size\n\nmin_samples_leaf: Minimum samples in final prediction groups\n\nBusiness impact: Ensures predictions based on substantial evidence\nTypical values: 10-50 for stable predictions\n\n\nLet’s rebuild our heart disease tree with reasonable constraints and see how it impacts generalization:\n\n# Build a constrained tree with reasonable parameters\nheart_tree_tuned = DecisionTreeClassifier(\n    max_depth=5,           # Limit tree depth\n    min_samples_split=20,  # Require at least 20 samples to split\n    min_samples_leaf=10,   # Require at least 10 samples in each leaf\n    random_state=42\n)\n\nheart_tree_tuned.fit(X_train_heart, y_train_heart)\n\n# Compare default vs constrained tree\nprint(\"Model Comparison:\")\nprint(\"\\nDefault Tree (unconstrained):\")\nprint(f\"  Training accuracy: {heart_tree.score(X_train_heart, y_train_heart):.3f}\")\nprint(f\"  Test accuracy: {heart_tree.score(X_test_heart, y_test_heart):.3f}\")\nprint(f\"  Overfitting gap: {heart_tree.score(X_train_heart, y_train_heart) - heart_tree.score(X_test_heart, y_test_heart):.3f}\")\nprint(f\"  Tree depth: {heart_tree.get_depth()}\")\nprint(f\"  Number of leaves: {heart_tree.get_n_leaves()}\")\n\nprint(\"\\nConstrained Tree (tuned parameters):\")\nprint(f\"  Training accuracy: {heart_tree_tuned.score(X_train_heart, y_train_heart):.3f}\")\nprint(f\"  Test accuracy: {heart_tree_tuned.score(X_test_heart, y_test_heart):.3f}\")\nprint(f\"  Overfitting gap: {heart_tree_tuned.score(X_train_heart, y_train_heart) - heart_tree_tuned.score(X_test_heart, y_test_heart):.3f}\")\nprint(f\"  Tree depth: {heart_tree_tuned.get_depth()}\")\nprint(f\"  Number of leaves: {heart_tree_tuned.get_n_leaves()}\")\n\nModel Comparison:\n\nDefault Tree (unconstrained):\n  Training accuracy: 1.000\n  Test accuracy: 0.747\n  Overfitting gap: 0.253\n  Tree depth: 9\n  Number of leaves: 37\n\nConstrained Tree (tuned parameters):\n  Training accuracy: 0.868\n  Test accuracy: 0.780\n  Overfitting gap: 0.088\n  Tree depth: 4\n  Number of leaves: 13\n\n\nThe constrained tree demonstrates significant improvements across all metrics. While training accuracy dropped slightly from 1.000 to around 0.86, test accuracy actually improved to approximately 0.82—a clear sign that the model now generalizes better to unseen data. Most importantly, the overfitting gap shrunk dramatically from 0.253 to just 0.04, indicating the tree has learned true patterns rather than memorizing noise. The tree structure itself became much more interpretable, with depth reduced from 9 to 5 levels and leaf count dropping from 37 to just 13 nodes—making it practical for clinical decision-making.\n\n\n\n\n\n\nImportantLooking Ahead: Systematic Hyperparameter Tuning\n\n\n\nIn this example, we manually specified parameter values based on general guidelines. In a future chapter on model optimization, you’ll learn systematic approaches like cross-validation and grid search that automatically find optimal parameter combinations by testing many values and selecting those that maximize generalization performance. These techniques remove the guesswork and ensure you’re getting the best possible model for your data.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Decision Trees: Foundations and Interpretability</span>"
    ]
  },
  {
    "objectID": "25-decision-trees.html#when-to-use-decision-trees",
    "href": "25-decision-trees.html#when-to-use-decision-trees",
    "title": "25  Decision Trees: Foundations and Interpretability",
    "section": "25.4 When to Use Decision Trees",
    "text": "25.4 When to Use Decision Trees\n\nAdvantages Over Linear Models\nDecision trees offer several compelling advantages over linear models, particularly in business contexts where relationships are complex and stakeholder buy-in requires clear explanations. These advantages make trees especially valuable when moving beyond the assumptions of linear modeling.\n\nAutomatic handling of non-linear relationships: Trees naturally discover threshold effects, saturation points, and diminishing returns without manual feature engineering.\nNo assumptions about data distribution: Unlike linear models, trees don’t assume your data follows normal distributions or has linear relationships.\nNatural handling of missing values: Trees can make decisions even when some features are missing—they simply use alternative splitting rules.\nBuilt-in feature interaction detection: Trees automatically find complex interactions like “high income AND young age” without requiring you to specify them upfront.\nComplete interpretability: Every prediction can be explained as a series of simple yes/no decisions that business stakeholders can understand and verify.\nMixed data type handling: Trees conceptually work with numerical, categorical, and ordinal features, though sklearn requires encoding categorical variables.\n\n\n\nLimitations to Consider\nWhile decision trees offer significant advantages, they also come with important limitations that can impact their effectiveness in certain business scenarios. Understanding these constraints helps you choose the right tool for each situation and avoid common pitfalls.\n\nTendency to overfit with complex trees: Without careful parameter tuning, trees can memorize training data rather than learning generalizable patterns.\nInstability: Small changes in training data can lead to completely different trees, making them less reliable for consistent rule generation.\nBias toward features with more levels: Categorical features with many categories get artificially inflated importance scores.\nDifficulty capturing linear relationships efficiently: Simple linear trends might require many splits to approximate, making trees unnecessarily complex.\nStep-function predictions: Trees create abrupt decision boundaries that might not reflect gradual real-world changes.\n\nOvercoming These Limitations: Many of these challenges—particularly overfitting, instability, and prediction variance—can be significantly mitigated using ensemble methods. In the next chapter, we’ll explore random forests, which combine predictions from many trees to create more robust, accurate models while reducing sensitivity to individual data points and improving generalization.\n\n\nBusiness Scenarios Where Trees Excel\nCertain business contexts particularly benefit from decision trees’ unique strengths. These scenarios typically involve complex decision-making, regulatory requirements for explainability, or situations where stakeholders need to understand and trust the model’s reasoning process. For example…\nRisk Assessment and Credit Scoring\nExample rules:\n- If credit_score ≤ 650 AND debt_to_income &gt; 0.4: High Risk\n- If credit_score &gt; 750 AND employment_years &gt; 3: Low Risk\nCustomer Segmentation\nNatural groupings:\n- Premium customers: High spending + Long tenure\n- At-risk customers: Recent complaints + Short contracts\n- Growth opportunities: Medium spending + Young demographics\nMedical Diagnosis and Triage\nClinical decision support:\n- If fever &gt; 101°F AND age &lt; 2: Immediate attention\n- If symptoms = [cough, fatigue] AND duration &gt; 14 days: Further testing\nProduct Recommendation Systems\nPersonalized suggestions:\n- If purchase_history includes \"electronics\" AND budget &gt; $500: Recommend premium gadgets\n- If browsing_time &gt; 10min AND cart_abandonment = True: Send discount offer\nQuality Control and Manufacturing\nDefect detection:\n- If temperature &gt; 150°C AND pressure &lt; 20 PSI: Quality alert\n- If machine_age &gt; 5 years AND vibration &gt; threshold: Maintenance required",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Decision Trees: Foundations and Interpretability</span>"
    ]
  },
  {
    "objectID": "25-decision-trees.html#summary",
    "href": "25-decision-trees.html#summary",
    "title": "25  Decision Trees: Foundations and Interpretability",
    "section": "25.5 Summary",
    "text": "25.5 Summary\nDecision trees represent a fundamental shift from linear thinking to rule-based reasoning. Throughout this chapter, we’ve seen how trees excel at capturing the complex, context-dependent patterns that characterize real business data.\nKey Concepts Mastered:\n\nTree Fundamentals: Understanding how CART builds trees through recursive splitting, using Gini impurity (classification) or SSE (regression) to find optimal decision points\nTree Anatomy: Recognizing the components—nodes, splits, leaves, and depth—and how they combine to create interpretable decision paths\nBuilding Trees: Constructing both classification and regression trees using scikit-learn’s DecisionTreeClassifier and DecisionTreeRegressor\nControlling Complexity: Managing overfitting through parameter constraints (max_depth, min_samples_split, min_samples_leaf) to improve generalization\nBusiness Application: Evaluating when trees excel versus when linear models remain more appropriate for the task at hand\n\nWhen Trees Shine:\n\nNon-linear relationships with threshold effects and saturation points\nBusiness problems requiring transparent, explainable decision rules\nComplex feature interactions that are unknown upfront\nMixed data types (though encoding is still needed for sklearn)\nRegulatory environments requiring model interpretability\n\nWhen to Consider Alternatives:\n\nRelationships are primarily linear (use linear/logistic regression)\nModel stability and consistency are critical requirements\nSmooth, continuous decision boundaries are preferred\nSmall training datasets where overfitting risk is high\n\nLooking Ahead: While individual decision trees provide excellent interpretability, they suffer from instability and overfitting tendencies. In the next chapter, we’ll explore random forests—an ensemble method that combines multiple trees to overcome these limitations while maintaining much of the interpretability advantage. Random forests address the key weaknesses of single trees by averaging predictions across many diverse trees, resulting in more robust and accurate models.\n\n\n\n\n\n\nTip🎥 Video: Decision Trees in Python - Complete Walkthrough\n\n\n\nWatch this comprehensive end-to-end tutorial that walks through implementing decision trees in Python:\n\nPreparing data for decision trees\nBuilding and training decision tree models\nUnderstanding and interpreting tree structure\nValidating model performance\nComplete workflow with practical Python examples",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Decision Trees: Foundations and Interpretability</span>"
    ]
  },
  {
    "objectID": "25-decision-trees.html#end-of-chapter-exercise",
    "href": "25-decision-trees.html#end-of-chapter-exercise",
    "title": "25  Decision Trees: Foundations and Interpretability",
    "section": "25.6 End of Chapter Exercise",
    "text": "25.6 End of Chapter Exercise\nFor these exercises, you’ll apply decision trees to real business scenarios using datasets from previous chapters. Each scenario mirrors decision contexts where decision trees can provide interpretable insights and accurate predictions.\n\n\n\n\n\n\nNoneExercise 1: Baseball Salary Prediction (Regression)\n\n\n\n\n\n\nCompany: Professional baseball team\nGoal: Understand what drives player salaries to inform contract negotiations and player evaluation\nDataset: Hitters dataset from ISLP package\n\n\nfrom ISLP import load_data\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\nimport matplotlib.pyplot as plt\n\n# Load and prepare the data\nHitters = load_data('Hitters')\n\n# Remove missing salary values\nHitters_clean = Hitters.dropna(subset=['Salary'])\n\n# Select numeric features for our regression tree\nfeatures = ['Years', 'Hits', 'RBI', 'Walks', 'PutOuts']\nX_hitters = Hitters_clean[features]\ny_hitters = Hitters_clean['Salary']\n\nprint(f\"Dataset size: {len(Hitters_clean)} players\")\nprint(f\"Features used: {features}\")\nprint(f\"\\nFirst few rows:\")\nprint(Hitters_clean[features + ['Salary']].head())\n\nDataset size: 263 players\nFeatures used: ['Years', 'Hits', 'RBI', 'Walks', 'PutOuts']\n\nFirst few rows:\n   Years  Hits  RBI  Walks  PutOuts  Salary\n1     14    81   38     39      632   475.0\n2      3   130   72     76      880   480.0\n3     11   141   78     37      200   500.0\n4      2    87   42     30      805    91.5\n5     11   169   51     35      282   750.0\n\n\nYour Tasks:\n\nBuild and visualize a decision tree: Create a DecisionTreeRegressor to predict Salary using the features provided above. Start with default parameters, then try constraining with max_depth=4 to improve interpretability.\nEvaluate performance: Split your data into training (70%) and test (30%) sets. Calculate and compare:\n\nTraining R² and test R²\nMean Absolute Error on test set\nRoot Mean Squared Error on test set\n\nInterpret the tree: Visualize your constrained tree (max_depth=4) using plot_tree(). What are the most important features for predicting salary? What salary ranges do different paths lead to?\nExtract business rules: Trace through 2-3 different paths in your tree and write them out as if-then rules (e.g., “If Years &gt; 5 AND Hits &gt; 100, then predicted salary = $X”)\nBusiness reflection:\n\nHow could a player agent use these rules to negotiate higher salaries?\nWhat limitations might this model have for contract negotiations?\nWould you trust this model for a $10M contract decision? Why or why not?\n\n\n\n\n\n\n\n\n\n\n\nNoneExercise 2: Credit Default Classification\n\n\n\n\n\n\nCompany: Regional bank\nGoal: Predict which customers will default on credit card payments to inform risk management strategies\nDataset: Default dataset from ISLP package\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Load the Default dataset\nDefault = load_data('Default')\n\n# Prepare features - encode student as binary (already done by get_dummies)\nDefault_encoded = pd.get_dummies(Default, columns=['student'], drop_first=True)\nDefault_encoded['default_binary'] = (Default_encoded['default'] == 'Yes').astype(int)\n\n# Select features\nX_default = Default_encoded[['balance', 'income', 'student_Yes']]\ny_default = Default_encoded['default_binary']\n\nprint(f\"Dataset size: {len(Default)} customers\")\nprint(f\"Default rate: {y_default.mean():.1%}\")\nprint(f\"\\nFeatures prepared:\")\nprint(X_default.head())\n\nDataset size: 10000 customers\nDefault rate: 3.3%\n\nFeatures prepared:\n       balance        income  student_Yes\n0   729.526495  44361.625074        False\n1   817.180407  12106.134700         True\n2  1073.549164  31767.138947        False\n3   529.250605  35704.493935        False\n4   785.655883  38463.495879        False\n\n\nYour Tasks:\n\nBuild classification trees: Create two DecisionTreeClassifier models:\n\nModel A: Default parameters\nModel B: Constrained with max_depth=3, min_samples_split=50, min_samples_leaf=25\n\nCompare overfitting: Split data (70/30 train/test) and evaluate both models:\n\nTraining accuracy vs. test accuracy\nClassification report on test data\nTree complexity (depth, number of leaves)\nWhich model generalizes better?\n\nUnderstand the imbalance: This dataset has only ~3% default rate. How does this affect your model’s performance? Look at precision and recall for the “default” class specifically.\nVisualize decision boundaries: Plot your constrained tree (Model B). What balance threshold appears most important? How does student status affect default predictions?\nBusiness application:\n\nWrite 3 clear business rules from your tree (e.g., “If balance &gt; $X AND student = No, then high risk”)\nHow would the bank’s risk team use these rules for credit limit decisions?\nWhat are the costs of false positives vs. false negatives in credit default prediction?\n\n\n\n\n\n\n\n\n\n\n\nNoneExercise 3: Stock Market Direction Prediction (Optional Challenge)\n\n\n\n\n\n\nCompany: Investment management firm\nGoal: Predict whether the stock market will go up or down based on previous market performance\nDataset: Weekly dataset from ISLP package\n\n\n# Load Weekly stock market data\nWeekly = load_data('Weekly')\n\n# Prepare features and target\n# Convert Direction to binary: 1 for Up, 0 for Down\nWeekly_encoded = Weekly.copy()\nWeekly_encoded['Direction_binary'] = (Weekly_encoded['Direction'] == 'Up').astype(int)\n\n# Use lag variables as features\nlag_features = ['Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5']\nX_weekly = Weekly_encoded[lag_features]\ny_weekly = Weekly_encoded['Direction_binary']\n\nprint(f\"Dataset size: {len(Weekly)} weeks\")\nprint(f\"Up weeks: {y_weekly.mean():.1%}\")\nprint(f\"\\nLag features:\")\nprint(X_weekly.head())\n\nDataset size: 1089 weeks\nUp weeks: 55.6%\n\nLag features:\n    Lag1   Lag2   Lag3   Lag4   Lag5\n0  0.816  1.572 -3.936 -0.229 -3.484\n1 -0.270  0.816  1.572 -3.936 -0.229\n2 -2.576 -0.270  0.816  1.572 -3.936\n3  3.514 -2.576 -0.270  0.816  1.572\n4  0.712  3.514 -2.576 -0.270  0.816\n\n\nYour Tasks:\n\nBuild a market direction classifier: Create a decision tree to predict whether the market goes up or down based on the 5 lag variables.\nEvaluate predictive power:\n\nSplit data chronologically (first 80% train, last 20% test) since this is time-series data\nWhat’s the test accuracy?\nHow does this compare to always predicting “Up” (the majority class)?\n\nInterpret the tree: Which lag periods appear most influential? Do the relationships make financial sense?\nChallenge question: Decision trees struggle with this type of data. Why might financial market prediction be particularly difficult for decision trees? What characteristics of stock market data violate the assumptions that make trees effective?\nInvestment strategy: Based on your tree’s performance, would you recommend using it for actual trading decisions? What would be the financial risks?",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Decision Trees: Foundations and Interpretability</span>"
    ]
  },
  {
    "objectID": "26-random-forests.html",
    "href": "26-random-forests.html",
    "title": "26  Random Forests: Ensemble Power and Robustness",
    "section": "",
    "text": "26.1 From Single Trees to Forest Wisdom\nIn business, critical decisions rarely rely on a single perspective. When making important choices, successful organizations typically:\nThis chapter introduces random forests, one of the most popular and effective machine learning algorithms in practice. Random forests apply the “wisdom of crowds” principle to machine learning: instead of relying on a single decision tree (which can be unstable and prone to overfitting), they build hundreds of trees on different samples of data and combine their predictions. Through two sources of randomness—bootstrap sampling and feature selection—random forests create diverse trees whose errors cancel out when averaged, resulting in models that are more accurate, stable, and robust than individual trees.\nBy the end of this chapter, you will be able to:\nIn the previous chapter, you learned that decision trees are highly interpretable but can suffer from instability—small changes in the data can lead to very different trees. Additionally, complex trees tend to overfit, memorizing training data patterns that don’t generalize to new situations.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Random Forests: Ensemble Power and Robustness</span>"
    ]
  },
  {
    "objectID": "26-random-forests.html#from-single-trees-to-forest-wisdom",
    "href": "26-random-forests.html#from-single-trees-to-forest-wisdom",
    "title": "26  Random Forests: Ensemble Power and Robustness",
    "section": "",
    "text": "The Problem with Single Trees\nWhile decision trees offer the advantage of interpretability (we can visualize and understand exactly how they make decisions), they suffer from a critical weakness: high variance. This means that small changes in the training data can produce dramatically different trees.\nThink of it this way: if you asked three different analysts to build a decision tree using slightly different samples from the same dataset, you might get three very different models. One tree might split first on feature A, another on feature B, and a third on feature C—even though they’re all trying to solve the same problem with the same data!\nThis instability has serious business implications:\n\nUnreliable predictions: A model that changes drastically with minor data variations is hard to trust\nPoor generalization: Trees that are too sensitive to training data often overfit and perform poorly on new data\nInconsistent insights: Different trees might suggest different business strategies, making it unclear which features truly matter\n\nLet’s demonstrate this instability with a concrete example. In the code below, we’ll create three different bootstrap samples (random samples with replacement) from the same dataset and train a decision tree on each. Even though all three samples come from the same underlying data, watch how the resulting trees have different structures:\n\n\nShow code: Tree instability demonstration\n# Show how small data changes create different trees\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\n# Load a sample dataset (using the iris dataset as an example)\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = pd.Series(iris.target)\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Create three different bootstrap samples (sampling with replacement)\nn_samples = len(X)\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor i in range(3):\n    # Create bootstrap sample\n    bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n    X_bootstrap = X.iloc[bootstrap_indices]\n    y_bootstrap = y.iloc[bootstrap_indices]\n\n    # Train a decision tree on this bootstrap sample\n    dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n    dt.fit(X_bootstrap, y_bootstrap)\n\n    # Visualize the tree\n    tree.plot_tree(dt,\n                   feature_names=iris.feature_names,\n                   class_names=iris.target_names,\n                   filled=True,\n                   ax=axes[i],\n                   fontsize=8)\n    axes[i].set_title(f'Decision Tree from Bootstrap Sample {i+1}', fontsize=12, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice how the trees have different structures despite being trained on the same underlying dataset! This instability is a key weakness of individual decision trees that random forests address.\n\n\n\n\n\nThe Ensemble Solution\nRandom forests solve these problems by combining multiple trees that are each trained on slightly different versions of the data. The key insight is that while individual trees might make errors, their collective wisdom tends to be more reliable.\nTwo sources of diversity:\n\nBootstrap sampling: Each tree sees a different random sample of the training data\nFeature randomness: Each split considers only a random subset of features",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Random Forests: Ensemble Power and Robustness</span>"
    ]
  },
  {
    "objectID": "26-random-forests.html#how-random-forests-work",
    "href": "26-random-forests.html#how-random-forests-work",
    "title": "26  Random Forests: Ensemble Power and Robustness",
    "section": "26.2 How Random Forests Work",
    "text": "26.2 How Random Forests Work\nNow that we’ve seen the instability problem with individual decision trees, let’s understand how random forests solve it. The core idea is surprisingly simple: instead of relying on a single tree, build many trees and let them vote.\nThis approach is called ensemble learning—combining multiple models to create a more powerful predictor. Think of it like consulting a panel of experts rather than trusting a single opinion. While any individual expert might be wrong, the collective wisdom of the group tends to be more reliable.\nRandom forests specifically use a technique called bootstrap aggregating (or “bagging”) combined with random feature selection. Here’s the three-step process:\n\nCreate diversity: Build each tree on a different random sample of the data\nAdd more randomness: At each split, consider only a random subset of features\nAggregate predictions: Combine all tree predictions through voting (classification) or averaging (regression)\n\nThe beauty of this approach is that while individual trees might be unstable and overfit to their particular training sample, the forest as a whole becomes stable and generalizes well. The errors of individual trees tend to cancel out, leaving more accurate predictions.\nLet’s break down each component to understand how this works in practice.\n\nBootstrap Aggregating (Bagging)\nBootstrap aggregating, commonly called bagging, is the foundation of random forests. The technique involves two key steps:\n\nBootstrap: Create multiple random samples from your training data\nAggregate: Combine predictions from models trained on each sample\n\n\nWhat is Bootstrap Sampling?\nBootstrap sampling is a statistical technique where you create a new dataset by randomly selecting observations from the original data with replacement. This means:\n\nThe new sample has the same size as the original dataset\nSome observations appear multiple times (because of replacement)\nSome observations don’t appear at all (approximately 37% are left out)\n\nHere’s a simple analogy: Imagine you have a jar with 20 numbered balls (your training data). To create a bootstrap sample, you:\n\nRandomly pick a ball, record its number, and put it back in the jar\nRepeat this 20 times\nYou now have a new sample of 20 numbers—some will be duplicates, some won’t appear at all\n\n\n\nShow code: Bootstrap sampling visualization\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Create a simplified example with 10 data points for clarity\noriginal_data = np.arange(1, 21)\n\n# Create 3 bootstrap samples\nn_samples = 3\nfig, axes = plt.subplots(n_samples + 1, 1, figsize=(6, 6))\n\n# Plot original data\naxes[0].bar(original_data, [1]*len(original_data), color='steelblue', edgecolor='black', linewidth=1.5)\naxes[0].set_xlim(0, 21)\naxes[0].set_ylim(0, 5)\naxes[0].set_ylabel('Count', fontsize=8)\naxes[0].set_title('Original Data: 10 Unique Observations', fontsize=9, fontweight='bold')\naxes[0].set_xticks(original_data)\naxes[0].grid(axis='y', alpha=0.3)\n\n# Create and plot bootstrap samples\nfor i in range(n_samples):\n    # Create bootstrap sample (sample with replacement)\n    bootstrap_sample = np.random.choice(original_data, size=len(original_data), replace=True)\n\n    # Count occurrences\n    unique, counts = np.unique(bootstrap_sample, return_counts=True)\n\n    # Identify which observations are missing\n    missing = set(original_data) - set(unique)\n    n_missing = len(missing)\n\n    # Create bar plot\n    axes[i+1].bar(unique, counts, color='coral', edgecolor='black', linewidth=1.5)\n\n    # Mark missing values with light gray at bottom\n    if missing:\n        axes[i+1].bar(list(missing), [0.1]*len(missing), color='lightgray',\n                     edgecolor='red', linewidth=2, linestyle='--', alpha=0.5)\n\n    axes[i+1].set_xlim(0, 21)\n    axes[i+1].set_ylim(0, 5)\n    axes[i+1].set_ylabel('Count', fontsize=8)\n    axes[i+1].set_title(f'Bootstrap Sample {i+1}: {len(unique)} unique ({n_missing} missing, some duplicated)',\n                       fontsize=9, fontweight='bold')\n    axes[i+1].set_xticks(original_data)\n    axes[i+1].grid(axis='y', alpha=0.3)\n\naxes[n_samples].set_xlabel('Observation Number', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipUnderstanding the Visualization\n\n\n\nIn this simplified example with 20 observations:\n\nTop panel: The original dataset contains observations 1-20, each appearing exactly once\nBottom three panels: Three different bootstrap samples, each created by randomly selecting 20 observations with replacement\n\nNotice in each bootstrap sample:\n\nSome observations appear multiple times (taller bars) - these were randomly selected more than once\nSome observations are missing entirely (shown in gray with dashed outline) - they were never selected\nEach sample is different, creating the diversity we need for random forests\n\n\n\n\n\nHow Bagging Creates Diversity\nWhen you create multiple bootstrap samples and train a decision tree on each one, you get trees that:\n\nSee different data: Each tree trains on a unique random sample\nMake different splits: Different data leads to different optimal split points\nCapture different patterns: Some trees might focus on certain relationships, others on different ones\n\nThis diversity is precisely what we want! Remember the instability problem from earlier? Bagging embraces that instability and uses it to our advantage.\n\n\nWhy Averaging Helps\nHere’s the key insight: if you have multiple models that each make somewhat independent errors, averaging their predictions reduces the overall error. This works because:\n\nErrors cancel outt: When one tree overestimates, another might underestimate\nSignal reinforces: True patterns appear across most trees\nNoise diminishes: Random fluctuations don’t consistently appear\n\nMathematically, if you have n models with uncorrelated errors, the variance of the average prediction is approximately 1/n times the variance of a single model. This is why more trees generally lead to better performance.\n\n\nBagging in Practice\nLet’s see bagging in action with a simple example:\n\n\nShow code: Bagging demonstration\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Create data following a sin wave pattern with noise\nX = np.linspace(0, 4*np.pi, 100).reshape(-1, 1)\ny_true = np.sin(X).ravel()  # True underlying pattern\ny = y_true + np.random.normal(0, 0.3, len(X))  # Add noise\n\n# Create a fine grid for smooth predictions\nX_grid = np.linspace(0, 4*np.pi, 300).reshape(-1, 1)\ny_grid_true = np.sin(X_grid).ravel()  # True pattern on grid\n\n# Train 10 trees on bootstrap samples\nn_trees = 10\npredictions = []\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Plot the true underlying pattern\nax.plot(X_grid, y_grid_true, 'g--', linewidth=2, label='True pattern', alpha=0.7)\n\n# Plot training data\nax.scatter(X, y, alpha=0.3, s=20, color='gray', label='Training data (with noise)')\n\n# Train trees and plot predictions\nfor i in range(n_trees):\n    # Create bootstrap sample\n    indices = np.random.choice(len(X), size=len(X), replace=True)\n    X_bootstrap = X[indices]\n    y_bootstrap = y[indices]\n\n    # Train tree on bootstrap sample\n    tree = DecisionTreeRegressor(max_depth=5, random_state=i)\n    tree.fit(X_bootstrap, y_bootstrap)\n\n    # Predict on grid\n    y_pred = tree.predict(X_grid)\n    predictions.append(y_pred)\n\n    # Plot individual tree prediction\n    ax.plot(X_grid, y_pred, alpha=0.4, linewidth=1.5, color='cornflowerblue',\n            label='Individual tree' if i == 0 else '')\n\n# Calculate and plot the average prediction\navg_prediction = np.mean(predictions, axis=0)\nax.plot(X_grid, avg_prediction, color='red', linewidth=3, label='Bagged prediction (average)', zorder=10)\n\nax.set_xlabel('X', fontsize=12)\nax.set_ylabel('Y', fontsize=12)\nax.set_title('Bagging: Individual Trees vs. Averaged Prediction', fontsize=14, fontweight='bold')\nax.legend(loc='upper right', fontsize=8)\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey Observation\n\n\n\nThis visualization powerfully demonstrates the value of bagging:\n\nTrue pattern (green dashed line): The underlying sin wave pattern we’re trying to learn\nTraining data (gray points): Noisy observations that obscure the true pattern\nIndividual trees (blue lines): Each tree makes large errors and fits the noise differently, creating erratic predictions\nBagged prediction (red line): The average of all 10 trees smooths out individual errors and closely follows the true pattern\n\nNotice how the individual trees have high variance—some overshoot, others undershoot, and they all capture noise from their particular bootstrap sample. But when we average their predictions, these errors cancel out, leaving a smooth prediction that captures the true underlying pattern. This is the power of bagging!\n\n\n\n\nThe Power of More Trees\nA natural question emerges: how many trees should we use? The beauty of bagging is that as we add more trees and average their predictions, we typically see a sharp decrease in overall prediction error. However, this improvement eventually levels off—after a certain point, adding more trees provides diminishing returns.\nThis happens because the first few trees capture the major patterns and reduce variance substantially. As we add more trees, we continue to smooth out errors, but the incremental improvement becomes smaller. In practice, random forests often use anywhere from 100 to 500 trees, though the optimal number depends on your specific problem.\nLet’s visualize this relationship using our sin wave example:\n\n\nShow code: Error reduction with increasing trees\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Create the same sin wave data\nX = np.linspace(0, 4*np.pi, 100).reshape(-1, 1)\ny_true = np.sin(X).ravel()\ny = y_true + np.random.normal(0, 0.3, len(X))\n\n# Create test grid to evaluate predictions\nX_test = np.linspace(0, 4*np.pi, 200).reshape(-1, 1)\ny_test_true = np.sin(X_test).ravel()\n\n# Track errors as we add more trees\nmax_trees = 50\nerrors = []\ntree_counts = range(1, max_trees + 1)\n\n# Store all tree predictions\nall_predictions = []\n\n# Train trees one at a time\nfor i in range(max_trees):\n    # Create bootstrap sample\n    indices = np.random.choice(len(X), size=len(X), replace=True)\n    X_bootstrap = X[indices]\n    y_bootstrap = y[indices]\n\n    # Train tree\n    tree = DecisionTreeRegressor(max_depth=5, random_state=i)\n    tree.fit(X_bootstrap, y_bootstrap)\n\n    # Predict on test set\n    y_pred = tree.predict(X_test)\n    all_predictions.append(y_pred)\n\n    # Calculate error using average of all trees so far\n    avg_prediction = np.mean(all_predictions, axis=0)\n    mse = mean_squared_error(y_test_true, avg_prediction)\n    errors.append(mse)\n\n# Plot error vs number of trees\nfig, ax = plt.subplots(figsize=(7, 4))\n\nax.plot(tree_counts, errors, linewidth=2.5, color='darkblue')\nax.axhline(y=errors[-1], color='red', linestyle='--', linewidth=1.5,\n           label=f'Final error with {max_trees} trees', alpha=0.7)\n\nax.set_xlabel('Number of Trees', fontsize=12)\nax.set_ylabel('Mean Squared Error', fontsize=12)\nax.set_title('Prediction Error Decreases as More Trees Are Added', fontsize=14, fontweight='bold')\nax.grid(alpha=0.3)\nax.legend(fontsize=10)\n\n# Annotate the sharp decrease\nax.annotate('Sharp decrease\\nwith first few trees',\n            xy=(5, errors[4]), xytext=(15, errors[4] + 0.01),\n            arrowprops=dict(arrowstyle='-&gt;', color='red', lw=1.5),\n            fontsize=10, color='red')\n\n# Annotate the leveling off\nax.annotate('Leveling off:\\ndiminishing returns',\n            xy=(35, errors[34]), xytext=(28, errors[34] + 0.01),\n            arrowprops=dict(arrowstyle='-&gt;', color='red', lw=1.5),\n            fontsize=10, color='red')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Error with 1 tree: {errors[0]:.4f}\")\nprint(f\"Error with 10 trees: {errors[9]:.4f} (reduction: {(1 - errors[9]/errors[0])*100:.1f}%)\")\nprint(f\"Error with 50 trees: {errors[49]:.4f} (reduction: {(1 - errors[49]/errors[0])*100:.1f}%)\")\n\n\n\n\n\n\n\n\n\nError with 1 tree: 0.0583\nError with 10 trees: 0.0267 (reduction: 54.3%)\nError with 50 trees: 0.0222 (reduction: 62.0%)\n\n\n\n\n\n\n\n\nImportantThe Power of Averaging Across Diverse Bootstrap Samples\n\n\n\nThis plot beautifully illustrates why random forests are so effective:\n\nInitial sharp decrease: The first 10-15 trees provide dramatic error reduction as diverse perspectives average out individual mistakes\nDiminishing returns: After ~20 trees, each additional tree provides smaller improvements\nStability: With enough trees, prediction error stabilizes at a low level\n\nThe key insight: diversity through bootstrap sampling allows individual noisy predictions to cancel out when averaged, revealing the true underlying pattern. More trees mean more diverse perspectives, which means more accurate collective wisdom—at least up to a point!\nIn practice, random forests typically use 100-500 trees to ensure stable predictions while balancing computational cost.\n\n\nWhat we’ve described so far—building multiple decision trees on bootstrap samples and averaging their predictions—is actually a complete machine learning technique called bagged trees (or bootstrap aggregated decision trees). Bagged trees are effective and widely used in practice.\nHowever, they have a limitation: because all trees consider the same features when making splits, the trees can still be somewhat correlated, especially if there are a few very strong predictive features in the dataset. For example, if one feature is far more predictive than others, most trees will split on that feature first, making their predictions somewhat similar despite using different bootstrap samples.\nRandom forests address this limitation by adding one more crucial source of diversity: feature randomness.\n\n\n\nFeature Randomness\nFeature randomness is the key innovation that distinguishes random forests from simple bagged trees. Here’s how it works:\n\n\n\n\n\n\nAt each split in each tree, only consider a random subset of features (rather than all features).\n\n\n\nThis seemingly small change has a profound impact. By forcing each tree to work with different feature subsets, we ensure that trees develop different structures and capture different patterns—even when they’re trained on similar bootstrap samples.\n\nHow Feature Randomness Works\nFor a dataset with p total features, at each node when the tree needs to make a split:\n\nRandomly select a subset of features (typically √p for classification, p/3 for regression)\nFind the best split using only these randomly selected features\nRepeat this process at every node in every tree\n\nThis means:\n\nDifferent trees will split on different features at the same depth\nA strong predictive feature won’t dominate every tree\nWeaker features get a chance to contribute in some trees\nTrees become more diverse and less correlated\n\n\n\nThe Impact of Feature Randomness\nConsider a dataset where one or two features are very strong predictors. Without feature randomness:\n\nMost trees would split on these dominant features first\nTrees would have similar structures despite bootstrap sampling\nPredictions would be correlated, limiting the benefit of averaging\n\nWith feature randomness:\n\nSome trees split on dominant features, others on different ones\nTrees explore different feature combinations\nPredictions are more diverse, so averaging provides greater error reduction\n\nLet’s visualize this concept:\n\n\nShow code: Feature randomness visualization\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as mpatches\n\n# Create a visualization showing feature selection at different nodes\nnp.random.seed(42)\n\nn_features = 8\nn_trees = 3\nn_nodes_per_tree = 3\n\nfig, axes = plt.subplots(1, n_trees, figsize=(12, 4))\nfeature_names = [f'F{i+1}' for i in range(n_features)]\ncolors = plt.cm.Set3(np.linspace(0, 1, n_features))\n\nfor tree_idx in range(n_trees):\n    ax = axes[tree_idx]\n\n    # For each node in this tree, randomly select features to consider\n    for node_idx in range(n_nodes_per_tree):\n        # Random subset of features (using sqrt(p) ~ 3 features for this example)\n        n_selected = 3\n        selected_features = np.random.choice(n_features, size=n_selected, replace=False)\n\n        y_pos = n_nodes_per_tree - node_idx - 1\n\n        # Draw all features as light gray (not selected)\n        for feat_idx in range(n_features):\n            x_pos = feat_idx * 0.12\n            if feat_idx in selected_features:\n                # Selected features are colored\n                rect = Rectangle((x_pos, y_pos*0.3), 0.1, 0.25,\n                               facecolor=colors[feat_idx], edgecolor='black', linewidth=2)\n            else:\n                # Not selected features are gray and faded\n                rect = Rectangle((x_pos, y_pos*0.3), 0.1, 0.25,\n                               facecolor='lightgray', edgecolor='gray',\n                               linewidth=0.5, alpha=0.3)\n            ax.add_patch(rect)\n\n            # Add feature labels only on bottom row\n            if node_idx == n_nodes_per_tree - 1:\n                ax.text(x_pos + 0.05, -0.15, feature_names[feat_idx],\n                       ha='center', fontsize=9)\n\n        # Add node label\n        ax.text(-0.15, y_pos*0.3 + 0.125, f'Node {node_idx+1}',\n               ha='right', va='center', fontsize=9, fontweight='bold')\n\n    ax.set_xlim(-0.2, n_features * 0.12)\n    ax.set_ylim(-0.25, n_nodes_per_tree * 0.3 + 0.1)\n    ax.axis('off')\n    ax.set_title(f'Tree {tree_idx + 1}', fontsize=12, fontweight='bold')\n\n# Add legend\nlegend_elements = [\n    mpatches.Patch(facecolor='lightblue', edgecolor='black', label='Selected for consideration'),\n    mpatches.Patch(facecolor='lightgray', edgecolor='gray', label='Not selected', alpha=0.3)\n]\nfig.legend(handles=legend_elements, loc='lower center', ncol=2, fontsize=10,\n          bbox_to_anchor=(0.5, -0.05))\n\nplt.suptitle('Feature Randomness: Different Trees Consider Different Features at Each Node',\n            fontsize=13, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipUnderstanding Feature Selection\n\n\n\nThis visualization shows three trees, each with three decision nodes. The eight features (F1-F8) are shown for each node:\n\nColored boxes: Features randomly selected for consideration at this node\nGray boxes: Features not available for this split\n\nNotice how:\n\nEach tree considers different random subsets of features at each node\nNo single feature dominates across all trees\nThe diversity of feature combinations leads to diverse tree structures\n\n\n\n\n\nBagged Trees vs. Random Forest Performance\nNow let’s demonstrate the performance improvement that feature randomness provides. We’ll compare bagged trees (bootstrap only) against random forests (bootstrap + feature randomness) using the Boston housing data—a classic dataset for predicting median home values:\n\n\nShow code: Bagged trees vs random forest comparison on housing data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom ISLP import load_data\n\n# Set random seed\nnp.random.seed(42)\n\n# Load Boston housing data\nBoston = load_data('Boston')\n\n# Separate features and target\nX = Boston.drop('medv', axis=1).values\ny = Boston['medv'].values\nfeature_names = Boston.drop('medv', axis=1).columns.tolist()\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(f\"Dataset: {len(X_train)} training samples, {X.shape[1]} features\")\nprint(f\"Features: {feature_names}\")\nprint(f\"Target: Median home value (in $1000s)\\n\")\n\n# Track performance as we add trees\nmax_trees = 100\ntree_range = range(1, max_trees + 1)\n\n# Bagged trees (all features at each split)\n# Using deeper trees (max_depth=None) to allow more overfitting, which makes\n# feature randomness more beneficial\nbagged_predictions = []\nbagged_mse = []\n\nfor i in range(max_trees):\n    # Bootstrap sample\n    indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n    X_bootstrap = X_train[indices]\n    y_bootstrap = y_train[indices]\n\n    # Train tree with ALL features at each split - no depth limit\n    tree = DecisionTreeRegressor(min_samples_split=10, min_samples_leaf=4, random_state=i)\n    tree.fit(X_bootstrap, y_bootstrap)\n\n    # Predict on test set\n    y_pred = tree.predict(X_test)\n    bagged_predictions.append(y_pred)\n\n    # Calculate ensemble MSE (average predictions)\n    avg_prediction = np.mean(bagged_predictions, axis=0)\n    mse = mean_squared_error(y_test, avg_prediction)\n    bagged_mse.append(mse)\n\n# Random forest (random feature subset at each split)\nrf_predictions = []\nrf_mse = []\nmax_features = X_train.shape[1] // 3  # p/3 for regression\n\nfor i in range(max_trees):\n    # Bootstrap sample\n    indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n    X_bootstrap = X_train[indices]\n    y_bootstrap = y_train[indices]\n\n    # Train tree with RANDOM SUBSET of features at each split - no depth limit\n    tree = DecisionTreeRegressor(max_features=max_features, min_samples_split=10,\n                                 min_samples_leaf=4, random_state=i)\n    tree.fit(X_bootstrap, y_bootstrap)\n\n    # Predict on test set\n    y_pred = tree.predict(X_test)\n    rf_predictions.append(y_pred)\n\n    # Calculate ensemble MSE (average predictions)\n    avg_prediction = np.mean(rf_predictions, axis=0)\n    mse = mean_squared_error(y_test, avg_prediction)\n    rf_mse.append(mse)\n\n# Plot comparison\nfig, ax = plt.subplots(figsize=(10, 5))\n\nax.plot(tree_range, bagged_mse, linewidth=2.5, color='orange',\n        label='Bagged Trees (all features)', alpha=0.8)\nax.plot(tree_range, rf_mse, linewidth=2.5, color='darkgreen',\n        label='Random Forest (feature randomness)', alpha=0.8)\n\nax.set_xlabel('Number of Trees', fontsize=12)\nax.set_ylabel('Test MSE', fontsize=12)\nax.set_title('Random Forest vs. Bagged Trees: Boston Housing Data',\n            fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal Performance Comparison ({max_trees} trees):\")\nprint(f\"Bagged Trees MSE:  {bagged_mse[-1]:.4f}\")\nprint(f\"Random Forest MSE: {rf_mse[-1]:.4f}\")\nprint(f\"Improvement: {((bagged_mse[-1] - rf_mse[-1]) / bagged_mse[-1] * 100):.1f}%\")\nprint(f\"\\nRMSE (in $1000s):\")\nprint(f\"Bagged Trees:  ${np.sqrt(bagged_mse[-1]):.3f}k\")\nprint(f\"Random Forest: ${np.sqrt(rf_mse[-1]):.3f}k\")\n\n\nDataset: 354 training samples, 12 features\nFeatures: ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'lstat']\nTarget: Median home value (in $1000s)\n\n\n\n\n\n\n\n\n\n\n\nFinal Performance Comparison (100 trees):\nBagged Trees MSE:  11.8729\nRandom Forest MSE: 11.1917\nImprovement: 5.7%\n\nRMSE (in $1000s):\nBagged Trees:  $3.446k\nRandom Forest: $3.345k\n\n\n\n\n\n\n\n\nImportantThe Power of Feature Randomness\n\n\n\nThis real-world example demonstrates why random forests outperform simple bagged trees:\n\nBagged trees (orange): Achieve good performance through bootstrap sampling alone\nRandom forests (green): Achieve consistently better performance by adding feature randomness\n\nThe improvement comes from decorrelating the trees. When trees are forced to consider different feature subsets, they:\n\nExplore different feature combinations and interactions\nMake more independent errors that cancel out when averaged\nDiscover useful patterns that might be masked by dominant features\nProvide more stable and accurate predictions\n\nThis is especially powerful in datasets with:\n\nStrong dominant features that would otherwise control all trees\nMany correlated or redundant features\nComplex interactions between features that different trees can explore differently\n\nThe combination of bootstrap sampling AND feature randomness is what makes random forests one of the most effective machine learning algorithms available.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Random Forests: Ensemble Power and Robustness</span>"
    ]
  },
  {
    "objectID": "26-random-forests.html#building-random-forest-models",
    "href": "26-random-forests.html#building-random-forest-models",
    "title": "26  Random Forests: Ensemble Power and Robustness",
    "section": "26.3 Building Random Forest Models",
    "text": "26.3 Building Random Forest Models\nNow that you understand how random forests work conceptually—combining bootstrap sampling and feature randomness to create diverse trees—let’s put this into practice. In this section, we’ll use scikit-learn’s RandomForestClassifier and RandomForestRegressor to build models for both classification and regression problems. You’ll see how simple it is to implement random forests in practice, and how to understand what’s happening under the hood when the algorithm makes predictions.\n\nClassification with Random Forests\nLet’s start with a classification example using the Default dataset from ISLP, which predicts whether credit card customers will default on their payments.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom ISLP import load_data\n\n# Set random seed\nnp.random.seed(42)\n\n# Load Default dataset\nDefault = load_data('Default')\n\n# Prepare features and target\nX_default = pd.get_dummies(Default[['balance', 'income', 'student']], drop_first=True)\ny_default = (Default['default'] == 'Yes').astype(int)\n\n# Split data\nX_train_default, X_test_default, y_train_default, y_test_default = train_test_split(\n    X_default, y_default, test_size=0.3, random_state=42\n)\n\n# Build and train random forest classifier with 100 trees\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train_default, y_train_default)\n\n# Make predictions\ny_pred_default = rf_classifier.predict(X_test_default)\n\n# Evaluate performance\ncm = confusion_matrix(y_test_default, y_pred_default)\n\nprint(\"Confusion Matrix:\")\nprint(cm)\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_default, y_pred_default, target_names=['No Default', 'Default']))\n\nConfusion Matrix:\n[[2880   26]\n [  65   29]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n  No Default       0.98      0.99      0.98      2906\n     Default       0.53      0.31      0.39        94\n\n    accuracy                           0.97      3000\n   macro avg       0.75      0.65      0.69      3000\nweighted avg       0.96      0.97      0.97      3000\n\n\n\n\n\n\n\n\n\nNoteHow Classification Predictions Work: Majority Voting\n\n\n\nRandom forests make classification predictions through majority voting:\n\nEach tree votes: All 100 trees independently predict a class (0 or 1)\nCount the votes: Tally how many trees predict each class\nMajority wins: The class with the most votes becomes the final prediction\nProbabilities: The predicted probability is the proportion of votes (e.g., 73/100 = 0.73)\n\nThis voting mechanism makes random forests robust—even if some individual trees make mistakes, the collective wisdom of the majority typically gets it right!\n\n\n\n\nRegression with Random Forests\nNow let’s see a regression example using the College dataset from ISLP, which predicts the number of applications received by colleges.\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import root_mean_squared_error\n\n# Set random seed\nnp.random.seed(42)\n\n# Load College dataset\nCollege = load_data('College')\n\n# Select features and target\n# We'll predict number of applications (Apps) using other college characteristics\nfeature_cols = ['Accept', 'Enroll', 'Top10perc', 'Top25perc', 'F.Undergrad',\n                'P.Undergrad', 'Outstate', 'Room.Board', 'Books', 'Personal',\n                'PhD', 'Terminal', 'S.F.Ratio', 'perc.alumni', 'Expend', 'Grad.Rate']\nX_college = College[feature_cols]\ny_college = College['Apps']\n\n# Split data\nX_train_college, X_test_college, y_train_college, y_test_college = train_test_split(\n    X_college, y_college, test_size=0.3, random_state=42\n)\n\n# Build and train random forest regressor with 100 trees\nrf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_regressor.fit(X_train_college, y_train_college)\n\n# Make predictions\ny_pred_college = rf_regressor.predict(X_test_college)\n\n# Evaluate performance\nrmse = root_mean_squared_error(y_test_college, y_pred_college)\n\nprint(\"Model Performance:\")\nprint(f\"RMSE: {rmse:,.0f} applications\")\nprint(f\"Mean actual applications: {y_test_college.mean():,.0f}\")\nprint(f\"RMSE as % of mean: {(rmse / y_test_college.mean() * 100):.1f}%\")\n\nModel Performance:\nRMSE: 1,207 applications\nMean actual applications: 3,088\nRMSE as % of mean: 39.1%\n\n\n\n\n\n\n\n\nNoteHow Regression Predictions Work: Averaging\n\n\n\nRandom forests make regression predictions through averaging:\n\nEach tree predicts: All 100 trees independently predict a numeric value\nCalculate the mean: Average all predictions: (tree₁ + tree₂ + … + tree₁₀₀) / 100\nAverage is the final prediction: This becomes the model’s prediction\n\nThis averaging mechanism reduces prediction variance. While individual trees might over or underestimate based on their specific bootstrap sample, the average smooths out these fluctuations, resulting in more stable and accurate predictions!\n\n\n\n\nKey Hyperparameters\nRandom forests have several important hyperparameters that control how the forest is built and how individual trees behave. Understanding these parameters helps you build effective models. Let’s explore the most critical ones and see how they impact model performance. We’ll cover hyperparameter tuning strategies in a later chapter, but for now, let’s understand what each parameter does.\n\nn_estimators: Number of Trees in the Forest\nThe n_estimators parameter controls how many decision trees are built in the forest. More trees generally lead to better performance, but with diminishing returns.\n\nMore trees = better performance (up to a point)\nDiminishing returns after ~100-500 trees\nComputational tradeoff: more trees = longer training time\n\n\n\nShow code: Impact of n_estimators\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load data\nBoston = load_data('Boston')\nX = Boston.drop('medv', axis=1)\ny = Boston['medv']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Test different numbers of trees\ntree_counts = [1, 5, 10, 25, 50, 100, 200, 300, 500]\ntrain_errors = []\ntest_errors = []\n\nfor n_trees in tree_counts:\n    rf = RandomForestRegressor(n_estimators=n_trees, random_state=42, max_features='sqrt')\n    rf.fit(X_train, y_train)\n\n    train_pred = rf.predict(X_train)\n    test_pred = rf.predict(X_test)\n\n    train_errors.append(mean_squared_error(y_train, train_pred))\n    test_errors.append(mean_squared_error(y_test, test_pred))\n\n# Plot\nfig, ax = plt.subplots(figsize=(7, 5))\nax.plot(tree_counts, train_errors, 'o-', label='Training Error', linewidth=2)\nax.plot(tree_counts, test_errors, 's-', label='Test Error', linewidth=2)\nax.set_xlabel('Number of Trees (n_estimators)', fontsize=12)\nax.set_ylabel('Mean Squared Error', fontsize=12)\nax.set_title('Impact of Number of Trees on Performance', fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Test MSE with 10 trees:  {test_errors[2]:.4f}\")\nprint(f\"Test MSE with 100 trees: {test_errors[5]:.4f}\")\nprint(f\"Test MSE with 500 trees: {test_errors[8]:.4f}\")\nprint(f\"\\nImprovement from 10 to 100 trees: {((test_errors[2] - test_errors[5])/test_errors[2]*100):.1f}%\")\nprint(f\"Improvement from 100 to 500 trees: {((test_errors[5] - test_errors[8])/test_errors[5]*100):.1f}%\")\n\n\n\n\n\n\n\n\n\nTest MSE with 10 trees:  14.1304\nTest MSE with 100 trees: 10.4205\nTest MSE with 500 trees: 10.3674\n\nImprovement from 10 to 100 trees: 26.3%\nImprovement from 100 to 500 trees: 0.5%\n\n\n\n\nmax_depth: Maximum Depth of Individual Trees\nThe max_depth parameter limits how deep each tree can grow. Deeper trees can capture more complex patterns but risk overfitting.\n\nShallow trees (depth 3-10): Simple patterns, less overfitting, faster training\nMedium trees (depth 10-20): Balance complexity and generalization\nDeep trees (no limit): Maximum flexibility, relies on ensemble averaging to prevent overfitting\n\nRandom forests can handle deeper trees than individual decision trees because the ensemble averaging reduces overfitting.\n\n\nShow code: Impact of max_depth\n# Test different maximum depths\ndepths = [3, 5, 10, 15, 20, None]  # None means no limit\ndepth_labels = ['3', '5', '10', '15', '20', 'No Limit']\ntrain_errors = []\ntest_errors = []\n\nfor depth in depths:\n    rf = RandomForestRegressor(n_estimators=100, max_depth=depth, random_state=42, max_features='sqrt')\n    rf.fit(X_train, y_train)\n\n    train_pred = rf.predict(X_train)\n    test_pred = rf.predict(X_test)\n\n    train_errors.append(mean_squared_error(y_train, train_pred))\n    test_errors.append(mean_squared_error(y_test, test_pred))\n\n# Plot\nfig, ax = plt.subplots(figsize=(7, 5))\nx_pos = np.arange(len(depths))\nax.plot(x_pos, train_errors, 'o-', label='Training Error', linewidth=2, markersize=8)\nax.plot(x_pos, test_errors, 's-', label='Test Error', linewidth=2, markersize=8)\nax.set_xticks(x_pos)\nax.set_xticklabels(depth_labels)\nax.set_xlabel('Maximum Tree Depth (max_depth)', fontsize=12)\nax.set_ylabel('Mean Squared Error', fontsize=12)\nax.set_title('Impact of Tree Depth on Performance', fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Best test MSE achieved with max_depth = {depth_labels[np.argmin(test_errors)]}\")\nprint(f\"Test MSE: {min(test_errors):.4f}\")\n\n\n\n\n\n\n\n\n\nBest test MSE achieved with max_depth = 15\nTest MSE: 9.7002\n\n\n\n\nmax_features: Features Considered at Each Split\nThe max_features parameter controls how many features are randomly selected at each split. This is the key parameter for controlling tree diversity!\n\n‘sqrt’ or √p: Common choice for classification - good default, high diversity\np/3: Common choice for regression - provides good balance of diversity and performance\n‘log2’: Consider log₂(p) features - even more diversity, useful for high-dimensional data\nNone or p: Consider all p features - equivalent to bagged trees, less diversity\nInteger or float: Specific number or fraction of features to consider\n\nNote: Sklearn’s default is ‘sqrt’ for RandomForestClassifier and 1.0 (all features) for RandomForestRegressor, but p/3 is often recommended for regression in practice.\n\n\nShow code: Impact of max_features\n# Test different max_features settings\nn_features = X_train.shape[1]\nmax_features_options = [1, int(np.sqrt(n_features)), n_features // 3, n_features // 2, n_features]\nfeature_labels = ['1', f'√p ({int(np.sqrt(n_features))})', f'p/3 ({n_features // 3})',\n                  f'p/2 ({n_features // 2})', f'All ({n_features})']\ntrain_errors = []\ntest_errors = []\n\nfor max_feat in max_features_options:\n    rf = RandomForestRegressor(n_estimators=100, max_features=max_feat, random_state=42)\n    rf.fit(X_train, y_train)\n\n    train_pred = rf.predict(X_train)\n    test_pred = rf.predict(X_test)\n\n    train_errors.append(mean_squared_error(y_train, train_pred))\n    test_errors.append(mean_squared_error(y_test, test_pred))\n\n# Plot\nfig, ax = plt.subplots(figsize=(7, 5))\nx_pos = np.arange(len(max_features_options))\nax.plot(x_pos, train_errors, 'o-', label='Training Error', linewidth=2, markersize=8)\nax.plot(x_pos, test_errors, 's-', label='Test Error', linewidth=2, markersize=8)\nax.set_xticks(x_pos)\nax.set_xticklabels(feature_labels, rotation=0)\nax.set_xlabel('Features Considered at Each Split (max_features)', fontsize=12)\nax.set_ylabel('Mean Squared Error', fontsize=12)\nax.set_title('Impact of Feature Randomness on Performance', fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nBest test MSE achieved with max_features = {feature_labels[np.argmin(test_errors)]}\")\n\n\n\n\n\n\n\n\n\n\nBest test MSE achieved with max_features = p/2 (6)\n\n\n\n\nmin_samples_split and min_samples_leaf: Controlling Tree Complexity\nThese parameters prevent trees from making splits on very small groups of samples:\n\nmin_samples_split: Minimum samples required to split a node (default: 2)\nmin_samples_leaf: Minimum samples required in a leaf node (default: 1)\n\nIncreasing these values creates simpler, more regularized trees that are less likely to overfit but are also less accurate.\n\n\nShow code: Impact of min_samples_split\n# Test different min_samples_split values\nmin_samples_options = [2, 5, 10, 20, 50, 100]\ntrain_errors = []\ntest_errors = []\n\nfor min_samp in min_samples_options:\n    rf = RandomForestRegressor(n_estimators=100, min_samples_split=min_samp,\n                               random_state=42, max_features='sqrt')\n    rf.fit(X_train, y_train)\n\n    train_pred = rf.predict(X_train)\n    test_pred = rf.predict(X_test)\n\n    train_errors.append(mean_squared_error(y_train, train_pred))\n    test_errors.append(mean_squared_error(y_test, test_pred))\n\n# Plot\nfig, ax = plt.subplots(figsize=(7, 5))\nax.plot(min_samples_options, train_errors, 'o-', label='Training Error', linewidth=2, markersize=8)\nax.plot(min_samples_options, test_errors, 's-', label='Test Error', linewidth=2, markersize=8)\nax.set_xlabel('Minimum Samples to Split (min_samples_split)', fontsize=12)\nax.set_ylabel('Mean Squared Error', fontsize=12)\nax.set_title('Impact of Minimum Sample Requirements on Performance', fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Best test MSE achieved with min_samples_split = {min_samples_options[np.argmin(test_errors)]}\")\n\n\n\n\n\n\n\n\n\nBest test MSE achieved with min_samples_split = 2\n\n\n\n\n\n\n\n\nTipPractical Guidelines for Hyperparameter Selection\n\n\n\nBased on these demonstrations, here are some practical starting points:\n\n\n\n\n\n\n\n\n\nHyperparameter\nClassification\nRegression\nPurpose\n\n\n\n\nn_estimators\n100-200\n100-200\nNumber of trees (more = better, but diminishing returns)\n\n\nmax_features\n‘sqrt’\np/3 or ‘sqrt’\nFeatures to consider at each split (controls diversity)\n\n\nmax_depth\nNone\nNone\nMaximum tree depth (None = no limit)\n\n\nmin_samples_split\n2\n2\nMinimum samples to split a node (increase if overfitting)\n\n\nmin_samples_leaf\n1\n1\nMinimum samples in leaf node (increase if overfitting)\n\n\n\nThe beauty of random forests is that they work well “out of the box” with default parameters. The ensemble averaging makes them robust to overfitting, so you often don’t need extensive tuning. That said, systematic hyperparameter tuning (covered in a later chapter) can squeeze out additional performance when needed.\n\n\n\n\n\nDefault vs. Tuned Random Forests\nIn a later chapter, we’ll discuss efficient ways to find the optimal combination of hyperparameters using techniques like grid search and randomized search. For now, let’s see how a default random forest compares to one with manually tuned hyperparameters on the Ames housing dataset.\n\n\nShow code: Prepare Ames data\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load and prepare Ames data\names = pd.read_csv('../data/ames_clean.csv')\n\n# Select only numeric features\nnumeric_features = ames.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_features.remove('SalePrice')\n\nX_ames = ames[numeric_features]\ny_ames = ames['SalePrice']\n\n# Split data\nX_train_ames, X_test_ames, y_train_ames, y_test_ames = train_test_split(\n    X_ames, y_ames, test_size=0.3, random_state=42\n)\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Default random forest (using sklearn defaults)\nrf_default = RandomForestRegressor(random_state=42)\nrf_default.fit(X_train_ames, y_train_ames)\n\n# Predict and evaluate\ny_pred_default = rf_default.predict(X_test_ames)\nrmse_default = root_mean_squared_error(y_test_ames, y_pred_default)\n\nprint(\"Default Random Forest Performance:\")\nprint(f\"  RMSE: ${rmse_default:,.0f}\")\nprint(f\"  RMSE as % of mean: {(rmse_default / y_test_ames.mean() * 100):.1f}%\")\nprint(f\"\\nDefault hyperparameters used:\")\nprint(f\"  n_estimators: {rf_default.n_estimators}\")\nprint(f\"  max_features: {rf_default.max_features}\")\nprint(f\"  max_depth: {rf_default.max_depth}\")\nprint(f\"  min_samples_split: {rf_default.min_samples_split}\")\nprint(f\"  min_samples_leaf: {rf_default.min_samples_leaf}\")\n\nDefault Random Forest Performance:\n  RMSE: $26,923\n  RMSE as % of mean: 15.0%\n\nDefault hyperparameters used:\n  n_estimators: 100\n  max_features: 1.0\n  max_depth: None\n  min_samples_split: 2\n  min_samples_leaf: 1\n\n\n\n# Tuned random forest (using optimal hyperparameters from grid search)\nrf_tuned = RandomForestRegressor(\n    n_estimators=300,              # vs. default 100 - more trees improve performance\n    max_features=18,                # vs. default all features - p/3 increases tree diversity\n    max_depth=20,                   # vs. default None - limits overfitting\n    min_samples_split=2,            # same as default - no change needed\n    min_samples_leaf=1,             # same as default - no change needed\n    random_state=42\n)\nrf_tuned.fit(X_train_ames, y_train_ames)\n\n# Predict and evaluate\ny_pred_tuned = rf_tuned.predict(X_test_ames)\nrmse_tuned = root_mean_squared_error(y_test_ames, y_pred_tuned)\n\nprint(\"\\nTuned Random Forest Performance:\")\nprint(f\"  RMSE: ${rmse_tuned:,.0f}\")\nprint(f\"  RMSE as % of mean: {(rmse_tuned / y_test_ames.mean() * 100):.1f}%\")\nprint(f\"\\nTuned hyperparameters used:\")\nprint(f\"  n_estimators: {rf_tuned.n_estimators}\")\nprint(f\"  max_features: {rf_tuned.max_features}\")\nprint(f\"  max_depth: {rf_tuned.max_depth}\")\nprint(f\"  min_samples_split: {rf_tuned.min_samples_split}\")\nprint(f\"  min_samples_leaf: {rf_tuned.min_samples_leaf}\")\n\n\nTuned Random Forest Performance:\n  RMSE: $26,843\n  RMSE as % of mean: 14.9%\n\nTuned hyperparameters used:\n  n_estimators: 300\n  max_features: 18\n  max_depth: 20\n  min_samples_split: 2\n  min_samples_leaf: 1\n\n\n\n# Compare the two models\nimprovement = ((rmse_default - rmse_tuned) / rmse_default) * 100\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Performance Comparison\")\nprint(\"=\"*60)\nprint(f\"Default RMSE:  ${rmse_default:,.0f}\")\nprint(f\"Tuned RMSE:    ${rmse_tuned:,.0f}\")\nprint(f\"Improvement:   {improvement:.1f}%\")\n\n\n============================================================\nPerformance Comparison\n============================================================\nDefault RMSE:  $26,923\nTuned RMSE:    $26,843\nImprovement:   0.3%\n\n\n\n\n\n\n\n\nNoteKey Takeaways\n\n\n\n\nDefault random forests work well: Even with default settings, random forests achieve strong performance\nTuning can help: Thoughtful hyperparameter selection can improve performance, though gains are often modest\nDiminishing returns: The improvement from tuning is typically smaller than the jump from a single tree to a random forest\nEfficient tuning matters: In later chapters, we’ll learn systematic approaches (grid search, random search) to efficiently explore the hyperparameter space rather than manual trial-and-error",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Random Forests: Ensemble Power and Robustness</span>"
    ]
  },
  {
    "objectID": "26-random-forests.html#random-forests-in-business-context",
    "href": "26-random-forests.html#random-forests-in-business-context",
    "title": "26  Random Forests: Ensemble Power and Robustness",
    "section": "26.4 Random Forests in Business Context",
    "text": "26.4 Random Forests in Business Context\nRandom forests have become one of the most widely used machine learning algorithms in practice due to their strong performance, robustness, and ease of use. Understanding when and where to apply them is crucial for business analytics.\n\nAdvantages Over Other Methods\nWhy random forests are popular in business applications:\n\nRobust performance: Handle outliers, noisy data, and missing values without extensive preprocessing\nMinimal tuning required: Work well with default parameters, reducing time from development to deployment\nVersatile: Effective for both classification and regression across diverse problem types\nReduce overfitting: Ensemble averaging makes them more reliable than single decision trees\nHandle mixed data: Work naturally with both numeric and categorical features\nProvide insights: Built-in feature importance (covered in next chapter) helps identify key drivers\n\n\n\nLimitations to Consider\nWhen random forests may not be the best choice:\n\nInterpretability: Harder to explain predictions than a single decision tree (though feature importance helps—see next chapter)\nComputational cost: Training hundreds of trees requires more time and memory than simpler models\nLarge datasets: Can become memory-intensive with millions of observations\nReal-time predictions: Slower prediction time than linear models due to querying many trees\nExtrapolation: Like all tree-based methods, struggle to predict beyond the range of training data\n\n\n\nIndustry Applications\nRandom forests excel in business environments where decisions need to balance accuracy with interpretability and where data is often messy or incomplete. Here are some common applications across industries:\n\nFinance: Banks and financial institutions use random forests to assess credit risk by predicting loan default probability and detect fraudulent transactions in real-time by identifying unusual patterns (Li and Zhang 2021; Khan and Malik 2024). Random forests have proven particularly effective for these applications due to their ability to handle imbalanced datasets and capture complex nonlinear relationships in financial data.\nHealthcare: Hospitals deploy random forests to predict which patients are at high risk for readmission (helping allocate resources), assist in disease diagnosis by analyzing patient symptoms and test results, and forecast treatment outcomes to personalize care plans (Hussain and Al-Obeidat 2024; Kim and Park 2021). The algorithm’s robustness to missing data makes it especially valuable in healthcare settings.\nMarketing & Retail: Marketing teams rely on random forests to predict customer churn (identifying at-risk customers before they leave), score leads for sales prioritization, segment customers into actionable groups, and model response rates for targeted campaigns (Wang and Chen 2023; Calvo and Fernandez 2024). The ability to handle large numbers of customer behavior features makes random forests ideal for these applications.\nE-commerce: Online retailers use random forests to power recommendation engines (predicting what customers might purchase next), optimize pricing strategies based on demand and competition, and forecast product demand across different regions and time periods.\nOperations & Manufacturing: Companies implement random forests for predictive maintenance (forecasting equipment failures before they occur) and quality control (identifying defective products) (Schmidt and Hoffmann 2023; Zhao 2024). The algorithm’s capacity to learn from historical sensor data and equipment logs helps prevent costly downtime.\nHuman Resources: HR departments apply random forests to predict employee attrition (helping retain valuable talent), score job candidates to improve hiring decisions, and forecast employee performance to guide development programs.\n\n\n\n\n\n\n\nTipWhy Random Forests Work Well in Business\n\n\n\nRandom forests’ ability to handle complex, messy real-world data with minimal preprocessing makes them a go-to algorithm for many business analytics teams. While they may not always achieve the absolute highest accuracy (gradient boosting methods sometimes edge them out), their reliability, ease of use, and ability to provide feature importance insights often make them the practical choice for production systems.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Random Forests: Ensemble Power and Robustness</span>"
    ]
  },
  {
    "objectID": "26-random-forests.html#summary",
    "href": "26-random-forests.html#summary",
    "title": "26  Random Forests: Ensemble Power and Robustness",
    "section": "26.5 Summary",
    "text": "26.5 Summary\nRandom forests transform the instability of individual decision trees into a strength by combining hundreds of trees trained on different data samples. The key insight: while any single tree might overfit or make errors, the collective wisdom of diverse trees averages out mistakes, producing accurate and stable predictions.\nRandom forests create diversity through two mechanisms: bootstrap aggregating (each tree trains on a random sample with replacement) and feature randomness (each split considers only a random subset of features—typically √p for classification, p/3 for regression). These mechanisms decorrelate trees and prevent dominant features from controlling the forest. Predictions aggregate through majority voting (classification) or averaging (regression).\nWe explored key hyperparameters including n_estimators, max_features, max_depth, and minimum sample requirements. An important practical finding: random forests work remarkably well with default settings, requiring minimal tuning. This “out-of-the-box” effectiveness makes them ideal for business applications across industries—from credit risk assessment and fraud detection to customer churn prediction and predictive maintenance.\nHowever, one question remains: which features actually matter? In the next chapter, we’ll explore feature importance—a built-in mechanism for quantifying each feature’s contribution to predictions and identifying the key drivers behind your outcomes.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Random Forests: Ensemble Power and Robustness</span>"
    ]
  },
  {
    "objectID": "26-random-forests.html#end-of-chapter-exercise",
    "href": "26-random-forests.html#end-of-chapter-exercise",
    "title": "26  Random Forests: Ensemble Power and Robustness",
    "section": "26.6 End of Chapter Exercise",
    "text": "26.6 End of Chapter Exercise\nFor these exercises, you’ll compare decision trees to random forests across real business scenarios. Each exercise will help you understand how ensemble methods improve upon individual trees and how hyperparameter tuning can further enhance performance.\n\n\n\n\n\n\nNoneExercise 1: Baseball Salary Prediction (Regression)\n\n\n\n\n\n\nCompany: Professional baseball team\nGoal: Improve salary predictions by comparing individual decision trees to random forest ensembles\nDataset: Hitters dataset from ISLP package\n\n\nfrom ISLP import load_data\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\nimport matplotlib.pyplot as plt\n\n# Load and prepare the data\nHitters = load_data('Hitters')\n\n# Remove missing salary values\nHitters_clean = Hitters.dropna(subset=['Salary'])\n\n# Select numeric features for our models\nfeatures = ['Years', 'Hits', 'RBI', 'Walks', 'PutOuts']\nX_hitters = Hitters_clean[features]\ny_hitters = Hitters_clean['Salary']\n\nprint(f\"Dataset size: {len(Hitters_clean)} players\")\nprint(f\"Features used: {features}\")\nprint(f\"\\nFirst few rows:\")\nprint(Hitters_clean[features + ['Salary']].head())\n\nDataset size: 263 players\nFeatures used: ['Years', 'Hits', 'RBI', 'Walks', 'PutOuts']\n\nFirst few rows:\n   Years  Hits  RBI  Walks  PutOuts  Salary\n1     14    81   38     39      632   475.0\n2      3   130   72     76      880   480.0\n3     11   141   78     37      200   500.0\n4      2    87   42     30      805    91.5\n5     11   169   51     35      282   750.0\n\n\nYour Tasks:\n\nBuild three models and compare performance:\n\nModel A: DecisionTreeRegressor with max_depth=4\nModel B: RandomForestRegressor with default parameters\nModel C: RandomForestRegressor with tuned parameters (try n_estimators=200, max_depth=6, min_samples_split=10)\n\nEvaluate and compare: Split data (70/30 train/test) and for each model calculate:\n\nTraining R² and test R²\nMean Absolute Error on test set\nRoot Mean Squared Error on test set\nOverfitting gap (training R² - test R²)\n\nPerformance analysis:\n\nWhich model generalizes best to the test set?\nHow much improvement did the default random forest provide over the single tree?\nDid tuning the hyperparameters further improve performance?\n\nTrade-offs:\n\nCompare model interpretability: Can you still extract clear business rules from the random forest?\nConsider computational cost: How much longer did the random forest take to train?\nWhich model would you recommend to the team’s management? Why?\n\nBusiness reflection:\n\nHow confident would you be using each model for actual salary negotiations?\nWhat are the risks of relying on the more complex random forest vs. the simpler tree?\n\n\n\n\n\n\n\n\n\n\n\nNoneExercise 2: Credit Default Classification\n\n\n\n\n\n\nCompany: Regional bank\nGoal: Improve default prediction accuracy while understanding the performance gains from ensemble methods\nDataset: Default dataset from ISLP package\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Load the Default dataset\nDefault = load_data('Default')\n\n# Prepare features - encode student as binary\nDefault_encoded = pd.get_dummies(Default, columns=['student'], drop_first=True)\nDefault_encoded['default_binary'] = (Default_encoded['default'] == 'Yes').astype(int)\n\n# Select features\nX_default = Default_encoded[['balance', 'income', 'student_Yes']]\ny_default = Default_encoded['default_binary']\n\nprint(f\"Dataset size: {len(Default)} customers\")\nprint(f\"Default rate: {y_default.mean():.1%}\")\nprint(f\"\\nFeatures prepared:\")\nprint(X_default.head())\n\nDataset size: 10000 customers\nDefault rate: 3.3%\n\nFeatures prepared:\n       balance        income  student_Yes\n0   729.526495  44361.625074        False\n1   817.180407  12106.134700         True\n2  1073.549164  31767.138947        False\n3   529.250605  35704.493935        False\n4   785.655883  38463.495879        False\n\n\nYour Tasks:\n\nBuild and compare three classification models:\n\nModel A: DecisionTreeClassifier with max_depth=3, min_samples_split=50\nModel B: RandomForestClassifier with default parameters\nModel C: RandomForestClassifier with tuned parameters (try n_estimators=200, max_depth=10, min_samples_split=20, max_features='sqrt')\n\nEvaluate on the imbalanced data: Split data (70/30) and for each model examine:\n\nTraining vs. test accuracy\nPrecision and recall for the “default” class specifically\nConfusion matrix on test set\nWhich model has the best balance of precision and recall?\n\nUnderstand the ensemble advantage:\n\nCalculate the overfitting gap for each model\nHow did random forests reduce overfitting compared to the single tree?\nDid the tuned random forest improve further on precision/recall for defaults?\n\nFeature importance (Random Forest only):\n\nUse .feature_importances_ on your best random forest model\nWhich feature is most important for predicting defaults?\nHow does this align with business intuition?\n\nBusiness application:\n\nThe bank loses $10,000 on each default but gains $500 in interest from good customers\nUsing the confusion matrices, calculate the expected profit/loss for each model\nWhich model would you deploy in production? Why?\nWhat threshold adjustments might you make given the cost structure?\n\n\n\n\n\n\n\n\n\n\n\nNoneExercise 3: Stock Market Direction Prediction (Optional Challenge)\n\n\n\n\n\n\nCompany: Investment management firm\nGoal: Test whether random forests can capture market patterns better than single decision trees\nDataset: Weekly dataset from ISLP package\n\n\n# Load Weekly stock market data\nWeekly = load_data('Weekly')\n\n# Prepare features and target\nWeekly_encoded = Weekly.copy()\nWeekly_encoded['Direction_binary'] = (Weekly_encoded['Direction'] == 'Up').astype(int)\n\n# Use lag variables as features\nlag_features = ['Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5']\nX_weekly = Weekly_encoded[lag_features]\ny_weekly = Weekly_encoded['Direction_binary']\n\nprint(f\"Dataset size: {len(Weekly)} weeks\")\nprint(f\"Up weeks: {y_weekly.mean():.1%}\")\nprint(f\"\\nLag features:\")\nprint(X_weekly.head())\n\nDataset size: 1089 weeks\nUp weeks: 55.6%\n\nLag features:\n    Lag1   Lag2   Lag3   Lag4   Lag5\n0  0.816  1.572 -3.936 -0.229 -3.484\n1 -0.270  0.816  1.572 -3.936 -0.229\n2 -2.576 -0.270  0.816  1.572 -3.936\n3  3.514 -2.576 -0.270  0.816  1.572\n4  0.712  3.514 -2.576 -0.270  0.816\n\n\nYour Tasks:\n\nBuild three models for market prediction:\n\nModel A: DecisionTreeClassifier with max_depth=3\nModel B: RandomForestClassifier with default parameters\nModel C: RandomForestClassifier with tuned parameters of your choice\n\nTime-series evaluation:\n\nSplit data chronologically (first 80% train, last 20% test)\nCalculate test accuracy for each model\nCompare to baseline: what’s the accuracy of always predicting “Up”?\n\nPerformance comparison:\n\nDid random forests improve over the single tree?\nAre any of the models beating the baseline?\nWhat does this tell you about market predictability?\n\nFeature importance analysis:\n\nExamine feature importance from your best random forest\nWhich lag periods matter most?\nDo these patterns make financial sense?\n\nChallenge questions:\n\nWhy might even random forests struggle with financial market prediction?\nWhat characteristics of stock market data make it fundamentally difficult for tree-based methods?\nWould you recommend using any of these models for actual trading? What would be the risks?\nHow might you modify the approach to make it more suitable for financial prediction?\n\n\n\n\n\n\n\n\n\nCalvo, J., and M. Fernandez. 2024. “Enhancing Customer Retention with Machine Learning: A Comparative Study of Ensemble Models.” Journal of Retail Analytics. https://www.sciencedirect.com/science/article/pii/S2667096825000138.\n\n\nHussain, A., and F. Al-Obeidat. 2024. “A Machine Learning Model to Predict Heart Failure Readmission: Toward Optimal Feature Set.” Frontiers in Artificial Intelligence 7: 1363226. https://doi.org/10.3389/frai.2024.1363226.\n\n\nKhan, A., and R. Malik. 2024. “Predictive Power of Random Forests in Analyzing Risk Management Practices and Concerns in Islamic Banks.” Journal of Risk and Financial Management 17 (3): 104. https://doi.org/10.3390/jrfm17030104.\n\n\nKim, H., and S. Park. 2021. “Machine Learning for Predicting Readmission Risk Among Hospitalized Patients: A Systematic Review.” Digital Health. https://www.sciencedirect.com/science/article/pii/S2666389921002622.\n\n\nLi, H., and Y. Zhang. 2021. “Financial Credit Risk Control Strategy Based on Weighted Random Forest Algorithm.” Journal of Applied Mathematics, 1–9. https://doi.org/10.1155/2021/6276155.\n\n\nSchmidt, L., and M. Hoffmann. 2023. “Using Machine Learning Prediction Models for Quality Control: A Case Study from the Automotive Industry.” Journal of Intelligent Manufacturing. https://doi.org/10.1007/s10287-023-00448-0.\n\n\nWang, L., and J. Chen. 2023. “Customer Churn Prediction Based on the Decision Tree and Random Forest Model.” International Journal of Computer Applications. https://www.researchgate.net/publication/370571328_Customer_Churn_Prediction_Based_on_the_Decision_Tree_and_Random_Forest_Model.\n\n\nZhao, Q. 2024. “Random Forest-Based Machine Failure Prediction in Industrial Equipment.” Applied Sciences 15 (16): 8841. https://doi.org/10.3390/app15168841.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Random Forests: Ensemble Power and Robustness</span>"
    ]
  },
  {
    "objectID": "27-feature-importance.html",
    "href": "27-feature-importance.html",
    "title": "27  Understanding Feature Importance: Peeking Inside the Black Box",
    "section": "",
    "text": "27.1 From Transparency to Black Boxes\nThroughout this course, you’ve progressed from simple, interpretable models like linear regression—where coefficients directly tell you how features affect predictions—to more powerful ensemble methods like random forests that combine hundreds of decision trees. As our models have grown more accurate, they’ve also become more opaque. A random forest might achieve 95% accuracy, but can you explain why it made a specific prediction? Which features drove that decision?\nThis chapter bridges the gap between model accuracy and model interpretability. You’ll learn techniques for understanding which features drive your model’s predictions, how to quantify their importance, and how to visualize these insights effectively. These skills are crucial not just for building stakeholder trust, but also for debugging models, ensuring they’re learning the right patterns, and making better business decisions.\nBy the end of this chapter, you will be able to:\nThink back to your first encounter with linear regression. Remember how straightforward it was to explain? “For each additional square foot, the house price increases by $45.” The model’s decision-making process was completely transparent—just a simple equation with coefficients you could point to and explain to anyone.\nBut as you’ve progressed through this course, you’ve learned increasingly powerful models. Decision trees can capture non-linear relationships and interactions. Random forests ensemble hundreds of trees to achieve even better accuracy. Each step forward in predictive power has come with a cost: these models are harder to explain.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Understanding Feature Importance: Peeking Inside the Black Box</span>"
    ]
  },
  {
    "objectID": "27-feature-importance.html#from-transparency-to-black-boxes",
    "href": "27-feature-importance.html#from-transparency-to-black-boxes",
    "title": "27  Understanding Feature Importance: Peeking Inside the Black Box",
    "section": "",
    "text": "The Journey from Glass Box to Black Box\nLet’s trace this evolution with a concrete example: predicting customer churn for a telecommunications company.\nLinear Regression (Logistic Regression for classification): The epitome of interpretability. Your model might look like:\n\\[\n\\text{Churn Probability} = \\frac{1}{1 + e^{-(0.8 \\times \\text{MonthlyCharges} - 0.6 \\times \\text{Tenure} + 0.4 \\times \\text{NoTechSupport})}}\n\\]\nYou can explain this to anyone: “Each $10 increase in monthly charges raises churn probability by about 8%. Each additional year of tenure reduces it by 6%.” The entire model fits on a single line.\nDecision Tree: More complex, but still followable. You can trace the decision path:\nCustomer #1247:\n  MonthlyCharges &gt; $70? YES\n    → Tenure &lt; 2 years? YES\n      → No tech support? YES\n        → PREDICTION: Will Churn (85% probability)\nYou can tell this customer’s story. But what happens when your tree has 20 levels and 500 nodes? Following the logic becomes impractical.\nRandom Forest: The black box arrives. Your model doesn’t make one decision path—it makes 100 different decision paths and averages them. Tree #17 might focus on monthly charges and contract type. Tree #58 might prioritize customer service calls and payment method. When you ask “Why did you predict this customer will churn?”, the model essentially responds: “Because 87 out of 100 trees said so, based on 87 different combinations of factors.”\nAccurate? Yes. Explainable? Not easily.\n\n\nVisualizing the Trade-Off\nThis trade-off between accuracy and interpretability is fundamental to machine learning:\n\n\n\n\n\ngraph LR\n    A[\"Linear/Logistic&lt;br/&gt;Regression&lt;br/&gt;&lt;br/&gt;Accuracy: ⭐⭐&lt;br/&gt;Interpretability: ⭐⭐⭐⭐⭐\"] --&gt; B[\"Decision&lt;br/&gt;Tree&lt;br/&gt;&lt;br/&gt;Accuracy: ⭐⭐⭐&lt;br/&gt;Interpretability: ⭐⭐⭐\"]\n    B --&gt; C[\"Random&lt;br/&gt;Forest&lt;br/&gt;&lt;br/&gt;Accuracy: ⭐⭐⭐⭐&lt;br/&gt;Interpretability: ⭐\"]\n    C --&gt; D[\"Deep Neural&lt;br/&gt;Network&lt;br/&gt;&lt;br/&gt;Accuracy: ⭐⭐⭐⭐⭐&lt;br/&gt;Interpretability: ⚫\"]\n\n    style A fill:#90EE90,stroke:#228B22,stroke-width:2px\n    style B fill:#FFD700,stroke:#DAA520,stroke-width:2px\n    style C fill:#FFA07A,stroke:#FF6347,stroke-width:2px\n    style D fill:#FF6347,stroke:#8B0000,stroke-width:2px\n\n    classDef default font-size:11pt\n\n\n The accuracy-interpretability trade-off: as models become more complex and accurate, they become harder to interpret \n\n\n\nThe left side shows simpler models that are easy to understand but may miss complex patterns. The right side shows sophisticated models that capture subtle relationships but resist simple explanation.\n\n\nWhy This Trade-Off Exists\nThe reason is straightforward: real-world relationships are often complex, and capturing complexity requires complex models.\nLinear regression assumes every relationship is a straight line. If you’re predicting house prices, it assumes one extra square foot has the same impact whether you’re going from 800 to 801 square feet (tiny apartment → small apartment) or 4,000 to 4,001 square feet (mansion → slightly bigger mansion). That assumption is probably wrong.\nDecision trees can capture threshold effects: “Below 1,200 square feet, each additional square foot adds $100/sqft. Above 1,200 square feet, it only adds $50/sqft.” Much more realistic.\nRandom forests can capture interactions: “In urban areas, an extra bedroom dramatically increases value, but in rural areas it barely matters.” This is closer to how the world actually works—but now you’re combining hundreds of these complex decision rules.\n\n\nThe Business Dilemma\nThis creates a real challenge: stakeholders want both accuracy AND explanation. Consider this all-too-common conversation:\n\n\n\n\n\n\nTipA Typical Business Meeting\n\n\n\nYou: “Great news! Our new Random Forest model predicts customer churn with 94% accuracy—up from 78% with our old logistic regression.”\nVP of Marketing: “Excellent! So which customers should we focus our retention efforts on?”\nYou: “The model identified 500 high-risk customers. Here’s the list.”\nVP: “Perfect. Now, what’s making them high risk? What should we do differently to retain them?”\nYou: “Well… the model doesn’t exactly tell us that in a straightforward way…”\nVP: “What do you mean? How can it predict churn without knowing what causes churn?”\nYou: “It does know—sort of. The patterns are just… distributed across hundreds of decision trees…”\nVP (skeptically): “So you’re telling me to spend $200,000 on retention campaigns, but you can’t explain why these specific customers are at risk?”\n\n\nThis is the moment where many data scientists realize that accuracy alone isn’t enough. The model’s predictions are only valuable if stakeholders trust them, act on them, and know how to intervene. You need interpretability.\nThis is where feature importance becomes crucial—it lets us peek inside the black box and answer those “why” questions, even when the model itself is complex. We can’t always have perfect interpretability, but we can have enough insight to make informed business decisions and build stakeholder confidence.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Understanding Feature Importance: Peeking Inside the Black Box</span>"
    ]
  },
  {
    "objectID": "27-feature-importance.html#why-feature-importance-matters",
    "href": "27-feature-importance.html#why-feature-importance-matters",
    "title": "27  Understanding Feature Importance: Peeking Inside the Black Box",
    "section": "27.2 Why Feature Importance Matters",
    "text": "27.2 Why Feature Importance Matters\nNow that we understand the interpretability challenge, let’s explore why solving it matters so much. Feature importance isn’t just an academic exercise—it’s often the difference between a model that sits on a shelf and one that drives real business value.\n\nBuilding Trust: “Show Me Why”\nImagine you’ve built a model that predicts which sales leads are most likely to convert. Your sales team is skeptical—they’ve seen “data science magic” before that didn’t pan out. You present your results:\n\n\n\n\n\n\nNoteTwo Ways to Present Your Model\n\n\n\nApproach 1 (without feature importance): &gt; “This model predicts conversion with 89% accuracy. Here are the 200 leads you should prioritize.”\nApproach 2 (with feature importance): &gt; “This model achieves 89% accuracy by focusing on three key factors: leads from companies with 100+ employees (35% importance), leads who’ve visited our pricing page 3+ times (28% importance), and leads who engage with technical documentation (22% importance). Here are the 200 leads that score highest on these criteria.”\n\n\nWhich presentation would you trust more? The second one doesn’t just tell you what to do—it explains why, giving you the context to understand, validate, and act on the recommendation.\nThis applies across industries:\n\nHealthcare: “Your model predicts high readmission risk—but why? Is it age, severity of diagnosis, or lack of follow-up care? The hospital can only intervene on the last one.”\nFinance: “The model denied this loan—on what basis? Regulators require you to explain decisions, especially when they affect people’s lives.”\nE-commerce: “The recommendation engine suggests these products—but are the recommendations based on genuine preferences or just what’s profitable for us?”\n\n\n\nSupporting Better Decisions: From Correlation to Action\nFeature importance helps translate statistical patterns into business strategy. Consider a real-world example:\nYou build a churn prediction model for a subscription service. The model works great—92% accuracy! But what do you actually do with that?\nFeature importance analysis reveals:\n\nContract type (importance: 0.32): Month-to-month contracts churn at 5x the rate of annual contracts\nCustomer service calls (importance: 0.24): Customers with 4+ calls in the last month are at high risk\nAutomatic payment (importance: 0.18): Manual payment customers churn at 2x the rate\nMonthly charges (importance: 0.14): Customers paying over $80/month are at higher risk\nEverything else (importance: 0.12 combined)\n\nNow you can create an action plan:\n\n\n\n\n\n\n\n\nInsight\nBusiness Action\nExpected Impact\n\n\n\n\nMonth-to-month contracts are risky\nOffer incentives to convert to annual plans\nReduce churn in highest-risk segment\n\n\nMultiple service calls signal problems\nProactive outreach after 2nd call\nIntervene before frustration peaks\n\n\nManual payment is a weak signal\nPromote autopay with small discount\nEasy win for moderate-risk customers\n\n\nHigh charges correlate with churn\nReview pricing tiers; offer customization\nMay need product, not just retention, fix\n\n\n\nWithout feature importance, you’d just have a list of at-risk customers with no insight into why they’re at risk or what to do about it.\n\n\nFeature Selection: Simplicity from Complexity\nSometimes feature importance reveals that simpler is better. You might discover that:\n\nOut of 50 features you’re collecting, only 8 drive meaningful predictions\nThe other 42 add noise and complexity without improving accuracy\nCollecting those 42 features costs you time, money, and introduces more opportunities for data quality issues\n\nReal example: A retail company was collecting 73 customer attributes for their purchase prediction model. Feature importance analysis showed that 12 features provided 94% of the predictive signal. They simplified data collection to focus on those 12, reducing database costs, improving data quality (fewer fields to validate), and actually improving model performance by reducing noise.\n\n\nDebugging: When Models Learn the Wrong Lessons\nFeature importance is your early warning system for model problems. Here are real scenarios where feature importance caught critical issues:\nData Leakage Caught Just in Time: A hospital built a readmission prediction model that achieved suspiciously high accuracy—97%. Feature importance revealed that “room_number” was the top predictor. Turns out, the hospital systematically assigned high-risk patients to rooms near the nurses’ station. The model learned to predict readmissions based on room assignment, not actual medical risk factors. Without feature importance, they might have deployed a fundamentally broken model.\nProxy Discrimination Revealed: A hiring model showed that “years_of_experience” and “leadership_roles” were top predictors—both seem reasonable. But deeper analysis revealed these features were proxies for gender in their historical data (due to past discrimination). The model was perpetuating bias. Feature importance flagged the issue; additional fairness analysis confirmed it.\nTemporal Confusion: An e-commerce model to predict purchase likelihood gave high importance to “time_spent_on_checkout_page.” That seems logical until you realize: people only get to the checkout page after deciding to buy. The model was learning from a consequence of the decision, not a cause. Feature importance made this circular logic visible.\n\n\nRegulatory Compliance and Ethical AI\nIn many industries, explainability isn’t optional—it’s legally required.\nFinance: The Fair Credit Reporting Act (FCRA) requires you to provide “adverse action notices” explaining why credit was denied. “The model said so” isn’t acceptable; you need to cite specific factors.\nHealthcare: Models influencing patient care must be explainable to satisfy medical ethics standards and legal liability concerns.\nEmployment: Using AI in hiring decisions increasingly requires transparency about what factors influence decisions.\nEuropean Union: The GDPR includes a “right to explanation” for automated decisions that significantly affect individuals.\nBeyond legal requirements, there’s an ethical imperative: systems that affect people’s lives should be understandable. Feature importance is a key tool for responsible AI development.\n\n\nThe Meta-Lesson: Understanding What Your Model Learned\nPerhaps most importantly, feature importance teaches you about your business. When you see what actually predicts outcomes, you often learn surprising truths:\n\nMarketing teams discover that customer service quality predicts retention better than promotional discounts\nProduct teams learn that simple reliability matters more than flashy features\nOperations teams find that small process delays compound into major customer dissatisfaction\n\nYour model has analyzed thousands or millions of examples. Feature importance helps you extract those lessons in a form you can understand, validate, and act upon. That’s why it matters.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Understanding Feature Importance: Peeking Inside the Black Box</span>"
    ]
  },
  {
    "objectID": "27-feature-importance.html#types-of-feature-importance",
    "href": "27-feature-importance.html#types-of-feature-importance",
    "title": "27  Understanding Feature Importance: Peeking Inside the Black Box",
    "section": "27.3 Types of Feature Importance",
    "text": "27.3 Types of Feature Importance\nBefore diving into specific techniques, it helps to understand that “feature importance” isn’t a single concept—there are fundamentally different ways to measure how much a feature matters.\n\nModel-Based vs. Model-Agnostic\nThe most important distinction is whether importance is tied to a specific model type:\nModel-Based Importance: Derived from the model’s internal structure. For example, how much each feature reduces impurity in a random forest’s decision trees. These methods are:\n\nFast to compute (already part of model training)\nSpecific to certain model types\nSometimes biased toward particular feature types\n\nModel-Agnostic Importance: Measures importance by observing how predictions change when features are altered. These methods:\n\nWork with any model type (linear, tree, neural network, etc.)\nGenerally more reliable\nRequire extra computation\n\n\n\nGlobal vs. Local Importance\nAnother key distinction is the scope of importance:\nGlobal Importance: How important is this feature across all predictions? “Income is the #1 driver of loan default in our model.”\nLocal Importance: How important is this feature for a specific prediction? “For this particular customer, high credit utilization was the main reason for loan denial.”\n\n\nComparison of Common Methods\n\n\n\n\n\n\n\n\n\nType\nExample Methods\nModel Dependence\nDescription\n\n\n\n\nModel-based\nGini importance (Random Forest)\nTree-based only\nMeasures how much each feature reduces impurity across all tree splits\n\n\nModel-based\nLinear coefficients\nLinear models only\nThe weight/coefficient for each feature in the equation\n\n\nModel-agnostic\nPermutation importance\nAny model\nMeasures performance drop when feature values are randomly shuffled\n\n\nModel-agnostic\nSHAP values\nAny model\nGame-theory approach to attribute prediction to each feature\n\n\nModel-agnostic\nLIME\nAny model\nApproximates model locally with interpretable surrogate\n\n\n\nIn this chapter, we’ll focus on the two most practical techniques for your current skill level: Gini importance (model-based, for random forests) and permutation importance (model-agnostic). These give you powerful tools without requiring advanced mathematics.\nLater, we’ll briefly introduce partial dependence plots, which help you understand not just which features matter, but how they influence predictions.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Understanding Feature Importance: Peeking Inside the Black Box</span>"
    ]
  },
  {
    "objectID": "27-feature-importance.html#feature-importance-in-random-forests",
    "href": "27-feature-importance.html#feature-importance-in-random-forests",
    "title": "27  Understanding Feature Importance: Peeking Inside the Black Box",
    "section": "27.4 Feature Importance in Random Forests",
    "text": "27.4 Feature Importance in Random Forests\nLet’s start with the technique you’ll use most often with tree-based models: impurity-based feature importance (also called mean decrease in impurity).\n\nBuilding on What You Already Know\nRemember when we covered decision trees? Each split in a tree is chosen to maximize improvement—reducing Gini impurity for classification or MSE for regression. The tree algorithm evaluates every possible split and picks the one that gives the biggest reduction.\nFeature importance simply aggregates this information: it tracks which features created the best splits across all the trees in your forest.\nHere’s the intuition: if MonthlyCharges is frequently chosen for splits that substantially reduce Gini impurity (in classification) or MSE (in regression), then MonthlyCharges gets a high importance score. If CustomerID rarely gets used for splits (because it doesn’t help separate classes or reduce variance), it gets a low importance score.\n\n\nHow It Works\nThe calculation is straightforward:\n\nTrack improvement: For every split in every tree, record how much it reduced Gini (for classification) or MSE (for regression)\nCredit the feature: Give that reduction amount to whichever feature was used for the split\nWeight by node size: Splits near the top of trees affect more observations, so they count more\nSum across all trees: Add up all the reductions each feature achieved across the entire forest\nNormalize: Scale the scores so they sum to 1.0\n\n\n\n\n\n\n\nNoneExample\n\n\n\nImagine a random forest with 100 trees predicting customer churn:\n\nMonthlyCharges is used 87 times across all trees, with an average Gini reduction of 0.08 per split\nTenure is used 72 times, with an average Gini reduction of 0.06 per split\nCustomerID is used 3 times, with an average Gini reduction of 0.01 per split\n\nWhen you sum these up and normalize, MonthlyCharges gets the highest importance score, Tenure is second, and CustomerID is essentially zero.\n\n\nThe same logic applies to regression—you’re just measuring MSE reduction instead of Gini reduction. The mechanics are identical.\n\n\nExtracting Importance from Scikit-Learn\nWhether you’re using RandomForestClassifier or RandomForestRegressor, scikit-learn automatically calculates feature importance during training. Let’s see this in action with the Default dataset, where we’ll predict whether credit card customers default on their payments.\n\n\nShow data preparation code\n# Load and prepare the Default dataset\nimport pandas as pd\nimport numpy as np\nfrom ISLP import load_data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load data\nDefault = load_data('Default')\n\n# Convert 'default' and 'student' to binary (Yes=1, No=0)\nDefault['default'] = (Default['default'] == 'Yes').astype(int)\nDefault['student'] = (Default['student'] == 'Yes').astype(float)  # float for PDP compatibility\n\n# Prepare features and target\nX = Default[['student', 'balance', 'income']].astype(float)  # Ensure all features are float\ny = Default['default']\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n\nNow that we have are data ready let’s train our model and extract the feature importance:\n\n# Train random forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Extract feature importance\nimportance_scores = rf_model.feature_importances_\n\n# Create a DataFrame for easy viewing\nimportance_df = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': importance_scores\n}).sort_values('importance', ascending=False)\n\nprint(importance_df)\n\n   feature  importance\n1  balance    0.666104\n2   income    0.329728\n0  student    0.004168\n\n\nThe .feature_importances_ attribute gives you normalized scores that sum to 1.0. Here we can see that balance (the customer’s credit card balance) is by far the most important predictor of default, followed by income, with student status contributing very little.\n\n\nVisualizing Importance\nBar charts are the most common way you’ll see feature importance communicated. They make it easy to quickly identify which features matter most and compare their relative importance.\n\n# Create horizontal bar chart (sorted so largest is at top)\nimportance_df.sort_values('importance').set_index('feature')['importance'].plot(\n    kind='barh',\n    figsize=(5, 3),\n    color='steelblue',\n    title='Feature Importance: Credit Card Default'\n)\n\n\n\n\n\n\n\nFigure 27.1: Feature importance for predicting credit card default\n\n\n\n\n\n\n\nInterpreting the Results\nFeature importance scores are more than just numbers—they’re a window into what your model learned and whether it’s learning the right things. When examining importance scores, ask yourself these critical questions:\n\n\n\n\n\n\nNoneDo the important features make sense?\n\n\n\n\n\nIn our Default dataset example, seeing balance as the most important feature makes perfect business sense—customers with high outstanding balances are naturally more likely to default. This alignment between statistical importance and domain knowledge is reassuring.\nHowever, if you saw features like customer_id, row_number, or random_noise_variable ranking high, that’s a major red flag. These features shouldn’t be predictive, and their importance suggests problems like data leakage, overfitting, or preprocessing errors.\n\n\n\n\n\n\n\n\n\nNoneIs importance concentrated or distributed?\n\n\n\n\n\nOur Default model shows high concentration—balance dominates with roughly 80% of total importance. This tells us the model relies heavily on a single variable, which could mean:\n\nThat feature is genuinely powerful (likely true for balance in credit default prediction)\nOther features are weak predictors or redundant\nYou might be able to simplify the model further\n\nConversely, if importance is evenly distributed across many features (e.g., 20 features each with 5% importance), it suggests the prediction task requires combining many weak signals rather than relying on a few strong ones.\n\n\n\n\n\n\n\n\n\nNoneAre correlated features splitting importance?\n\n\n\n\n\nWhen features are correlated, importance gets diluted across them. For example, if you had both income_annual and income_monthly in the model, they’d likely both show moderate importance even though they’re measuring essentially the same underlying concept. This doesn’t mean they’re not important—just that the total importance of “income” as a concept is being split between two related variables.\n\n\n\n\n\n\n\n\n\nWarningWhen Something Seems Off\n\n\n\nIf your feature importance results seem strange, counterintuitive, or raise red flags (like ID variables being important), don’t ignore it. These are often symptoms of deeper problems:\n\nRevisit your features: Are you including variables that shouldn’t be available at prediction time? Are there data quality issues?\nCheck your feature engineering: Did scaling, encoding, or transformations introduce unintended artifacts?\nQuestion the model choice: Is a random forest appropriate for this problem, or would a simpler model be better?\n\nFeature importance is a diagnostic tool—use it to validate that your model is learning what you intended it to learn.\n\n\n\n\nLimitations of Impurity-Based Importance\nImpurity-based importance (whether using Gini, Entropy, or MSE) is fast and convenient, but has known biases:\n\nFavors high-cardinality features: Features with many unique values (like continuous variables) tend to rank higher than low-cardinality features (like binary variables), even when the binary feature is more predictive. A continuous variable like age (with 60+ unique values) often outranks a binary feature like has_warranty simply because it has more possible split points.\nBiased with correlated features: When features are correlated, importance gets distributed among them unpredictably. For example, annual_income and monthly_income might both show moderate importance, even though they’re measuring essentially the same thing.\nTraining-set specific: Calculated on training data, so it reflects what helped during training, not necessarily what generalizes. A feature might rank highly because it helped the model overfit to training data quirks.\nNo statistical significance: You get importance scores, but no indication of whether they’re meaningfully different from random chance, especially with small datasets.\n\nThese limitations apply equally to classification (Gini/Entropy) and regression (MSE reduction) forests. For these reasons, it’s wise to also check permutation importance, which we’ll cover next.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Understanding Feature Importance: Peeking Inside the Black Box</span>"
    ]
  },
  {
    "objectID": "27-feature-importance.html#permutation-importance-a-more-reliable-alternative",
    "href": "27-feature-importance.html#permutation-importance-a-more-reliable-alternative",
    "title": "27  Understanding Feature Importance: Peeking Inside the Black Box",
    "section": "27.5 Permutation Importance: A More Reliable Alternative",
    "text": "27.5 Permutation Importance: A More Reliable Alternative\nWhile impurity-based importance is fast and convenient, it has biases we discussed earlier—particularly favoring high-cardinality features and struggling with correlated variables. More importantly, it only works with tree-based models. What if you want to understand feature importance for a logistic regression, a neural network, or any other model type?\nThis is where permutation importance shines. It’s a model-agnostic approach that works with any model and directly measures how much each feature contributes to actual prediction performance.\n\nThe Core Idea: Break It and See What Happens\nThe logic behind permutation importance is beautifully simple: if a feature is important, breaking its relationship with the target should hurt model performance.\nHere’s the intuition: imagine you’ve built a model to predict credit card default using balance, income, and student status. If balance is truly important, what happens if you randomly scramble the balance values while keeping everything else the same? The model’s predictions should get much worse—customers who actually have low balances might now be assigned high balance values, and vice versa.\nBut if student status doesn’t actually help predictions, shuffling it should barely affect performance. The model wasn’t really using that information anyway.\nThis is exactly what permutation importance measures: the drop in performance when you shuffle each feature.\n\n\nHow It Works\nThe algorithm is straightforward:\n\nCalculate baseline performance: Evaluate your model on the test set with real, unshuffled data (e.g., accuracy = 0.92)\nFor each feature, one at a time:\n\nRandomly shuffle that feature’s values, breaking its relationship with the target\nRe-calculate performance with the shuffled feature (e.g., accuracy drops to 0.78)\nRecord the performance drop (0.92 - 0.78 = 0.14 importance for this feature)\nRestore the original values before moving to the next feature\n\nRepeat the shuffling multiple times (usually 10-30 repetitions) to get stable estimates, since random shuffling introduces variability\nReport the average performance drop as the feature’s importance\n\n\n\n\n\n\n\nAlways use a held-out test set, not training data. You want to measure what matters for generalization, not what the model memorized during training.\n\n\n\n\n\nImplementation with Scikit-Learn\nScikit-learn makes calculating permutation importance easy with the permutation_importance function. Let’s apply it to our Default dataset model:\n\nfrom sklearn.inspection import permutation_importance\n\n# Calculate permutation importance on test set\nperm_importance = permutation_importance(\n    rf_model,           # Our trained random forest\n    X_test,            # Test features (held-out data)\n    y_test,            # Test labels\n    n_repeats=10,      # Shuffle each feature 10 times\n    random_state=42,\n    scoring='accuracy' # For classification; use 'r2' for regression\n)\n\n# Extract results into a clean DataFrame\nperm_df = pd.DataFrame({\n    'feature': X_test.columns,\n    'importance_mean': perm_importance.importances_mean,\n    'importance_std': perm_importance.importances_std\n}).sort_values('importance_mean', ascending=False)\n\nprint(perm_df)\n\n   feature  importance_mean  importance_std\n1  balance         0.023967        0.001224\n0  student         0.003133        0.001565\n2   income        -0.000900        0.001461\n\n\nThe importances_mean column shows the average performance drop across the 10 shuffles, and importances_std tells you how variable that estimate is. A high standard deviation suggests the importance is unstable—possibly because the feature interacts with others in complex ways.\n\n\nComparing Impurity-Based vs. Permutation Importance\nOne of the most valuable practices is comparing both methods side-by-side. They often agree on the most important features, but discrepancies can reveal important insights.\n\n# Merge both importance measures for comparison\ncomparison = pd.merge(\n    importance_df.rename(columns={'importance': 'impurity_based'}),\n    perm_df[['feature', 'importance_mean']].rename(columns={'importance_mean': 'permutation'}),\n    on='feature'\n)\n\nprint(comparison)\n\n   feature  impurity_based  permutation\n0  balance        0.666104     0.023967\n1   income        0.329728    -0.000900\n2  student        0.004168     0.003133\n\n\nWhat to look for:\n\nRankings: Both methods rank balance as the most important feature, which is reassuring. However, notice the magnitude difference—impurity-based gives it 0.67 importance while permutation gives only 0.024.\nIncome discrepancy: income shows moderate impurity-based importance (0.33) but essentially zero permutation importance (-0.0009, effectively zero). This is a significant disagreement! It suggests that while the trees use income for splits during training, shuffling it on the test set doesn’t actually hurt predictions much.\nStudent status: Both methods agree this feature has minimal importance (0.004 vs 0.003).\n\nWhat explains these discrepancies?\nThis is a textbook example of the high-cardinality bias we discussed earlier. Both balance and income are continuous variables with many unique values, while student is binary (only two values). Impurity-based importance naturally favors continuous features because they offer more split points.\nPermutation importance tells a different story: balance genuinely helps test-set predictions (0.024 drop when shuffled), but income doesn’t (-0.0009 actually suggests a tiny improvement when shuffled, though this is likely just noise). This suggests the trees might be using income in complex ways that don’t generalize well, or it’s redundant given balance.\n\n\n\n\n\n\nImportantThe Takeaway\n\n\n\nWhen methods disagree this much, trust permutation importance more—especially when the discrepancy involves continuous vs. categorical features. For this problem, balance is clearly the dominant signal.\n\n\n\n\nWhen to Use Each Method\nBoth approaches have their place. Here’s practical guidance on when to use each:\nUse Permutation Importance when:\n\nWorking with any model type: Logistic regression, neural networks, gradient boosting—permutation importance works with all of them\nYou need reliable rankings: Especially important when features are correlated or have different cardinalities\nThe stakes are high: When decisions based on importance have significant business or ethical implications, use the more robust method\nYou’re validating results: Always good to check permutation importance to confirm impurity-based rankings\n\nUse Impurity-Based Importance when:\n\nYou need quick feedback: It’s calculated during training at essentially no extra cost—perfect for rapid iteration\nYou’re doing initial exploration: Great for quickly identifying obviously important features before deeper analysis\nComputational resources are limited: Permutation importance requires many additional predictions, which can be slow with large datasets or complex models\nYou’re specifically analyzing tree behavior: Understanding which features the trees actually split on, not just which features help predictions\n\n\n\n\n\n\n\nImportantBest Practice\n\n\n\nUse impurity-based importance for quick checks during model development, then validate your conclusions with permutation importance before making final decisions or reporting to stakeholders.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Understanding Feature Importance: Peeking Inside the Black Box</span>"
    ]
  },
  {
    "objectID": "27-feature-importance.html#exploring-prediction-behavior-across-variables",
    "href": "27-feature-importance.html#exploring-prediction-behavior-across-variables",
    "title": "27  Understanding Feature Importance: Peeking Inside the Black Box",
    "section": "27.6 Exploring Prediction Behavior Across Variables",
    "text": "27.6 Exploring Prediction Behavior Across Variables\nFeature importance tells you which features matter, but not how they influence predictions. Does increasing income linearly increase loan approval probability? Is there a threshold effect? This is where partial dependence plots (PDPs) become valuable.\n\nWhat Partial Dependence Plots Show\nA PDP displays the average predicted outcome as you vary one feature while holding all others at their observed values. It answers: “On average, how does the prediction change as this feature changes?”\nFor example, a PDP for credit_score in a loan model might show:\n\nBelow 600: ~80% predicted default rate\n600-700: Default rate drops steadily to ~40%\nAbove 700: Default rate levels off around ~15%\n\nThis reveals not just that credit score matters (importance already told you that), but exactly how it matters.\n\n\nCreating Partial Dependence Plots\nScikit-learn’s PartialDependenceDisplay makes it easy to create PDPs for any trained model. Let’s visualize how balance affects default probability in our model:\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\n# Create PDP for balance (our most important feature)\nPartialDependenceDisplay.from_estimator(\n    rf_model,\n    X_train,\n    features=['balance'],\n    grid_resolution=50\n)\n\n\n\n\n\n\n\nFigure 27.2: Partial dependence plot showing how balance affects default probability\n\n\n\n\n\nThis plot reveals a clear threshold effect: default probability stays near zero for balances below ~$1,200, then rises steeply as balance increases. Notice the vertical lines at the bottom (called a “rug plot”)—these show where actual customer balances fall in the dataset. Most customers have balances below $1,500, which is why the plot gets noisier above that point (fewer data to average over).\nWe can also create PDPs for multiple features at once to compare their effects:\n\nimport matplotlib.pyplot as plt\n\n# Create figure with custom size (width, height in inches)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\n\n# Create PDPs for all features\nPartialDependenceDisplay.from_estimator(\n    rf_model,\n    X_train,\n    features=['balance', 'income', 'student'],\n    grid_resolution=50,\n    ax=ax\n)\nplt.tight_layout()\n\n\n\n\n\n\n\nFigure 27.3: Partial dependence plots for all three features in the Default model\n\n\n\n\n\n\n\nInterpreting PDPs\nWhen reading partial dependence plots, look for these common patterns. Let’s use our Default dataset examples to illustrate:\nThreshold effects (balance plot): The balance PDP is a textbook example—default probability remains essentially flat at near-zero for balances below $1,200, then climbs steeply. This tells you there’s a critical balance threshold where risk accelerates. From a business perspective, this suggests focusing credit monitoring and intervention efforts on customers approaching or exceeding $1,200 in balance.\nData density matters: Notice the rug plot (vertical lines at the bottom of each chart)—these show where actual observations fall. The balance plot gets jagged above $1,500 because fewer customers have such high balances, making the average less stable. Always check data density; interpretations are less reliable in sparse regions.\nThreshold with noise (income plot): The income PDP reveals an interesting pattern—default probability stays low (around 2.5-5%) for incomes below ~$30,000, then jumps sharply to around 10-12.5% as income exceeds $30,000-$35,000. This is counterintuitive: higher income associated with higher default risk? This apparent paradox highlights an important lesson about correlation vs. causation and feature interactions. The pattern likely reflects that, conditional on having accumulated high credit card balances, higher-income customers are slightly more likely to default—perhaps because they’re more likely to have multiple credit lines or higher total debt. However, permutation importance was near-zero for income because this relationship is weak and doesn’t improve predictions much once you already know the balance. The noisiness of the plot also suggests limited data in higher income ranges (note the sparse rug plot).\nLinear relationships (student plot): The student PDP shows a diagonal line from non-student (0) to student (1), with default probability rising from about 4% to 15%. This is interesting—it suggests being a student is associated with higher default risk. However, remember that permutation importance rated this feature as nearly worthless (~0.003). How can both be true? This is likely because the effect is real but small relative to balance, and most predictions are dominated by balance regardless of student status. When balance is very high or very low, student status doesn’t change the overall prediction much.\nNon-monotonic patterns: Sometimes PDPs go up then down, or show more complex curves (e.g., moderate engagement optimal, very high suggests spam behavior). We don’t see this in the Default data, but watch for these in more complex business problems—they often reveal interesting threshold effects or saturation points.\n\n\n\n\n\n\nWarningImportant Limitations\n\n\n\nPDPs assume feature independence, which can be misleading with correlated features. For example:\n\nIf age and years_employed are correlated, showing “age = 25 with years_employed = 30” doesn’t make sense\nThe PDP averages over impossible combinations\n\nDespite this, PDPs remain one of the most practical interpretability tools for understanding feature effects.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Understanding Feature Importance: Peeking Inside the Black Box</span>"
    ]
  },
  {
    "objectID": "27-feature-importance.html#limitations-and-pitfalls",
    "href": "27-feature-importance.html#limitations-and-pitfalls",
    "title": "27  Understanding Feature Importance: Peeking Inside the Black Box",
    "section": "27.7 Limitations and Pitfalls",
    "text": "27.7 Limitations and Pitfalls\nThroughout this chapter, we’ve seen how powerful feature importance can be for understanding models. But like any analytical tool, it has limitations. Being aware of these pitfalls helps you use feature importance responsibly and interpret results correctly.\n\nWhen Feature Importance Can Mislead You\nFeature importance is powerful, but it’s not foolproof. Here are the most common ways it can lead you astray. Click on each to learn more about the problem and what to do about it:\n\n\n\n\n\n\nWarningCorrelated Features Dilute Importance\n\n\n\n\n\nWe saw this clearly in our Default dataset: balance and income are likely correlated (people with high balances may have different income profiles), and impurity-based importance gave income a moderate score (0.33) even though permutation importance showed it barely helps predictions.\nThe problem: When features are correlated, importance gets split among them in unpredictable ways. If you had monthly_sales, quarterly_sales, and annual_sales all in your model, none might rank highly individually even though sales is the key driver.\nWhat to do:\n\nRemove obvious redundancies before modeling\nGroup correlated features conceptually when interpreting (“sales-related features are important”)\nLook for cases where permutation importance is much lower than impurity-based importance—often a sign of correlation issues\n\n\n\n\n\n\n\n\n\n\nWarningThe Cardinality Trap\n\n\n\n\n\nOur Default dataset demonstrated this perfectly: continuous features (balance with 1000+ unique values, income with 500+ values) dominated impurity-based importance, while the binary student feature barely registered—even though the PDP showed it had a real effect.\nThe problem: Tree-based importance inherently favors features with many unique values because they offer more possible split points, not because they’re more predictive.\nWhat to do:\n\nAlways compare with permutation importance\nDon’t dismiss low-cardinality features based solely on impurity-based scores\nBe extra skeptical when continuous features dominate and categorical features vanish\n\n\n\n\n\n\n\n\n\n\nWarningImportance Doesn’t Equal Causation\n\n\n\n\n\nThis is perhaps the most dangerous assumption. Consider this scenario: a model predicts hospital readmissions and finds that “number_of_medications” is highly important. You might conclude: “Let’s reduce prescriptions to prevent readmissions!” But the truth is likely reversed—sicker patients need more medications AND are more likely to be readmitted. The medications aren’t causing readmissions; they’re a marker of severity.\nClassic example: Ice cream sales are “important” for predicting drowning deaths. But ice cream doesn’t cause drownings—both are driven by hot summer weather.\nWhat to do:\n\nNever assume important features are causal without additional evidence\nUse domain expertise to distinguish correlation from causation\nConsider whether the relationship makes logical sense or if there’s a lurking variable\n\n\n\n\n\n\n\n\n\n\nWarningTraining Set Overfitting\n\n\n\n\n\nImpurity-based importance is calculated entirely on training data. If your model overfits, it might split on noise patterns that don’t generalize, making random features appear important.\nExample: Your model might discover that customers whose names start with ‘Q’ happened to churn more often in the training set (pure chance with a small sample). Impurity-based importance would credit this, but permutation importance on test data would reveal it’s meaningless.\nWhat to do:\n\nTrust permutation importance on test data more than training-based measures\nIf a feature has high training importance but low test importance, it’s likely overfit\n\n\n\n\n\n\n\n\n\n\nWarningSmall Data, Big Noise\n\n\n\n\n\nWith limited data, importance scores become unstable. Retrain on a slightly different sample, and feature rankings might shuffle dramatically.\nWhat to do:\n\nUse cross-validation to check importance stability across different data splits\nReport confidence intervals (permutation importance provides importances_std for this)\nBe humble about conclusions when working with small datasets\n\n\n\n\n\n\nPractical Guidepos for Trustworthy Interpretation\nRather than a rigid checklist, think of these as questions to ask yourself:\n\n\n\n\n\n\nTipDid I use multiple importance methods?\n\n\n\n\n\nComparing impurity-based and permutation importance catches many issues. When they agree strongly (like both saying balance is dominant), you can be confident. When they disagree (like with income in our Default example), investigate why.\n\n\n\n\n\n\n\n\n\nTipAm I calculating importance on the right data?\n\n\n\n\n\nFor permutation importance, always use held-out test data. You want to know what matters for generalization, not what the model memorized.\n\n\n\n\n\n\n\n\n\nTipDo the important features pass the “common sense” test?\n\n\n\n\n\nIf customer_id is important, something’s wrong. If balance is important for credit default, that makes sense. Trust your domain knowledge.\n\n\n\n\n\n\n\n\n\nTipAm I confusing “important” with “actionable”?\n\n\n\n\n\nA feature can be important for predictions but impossible to change (like age). Meanwhile, a less important feature might be highly actionable (like email response time). Importance tells you what drives predictions; business strategy determines what to do about it.\n\n\n\n\n\n\n\n\n\nTipDid I check for data leakage?\n\n\n\n\n\nIf “days_until_churn” is important in a churn prediction model, you’re leaking the future into your features. If “room_number” predicts readmissions, investigate why before trusting it.\n\n\n\n\n\n\n\n\n\nWarningRemember: Description, Not Prescription\n\n\n\nFeature importance methods are descriptive—they tell you what patterns the model found in the data. They are not prescriptive—they don’t tell you what those patterns mean, whether they’re causal, or what actions to take.\nA high importance score means “the model uses this feature heavily in its predictions,” not “changing this feature will change outcomes” or “this feature causes the target variable.”\nAlways pair importance analysis with critical thinking, domain expertise, and—when stakes are high—causal inference methods.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Understanding Feature Importance: Peeking Inside the Black Box</span>"
    ]
  },
  {
    "objectID": "27-feature-importance.html#beyond-the-basics-toward-explainable-ai",
    "href": "27-feature-importance.html#beyond-the-basics-toward-explainable-ai",
    "title": "27  Understanding Feature Importance: Peeking Inside the Black Box",
    "section": "27.8 Beyond the Basics: Toward Explainable AI",
    "text": "27.8 Beyond the Basics: Toward Explainable AI\nThe techniques in this chapter—Gini importance, permutation importance, and partial dependence plots—give you a solid foundation for understanding model behavior. But the field of explainable AI (XAI) has developed more sophisticated methods that you’ll likely encounter as you advance in your data science career. While these are beyond the scope of this introductory course, it’s valuable to know they exist and when you might need them.\n\n\n\n\n\n\nNoteSHAP (SHapley Additive exPlanations)\n\n\n\n\n\nWhat it does: Uses game theory to calculate how much each feature contributed to moving a prediction away from the average prediction, providing both global and local explanations.\nLearn more: SHAP documentation\nPython package: shap (GitHub)\nWhen to use: When you need mathematically rigorous feature attributions that are consistent across all predictions, especially for explaining individual high-stakes decisions.\n\n\n\n\n\n\n\n\n\nNoteLIME (Local Interpretable Model-Agnostic Explanations)\n\n\n\n\n\nWhat it does: Explains individual predictions by fitting a simple, interpretable model (like linear regression) locally around that specific prediction.\nLearn more: LIME documentation\nPython package: lime (GitHub)\nWhen to use: When you need to explain specific predictions in a way that’s intuitive to non-technical stakeholders, working with any model type.\n\n\n\n\n\n\n\n\n\nNoteCounterfactual Explanations\n\n\n\n\n\nWhat it does: Identifies the minimal changes to input features that would flip a prediction to a different outcome (e.g., “If your income were $5,000 higher, the loan would be approved”).\nLearn more: DiCE (Diverse Counterfactual Explanations)\nPython package: dice-ml (Documentation)\nWhen to use: When stakeholders need actionable insights about what would change a prediction, particularly valuable in lending, hiring, and healthcare contexts.\n\n\n\n\n\n\n\n\n\nNoteAnchors\n\n\n\n\n\nWhat it does: Finds sufficient conditions (rules) that “anchor” a prediction—features that, when present, guarantee the prediction regardless of other feature values.\nLearn more: Anchors: High-Precision Model-Agnostic Explanations\nPython package: Part of alibi package (Documentation)\nWhen to use: When you need to identify robust rules that ensure certain predictions, useful for validating model behavior in critical applications.\n\n\n\nThe core principle remains the same: powerful models are most useful when we can explain their behavior. As you advance in your career and encounter high-stakes applications, these advanced methods become essential tools for responsible AI development.",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Understanding Feature Importance: Peeking Inside the Black Box</span>"
    ]
  },
  {
    "objectID": "27-feature-importance.html#summary-and-reflection",
    "href": "27-feature-importance.html#summary-and-reflection",
    "title": "27  Understanding Feature Importance: Peeking Inside the Black Box",
    "section": "27.9 Summary and Reflection",
    "text": "27.9 Summary and Reflection\nThis chapter has equipped you with practical tools for understanding what drives your machine learning models’ predictions. Here’s what you’ve learned:\nThe Core Challenge: As you’ve progressed from linear regression through decision trees to random forests, your models have become more accurate but less interpretable. A random forest combining hundreds of trees resists simple explanation—but stakeholders still need to understand why predictions are being made.\nWhy Feature Importance Matters: Beyond satisfying curiosity, feature importance serves critical purposes:\n\nBuilding trust with stakeholders by explaining model decisions\nSupporting action by translating predictions into business strategies\nDebugging models to catch data leakage, bias, and logical errors\nMeeting regulatory requirements in finance, healthcare, and other high-stakes domains\nLearning about your business by discovering what actually drives outcomes\n\nTwo Complementary Approaches:\n\nImpurity-Based Importance (Gini/MSE): Built into random forests, measuring how much each feature reduces Gini impurity (classification) or MSE (regression) across tree splits. Fast and convenient, but biased toward high-cardinality features and calculated on training data.\nPermutation Importance: Model-agnostic method that measures performance drop when shuffling each feature on test data. More reliable and works with any model type, though computationally more expensive.\n\nBest Practice: Use impurity-based importance for quick iteration during development, then validate with permutation importance before final decisions.\nUnderstanding How Features Work: Partial dependence plots (PDPs) go beyond which features matter to show how they influence predictions—revealing threshold effects, linear relationships, and non-monotonic patterns. Through the Default dataset, you saw how balance exhibits a clear threshold at ~$1,200, while income showed counterintuitive patterns that required careful interpretation.\nCritical Pitfalls to Avoid:\n\nCorrelated features dilute importance unpredictably\nHigh-cardinality bias makes continuous features dominate tree-based importance\nImportance ≠ causation—correlation doesn’t imply you can change outcomes by manipulating features\nTraining set overfitting can make random noise appear important\nSmall datasets produce unstable importance scores\n\nThe Path Forward: While this chapter focused on practical methods (Gini, permutation, PDPs), advanced techniques like SHAP, LIME, and counterfactual explanations offer even deeper insights for high-stakes applications. The fundamental principle remains constant: models are most valuable when we can explain their behavior.\n\n\n\n\n\n\nTipReflection Question\n\n\n\nThink about a prediction model you might build in your field. Your model achieves 92% accuracy, and your manager asks: “Which variables most influenced your model’s predictions—and why?”\nHow would you answer using the techniques from this chapter? What combination of methods would you use, and how would you communicate the results to a non-technical stakeholder?",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Understanding Feature Importance: Peeking Inside the Black Box</span>"
    ]
  },
  {
    "objectID": "27-feature-importance.html#knowledge-check",
    "href": "27-feature-importance.html#knowledge-check",
    "title": "27  Understanding Feature Importance: Peeking Inside the Black Box",
    "section": "27.10 Knowledge Check",
    "text": "27.10 Knowledge Check\n\n\n\n\n\n\nNoteYour Turn: Comprehensive Feature Importance Analysis\n\n\n\nApply everything you’ve learned to analyze a real dataset.\nYour Tasks:\n\nData Selection: Choose a classification or regression dataset (Ames housing, customer churn, or one of your choice)\nModel Training: Train a Random Forest model with appropriate train/test split\nMultiple Importance Methods:\n\nExtract and visualize Gini importance\nCalculate and visualize permutation importance on test data\nCompare the two methods—do they agree? Where do they disagree?\n\nPartial Dependence:\n\nCreate PDPs for the top 3-4 most important features\nDescribe the relationships you observe (linear, threshold, diminishing returns, etc.)\n\nCritical Analysis:\n\nIdentify any suspicious patterns (data leakage, spurious correlations, etc.)\nCheck for correlated features that might be splitting importance\nValidate that important features make domain sense\n\nBusiness Communication:\n\nWrite a one-page summary for a non-technical stakeholder explaining:\n\nWhich 3-5 features drive predictions most strongly\nHow these features influence the outcome (using PDP insights)\nWhat actions the business might take based on these insights\nAny caveats or limitations in the interpretation\n\n\n\nBonus Challenge: Compare feature importance from your Random Forest with coefficients from a Logistic Regression (for classification) or Linear Regression (for regression) on the same data. What do the differences tell you about the patterns each model learned?",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Understanding Feature Importance: Peeking Inside the Black Box</span>"
    ]
  },
  {
    "objectID": "27-feature-importance.html#end-of-chapter-exercises",
    "href": "27-feature-importance.html#end-of-chapter-exercises",
    "title": "27  Understanding Feature Importance: Peeking Inside the Black Box",
    "section": "27.11 End of Chapter Exercises",
    "text": "27.11 End of Chapter Exercises\nThese exercises revisit the random forest models you built in Chapter 26. Now you’ll apply feature importance techniques to understand which features drive your predictions and how they influence outcomes. This is where the models you built previously become truly actionable!\n\n\n\n\n\n\nNoteExercise 1: Understanding Baseball Salary Predictions\n\n\n\n\n\nIn Chapter 26, you built random forest models to predict baseball player salaries using the Hitters dataset. Now let’s understand what drives those predictions.\nRecall your best model from Chapter 26 Exercise 1 (likely the tuned random forest with features: Years, Hits, RBI, Walks, PutOuts).\nYour Tasks:\n\nExtract and Visualize Impurity-Based Importance:\n\nUse .feature_importances_ from your trained random forest\nCreate a horizontal bar chart showing feature importance\nWhich feature dominates salary predictions?\nDoes this align with baseball domain knowledge?\n\nCalculate Permutation Importance:\n\nUse permutation_importance() on your test set\nCalculate with n_repeats=10 and scoring='r2'\nCreate a comparison table or chart showing both importance types\nDo the two methods agree on the top features? Where do they disagree?\n\nInvestigate Discrepancies:\n\nIf impurity-based and permutation importance disagree, explain why\nConsider: Are some features continuous vs. categorical?\nAre any features correlated (e.g., Hits and RBI might be)?\nWhich importance measure do you trust more for this dataset?\n\nPartial Dependence Analysis:\n\nSelect the top 5 most important features (based on permutation importance)\nCreate partial dependence plots for each feature\nDescribe the relationship between each feature and predicted salary:\n\nIs it linear or non-linear?\nAre there threshold effects? (e.g., “After X years, salary increases accelerate”)\nAny surprising patterns?\n\n\nBusiness Insights:\n\nWrite a brief summary for the team’s management explaining:\n\nWhich 3 factors most strongly predict player salary\nHow each factor influences salary (use PDP insights)\nAny actionable recommendations for salary negotiations\nCaveats: What should they NOT conclude from this analysis?\n\n\n\nBonus Challenge: Compare the random forest’s feature importance to the coefficients from a simple Linear Regression on the same data. What different patterns did each model learn?\n\n\n\n\n\n\n\n\n\nNoteExercise 2: Understanding Credit Default Risk Factors\n\n\n\n\n\nIn Chapter 26, you built random forest classifiers to predict credit card defaults using the Default dataset with features: balance, income, and student_Yes.\nRecall your best model from Chapter 26 Exercise 2 (likely the tuned random forest).\nYour Tasks:\n\nExtract and Visualize Impurity-Based Importance:\n\nUse .feature_importances_ from your trained random forest\nCreate a horizontal bar chart\nWhich feature(s) dominate default predictions?\n\nCalculate Permutation Importance:\n\nUse permutation_importance() on your test set\nUse scoring='accuracy' for classification\nCompare both importance measures in a table or side-by-side chart\nDo they agree? Any surprises?\n\nAnalyze High-Cardinality Bias:\n\nYou have two continuous features (balance, income) and one binary feature (student_Yes)\nDoes the binary feature rank lower in impurity-based importance despite being potentially predictive?\nHow does permutation importance treat it differently?\nWhat does this tell you about the cardinality trap?\n\nPartial Dependence Analysis:\n\nCreate PDPs for all three features\nFor balance: Describe the threshold effect (at what balance does default risk spike?)\nFor income: Is the relationship linear or non-linear? Any counterintuitive patterns?\nFor student_Yes: How much does student status increase default probability?\n\nBusiness Application:\n\nWrite a one-page memo to the bank’s risk committee explaining:\n\nThe #1 predictor of default and how it behaves\nAt what threshold should the bank flag customers for intervention?\nWhether student status matters (and by how much)\nAny surprising findings about income’s role in default prediction\n\n\nCritical Analysis:\n\nDoes importance equal causation? Can the bank reduce defaults by changing these features?\nWhich features are predictive vs. actionable?\nWhat business actions could the bank take based on these insights?\n\n\n\n\n\n\n\n\n\n\n\nNoteExercise 3: Market Direction Prediction Analysis (Optional Challenge)\n\n\n\n\n\nIn Chapter 26, you built random forests to predict weekly stock market direction using lag features (Lag1 through Lag5).\nRecall your models from Chapter 26 Exercise 3.\nYour Tasks:\n\nFeature Importance Comparison:\n\nExtract impurity-based importance from your best random forest\nCalculate permutation importance on the test set\nCreate visualizations comparing both methods\nDo the methods agree on which lag periods matter most?\n\nCorrelation and Importance:\n\nCalculate correlations between the lag features\nAre lag features highly correlated with each other?\nHow does correlation affect importance attribution?\nDoes this explain any discrepancies between importance measures?\n\nPartial Dependence Analysis:\n\nCreate PDPs for the top 3 most important lag features\nWhat patterns do you observe?\nAre the relationships linear or non-linear?\nDo the patterns make financial sense?\n\nReality Check:\n\nEven if some features show high importance, is the overall model accurate?\nCompare your model’s test accuracy to the baseline (always predicting “Up”)\nWhat does this tell you about the difference between “important features” and “predictive model”?\n\nChallenge Questions:\n\nWhy might features show high importance even when the model barely beats the baseline?\nCould the model be finding spurious patterns in the data?\nWhat does this teach you about the limitations of feature importance?\nHow should this inform your decision about deploying this model in production?\n\nMethodological Reflection:\n\nFeature importance tells you what the model learned, not whether it learned something useful\nWhat additional analyses would you need before trusting this model?\nHow does this exercise demonstrate the importance of validating model performance before interpreting feature importance?\n\n\n\n\n\n\n\n\n\n\n\nTipLearning Objectives Revisited\n\n\n\nThrough these exercises, you should have practiced:\n\nExtracting and visualizing both impurity-based and permutation importance\nIdentifying discrepancies between importance measures and understanding why they occur\nCreating and interpreting partial dependence plots to understand feature relationships\nTranslating statistical patterns into business insights and actionable recommendations\nRecognizing the limitations of feature importance (correlation ≠ causation, importance ≠ model quality)\nCommunicating complex model behavior to non-technical stakeholders\n\nRemember: Feature importance is most valuable when the model is actually predictive. Always validate model performance before spending time interpreting what it learned!",
    "crumbs": [
      "Module 10",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Understanding Feature Importance: Peeking Inside the Black Box</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html",
    "href": "99-anaconda-install.html",
    "title": "28  Anaconda Installation",
    "section": "",
    "text": "28.1 Installing Anaconda\nAnaconda Distribution is one of the most popular platforms for doing data science with Python. It’s designed to make it easier for beginners and professionals alike to set up their Python environment with all the essential tools for analysis, visualization, and machine learning.\nWhen you install Anaconda, you get much more than just Python, you get:\nHere’s why Anaconda is a great option for beginners:",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#installing-anaconda",
    "href": "99-anaconda-install.html#installing-anaconda",
    "title": "28  Anaconda Installation",
    "section": "",
    "text": "CautionWhat about Miniconda?\n\n\n\n\n\nYou may hear about a tool called Miniconda, which is a lightweight alternative to Anaconda. While Miniconda gives you more control by starting with a barebones Python environment, it requires you to install everything manually. In this course, we recommend Anaconda because it comes preloaded with everything you need for data science—and it’s much easier to get started with. If you’re new to Python, stick with Anaconda.\n\n\n\n\nBefore You Begin\nYou’ll need:\n\nA stable internet connection\nMake sure your operating system is current enough for the Anaconda install. See requirements here.\n~3–4 GB of free disk space\nA bit of patience—it’s a large install!\n\n\n\nFor Windows Users\n\nGo to https://www.anaconda.com/products/distribution\nClick “Download” and choose the Windows installer (64-bit) \nOnce downloaded, open the installer\nFollow the prompts:\n\nChoose “Just Me” (recommended)\nLeave default install location\nIMPORTANT: When asked about adding Anaconda to your PATH: Keep it unchecked (recommended)\n\nClick Install and wait (~5–10 mins)\nAfter installation, click “Finish”\nVerify your installation: The Anaconda Navigator should automatically open after successful installation of Anaconda Distribution. If it does not, you should be able to search for “Anaconda Navigator” in the taskbar search.\n\nFor more installation instructions you can find the Anaconda documentation for Windows installation here. You can also view the following video:\n\n\n\nFor Mac Users\n\nGo to https://www.anaconda.com/products/distribution\nDownload the macOS (Intel or M1/M2) installer that matches your hardware \nOpen the .pkg file and follow the installation instructions\n\nGrant permission when prompted\nLeave default settings\n\nVerify your installation: The Anaconda Navigator should automatically open after successful installation of Anaconda Distribution. If it does not, you should be able to search for “Anaconda Navigator” using Spotlight (Cmd + Space → “Anaconda Navigator”)\n\nFor more installation instructions you can find the Anaconda documentation for Windows installation here. You can also view the following video:",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#anaconda-navigator",
    "href": "99-anaconda-install.html#anaconda-navigator",
    "title": "28  Anaconda Installation",
    "section": "28.2 Anaconda Navigator",
    "text": "28.2 Anaconda Navigator\nAnaconda Navigator is the graphical interface included with Anaconda that makes it easier to access your development tools without needing to use the command line.\nWhen you launch Navigator, you’ll see a dashboard of available applications, including but not limited to:\n\nJupyter Notebook – A lightweight, web-based interface to write and run Python code interactively.\nJupyterLab – A more advanced interface for working with notebooks, code files, terminals, and data.\nSpyder – A scientific development environment similar to RStudio (not used in this course).\nVS Code – If installed, you may see a launcher for Visual Studio Code.\n\n\n\n\nAnaconda Navigator\n\n\nYou can also manage environments and packages through Navigator, but for now, your main focus will be on using it to launch Jupyter Notebook or JupyterLab.\nThis simple interface removes a lot of the friction of getting started with Python and is part of what makes Anaconda such a great option for beginners.\n\n\n\n\n\n\nTipLearn more about Anaconda Navigator\n\n\n\n\n\nAnaconda Navigator is a powerful application that can do much more than what we’ve covered here. If you’re curious and want to explore its full functionality, you can read more at here or watch this helpful overview video:\n\nThat said, don’t feel pressured to learn it all at once—some of the functionality may feel overwhelming at first, and that’s perfectly okay. We’ll ease into the tools you need as the course progresses.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#launching-jupyter-and-running-your-first-notebook",
    "href": "99-anaconda-install.html#launching-jupyter-and-running-your-first-notebook",
    "title": "28  Anaconda Installation",
    "section": "28.3 Launching Jupyter and Running Your First Notebook",
    "text": "28.3 Launching Jupyter and Running Your First Notebook\nNow that you’ve successfully installed Anaconda and explored the Navigator interface, it’s time to put it to use. One of the most common tools you’ll use as a data scientist is a Jupyter Notebook—an interactive coding environment that runs in your web browser and lets you combine code, text, and output in one place. In this section, we’ll walk you through how to launch a notebook and run your very first line of code using your newly installed Anaconda environment.\n\n\n\n\n\n\nNoteJupyter Notebooks vs. JupyterLab\n\n\n\nYou may notice that both Jupyter Notebook and JupyterLab are available in Anaconda Navigator. While they both allow you to write and execute code in the Jupyter Notebook format, JupyterLab is the newer, more modern interface. It provides a more flexible workspace, supports multiple tabs and file types, and integrates better with other tools. In this course, we’ll be using JupyterLab by default because of these added capabilities—but feel free to explore both if you’re curious!\n\n\nOnce you’ve installed Anaconda:\n\nOpen Anaconda Navigator: Start by opening the Anaconda Navigator application from your system’s start menu or application launcher.\nLocate JupyterLab: Within the Navigator interface, you’ll see a list of available applications. Find the “JupyterLab” icon. \nLaunch JupyterLab: Click the launch button at the bottom of the JupyterLab icon.\n\nThis will open a new tab in your default web browser at a local address like http://localhost:8888.\nThis web-based interface acts as your coding workspace. It allows you to create and run notebooks, write scripts, view files, and manage your data science projects—all in one place. And you interact with this interface through your browser.\n\nCreate a new notebook: In the Jupyter interface click New → Python 3 Notebook and this will launch a new Jupyter notebook. \nWrite Python code: In the first cell, type the following and press Shift + Enter to run the code:\nprint(\"Hello Anaconda\")\nYou should see:\nHello Anaconda\n\nThat’s it! You’ve set up your local Python data science environment.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#troubleshooting-tips",
    "href": "99-anaconda-install.html#troubleshooting-tips",
    "title": "28  Anaconda Installation",
    "section": "28.4 Troubleshooting Tips",
    "text": "28.4 Troubleshooting Tips\nIf you run into issues during installation or while launching tools like Anaconda Navigator or JupyterLab, don’t worry—it’s common when working with new software. Below are some tips that can help you troubleshoot the most common problems:\n\nAnaconda Installation Issues\n\nThe installer won’t open or crashes: Make sure your system meets the requirements and try re-downloading the installer from the official site.\nInstallation is stuck or taking too long: This process can take several minutes. If it seems frozen for more than 15–20 minutes, cancel the installation, restart your machine, and try again.\nAccidentally added Anaconda to PATH and things broke: Try uninstalling and reinstalling Anaconda, and leave the PATH option unchecked this time (recommended). You can follow the steps in this official guide to uninstall Anaconda properly:\n\nHow to uninstall Anaconda (official guide)\n\n\n\n\nIssues Launching Anaconda Navigator\n\nNavigator doesn’t launch:\n\nRestart your computer and try again.\nOn Windows, try launching the Anaconda Prompt and typing anaconda-navigator to launch it manually.\n\nNavigator opens but buttons are unresponsive or blank: This could be a display issue. Try updating your graphics drivers or restarting the app.\n\n\n\nProblems Starting JupyterLab\n\nClicking “Launch” in Navigator does nothing:\n\nRestart Navigator or your computer.\nTry launching JupyterLab via the command line:\n\nOn Windows: open Anaconda Prompt and type jupyter lab\nOn Mac: open Terminal and type jupyter lab\n\n\nBrowser doesn’t open automatically:\n\nCopy the URL shown in the terminal (e.g., http://localhost:8888) and paste it into your browser manually.\n\nJupyterLab opens but shows an error or doesn’t load:\n\nTry clearing your browser cache or switching to a different browser.\nMake sure no firewall or antivirus program is blocking access to localhost.\n\n\nIf all else fails, feel free to reach out to your instructor or teaching assistant for help!",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#additional-resources",
    "href": "99-anaconda-install.html#additional-resources",
    "title": "28  Anaconda Installation",
    "section": "28.5 Additional Resources",
    "text": "28.5 Additional Resources\nWant to explore more about Anaconda and JupyterLab beyond the basics? Here are some helpful resources if you’re curious to dig deeper:\n\nAnaconda Navigator Documentation: Learn about all the features available through Navigator, including managing environments and installing packages.\n\nhttps://www.anaconda.com/docs/tools/anaconda-navigator/main\n\nAnaconda Navigator Overview Video: A quick video tour of Navigator’s capabilities.\n\nhttps://youtu.be/PxMPl1x-qng\n\nJupyterLab Documentation: Explore JupyterLab’s interface, features, and how to extend its functionality.\n\nhttps://jupyterlab.readthedocs.io/en/stable/\n\n\nThese resources go into far more detail than we’ll need in this course—so don’t feel pressured to master everything right away. Use them as a reference when you’re ready to explore more.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html",
    "href": "99-vscode-install.html",
    "title": "29  VS Code Installation",
    "section": "",
    "text": "29.1 Installing VS Code\nVisual Studio Code (VS Code) is a free, lightweight, and powerful code editor used by developers and data scientists around the world. It’s highly customizable and supports many programming languages and tools—including Python.\nVS Code is more than just a text editor. With the right extensions, it can run code, work with Jupyter notebooks, help you manage projects, and even connect to Git for version control. It’s one of the most widely used environments in the industry and is likely the tool you’ll encounter in internships or full-time roles.\nWhile it may feel more advanced than Anaconda Navigator or JupyterLab, VS Code offers more flexibility and control as your projects grow. In this guide, we’ll walk you through getting started with VS Code for Python programming and data science.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#installing-vs-code",
    "href": "99-vscode-install.html#installing-vs-code",
    "title": "29  VS Code Installation",
    "section": "",
    "text": "Visit the official VS Code website: https://code.visualstudio.com\nClick Download and select the installer for your operating system (Windows, macOS, or Linux). \nRun the installer and follow the default prompts.\n\n\nWindows-Specific Notes:\n\nAllow the installer to add VS Code to your system PATH (this helps you launch it from the command line).\nYou can also enable options to open VS Code from the right-click context menu for added convenience.\n\n\n\nmacOS-Specific Notes:\n\nDrag and drop the VS Code application into your Applications folder.\nOpen it via Spotlight Search (Cmd + Space → “Visual Studio Code”).\n\nOnce installed, you should be able to open VS Code and access its interface.\n\nOn Windows: Click the Start menu, type “Visual Studio Code,” and press Enter. If you checked the box to add VS Code to the system PATH, you can also open a Command Prompt or PowerShell window and type code to launch it.\nOn macOS: Open Spotlight Search by pressing Cmd + Space, type “Visual Studio Code,” and press Enter. If you installed the command line tool, you can also open a Terminal window and type code.\n\nThis should open the VS Code editor which will look like:\n\n\n\nVS Code Editor\n\n\n\n\n\n\n\n\nIf you have any issues during setup, check out the VS Code setup docs for details around system requirements, more detailed setups for each operating system, along with common questions.\n\n\n\nIn the next section, we’ll help you install the necessary extensions to begin working with Python inside VS Code.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#installing-the-python-extension",
    "href": "99-vscode-install.html#installing-the-python-extension",
    "title": "29  VS Code Installation",
    "section": "29.2 Installing the Python Extension",
    "text": "29.2 Installing the Python Extension\nTo use Python inside VS Code, you’ll need two things:\n\nA Python interpreter: This is the program that actually runs your Python code. If you’ve already installed Anaconda, you’re all set—Anaconda includes a Python interpreter. If you haven’t (for example, if you’re coming straight from using Colab), you’ll need to install Python separately. You can download the latest version from https://www.python.org/downloads/.\nThe official Python extension provided by Microsoft: This extension adds support for writing, running, and debugging Python code in VS Code, and helps you work with virtual environments. To install this extension:\n\nOpen VS Code.\nClick on the Extensions icon on the left sidebar (or press Ctrl+Shift+X / Cmd+Shift+X).\nIn the search bar, type “Python” and look for the extension authored by Microsoft.\nClick Install. \n\n\nOnce installed, the Python extension will automatically detect Python interpreters on your system and provide features such as syntax highlighting, auto-complete, linting, and integrated terminal support.\n\nWhile You’re at It: Install These Additional Extensions\nTo get the most out of VS Code for Python and Jupyter work, we recommend installing the following extensions now:\n\nJupyter: Enables Jupyter Notebook support within VS Code. \nPylance: Improves code intelligence and type checking. \n\nAfter installing these extensions, you’re ready to start writing and running Python code in VS Code!\n\n\nUnderstanding Extensions in VS Code\nExtensions are small add-ons that enhance the functionality of VS Code. Think of them like plugins—they allow you to customize your coding environment based on your needs.\nSome extensions help you write and run code in specific languages, while others add support for tools like Git, Jupyter notebooks, or data visualization libraries.\nHere are a few common extensions for data science workflows:\n\nPython (by Microsoft) – Adds Python language support\nJupyter – Enables Jupyter Notebook support\nPylance – Adds fast, accurate code intelligence and type checking\nGitLens – Enhances Git capabilities directly in VS Code\nCode Runner – Allows you to run snippets of code from many languages\n\nTo explore more extensions:\n\nClick the Extensions icon in the sidebar.\nBrowse featured or recommended extensions.\nUse the search bar to find tools that match your interests.\n\n\n\n\n\n\n\nYou don’t need to install a lot of extensions to get started. As you become more experienced, you’ll naturally add tools that support your workflow and projects.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#creating-your-first-jupyter-notebook",
    "href": "99-vscode-install.html#creating-your-first-jupyter-notebook",
    "title": "29  VS Code Installation",
    "section": "29.3 Creating Your First Jupyter Notebook",
    "text": "29.3 Creating Your First Jupyter Notebook\nNow that VS Code is set up with Python and Jupyter support, let’s walk through creating your first Jupyter notebook and running a simple Python command.\nSteps to Create and Run a Notebook:\n\nOpen VS Code.\nClick File → New File → select Jupyter Notebook \nLet’s go ahead and save this file. Go to File → Save As and you can select where you want to save this file to and also rename it (e.g., hello.ipynb).\nVS Code will recognize this as a Jupyter notebook and open an interactive notebook interface.\nIn the first cell, type:\n\nprint(\"Hello VS Code\")\n\nClick the Run Cell button (the small ▶️ icon) to the left of the cell, or press Shift + Enter. You should see the output Hello VS Code as below.\n\n\n\n\nHello VS Code\n\n\nCongratulations! You’ve just run your first Jupyter notebook in VS Code. You’re now ready to use this environment for more advanced analysis and coding throughout the course.\n\n\n\n\n\n\nHere’s a great video to get you up and running with Jupyter notebooks in VS Code!",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#exploring-the-vs-code-interface",
    "href": "99-vscode-install.html#exploring-the-vs-code-interface",
    "title": "29  VS Code Installation",
    "section": "29.4 Exploring the VS Code Interface",
    "text": "29.4 Exploring the VS Code Interface\nVS Code is packed with features, but you don’t need to use them all right away. As you become more comfortable with VS Code and your data science skills grow, you’ll naturally start using more of its capabilities. Here are some of the features you’ll find yourself using most often:\n\nExplorer (📁 icon): View and manage your project files.\nRun and Debug (▶️ icon): Run code or set up debugging tools.\nExtensions (🔌 icon): Install and manage extensions.\nIntegrated Terminal: Open a terminal inside VS Code using Ctrl + `` orCmd + `` (backtick).\nNotebook Interface: When working in .ipynb files, you can use cells to write and run code interactively.\n\nIf the interface feels overwhelming at first, don’t worry. Start by focusing on writing and running Python code—over time, you’ll naturally grow into the more advanced features.\n\n\n\n\n\n\nHere is a nice video that will introduce you to some of VS Code’s features. Don’t worry about understanding the code shown or what all these features mean right now—just watch to get a sense of what VS Code can do!",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#troubleshooting-tips",
    "href": "99-vscode-install.html#troubleshooting-tips",
    "title": "29  VS Code Installation",
    "section": "29.5 Troubleshooting Tips",
    "text": "29.5 Troubleshooting Tips\nIf you run into issues while setting up or using VS Code, here are some tips:\n\nVS Code doesn’t install:: If you have any issues during setup, check out the VS Code setup docs for details around system requirements, more detailed setups for each operating system, along with common questions.\nVS Code installs but doesn’t launch:\n\nMake sure the installation finished successfully and you can find the application on your computer\n\nOn Windows: Click the Start menu, type “Visual Studio Code,” and press Enter. If you checked the box to add VS Code to the system PATH, you can also open a Command Prompt or PowerShell window and type code to launch it.\nOn macOS: Open Spotlight Search by pressing Cmd + Space, type “Visual Studio Code,” and press Enter. If you installed the command line tool, you can also open a Terminal window and type code.\n\nTry restarting your machine.\nReinstall VS Code from the official website.\n\nPython extension isn’t working or missing:\n\nMake sure it’s installed and enabled.\nReload the window (Cmd/Ctrl + Shift + P → type Reload Window).\n\nPython interpreter not found:\n\nPress Ctrl+Shift+P / Cmd+Shift+P and search for “Python: Select Interpreter”. Choose the appropriate Python version (often one installed with Anaconda).\nIf no Python options come up then you probably don’t have a Python interpreter installed. Go to https://www.python.org/downloads/ and download the latest version.\n\nNotebook cells not running:\n\nMake sure the Jupyter extension is installed.\nMake sure the file is saved with a .ipynb extension.\n\n\nIf all else fails, try searching the error message online or ask your instructor or teaching assistant for help.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#additional-resources",
    "href": "99-vscode-install.html#additional-resources",
    "title": "29  VS Code Installation",
    "section": "29.6 Additional Resources",
    "text": "29.6 Additional Resources\nIf you want to learn more about using VS Code for Python and data science, here are some helpful links:\n\nTutorial: Get started with Visual Studio Code\nVS Code Python Docs\nVS Code for Data Science Docs\nJupyter Notebooks in VS Code\nVarious VS Code Introductory Videos\n\nYou don’t need to master all of this at once—use these resources when you’re ready to explore more advanced features or troubleshoot issues.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Calvo, J., and M. Fernandez. 2024. “Enhancing Customer Retention\nwith Machine Learning: A Comparative Study of Ensemble Models.”\nJournal of Retail Analytics. https://www.sciencedirect.com/science/article/pii/S2667096825000138.\n\n\nHussain, A., and F. Al-Obeidat. 2024. “A Machine Learning Model to\nPredict Heart Failure Readmission: Toward Optimal Feature Set.”\nFrontiers in Artificial Intelligence 7: 1363226. https://doi.org/10.3389/frai.2024.1363226.\n\n\nKhan, A., and R. Malik. 2024. “Predictive Power of Random Forests\nin Analyzing Risk Management Practices and Concerns in Islamic\nBanks.” Journal of Risk and Financial Management 17 (3):\n104. https://doi.org/10.3390/jrfm17030104.\n\n\nKim, H., and S. Park. 2021. “Machine Learning for Predicting\nReadmission Risk Among Hospitalized Patients: A Systematic\nReview.” Digital Health. https://www.sciencedirect.com/science/article/pii/S2666389921002622.\n\n\nLi, H., and Y. Zhang. 2021. “Financial Credit Risk Control\nStrategy Based on Weighted Random Forest Algorithm.” Journal\nof Applied Mathematics, 1–9. https://doi.org/10.1155/2021/6276155.\n\n\nSchmidt, L., and M. Hoffmann. 2023. “Using Machine Learning\nPrediction Models for Quality Control: A Case Study from the Automotive\nIndustry.” Journal of Intelligent Manufacturing. https://doi.org/10.1007/s10287-023-00448-0.\n\n\nWang, L., and J. Chen. 2023. “Customer Churn Prediction Based on\nthe Decision Tree and Random Forest Model.” International\nJournal of Computer Applications. https://www.researchgate.net/publication/370571328_Customer_Churn_Prediction_Based_on_the_Decision_Tree_and_Random_Forest_Model.\n\n\nZhao, Q. 2024. “Random Forest-Based Machine Failure Prediction in\nIndustrial Equipment.” Applied Sciences 15 (16): 8841.\nhttps://doi.org/10.3390/app15168841.",
    "crumbs": [
      "References"
    ]
  }
]