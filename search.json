[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BANA 4080: Data Mining",
    "section": "",
    "text": "Welcome\nWelcome to BANA 4080: Introduction to Data Mining with Python. This course provides an immersive, hands-on introduction to the tools and techniques used in modern data science. You’ll learn how to explore, analyze, and model data using Python and gain practical experience through labs, projects, and real-world datasets.\nAlong the way you will develop core skills in data wrangling, exploratory data analysis, data visualization, and even key machine learning techniques such as supervised, unsupervised, and deep learning model. We’ll even take a quick detour into generative AI and large language models (LLMs). Throughout this process we’ll use real-world data and experiential learning to guide your learning.\nBy the end of the course, students will be able to:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#who-should-read-this",
    "href": "index.html#who-should-read-this",
    "title": "BANA 4080: Data Mining",
    "section": "Who should read this?",
    "text": "Who should read this?\nThis book is designed for upper-level undergraduate students who may have little to no prior programming experience but are eager to explore the world of data science using Python. It’s also an ideal resource for early-career professionals or students in analytics, business, or quantitative fields who are looking to upskill—whether by learning Python for the first time or by building a deeper understanding of how to explore, visualize, and model data. The content is structured to be accessible and hands-on, guiding readers step-by-step through the core tools and techniques used in modern data-driven problem solving.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-this-book-is-structured",
    "href": "index.html#how-this-book-is-structured",
    "title": "BANA 4080: Data Mining",
    "section": "How this book is structured",
    "text": "How this book is structured\nThis book is broken into 14 modules, each aligned with a week of instruction in the BANA 4080 course. Every module introduces key concepts or techniques in data science, combining concise explanations with interactive, hands-on code examples. Whether you’re reading independently or following along with the course, the modular structure makes it easy to work through the content at your own pace, week by week.\n\n\n\n\n\n\n\nModule & Topics\nSummary of Concepts Covered\n\n\n\n\n1. Fundamentals I\nCourse overview, coding environment setup, Python basics\n\n\n2. Fundamentals II\nUsing Jupyter notebooks, data structures, Python libraries\n\n\n3. Pandas DataFrames\nImporting data, DataFrame fundamentals, subsetting DataFrames\n\n\n4. Data Wrangling I\nCleaning, filtering, aggregating, and merging tabular data\n\n\n5. Data Wrangling II\nWorking with datetime, text data, and joining data like SQL\n\n\n6. Data Visualization\nCreating plots using matplotlib and seaborn, exploratory data analysis\n\n\n7. Writing Efficient Python Code\nControl flow, defining functions, loops, list comprehensions\n\n\n8. Introduction to Machine Learning\nOverview of ML, features/labels, train/test split, scikit-learn basics\n\n\n9. Unsupervised Learning\nClustering (k-means), PCA, dimensionality reduction, t-SNE visualization\n\n\n10. Supervised Learning\nRegression and classification models: linear, logistic regression\n\n\n11. Deep Learning & Neural Networks\nNeural networks using Keras; simple classification tasks\n\n\n12. Generative AI & Prompt Engineering\nWorking with LLMs, OpenAI API, prompt design, building AI agents\n\n\n13. Final Project Kickoff\nScoping and starting a capstone data science project\n\n\n14. Final Project Presentations & Wrap-Up\nPresenting project findings, course reflection, and next steps",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#conventions-used-in-this-book",
    "href": "index.html#conventions-used-in-this-book",
    "title": "BANA 4080: Data Mining",
    "section": "Conventions used in this book",
    "text": "Conventions used in this book\nThe following typographical conventions are used in this book:\n\nstrong italic: indicates new terms,\nbold: indicates package & file names,\ninline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user,\ncode chunk: indicates commands or other text that could be typed literally by the user\n\n\n1 + 2\n\n3\n\n\nIn addition to the general text used throughout, you will notice the following code chunks with images:\n\n\n\n\n\n\nSignifies a tip or suggestion\n\n\n\n\n\n\n\n\n\nSignifies a general note\n\n\n\n\n\n\n\n\n\nSignifies a warning or caution",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#software-used-throughout-this-book",
    "href": "index.html#software-used-throughout-this-book",
    "title": "BANA 4080: Data Mining",
    "section": "Software used throughout this book",
    "text": "Software used throughout this book\nThis book is built around an open-source Python-based data science ecosystem. While the list of tools evolves with the field, the examples and exercises in this book are designed to work with Python 3.x, currently using…\n\n\nCode\n# Display the Python version\nimport sys\nprint(\"Python version:\", sys.version.split()[0])\n\n\nPython version: 3.13.7\n\n\n…and are executed within Jupyter Notebooks, which provide an interactive, beginner-friendly environment for writing and running code.\nThroughout the modules, we use foundational Python libraries such as:\n\npandas and numpy for data wrangling and numerical computing,\nmatplotlib and seaborn for data visualization,\nscikit-learn and keras for machine learning and deep learning, and\nopenai and transformers for generative AI and large language model exploration.\n\nEach module explicitly introduces the relevant software and libraries, explains how and why they are used, and provides reproducible code so that readers can follow along and generate similar results in their own environment.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "BANA 4080: Data Mining",
    "section": "Additional resources",
    "text": "Additional resources\nTBD",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html",
    "href": "01-intro-data-mining.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Why are we here?\nWelcome to your journey into the world of data science. Whether you’re here out of curiosity, career aspirations, or because it’s a required course, you’re stepping into a field that blends logic, creativity, and curiosity to solve real-world problems using data. But let’s be honest—if you’ve never written a line of code before, the idea of learning Python or building machine learning models might feel a bit intimidating. You might even be wondering: “Why do I need to learn this when tools like ChatGPT or Copilot can just write code for me?” That’s a fair question — and one we’ll unpack in this chapter. Our goal is to give you a clear picture of why learning these foundational skills still matters, why Python is at the heart of modern data science, and how to approach the learning process with the right expectations and mindset.\nBy the end of this chapter, you will:\nLet’s start with a story.\nImagine you’re a college junior named Taylor who just landed a summer internship at a local marketing analytics firm. On your first day, your manager hands you a file — it’s a messy spreadsheet filled with customer purchase data — and says, “We’re trying to understand what drives repeat purchases. Can you dig into this and see what you find?”\nTaylor freezes.\nYou’ve taken calculus, stats, and maybe even a regression course. You’ve know Excel, used it to build a few dashboards in your business classes, and maybe dabbled in a little VBA. You know how to analyze formulas on paper and interpret statistical summaries. You’ve studied how marketing campaigns influence consumer behavior, how pricing affects demand, how supply chains function, and how companies generate profits.\nBut now, faced with a real dataset and a vague question, you’re not sure where to begin.\nYou know there’s a story in the data; something important about customer behavior that the business wants to uncover. But without a roadmap, the numbers feel more overwhelming than enlightening.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#why-are-we-here",
    "href": "01-intro-data-mining.html#why-are-we-here",
    "title": "1  Introduction",
    "section": "",
    "text": "Do you start sorting columns manually?\nWrite an IF statement?\nAsk ChatGPT to explain the data?\nLook for a “correlation” button in Excel?\n\n\n\nYou’ve built the foundation—now it’s time to apply it.\nThat feeling Taylor has? That’s not a sign of being unprepared, it’s a sign of the gap that exists between traditional classroom learning and the real-world application of data science.\nYou already know more than you think:\n\nYou understand business operations, customer behavior, and the bottom-line goals companies care about.\nYou’ve studied quantitative methods and statistical inference.\nYou know how to think critically and ask good questions.\n\nWhat you haven’t had (yet) is the experience of turning raw data into actionable insight using tools like Python to clean, explore, visualize, and model information in a repeatable, scalable way.\nThat’s where this course comes in.\n\n\nThis course is about doing.\nWe’re going to close the gap between the theory you’ve learned and the practice that’s expected in today’s data-driven workplace. You’ll work with messy datasets just like Taylor’s. You’ll write code to clean, organize, and analyze real data. And you’ll build up your own toolkit so that, the next time someone asks you to “see what you can find,” you won’t freeze — you’ll get to work.\nAnd yes, we’ll even talk about when to use tools like ChatGPT to help you along the way (and when not to).\n\n\n\n\n\n\nBy the end of this course, you’ll have the confidence and experience to tackle open-ended data problems, the skills to automate and scale your work, and the mindset to keep growing as a data-driven problem solver.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#isnt-ai-supposed-to-do-all-this-for-us-now",
    "href": "01-intro-data-mining.html#isnt-ai-supposed-to-do-all-this-for-us-now",
    "title": "1  Introduction",
    "section": "1.2 Isn’t AI supposed to do all this for us now?",
    "text": "1.2 Isn’t AI supposed to do all this for us now?\nLet’s address the question that’s probably been lingering in your mind since the moment you signed up for this course:\n\n“Why do I need to learn how to code when tools like ChatGPT, Copilot, and Claude can just do it for me?”\n\nIt’s a fair question.\nIn fact, you may have already used one of these tools to generate code. Maybe you asked ChatGPT to “write a Python script that reads a CSV file and finds the average,” and in seconds — boom — you had something that worked.\nSo… case closed, right? Not quite.\n\nAI is helpful—but it’s not a substitute for understanding.\nGenerative AI tools are incredible accelerators. They can save time, reduce friction, and provide helpful starting points. But they’re not magic. They don’t know your data. They don’t understand the business context. And they certainly don’t guarantee correct answers.\nThese tools work by predicting the next most likely word or line of code based on patterns in data they’ve seen. They don’t reason like humans. They can’t debug your logic or decide which metric is appropriate for your analysis. And sometimes? They just make stuff up.\n\n\n\n\n\n\nWarningCallout: It’s like autocorrect but for code!\n\n\n\n\n\nIf you’ve ever had your phone turn “on my way!” into “omg my weasel!” — you already understand how these tools work.\nGenAI models, like ChatGPT and Copilot, are basically super-powered autocomplete engines. They’re predicting what comes next based on what they’ve seen before. That means they can sometimes nail it… and other times leave you wondering, “What were you even trying to say?”\n\nJust like with autocorrect, the key is knowing when the suggestion is helpful—and when to hit backspace.\n\n\n\nIf you don’t have the foundational skills, you won’t know when they’re wrong—or worse, when they’re subtly wrong.\n\n\nAI tools are assistants, not autopilots.\nLearning how to code and analyze data yourself is what allows you to:\n\nSpot mistakes in generated code\nAsk better questions (better prompts = better results)\nCustomize and build on what AI gives you\nEvaluate whether an approach is valid or helpful\nExplain what your analysis means and how it should be used\n\nYou don’t need to fear these tools. But you also don’t want to depend on them blindly. This course will teach you the skills needed to use AI as a powerful assistant — not a crutch.\n\n\n\n\n\n\nNoteStudent Reflection Prompt\n\n\n\n\n\nHave you ever used a tool like ChatGPT, Claude, or Copilot to generate code or solve a problem?\n\nWhat did it get right?\nWhat did it miss?\nHow confident were you in the result?\n\n\n\n\nIn the chapters ahead, we’ll sometimes bring GenAI tools into the learning process, especially as helpers for things like brainstorming or debugging. But we’ll always emphasize the importance of understanding the code you’re working with. Otherwise, you’re just copying answers without learning anything—which is like using a calculator to solve a math problem you never actually learned.\nAnd trust us: if you ever find yourself in Taylor’s shoes—staring at a spreadsheet and needing to figure out what matters—you’ll want more than a chatbot. You’ll want skill.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#why-python-and-why-now",
    "href": "01-intro-data-mining.html#why-python-and-why-now",
    "title": "1  Introduction",
    "section": "1.3 Why Python, and why now?",
    "text": "1.3 Why Python, and why now?\nSo far, we’ve talked about why it’s important to build foundational skills and not overly rely on AI tools. But that probably leaves you with another question:\n\n“Out of all the programming languages out there, why are we learning Python?”\n\nIt’s a good question — and the answer is simple: Python is the language of modern data science.\n\nPython is beginner-friendly, but not just for beginners.\nPython was designed to be easy to read and write. Its syntax is clean and (mostly) intuitive, which means you won’t spend hours trying to remember obscure symbols or puzzling over where the semicolon went. That makes it a great first language for students who are new to programming.\nBut don’t mistake simplicity for lack of power. Python is also used by:\n\nData scientists at companies like Google, Netflix, and Spotify\nAnalysts at healthcare organizations, banks, and nonprofits\nResearchers running simulations and training machine learning models\nEngineers building large-scale data pipelines and AI tools\n\nSo while Python is accessible to beginners, it’s also respected and widely used by professionals. In fact, according to the 2023 Stack Overflow Developer Survey, Python ranks as one of the top three most commonly used programming languages overall, and is consistently a favorite among developers working in data science, machine learning, and academic research. Its combination of readability, power, and a rich ecosystem of libraries makes it a go-to language across industries and roles.\n\n\nIt’s not just the language, it’s the ecosystem.\nPython has a massive collection of libraries and packages built specifically for data work. Here are just a few you’ll get to know in this course but realize there are tens of thousands of open source Python libraries that you can use for various data science tasks:\n\npandas – for working with data tables\nnumpy – for efficient numerical computation\nmatplotlib and seaborn – for data visualization\nscikit-learn – for building machine learning models\n\nThis ecosystem means you won’t have to build everything from scratch. You’ll learn to stand on the shoulders of open-source giants so you can focus more on solving problems and less on reinventing the wheel.\n\n\nPython is your toolset. This course is your training ground.\nIn this course, you’ll learn how to:\n\nWrite Python code that manipulates and explores real datasets\nUse libraries like pandas and matplotlib to summarize and visualize your data\nBuild your first machine learning models with tools like scikit-learn\nGain confidence in reading and modifying code — whether it came from a textbook, a blog post, or a GenAI assistant\n\nBy the end of this course, Python won’t feel like some intimidating, foreign technical concept you’ve been avoiding or unsure how to approach. It will become a practical skill—a tool you know how to use to explore data, uncover insights, and drive real decisions. And here’s the best part: you’ll be learning the same tool that analysts, data scientists, and business teams across your future organization are already using. Instead of feeling left out of the conversation, you’ll be equipped to lead it.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#learning-to-code-a-reality-check",
    "href": "01-intro-data-mining.html#learning-to-code-a-reality-check",
    "title": "1  Introduction",
    "section": "1.4 Learning to code: a reality check",
    "text": "1.4 Learning to code: a reality check\nLet’s be real for a minute.\nLearning to code can be frustrating at first. It’s not always fun to stare at an error message that makes zero sense. It’s even less fun when you fix that error… only to get a brand-new one. You might feel stuck, confused, or like everyone else “gets it” but you.\nThat’s normal. In fact, it’s expected.\n\nLearning to code is like learning a new language (because it is)\nWhen you’re first learning a spoken language, you stumble over basic phrases. You forget vocabulary. You mess up grammar. But over time, with practice, you start to think in the new language — and eventually, it just clicks.\nCoding is the same way. You’ll start by copying examples and Googling error messages. That’s fine. That’s part of the process. Over time, those patterns will stick. You’ll stop memorizing and start thinking in code.\n\n\nThis course is designed for beginners\nYou don’t need prior programming experience to succeed here. We’ll start from the very beginning: writing simple statements, understanding variables and data types, building up to loops, functions, and eventually full data science workflows.\nYou’ll go from:\n\n“What’s a variable?”\n\nto…\n\n“I just built a machine learning model to predict customer churn.”\n\nStep by step. Week by week. You’ll get there. And don’t feel like you have to go it alone.\n\n\n\n\n\n\nTipLean on your classmates.\n\n\n\n\n\nIf you’re stuck, chances are someone else is too. Use the classroom discussion board to ask questions, share what you’re learning, and help each other out. This is a collaborative course, and we’re building a community of learners who grow together.\n\n\n\n\n\nIt’s okay to use AI tools but don’t skip the struggle\nYou’ll be encouraged to use tools like ChatGPT, Copilot, and Claude throughout this course. But here’s the rule: use them to learn, not to avoid learning.\nThink of these tools as tutors, not answer keys:\n\nUse them to check your understanding\nAsk them for help when you’re stuck\nCompare their answers to your own and figure out the differences\nBut always make sure you understand what the code is doing\n\nShortcuts don’t help if you skip the part where you build skill.\nLearning to code is like learning anything meaningful—it takes effort, patience, and a little bit of resilience. You won’t get everything right the first time, and that’s okay. You don’t need to be perfect. You just need to keep showing up and keep trying.\nAnd we’ll be right here to help every step of the way.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#wrap-up",
    "href": "01-intro-data-mining.html#wrap-up",
    "title": "1  Introduction",
    "section": "1.5 Wrap-Up",
    "text": "1.5 Wrap-Up\nYou’ve made it through your first chapter — nicely done!\nWe’ve talked about why data science matters, why Python is the language we’re using, and why learning these skills (even in the age of AI) still gives you a powerful edge. We’ve also tried to set realistic expectations: learning to code can be challenging, but it’s absolutely doable—with patience, practice, and support from your classmates, instructors, and tools like ChatGPT when used wisely.\nThe big takeaway? You’re not here to memorize formulas or passively watch someone else code. You’re here to build real skills that will help you ask better questions, explore real-world data, and create insights that drive decisions.\nAnd we’re not wasting any time. In the next chapter, we’ll jump right in — you’ll get your Python environment set up and write your first lines of code. This is where the hands-on part of the journey begins.\nThis course moves fast, so buckle up and enjoy the ride. Let’s get started!",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro-data-mining.html#exercise-can-you-help-taylor",
    "href": "01-intro-data-mining.html#exercise-can-you-help-taylor",
    "title": "1  Introduction",
    "section": "1.6 Exercise: “Can You Help Taylor?”",
    "text": "1.6 Exercise: “Can You Help Taylor?”\n\n\n\n\n\n\nNoneScenario\n\n\n\n\n\nYou’re stepping into Taylor’s shoes. You’ve just been handed three datasets by your manager at a local marketing analytics firm:\n\ntransactions.csv — individual transactions from various customers, but… it’s a litle messy.\nhh_demographics.csv — customer-level information including age, household size, income level, etc.\nproduct.csv - product-level information such as manufacturer, brand, commodity type, package size, etc.\n\nYour job is to begin exploring what factors might be influencing repeat purchases. But before you can even analyze anything, you’ll need to clean, combine, and make sense of the data.\nThroughout this exercise your objective is to:\n\nInspect and describe real-world “messy” data\nIdentify issues that must be addressed before analysis\nStrategize a plan for cleaning and joining datasets\nExperiment with using ChatGPT to assist your data work\nReflect on how foundational skills + GenAI can be used together\n\n\n\n\n\n\n\n\n\n\nNonePart 1 – Get Oriented\n\n\n\n\n\nStart by opening the transactions data and take a few minutes to explore it.\nQuestions:\n\nWhat’s messy or confusing about this data?\nWhat would your first 2–3 steps be if you were asked to clean or prepare it for analysis?\nBased on this table alone, what kinds of analyses might you try to do?\n\n\n\n\n\n\n\n\n\n\nNonePart 2 – Meet the Second Dataset\n\n\n\n\n\nNow open the household demographic and the product information data.\nQuestions:\n\nWhat types of information does this dataset contain that might help you understand customer behavior?\nHow would you combine these with the transactions data?\nWhat issues do you foresee when trying to join them?\n\n\n\n\n\n\n\n\n\n\nNonePart 3 – Ask for Help (But Be Smart About It)\n\n\n\n\n\nUse ChatGPT (or any GenAI tool you’re comfortable with) to begin cleaning or analyzing the data.\nInstructions:\n\nTry writing 2–3 different prompts to help with basic data cleaning, joining, or summarizing the data.\nTry at least one prompt that combines both datasets.\n\nThen reflect:\n\nWhat prompts did you use?\nDid the AI return usable code?\nWere there any errors, misunderstandings, or surprises?\nDid you understand what the code was doing? If not, what helped you figure it out?\n\n\n\n\n\n\n\n\n\n\nNonePart 4 – Pull It All Together\n\n\n\n\n\nWrite a short summary (5–7 sentences) answering the following:\n\nWhat was your biggest takeaway from working with these datasets?\nHow did this exercise reinforce the ideas from Chapter 1?\nDid this change your perspective on what your role is when using AI tools like ChatGPT in data analysis?\nHow do you feel about starting your journey into Python after completing this activity?",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html",
    "href": "02-preparing-for-code.html",
    "title": "2  Setting Up Your Python Environment",
    "section": "",
    "text": "2.1 Hello World\nBefore we dive deep into the world of data science, we need to get you set up with the tools of the trade. Just like a carpenter needs a good workbench and set of tools, a data scientist needs a place to write and run code. The good news? There are several ways to do this—and even better, there’s no one-size-fits-all answer. You’ll get to choose the approach that works best for you.\nIn this chapter, we’ll introduce three common ways to set up a Python environment: Google Colab, the Anaconda distribution, and Visual Studio Code. We’ll start with the path of least resistance (Colab), then explore more powerful and flexible setups as we go.\nWe’ll kick things off by writing your very first Python program—a classic:\nBy the end of this chapter, you’ll not only have a working Python environment, but you’ll also understand the pros and cons of each approach and know how to get started coding in any of them. Let’s get your hands on the keyboard.\nWelcome to your first moment of writing real Python code. We’re not going to lecture you about the perfect setup right away. Instead, let’s just do something. This short activity gives you a quick win and shows you how easy it can be to get started with Python—no installations or configurations required.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html#hello-world",
    "href": "02-preparing-for-code.html#hello-world",
    "title": "2  Setting Up Your Python Environment",
    "section": "",
    "text": "Running Python in Google Colab\nGoogle Colab is a free, cloud-based tool that lets you run Python code in your browser. It requires no setup and is perfect for getting started.\n\nGo to: https://colab.research.google.com/\nSign in with your Google account (you’ll need one).\nClick on “New Notebook.”\nIn the cell that appears, type:\nprint(\"Hello World\")\nPress Shift + Enter to run the cell.\n\nYou should see:\nHello World\nYou just wrote and ran your first line of Python code! 🎉🎉🎉\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneExample Notebook\n\n\n\nLet’s put your new skills to the test by working through a simple example notebook.\n📓 We’ve created a sample notebook that introduces:\n\nThe print() function\nHow to format and write some basic text with Markdown\nHow to combine code and notes in one place\n\n👉 Click the “Open in Colab” link at the top of the notebook to launch it in Google Colab and run through the notebook.\n\n\n\n\nTry This: Generate Python Code with AI\nAfter you run your print(\"Hello World\"), take a moment to explore the built-in AI assistant in Google Colab. You might notice a prompt that says:\n\n“Start coding or generate with AI”\n\nThis is your chance to see how AI tools can help you write Python code. Try typing in a prompt or click on the suggestion box to let Colab help generate code for you. You can run the code to see what it does—and even tweak it to make it your own.\nFeel free to come up with your own prompt, or try one of these to get started:\n\n“Write Python code to compute the area of a 12-inch pizza.”\n“Write Python code to find all prime numbers between 2 and 100.”\n“Write a Python program that asks for a user’s name and prints a greeting.”\n\n\n\n\n\n\n\nNoneReflect: Your First Python Experience\n\n\n\n\n\nTake a few minutes to reflect on your first hands-on experience writing Python code and using an AI assistant. You can jot your answers in a notebook, a note-taking app, or even directly in your Colab notebook using a text cell (don’t know how to do this, that’s ok, ask Colab’s AI to help 😉).\nConsider the following questions:\n\nWas writing and running your first lines of Python code easier or harder than you expected?\nHow did it feel to use the AI assistant to generate code? Do you think tools like this can make you more productive? How confident are you that the code was actually correct?\nGoogle Colab makes it easy to get started, but do you think this is the kind of environment you would use at work? Why or why not?\n\nYou don’t need to write a long answer—just a few thoughtful sentences to capture your perspective at this early stage in your learning journey.\n\n\n\nNext, we’ll step back and look at what just happened. Then, we’ll dive into the different ways you can set up your Python environment, from easy to advanced.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html#development-environments",
    "href": "02-preparing-for-code.html#development-environments",
    "title": "2  Setting Up Your Python Environment",
    "section": "2.2 Development Environments",
    "text": "2.2 Development Environments\nNow that you’ve written and run your first line of Python code, it’s time to step back and understand where that code actually runs—and what your options are going forward.\nIn programming, the place where you write and run your code is called your development environment. Think of it like your digital workspace: it includes the tools, interface, and underlying systems that allow you to code, debug, and manage projects.\nThere’s no single “best” environment—just the one that’s best for your current needs. In this course, you’ll be exposed to three common Python environments used by data scientists, ranging from beginner-friendly to professional-grade. Each has its own strengths and trade-offs, and over time you may find yourself using all three depending on the situation.\nBelow is a quick overview of the three environments we’ll focus on in this course.\n\nGoogle Colab\nYou’ve already used this! Colab is a cloud-based environment that lets you run Python in your browser, no installation required. It’s perfect for beginners or anyone who wants to start quickly and painlessly.\nPros:\n\nNo installation needed—works in your browser.\nBuilt-in support for Jupyter notebooks.\nEasy to share and collaborate via Google Drive.\nIncludes access to an AI assistant to help generate code.\n\nCons:\n\nRequires an internet connection.\nLimited access to your local files or custom setups.\nNot commonly used in professional, production-level environments.\n\n\n\n\n\n\n\nWhile Colab is excellent for learning, prototyping, and sharing code, it isn’t typically used in workplace settings—especially for production code, version-controlled projects, or large-scale data workflows. Because of this, we encourage you to use Colab to get started quickly, but strive to set up one of the other environments (Anaconda or VS Code) as you progress through the course. These tools will better reflect the development environments you’re likely to use in internships, co-ops, or full-time roles\n\n\n\n\n\nAnaconda Distribution\nAnaconda is a local setup that installs Python along with most of the libraries and tools used in data science, including Jupyter Notebook and JupyterLab. It’s an excellent next step for learners who want more control while still keeping things simple.\n\n\n\n\n\n\nAnaconda\n\n\n\n\nFigure 2.1: Anaconda is one of the most widely used platforms for data science, offering an all-in-one distribution of Python, Jupyter, and essential packages for analysis and machine learning.\n\n\n\nPros:\n\nAll-in-one installation of Python + essential libraries.\nUser-friendly interface via Anaconda Navigator.\nIncludes JupyterLab for notebook-based development.\nWorks locally without internet access.\n\nCons:\n\nLarge download and install size (~3 GB).\nYou’ll need to learn how to manage packages and environments with Conda.\nSlightly more complex than Colab, but still beginner-friendly.\n\n\n\nVisual Studio Code (VS Code)\nVS Code is a lightweight but powerful code editor used by professional developers and data scientists. With the right extensions, it supports Python and Jupyter notebooks and is ideal for building larger or more complex projects.\n\n\n\n\n\n\nVS Code is my preferred environment for writing code, so throughout this course, most of the code you’ll see during lectures and demos will be displayed using VS Code. This will give you exposure to an industry-standard tool that’s commonly used across the kinds of organizations you’re likely to intern or work at.\n\n\n\n\n\n\n\n\n\nVS Code\n\n\n\n\nFigure 2.2: Visual Studio Code (VS Code) is one of the leading code editors used by data scientists and software developers. It’s widely adopted across industry and is likely the primary development environment you’ll encounter in internships or full-time roles.\n\n\n\nPros:\n\nHighly customizable and fast.\nGreat for real-world development workflows.\nIntegrated support for version control (Git), debugging, and extensions.\n\nCons:\n\nSteeper learning curve—more setup required.\nRequires separate installations of Python, extensions, and Jupyter support.\nBest suited for students who want to grow into more advanced tools.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html#choosing-your-path-forward",
    "href": "02-preparing-for-code.html#choosing-your-path-forward",
    "title": "2  Setting Up Your Python Environment",
    "section": "2.3 Choosing Your Path Forward",
    "text": "2.3 Choosing Your Path Forward\nHow you decide to move forward from here is up to you. For now, you’re welcome to continue using Google Colab, especially if it’s helping you build confidence and get comfortable writing Python code without worrying about software setup.\nThat said, by the end of this course, you’ll be expected to have a local Python development environment set up on your computer—either using Anaconda or Visual Studio Code. These environments are more reflective of what you’ll use in real-world internships, co-ops, or full-time roles, and they’ll give you greater flexibility and power as your projects grow in complexity.\nWhen you’re ready to make the leap, use the resources below to guide you:\n\nAnaconda Installation: Appendix 19\nVS Code Installation: Appendix 20\n\nYou don’t need to switch immediately, but the sooner you get comfortable working locally, the better prepared you’ll be for the kinds of tasks data scientists regularly tackle in the field.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "02-preparing-for-code.html#exercise-pick-your-environment-and-make-it-yours",
    "href": "02-preparing-for-code.html#exercise-pick-your-environment-and-make-it-yours",
    "title": "2  Setting Up Your Python Environment",
    "section": "2.4 Exercise: Pick Your Environment and Make It Yours",
    "text": "2.4 Exercise: Pick Your Environment and Make It Yours\nChoose one of the following paths based on your current comfort level and curiosity. Your goal is to take one step toward becoming confident in your development environment.\n\n\n\n\n\n\nNoneOption 1: Stay with Colab (for now)\n\n\n\n\n\n\nOpen a new Colab notebook.\nUse the AI assistant to generate and run a short piece of Python code.\nTry modifying the code and run it again to see how it changes.\nAdd a Markdown cell where you reflect on:\n\nWhat the code does.\nWhat you changed.\nWhat you’re still unsure about.\n\n\nNote: Not sure what a Markdown cell is? Great opportunity to Google or ask ChatGPT! But don’t worry, we’ll discuss this more next week.\n\n\n\n\n\n\n\n\n\nNoneOption 2: Install Anaconda\n\n\n\n\n\n\nFollow the instructions in appendix 19 to install Anaconda and launch JupyterLab.\nCreate a new notebook and run print(\"Hello from Anaconda!\").\nTake a screenshot of your working notebook or write a short note on how it went.\n\n\n\n\n\n\n\n\n\n\nNoneOption 3: Set Up VS Code\n\n\n\n\n\n\nFollow the instructions in appendix 20 to install VS Code and set up Python and Jupyter support.\nOpen a Jupyter notebook and run print(\"Hello from VS Code!\").\nExplore the editor: try renaming a file, changing themes, or opening the terminal.\nWrite a brief reflection on what you liked or didn’t like about this environment.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting Up Your Python Environment</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html",
    "href": "03-python-basics.html",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "",
    "text": "3.1 Try It Before We Start!\nIf you’re new to Python or programming in general, welcome—you’re in the right place. This chapter is where we start turning ideas into code. Just like learning a new language starts with learning a few key words and phrases, learning Python begins with understanding how to work with basic building blocks: numbers, text, and data containers.\nWhy is this important? Because whether you’re analyzing customer data, building a machine learning model, or automating a task at work, you’ll rely on these foundational concepts every single time you write code. Think of this chapter as your toolbox—it may seem simple now, but you’ll keep coming back to these tools throughout your data science journey.\nAnd don’t worry if it doesn’t all click right away. Everyone struggles with syntax or logic at first. Learning to code is more like learning to solve puzzles than memorizing rules. Take your time, experiment, and remember: errors are part of the process (even experienced coders Google stuff constantly).\nBy the end of this chapter, you’ll be able to:\nLet’s dive in.\nLet’s write a few lines of Python code. Run the following in a Python environment (like Google Colab, Jupyter Notebook, or VS Code) and see what happens:\n# Store your name as a string\nname = \"Taylor\"\n\n# Store your age as a number\nage = 22\n\n# Print a personalized message\nprint(\"Hi \" + name + \"! You are \" + str(age) + \" years old.\")\n\n# Calculate how many years until you turn 30\nyears_to_30 = 30 - age\nprint(\"You'll turn 30 in \" + str(years_to_30) + \" years.\")\n\nHi Taylor! You are 22 years old.\nYou'll turn 30 in 8 years.\nWhat just happened?\nDon’t worry if this isn’t totally clear yet—that’s what this chapter is for. We’ll break it all down step by step.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#try-it-before-we-start",
    "href": "03-python-basics.html#try-it-before-we-start",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "",
    "text": "You stored information using variables\nYou worked with different data types (a string and a number)\nYou used basic math and printed a personalized message",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#python-data-types",
    "href": "03-python-basics.html#python-data-types",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.2 Python Data Types",
    "text": "3.2 Python Data Types\nEverything in Python is an object, and every object has a type. A data type tells Python what kind of value you’re working with—whether it’s a number, a piece of text, or a simple True/False flag. Understanding Python’s core data types is a fundamental step toward writing useful programs.\nLet’s walk through a quick introduction to the most common data types you’ll encounter early on. As we progress through this class and book, you’ll explore many more ways to work with and manipulate these data types.\n\n\nNumeric Types\nPython has two main types of numbers:\n\nint (integers): whole numbers like 5, 42, or -100\nfloat (floating-point): numbers with decimals like 3.14, 0.5, or -2.718\n\nYou can do all kinds of math with numbers in Python using simple operators:\n\n10 + 3.5   # Addition\n\n13.5\n\n\n\n10 * 3.5   # Multiplication\n\n35.0\n\n\nGo ahead and run these lines of code in your notebook:\n# Basic operations\n10 - 3.5   # Subtraction\n10 * 3.5   # Multiplication\n10 / 3.5   # Division (always returns a float)\n\n# More math\n10 ** 3.5  # Exponentiation (10^2 = 100)\n10 // 3.5  # Integer division (result is an int)\n10 % 3.5   # Modulus (remainder)\n\n\n\n\n\n\nWarningHeads Up: Python Math Isn’t Always Perfect\n\n\n\nSometimes, Python (like all programming languages) can give results that look a little… off. This usually happens because computers represent decimal numbers using floating-point approximation, which isn’t always exact.\nFor example, you’d expect the following to equal 0.3 but instead…\n\n0.1 + 0.1 + 0.1\n\n0.30000000000000004\n\n\nThis tiny difference is due to how numbers like 0.1 are stored in computer memory. It’s a common issue when doing calculations with decimal values, especially in financial or scientific applications. As we progress, you’ll learn best practices to handle imperfections like this. For now, you could just round it.\n\nround(0.1 + 0.1 + 0.1, ndigits=2)\n\n0.3\n\n\n\n\n\n\nStrings\nA string is a sequence of characters, enclosed in single or double quotes - whichever you use is personal preference but the output in the Python environment will contain single quotes:\n\n\"Hello Taylor!\"\n\n'Hello Taylor!'\n\n\nStrings can be combined and manipulated in many ways:\n\n# Concatenation\n\"Hello\" + \" \" + \"Taylor!\"   # Hello Taylor!\n\n'Hello Taylor!'\n\n\nGo ahead and run these lines of code in your notebook:\n# String repetition\n\"ha\" * 3   # 'hahaha'\n\n# Get first letter (starts at 0)\n\"Taylor\"[0] # 'T'\n\n# Get first three letters\n\"Taylor\"[:3]   # 'Tay'\nStrings are extremely common—you’ll use them to label, format, and present data in readable ways.\n\n\n\n\n\n\nIn fact, as organizations increasingly rely on data-driven decision-making, a significant portion of their data comes in the form of text. Examples include product descriptions, customer feedback, social media posts, and even log files. Understanding how to work with strings is essential for extracting insights and making sense of this unstructured data.\n\n\n\n\n\nBooleans\nA Boolean is a special data type with only two values: True and False.\nis_raining = True\nhas_umbrella = False\n\n\n\n\n\n\nIn Python, True and False are capitalized. Writing true or false (lowercase) will result in a NameError. Always ensure proper capitalization when working with booleans.\n\n\n\nYou’ll mostly use booleans when you write logical conditions, such as comparing values or checking if something is true. Don’t worry, we’ll discuss comparison operators (i.e. &gt;, ==) in a moment.\n\n5 &gt; 3      # True - 5 is greater than 3\n\nTrue\n\n\n\n5 == 3   # False - 5 does not equal 3\n\nFalse\n\n\nGo ahead and run these lines of code in your notebook:\n10 &lt; 9\n10 &gt; 9\n10 &lt;= 9\n10 &gt;= 9\n10 == 10\n\n\nType Checking and Conversion\nYou can check the type of any object using the type() function:\n\nprint(type(10))        # &lt;class 'int'&gt;\nprint(type(\"hello\"))   # &lt;class 'str'&gt;\n\n&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n\n\n\n\n\n\n\n\nTipA Quick Note on print()\n\n\n\n\n\nWhen working in a Jupyter notebook, Python will automatically display the result of the last line in a cell if it’s a value (like a string or number). For example:\n\ntype(10)        \ntype(\"hello\")   # will only show the output of this last line\n\nstr\n\n\nBut in most real Python programs—like scripts, functions, or when running multiple lines—you need to use print() to explicitly display something to the user:\n\nprint(type(10))        # will print print out this output...\nprint(type(\"hello\"))   # and this output\n\n&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n\n\nUsing print()` is a way to say, “Hey Python, show this to me in the output.” You’ll use it all the time for debugging, building interfaces, or just (as in this case) just to tell Python to display the output of each line of code.\n\n\n\nYou can also convert between types using built-in functions. For example, the following converts the integer value of 5 to a string '5'.\n\nstr(5) # '5' (string to int)\n\n'5'\n\n\nAnd the following converts a string '5' to an integer 5:\n\nint('5') # 5 (int to string)\n\n5\n\n\nGo ahead and run these lines of code in your notebook:\nprint(type(3.5))     # &lt;class 'float'&gt;\nprint(type(True))    # &lt;class 'bool'&gt;\nprint(str(3.14))     # '3.14' (convert decimal to string)\nprint(bool(0))       # False (0 is treated as False)\nprint(float(2))      # 2.0 (convert integer to decimal)\nThese conversions come in handy when working with user input or cleaning messy data.\n\n\nKnowledge check\nWork through the following tasks in your notebook.\n\n\n\n\n\n\nNone🍕 What’s the best deal?\n\n\n\n\n\nA 12-inch pizza costs $8. Use the formula for the area of a circle (\\(A = \\pi × r^2\\)) to calculate the cost per square inch of the pizza.\nHints:\n\nRadius (\\(r\\)) is half the diameter\nUse 3.14159 as your approximation for \\(\\pi\\)\nDivide the price by the area to get cost per square inch\n\nNow repeate for a 15-inch pizza that costs $12. Which is a better deal?\n\n\n\n\n\n\n\n\n\nNonePlay with ‘strings’\n\n\n\n\n\nFirst, guess what each line of code will result in. Then run them in your notebook. Were the results what you expected?\nprint(\"Python\" + \"Rocks\")\nprint(\"ha\" * 5)\nprint(\"banana\"[1])\nprint(\"banana\"[::-1])   # Can you guess what this does?\nExtra challenge: Can you use slicing to print just the word \"ana\" from \"banana\"?\n\n\n\n\n\n\n\n\n\nNone🕵🏻‍♂️ Data type detective\n\n\n\n\n\nBefore you run the following, what do you think the data types are for each line? Then, run the code in your notebook to check your answers. Were your predictions correct?\nprint(type(\"True\"))\nprint(type(True))",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#variables-and-the-assignment-operator",
    "href": "03-python-basics.html#variables-and-the-assignment-operator",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.3 Variables and the Assignment Operator",
    "text": "3.3 Variables and the Assignment Operator\nIn the last section, we did a lot of math using the same numbers — 10 and 3.5 — over and over again:\n# Basic operations\n10 - 3.5\n10 * 3.5\n10 / 3.5\nThat works fine for short examples, but imagine writing a program that needs to use the same values in dozens of different places. What if you want to change one of those values later? You’d have to find and update every instance in your code.\nThat’s where variables come in.\n\nWhat Is a Variable?\nA variable is a name that refers to a value. You can think of it like labeling a container that holds something useful—like a number, a word, or even a list of things.\nHere’s how we could rewrite the examples above using variables:\nx = 10\ny = 3.5\n\nprint(x - y)\nprint(x * y)\nprint(x / y)\nMuch cleaner, right?\n\n\n\n\n\n\nImportantWhy Use Variables?\n\n\n\nVariables help you:\n\nAvoid repeating values\nMake your code easier to read and maintain\nUpdate values in one place instead of many\n\nAnd when your programs get more complex, variables become essential for storing user input, results from calculations, or intermediate steps in a data analysis.\n\n\n\n\nThe Assignment Operator: =\nTo create a variable, we use the assignment operator (=). This tells Python to take the value on the right and assign it to the name on the left:\n\ngreeting = \"Hello, world!\"\n\nThis means - store the string \"Hello, world!\" in a variable called greeting\nYou can then reuse that variable:\n\nprint(greeting)\n\nHello, world!\n\n\n\n\n\n\n\n\nIn Python, = does not mean “equal to” like in math. It’s an instruction: assign the value.\n\n\n\n\n\nNaming Variables\nHere are basic rules for naming variables in Python:\n\n✅ Must start with a letter (or an underscore _ as in _name, though that’s typically reserved for special cases—so avoid starting with _ unless you know what you’re doing)\n✅ Can include letters, numbers, and underscores\n🚫 Cannot start with a number\n🚫 Cannot use built-in Python keywords (like if, True, print, etc.)\n\nSome good examples:\nage = 25\nstudent_name = \"Taylor\"\nis_logged_in = True\nUse descriptive names when possible—it makes your code easier for others (and future-you) to understand.\n\n\nReassigning Variables\nVariables can change! When you assign a new value to an existing variable, it overwrites the old one:\n\nx = 5\nx = x + 1  # Now x is 6\n\nprint(x)\n\n6\n\n\nPython always uses the most recent value.\n\n\nYou Can Store Any Type of Value\nYou can assign any data type to a variable—numbers, strings, booleans, and more:\nname = \"Taylor\"           # string\ngpa = 3.85                # float\nis_honors_student = True  # boolean\nAnd Python is flexible — you can even change what type a variable holds:\ngpa = \"3.85\"  # Now it's a string!\nThis is called dynamic typing, and it’s part of what makes Python beginner-friendly.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry It!\n\n\n\n\n\nLet’s return to our earlier example from the start of the chapter:\n# Store your name and age\nname = \"Taylor\"\nage = 22\n\n# Print a custom message\nprint(\"Hi \" + name + \"! You are \" + str(age) + \" years old.\")\n\n# Do a little math\nprint(\"You'll turn 30 in \" + str(30 - age) + \" years.\")\nTry updating the values of name and age to reflect your info. Then tweak the message to include your graduation year or your major. Play around; don’t worry you won’t break anything!",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#comparison-operators",
    "href": "03-python-basics.html#comparison-operators",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.4 Comparison Operators",
    "text": "3.4 Comparison Operators\nIn the previous section on booleans, we saw that comparison expressions—like 5 &gt; 3 — evaluate to either True or False. These expressions are powered by comparison operators, which are used to compare values in Python.\nYou’ll use comparison operators all the time when writing conditions, checking data, filtering results, or writing logic into your programs.\nHere’s a quick cheat sheet of the most common ones:\n\n\n\nOperator\nDescription\nExample\nResult\n\n\n\n\n==\nEqual to\n5 == 5\nTrue\n\n\n!=\nNot equal to\n5 != 3\nTrue\n\n\n&gt;\nGreater than\n10 &gt; 7\nTrue\n\n\n&lt;\nLess than\n4 &lt; 2\nFalse\n\n\n&gt;=\nGreater than or equal to\n3 &gt;= 3\nTrue\n\n\n&lt;=\nLess than or equal to\n8 &lt;= 6\nFalse\n\n\n\nAll of these expressions return a boolean value: True or False. Try running the following lines of code in your notebook:\nprint(10 &gt; 3)      # True\nprint(2 &lt; 1)       # False\nprint(4 == 4.0)    # True (int and float are treated as equal in value)\nprint(4 != 5)      # True\nprint(6 &gt;= 7)      # False\nprint(5 &lt;= 5)      # True\nYou can also compare strings:\nprint(\"apple\" == \"apple\")    # True\nprint(\"Apple\" == \"apple\")    # False (case matters!)\nprint(\"cat\" &lt; \"dog\")         # True (compares alphabetically)\n\n\n\n\n\n\nWarningCommon Pitfalls\n\n\n\nDon’t confuse = and ==:\n\n= is the assignment operator (used to assign a value to a variable)\n== is the comparison operator (used to check if two values are equal)\n\nx = 5           # assignment\nprint(x == 5)   # comparison → True\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneWhich Pizza is the Better Deal?\n\n\n\n\n\nLet’s build on a problem you saw earlier. This time, we’ll:\n\nuse variables to store the cost per square inch of two pizzas and\nthen use a comparison operator to see which one is the better deal.\n\nThe Setup\n\nA 12-inch pizza costs $8\nA 15-inch pizza costs $12\nUse the formula for the area of a circle:\n\n\\(A = \\pi \\times r^2\\)\nUse 3.14159 for π\n\n\nYour Task\n\nCompute the cost per square inch for each pizza\nStore the results in two variables: small_pizza and large_pizza\nUse a comparison operator to check if the smaller pizza is a better or equal deal\n\nHere’s a starting point:\n# Calculate cost per square inch for each pizza\nsmall_pizza = 8 / (3.14159 * (12 / 2) ** 2)\nlarge_pizza = 12 / (3.14159 * (15 / 2) ** 2)\n\n# Compare them (insert proper comparison operator in the blanks)\nprint(small_pizza __ large_pizza)\nWhat does the output of the comparison tell you? Try printing both values first to see how they compare. Which pizza gives you more for your money?\nprint(small_pizza)\nprint(large_pizza)",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#putting-it-all-together-basic-python-in-action",
    "href": "03-python-basics.html#putting-it-all-together-basic-python-in-action",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.5 Putting It All Together: Basic Python in Action",
    "text": "3.5 Putting It All Together: Basic Python in Action\nNow that you’ve learned about Python’s core data types, how to assign values to variables, and how to make comparisons, let’s put it all together into a small real-world example.\nImagine you’re helping manage event registration for a student club. You want to:\n\nStore the number of attendees and the cost per ticket\nCalculate total revenue from the event\nSet a goal for how much you wanted to make\nPrint a basic summary report\nUse comparison logic to see if you met your goal\n\nHere’s how you might write that in Python:\n\n# Number of attendees and ticket price\nattendees = 48\nticket_price = 12.50\n\n# Calculate total revenue\ntotal_revenue = attendees * ticket_price\n\n# Set a revenue goal\nrevenue_goal = 600\n\n# Print a summary message\nprint(\"You sold \" + str(attendees) + \" tickets at $\" + str(ticket_price) + \" each.\")\nprint(\"Total revenue: $\" + str(total_revenue))\n\n# Compare to revenue goal\ngoal_met = total_revenue &gt;= revenue_goal\nprint(\"Did we meet our revenue goal: \" + str(goal_met))\n\nYou sold 48 tickets at $12.5 each.\nTotal revenue: $600.0\nDid we meet our revenue goal: True\n\n\n\nWhat’s going on here?\nIn these 8 lines of code we’ve combined everything we learned across this chapter:\n\nWe used variables to store numbers and reused them in calculations\nWe performed basic math operations using multiplication\nWe used print() to display helpful messages, combining strings and numeric values\nWe used str() to convert numeric and boolean data types to strings\nWe used a comparison operator (&gt;=) to return True or False based on whether our total revenue met the goal\n\n\n\nTry it Yourself!\n\nChange the number of attendees or the ticket price. What happens to total revenue?\nTry changing the revenue goal and see if the result of the comparison changes.\nStretch: Use a GenAI tool like ChatGPT, Claude, or Copilot and ask it to expand upon your code so that it prints ‘Yaaah!’ if we met the revenue goal, or ‘Booo!’ if we didn’t. Then copy the AI’s suggestion into your notebook and test it out. Can you understand what it did? Does the code work the way you expected? If not—can you fix it?\n\n\n\n\n\n\n\nRemember: GenAI tools are great helpers, but not always correct. Always try to understand the code they give you!\n\n\n\nThis is your first step toward building programs that do real work. It may not seem fancy now—but you’ve already written a script that stores, processes, and evaluates real-world data.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#summary-and-whats-next",
    "href": "03-python-basics.html#summary-and-whats-next",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.6 Summary and What’s Next",
    "text": "3.6 Summary and What’s Next\nIn this chapter, you learned how to:\n\nIdentify and use Python’s most common data types—numbers, strings, and booleans\nUse the assignment operator (=) to store and reuse values with variables\nWrite and evaluate comparison expressions that return True or False\nUse print statements to combine and display information\nStart thinking like a programmer by working through simple real-world examples\n\nThese are the essential tools that will support everything you do moving forward—whether you’re analyzing a spreadsheet, building a model, or writing a script to automate a task.\n\nWhat’s Next: From Basics to Real Data\nNow that you’ve learned how to work with individual values and variables, it’s time to start thinking bigger—about how we structure and analyze real-world data. In the next module, we’ll go deeper into three key topics:\n\nJupyter Notebooks: You’ve already seen Jupyter Notebooks in action, but now we’ll explore just how powerful they are. Mastering Jupyter is about more than writing code—it’s about communicating insights clearly. You’ll learn how to:\n\nCombine text and code in the same document using Markdown\nFormat your notebook with headers, bullets, and even equations to make your work easier to understand\nStructure your notebooks as professional, reproducible reports, just like a real data scientist would\n\nPython Data Structures: So far, you’ve worked with individual values like one number or one string. But in data science, we almost never work with just one thing—we work with collections of data. Understanding data structures is essential as we start to clean, transform, and analyze datasets so we’ll cover:\n\nStoring and accessing data using lists (ordered sequences of items)\nOrganizing key-value pairs using dictionaries (think of them like labeled data bins)\nLooping through and manipulate these collections efficiently\n\nImporting Real-World Datasets with Pandas: You’ll also take your first step into real data science work—bringing in external datasets and exploring them with the Pandas library. Pandas is one of the most important tools in any data scientist’s toolbox. It makes it easy to:\n\nRead data from CSV files, Excel, databases, and more\nView, clean, and filter your data\nBegin asking real questions and discovering patterns\n\n\nYou’ve built a strong foundation. Next, we’ll build on it and start working with data the way real analysts and scientists do. Let’s keep going!",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "03-python-basics.html#exercise-build-a-simple-event-summary",
    "href": "03-python-basics.html#exercise-build-a-simple-event-summary",
    "title": "3  Python Basics – Working with Data and Variables",
    "section": "3.7 Exercise: Build a Simple Event Summary",
    "text": "3.7 Exercise: Build a Simple Event Summary\n\n\n\n\n\n\nNoneThe Scenario\n\n\n\n\n\nYour student club is hosting an event and you’re in charge of summarizing registration data. Use what you’ve learned in this chapter to answer the following questions using Python.\nWrite all your code from scratch—no copy-pasting. Try to reason through the logic before typing.\n\nTickets sold: 56\nTicket price: $10.50\nRevenue goal: $600\nEvent name: “Python for Everyone”\n\n\n\n\n\n\n\n\n\n\nNoneYour Tasks\n\n\n\n\n\n\nCreate Variables: Assign appropriate values to variables for:\n\nEvent name\nTickets sold\nTicket price\nRevenue goal\n\nCalculate Total Revenue: Use math operations to calculate the total revenue earned from ticket sales.\nPrint a Summary Report: Use print() and string concatenation to display a message like:\nThe event \"Python for Everyone\" sold 56 tickets at $10.50 each.\nTotal revenue: $588.0\nMet or exceeded goal: False\nStretch Task (Optional): Add a comparison that checks whether your total revenue met or exceeded the revenue goal and prints:\n\n\"Yaaah! We met our goal!\" if the goal was met\n\"Booo! We missed our goal.\" if it was not\n\nHint: Ask a GenAI tool to help you construct the logic! If you can’t get it, don’t worry as we will talk about this later in the course.",
    "crumbs": [
      "Module 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics – Working with Data and Variables</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html",
    "href": "04-jupyter.html",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "",
    "text": "4.1 Benefits:\nIn data science, we don’t just write code — we tell stories with data. Jupyter notebooks are an essential tool because they allow us to blend code, explanatory text, and visualizations in a single document. This makes it easier to understand, share, and reflect on your work.\nHere are a few key reasons why Jupyter notebooks are so widely used and appreciated in the data science world:",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#benefits",
    "href": "04-jupyter.html#benefits",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "",
    "text": "Blending code and context: Jupyter notebooks allow you to seamlessly combine executable Python code with rich text elements using Markdown. This means you can explain your thought process, document your workflow, and display results all in one place—making your analysis easier to follow and reproduce.\nExploratory-friendly: Notebooks are designed for experimentation. You can write and run code in small, manageable chunks (cells), see immediate feedback, and iteratively refine your approach. This makes it easy to test ideas, debug, and learn as you go.\nSharable and visual: Notebooks support inline visualizations, tables, and formatted text, making your work more engaging and accessible. You can easily export notebooks as HTML or PDF files to share with others, ensuring your analysis is both readable and reproducible.\n\n\n\n\n\n\n\nThink of a notebook as your digital lab notebook. Instead of jumping between different tools to write your analysis, calculate results, and explain your thinking, a Jupyter notebook lets you do it all in one place—cleanly combining code, commentary, and visuals in a single, easy-to-follow narrative.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#notebook-anatomy-and-core-features",
    "href": "04-jupyter.html#notebook-anatomy-and-core-features",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.2 Notebook Anatomy and Core Features",
    "text": "4.2 Notebook Anatomy and Core Features\n\n\n\n\n\n\nNote📓 Follow Along in Colab!\n\n\n\nAs you read through this chapter, we encourage you to follow along using the companion notebook in Google Colab (or other editor of choice). This interactive notebook lets you run code examples covered in the chapter—and experiment with your own ideas.\n👉 Open the Jupyter Basics Notebook in Colab.\n\n\n\nCode vs Markdown Cells\nJupyter notebooks are made up of cells, and there are two main types you’ll use regularly: code cells and Markdown cells.\n\nCode cells are where you write and run Python code. When executed, these cells display output directly beneath the cell—whether it’s a simple calculation, a table, or a visualization.\nMarkdown cells allow you to format text to explain your work, provide instructions, insert images, or even write mathematical formulas. These cells support formatting tools like headers, bullet points, bold/italic text, and LaTeX for equations.\n\nBy combining these two types of cells, you can create a clear, readable narrative that both documents your process and shows your results.\n\n\n\n\n\n\nWatch from 5:20-8:45 in this video to see simple examples of code cells and Markdown cells.\n\n\n\n\nTo select the type of a cell (Code or Markdown), use the dropdown menu typically found at the top of the notebook interface. In Jupyter Lab or classic Jupyter Notebook, it will say ‘Code’ or ‘Markdown’—click it to change. In VS Code, you can right-click a cell and choose the type, or use shortcuts like Cmd+M M (Markdown) and Cmd+M Y (Code) on Mac (Ctrl+M equivalents on Windows).\nTo run a cell: Shift + Enter\n\nKnowledge check\n\n\n\n\n\n\nNoneTry It!\n\n\n\n\nCreate and execute a Markdown cell and write the following: “Today I’m learning about Jupyter notebooks!”\nBelow it, create and execute a Code cell that:\n\nDefines a variable, like x = 5\nPrints a message that includes the variable (e.g., print(f'The value of x is {x}'))\n\n\n\n\n\n\n\nMarkdown Basics\nThere’s a lot you can do with Markdown to make your notebooks clearer and more engaging. You can add headings, lists, formatting, and even mathematical expressions to help document your analysis and guide readers through your work.\nHere are a few basics to get you started but check out the cheat sheet below to see more:\n\nHeaders: Use #, ##, ### to create headings of different sizes\nBold and Italic: **bold**, *italic*\nLists: - item for bullet points or 1. item for numbered lists\nEquations: Use LaTeX syntax between dollar signs, like $a^2 + b^2 = c^2$\n\n\n\n\nMarkdown Cheat Sheet (Datacamp)\n\n\nWant to go deeper? You can explore more Markdown syntax in these helpful guides:\n\nJupyter Docs on Markdown: A quick and easy introduction to get you started with basic Markdown syntax.\nMarkdown Guide: A more detailed reference for adding more structure and polish to your notebook explanations.\nColab Markdown Guide notebook: An example Colab notebook that allows you to experiment directly with Markdown syntax - a great playground for practicing different Markdown elements in a live notebook environment. \n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry It!\n\n\n\nBuild on the previous activity by enhancing your notebook with more Markdown elements:\n\nAt the very top of your notebook, add a title using a level-1 header (#).\nAdd a second-level header (##) introducing a new section.\nUnder that header, create a bulleted list—for example, a grocery shopping list.\n(Stretch Goal) Try writing a simple equation in Markdown, such as the formula for the area of a circle: $A = \\pi r^2$.\n\nThis will help you practice using Markdown to better organize and explain your work.\n\n\n\n\n\nCode Execution & State\nOne of the key features of Jupyter notebooks is that they allow you to execute code one cell at a time. This supports experimentation, but also introduces some complexity.\n\nCode is executed in the order you run the cells, not necessarily from top to bottom. This means you can define a variable in one cell and use it later—even if that later cell is above the original one. While this can be convenient, it can also lead to confusion or bugs if you’re not careful.\n\n\n\n\n\n\n\nCautionExample:\n\n\n\n\nIn one code cell, type: x = 42\nNow scroll up and create a new code cell above that one.\nIn that new cell, type: print(x) and run it.\n\nIt will work—because the variable x was already defined in memory. But if someone runs the notebook from top to bottom, it will break. This illustrates why executing cells out of order can lead to unpredictable behavior.\n\n\n\nThe notebook maintains a running memory called the kernel. As long as the kernel is active, all the variables and functions you’ve defined persist in memory. This makes it easy to build on previous work, but if you make a mistake or want a clean slate, you may need to restart the kernel.\n\nA good habit is to periodically restart the kernel and run all cells in order to ensure your notebook works as expected from top to bottom.\n\n\n\nManaging Your Notebook\nWorking in Jupyter notebooks involves managing both your work and the underlying system that runs it. Here are a few key features and best practices to help keep things running smoothly:\n\nSave often: Notebooks autosave regularly, especially when working in a browser-based environment like Jupyter Lab or Colab. However, it’s still a good habit to manually save using Ctrl+S or Cmd+S, especially before running all cells or restarting the kernel.\nRestarting the Kernel: If your notebook starts behaving unpredictably—due to memory overload, undefined variables, or bugs—it’s a good idea to restart the kernel. Restarting wipes the slate clean by clearing all variables from memory, allowing you to re-run the notebook from the top.\nClear Outputs: Before sharing your notebook with others or submitting it for review, consider clearing all outputs (cell results, print statements, plots, etc.). This makes your notebook easier to read and ensures the reader sees only the final results when they run it themselves. You can usually do this via the Kernel or Edit menu.\n\n\n\n\n\n\n\nAgain, can’t highlight this enough, a good practice is to “Restart and Run All” to make sure your notebook runs cleanly from start to finish without relying on any hidden state or cell order.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#when-not-to-use-notebooks",
    "href": "04-jupyter.html#when-not-to-use-notebooks",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.3 When Not to Use Notebooks",
    "text": "4.3 When Not to Use Notebooks\nNotebooks are powerful and flexible, but they aren’t the best fit for every type of task—especially as your work becomes more complex or transitions into production environments.\n\nLimitations:\n\nHard to modularize and test: Notebooks encourage an exploratory, linear style of development. But when you need reusable functions, testable components, or well-structured projects, notebooks can become unwieldy.\nPoor version control: While tools like Git work well with text files, notebooks save code and output together in JSON format. This makes it difficult to track changes or resolve merge conflicts. This is improving but is still less than stellar.\nExecution order issues: Since notebooks allow you to run cells out of order, it’s easy to accidentally create dependencies that break when someone else (or you later) tries to run the notebook top to bottom.\n\n\n\nWhen Notebooks Shine\nJupyter notebooks truly excel in scenarios where exploration, explanation, and communication are key. Here are a few situations where notebooks are particularly well-suited:\n\nLearning and teaching: The ability to mix code, narrative, and output in one place makes notebooks an ideal format for both instruction and self-study.\nExploratory data analysis (EDA): Notebooks allow you to quickly test hypotheses, visualize data, and keep track of your insights along the way.\nPrototyping and trying out ideas: Need to test a small function or compare two approaches? Notebooks provide a fast and flexible way to experiment.\nCreating interactive reports and visualizations: With support for charts, tables, and even interactive widgets, notebooks are a powerful medium for communicating data-driven stories.\n\n\n\nWhen to Use Something Else\nWhile notebooks are fantastic for interactive work, they aren’t ideal when software engineering discipline and scalability are required. Here are scenarios where other tools—like .py scripts, IDEs, or structured repositories—are often a better choice:\n\nBuilding and deploying production applications: Applications that need to be deployed on servers or integrated with other systems require a more modular and structured codebase than notebooks typically provide.\nCreating libraries, APIs, or packages: These projects need reusable components, proper documentation, and automated testing, which are easier to maintain outside of notebooks.\nCollaborating with others using complex Git workflows: Notebooks don’t lend themselves well to detailed version control, especially with branching and merging.\nDeveloping software that requires extensive testing or CI/CD pipelines: Production-grade software often relies on unit testing, linting, and automated builds, which are best handled in traditional .py environments.\n\n\n\n\n\n\n\nIf you don’t yet know what all the bullet points above mean — like Git, APIs, CI/CD, or modular code — that’s completely fine. You’ll encounter these concepts as you progress in your data science journey. For now, just understand that as projects get more complex and move closer to production, notebooks often aren’t the best tool for the job.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#using-.py-files-in-notebooks",
    "href": "04-jupyter.html#using-.py-files-in-notebooks",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.4 Using .py Files in Notebooks",
    "text": "4.4 Using .py Files in Notebooks\nMixing .py scripts with notebooks is common in real-world workflows, especially as your projects grow in complexity or require reusable functions. Instead of writing everything directly in a notebook, you can keep your reusable logic—like data cleaning functions, utility tools, or modeling pipelines—in a .py script and then import or run that script from within your notebook.\nThis approach offers several benefits: you avoid cluttering your notebook with repeated code, you can test pieces of logic more easily, and your codebase becomes easier to maintain and share.\n\nMethods:\nThere are a few different ways to work with .py files inside your notebook, each useful in different situations:\n\n%run your_script.py – This runs the entire Python script as if you had pasted the code into the notebook. Any variables, functions, or classes defined in that script will be available in your notebook’s environment after execution.\n%load your_script.py – This loads the contents of the script into a new notebook cell so you can review or edit it before running. It’s a nice way to examine the code without opening another file.\nimport your_script – This is the standard Python approach for using modules. If your script is in the same directory as your notebook, you can import functions or variables from it using this command. For example, if your script has a function called clean_data(), you could use it in your notebook with your_script.clean_data(). Don’t worry if this is new—we’ll cover imports and module structure later in the class.\n\n\n\n\n\n\n\nNoneExample\n\n\n\n\n\nSee an example of running a .py script with %run your_script.py in this video:\n\n\n\n\n\n\nWhy use .py files?\nOrganizing your code into .py files offers several advantages, especially as your projects grow beyond a single notebook:\n\nReusability: Functions or logic stored in a .py file can be reused across multiple notebooks or scripts without duplication.\nOrganization: Separating logic from analysis helps keep your notebooks clean and focused on storytelling, while your .py files house the supporting code.\nVersion control: Unlike notebooks, .py files are plain text and work much better with Git and other version control tools—making it easier to track changes, review code, and collaborate with others.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry It!\n\n\n\n\nOpen your editor of choice (such as Colab, JupyterLab, or VS Code).\nCreate a new .py file and type the following line into it:\nprint(\"Hello .py scripts!\")\nSave this file as my_first_script.py.\nNow open a new Jupyter notebook.\nIn a code cell in the notebook, type and run:\n%run my_first_script.py\n\nYou should see the message “Hello .py scripts!” printed in the notebook output.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#best-practices-and-pro-tips",
    "href": "04-jupyter.html#best-practices-and-pro-tips",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.5 Best Practices and Pro Tips",
    "text": "4.5 Best Practices and Pro Tips\nFollowing a few best practices can make your Jupyter notebooks more readable, reliable, and professional—both for yourself and others who use your work.\n\nName files clearly: Use descriptive and consistent naming conventions for your notebooks. For example, eda_customer_data.ipynb is much more informative than untitled3.ipynb.\nUse Markdown generously: Narrate your thought process. Include headers, bullet points, and equations to guide the reader through your analysis. Also, take time to format your writing as you would for any professional report—pay attention to grammar, clarity, and structure to ensure your notebook is polished and easy to follow.\nRestart and clear output before sharing: This ensures the notebook runs top-to-bottom and that readers aren’t confused by leftover output or state.\nExport as HTML or PDF: If you’re turning in a notebook or sharing it for review, exporting to a static format can ensure others see exactly what you intended.\n\n\n\n\n\n\n\nTipPro Tip\n\n\n\nStructure your notebook like a story—introduce the goal, describe your approach, show the results, and summarize your findings. Your future self will thank you!\nHere’s a great article that walks through best practices to help you make this a reality.\n\n\n\nWant to Learn More?\nHere are a few helpful references that expand on Jupyter notebook best practices:\n\nProject Jupyter Guidelines\nReal Python – Jupyter Notebook Tips, Tricks, and Shortcuts",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#summary-and-whats-next",
    "href": "04-jupyter.html#summary-and-whats-next",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.6 6. Summary and What’s Next",
    "text": "4.6 6. Summary and What’s Next\nLet’s recap what you’ve learned in this chapter:\n\nJupyter notebooks are an essential tool for blending code, narrative, and output in one place.\nYou now know how to use different cell types, write Markdown, and execute code.\nYou’ve seen how notebooks maintain memory with the kernel and why it’s important to restart and re-run code in order.\nWe covered notebook limitations and when it’s better to switch to .py scripts or more robust development tools.\nYou explored how to organize your code using .py files and how to run them inside your notebooks.\nFinally, you picked up several best practices for writing professional and readable notebooks.\n\n\nNext Up:\nIn the next chapter, we’ll dive into Python’s basic data structures—like lists and dictionaries. These tools are essential for organizing and manipulating data efficiently, and they will serve as the foundation for the rest of your data analysis journey.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "04-jupyter.html#exercise-comparing-movie-theater-snacks",
    "href": "04-jupyter.html#exercise-comparing-movie-theater-snacks",
    "title": "4  Getting Started with Jupyter Notebooks",
    "section": "4.7 Exercise: Comparing Movie Theater Snacks",
    "text": "4.7 Exercise: Comparing Movie Theater Snacks\n\n\n\n\n\n\nNoneScenario\n\n\n\n\n\nYou’re deciding between three snack combos at the movie theater. Each combo includes popcorn and a drink. Your task is to figure out which option gives you the most food value for your money, based on calories per dollar.\nCombo Details:\n\nCombo 1: $7.50 and 500 calories\nCombo 2: $9.00 and 700 calories\nCombo 3: $10.50 and 900 calories\n\n\n\n\n\n\n\n\n\n\nNoneCreate Notebook\n\n\n\n\n\nCreate a new Jupyter notebook and name it something meaningful and relevant for this analysis.\n\n\n\n\n\n\n\n\n\nNoneExecute Code\n\n\n\n\n\n\nUse a code cell to define the following variables for each combo:\n\ncombo_1_price = 7.50\ncombo_1_calories = 500\ncombo_2_price = 9.00\ncombo_2_calories = 700\ncombo_3_price = 10.50\ncombo_3_calories = 900\n\nUse another code cell to calculate the calories per dollar for each combo:\ncombo_1_value = combo_1_calories / combo_1_price\nUse a final code cell to print the results for each combo.\n\n\n\n\n\n\n\n\n\n\nNoneDocument Your Notebook\n\n\n\n\n\nUse Markdown cells to:\n\nGive your notebook a proper title and write a short introduction explaining the goal of the analysis.\nDisplay the formula used: \\(Value = \\frac{Calories}{Price}\\)\nReport your findings: Which combo offers the best value, and how did you determine this?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Getting Started with Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html",
    "href": "05-data-structures.html",
    "title": "5  Introduction to Data Structures",
    "section": "",
    "text": "5.1 Why Data Structures?\nIn the real world, we rarely work with individual pieces of data. Whether we’re analyzing a dataset, building a model, or writing a simulation, we typically work with collections of data. Python provides several built-in ways to store and organize these collections. These tools are called data structures.\nIn this chapter, you will learn how to:\nImagine you want to store the test scores of a student across multiple exams. Instead of creating separate variables for each score, it makes more sense to group them together:\n# instead of this\ngrade_1 = 85\ngrade_2 = 90\ngrade_3 = 88\ngrade_4 = 92\n\n# more convenient to group them together like this\ngrades = [85, 90, 88, 92]\nThis approach allows you to access, modify, and analyze data more efficiently. That’s the power of data structures: they help us organize and operate on collections of data.\nPython includes different types of data structures that allow us to organize data with specific characteristics suited to the task at hand. For example, some data structures preserve the order of elements, while others do not. Some allow us to modify their contents (mutable), and others do not (immutable). Still others let us associate labels or keys with each item, such as names linked to phone numbers. Choosing the right data structure depends on how we need to access and manage the data.\nBuilding on this, each structure in Python has tradeoffs that make it more suitable for specific situations. If you need to keep track of items in a particular order and expect the data to change, a list might be ideal. If you want to store a fixed grouping of values like GPS coordinates, a tuple ensures the data remains unchanged. And when you need to link one piece of information to another—like a person’s name to their phone number—a dictionary’s key-value format is the perfect tool. Understanding these attributes helps you pick the right structure for the task and write more efficient, readable code.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#why-data-structures",
    "href": "05-data-structures.html#why-data-structures",
    "title": "5  Introduction to Data Structures",
    "section": "",
    "text": "Think about a recent experience filling out an online form.\n\nWhat types of data did you need to input?\nDo you think this data should be stored in a structure that allows for changes (mutability)?\nDoes the order of the data matter for how it’s used or displayed? Were there any key-value pairs—like a name linked to an email address—that need to be stored and accessed together?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#lists-ordered-and-mutable",
    "href": "05-data-structures.html#lists-ordered-and-mutable",
    "title": "5  Introduction to Data Structures",
    "section": "5.2 Lists: Ordered and Mutable",
    "text": "5.2 Lists: Ordered and Mutable\n\nWhat is a List?\nA list is an ordered, changeable collection of items. You can add, remove, or modify elements in a list. Lists are among the most commonly used data structures in Python due to their flexibility and ease of use. When you need to keep items in a specific order—like daily stock prices, game scores, or survey responses—a list is often the right choice.\nLists are mutable, meaning you can change their contents after they’re created. This makes them ideal for scenarios where your data updates frequently, such as tracking website visits or logging sensor readings.\n\n\nCreating Lists\nTo create a list, use square brackets [] and separate items with commas. Lists can hold values of any type: numbers, strings, Booleans, or even other lists. Here are three simple examples:\n\nfruits = ['apple', 'banana', 'cherry']\nscores = [85, 90, 88, 92]\nmixed = ['blue', 42, True, 3.14]\n\nThe order in which you define the elements is preserved. We can see this when we evaluate a list - the output will always be in the same order as it was input:\n\nmixed\n\n['blue', 42, True, 3.14]\n\n\nThis makes lists especially helpful for keeping track of sequences where position matters, such as chronological data or ranked preferences. Because lists are mutable, you can also build them incrementally as your data grows or changes during the course of a program.\n\n\nAccessing List Items\nTo access a specific item in a list, you use indexing - which uses brackets ([]) along with the specified location. So, say we want to get the first item from a list:\n\nfruits[1]\n\n'banana'\n\n\nWait a minute! Shouldn’t fruits[1] give the first item in the list? It seems to give the second. This is because indexing in Python starts at zero.\n\n\n\n\n\n\nImportantZero-based Indexing\n\n\n\nPython uses zero-based indexing, which means that the first element in a list has an index of 0, the second has an index of 1, and so on. You can also use negative indexing to count from the end of the list, where -1 is the last item, -2 is the second-to-last, etc.\n\nprint(fruits[0])   # first item\nprint(fruits[2])   # second item\nprint(fruits[-1])  # last item\n\napple\ncherry\ncherry\n\n\nFun reading: Why Python uses 0-based indexing\n\n\nUnderstanding indexing is essential because it affects how you loop through lists, retrieve elements, and manipulate values. For example, if you mistakenly assume that indexing starts at 1, you might accidentally skip or mislabel elements in your data, or you may specify an invalid location which will raise an error like the following:\n\nfruits[4]\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 fruits[4]\n\nIndexError: list index out of range\n\n\n\n\n\nModifying Lists\nOnce a list is created, you can change its contents easily. This mutability makes lists incredibly useful for situations where your data evolves over time. You can add new items using methods like .append() or change existing elements by assigning new values using their index.\n\nfruits.append('orange')\nfruits[1] = 'blueberry'\n\nfruits\n\n['apple', 'blueberry', 'cherry', 'orange']\n\n\nThese operations reflect how dynamic lists can be—whether you’re updating a list of customer names, recording daily measurements, or tracking items in a to-do list, lists let you adjust your data in place without rebuilding the structure from scratch.\n\n\nCommon List Operations\nLists come with many built-in functions and methods that make it easy to analyze and manipulate data. These operations can help you answer questions like: How many elements are in the list? Does it contain a specific item? Can I sort or remove items from it?\n\nlen(fruits)            # 4 (returns the number of elements in the list)\n'apple' in fruits      # True (checks if 'apple' is in the list)\nfruits.remove('apple') # removes the first occurrence of 'apple'\nfruits.sort()          # sorts the list in place (alphabetically or numerically)\nfruits.pop()           # removes and returns the last item\n\nThese methods are especially useful in data wrangling tasks, such as filtering survey responses, cleaning up logs, or ranking scores.\n\n\n\n\n\n\nYou can find many other list operations here: https://docs.python.org/3/tutorial/datastructures.html#data-structures\n\n\n\n\n\nUse Cases\n\nTime series data\nCollections of records (e.g., names, prices, grades)\nMaintaining a dynamic list of inputs that might grow or shrink over time\nStoring results from computations or simulations\nCollecting user inputs or responses in interactive programs\nAnd others!\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry It!\n\n\n\nSay the last 5 days had daily high temperatures of 79, 83, 81, 89, 78.\n\nStore these values in a list.\nNow add a new item to this list that represents today’s high temp of 85.\nNext, suppose the first day’s reading was inaccurate and the actual high temp was 76 — update that value in the list.\nFinally, sort the list to see the temperatures in ascending order.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#tuples-ordered-and-immutable",
    "href": "05-data-structures.html#tuples-ordered-and-immutable",
    "title": "5  Introduction to Data Structures",
    "section": "5.3 Tuples: Ordered and Immutable",
    "text": "5.3 Tuples: Ordered and Immutable\n\nWhat is a Tuple?\nA tuple is similar to a list in that it is an ordered collection of items. However, unlike lists, tuples are immutable, meaning their contents cannot be changed after creation. This immutability makes tuples useful for representing fixed sets of data that should not be altered during a program’s execution.\n\n\n\n\n\n\nThink of tuples as a read-only list. This may be contentious, as described in this blog post. Tuples do have many other capabilities beyond what you would expect from just being “a read-only list,” but for us just beginning now, we can think of it that way.\n\n\n\nTuples are typically used when you want to ensure data integrity, such as storing constant values or configuration settings. They are also commonly used to return multiple values from a function, or to represent simple groupings like coordinates, RGB color values, or database records.\nBecause of their immutability, tuples can be used as keys in dictionaries (we’ll learn what these are shortly), unlike lists. Additionally, they offer slightly better performance than lists when it comes to iteration.\n\n\nCreating Tuples\nTo create a tuple, use parentheses () and separate values with commas. Like lists, tuples can hold values of different data types. However, because tuples are immutable, they are particularly well-suited for storing data that should remain constant throughout your program.\nFor example, if you’re working with geographic coordinates, a tuple ensures the latitude and longitude values stay paired and unchanged:\n\ncoordinates = (39.76, -84.19)\n\nOr you might use a tuple to store a birthdate, where the structure will never need to be modified:\n\nbirthday = (7, 14, 1998)\n\nTuples are also frequently used when functions need to return multiple values, making them both practical and efficient in everyday programming. We’ll see this in action in later sections.\n\n\nAccessing Tuple Items\nIndexing tuples works exactly the same way as indexing lists in Python. You use square brackets [] with a zero-based index to access elements. For example:\n\ncoordinates[0]\n\n39.76\n\n\nJust like lists, you can use negative indices to access elements from the end:\n\nbirthday[-1]\n\n1998\n\n\n\n\n\n\n\n\nWhile both lists and tuples support indexing, remember that tuples are immutable. This means you can read elements by index, but you cannot change them:\n\ncoordinates[0] = 41.62\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 coordinates[0] = 41.62\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\n\n\n\nTuple Unpacking\nTuple unpacking allows you to assign each item in a tuple to its own variable in a single line. This is especially useful when a function returns multiple values or when you’re working with grouped data like coordinates, dimensions, or ranges. It improves readability and simplifies your code when working with known-length tuples.\n\nx, y = coordinates\n\n\nprint(x)\n\n39.76\n\n\n\nprint(y)\n\n-84.19\n\n\nHere, the value of x will be 39.76 and y will be -84.19, corresponding to the first and second elements of the coordinates tuple, respectively. If you try to unpack a tuple into a different number of variables than it has elements, Python will raise an error.\n\n\nWhy Use Tuples?\nTuples are ideal when you want to group together related values that should not be changed once set. Their immutability provides a built-in safeguard against accidental modifications, which is especially helpful when the data must remain consistent throughout the execution of a program. Because they are more lightweight than lists, tuples also offer faster performance in scenarios that involve iteration or large numbers of data groupings. Furthermore, since tuples can be used as keys in dictionaries (unlike lists), they provide a reliable way to map compound keys to values in advanced data structures.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry It!\n\n\n\nGiven the following tuple schooling = ('UC', 'BANA', '4080')\n\nUse indexing to grab the word “BANA”.\nChange the value of “BANA” to “Business Analytics”. What happens?\nUnpack the schooling tuple into three variables: university, program, class_id.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#dictionaries-key-value-pairs",
    "href": "05-data-structures.html#dictionaries-key-value-pairs",
    "title": "5  Introduction to Data Structures",
    "section": "5.4 Dictionaries: Key-Value Pairs",
    "text": "5.4 Dictionaries: Key-Value Pairs\nA dictionary is a collection of key-value pairs, where each unique key maps to a specific value. This structure allows you to organize and retrieve data using meaningful identifiers rather than relying on position, as with lists or tuples. Dictionaries are especially helpful when you need to store data that has a clear label or attribute—such as a student’s name, ID, or grade—making it easy to access or update values using their corresponding keys. This makes dictionaries one of the most powerful and flexible tools for working with structured data in Python.\n\nCreating Dictionaries\nTo create a dictionary, use curly braces {} with key-value pairs separated by colons. Each key must be unique and is typically a string, though it can also be a number or other immutable type. Dictionaries are ideal when you want to store data that has meaningful labels. For instance, instead of remembering that index 0 in a list corresponds to a name and index 1 corresponds to a score, a dictionary lets you associate 'name' directly with a value.\n\nstudent = {\n    'name': 'Jordan',\n    'score': 95,\n    'major': 'Data Science'\n}\n\n\n\nAccessing and Modifying Dictionaries\nTo access a value in a dictionary, you reference its key in square brackets. This allows you to retrieve values without knowing their position, unlike lists or tuples.\n\n# access the value associated with the 'score' key\nstudent['score']\n\n95\n\n\nYou can also update the value of an existing key or add a completely new key-value pair to the dictionary. This mutability makes dictionaries ideal for dynamic data, such as user profiles, configuration settings, or database records.\n\nstudent['score'] = 98  # update the score\nstudent['grad_year'] = 2025  # add a new key-value pair\n\nstudent\n\n{'name': 'Jordan', 'score': 98, 'major': 'Data Science', 'grad_year': 2025}\n\n\n\n\nDictionary Methods\nDictionaries come with several built-in methods that allow you to efficiently access and interact with their contents. These methods help you retrieve just the keys, just the values, or both together. They are particularly useful when you’re iterating over a dictionary to analyze or transform its contents.\nTry the following operations and see what their outputs are:\nstudent.keys()         # dict_keys(['name', 'score', 'major', 'grad_year'])\nstudent.values()       # dict_values(['Jordan', 98, 'Data Science', 2025])\nstudent.items()        # dict_items([('name', 'Jordan'), ('score', 98), ...])\n'name' in student      # True (checks if a key exists in the dictionary)\ndel student['major']   # removes the 'major' key and its associated value\nUsing these methods allows you to build flexible programs that can dynamically explore or modify structured data—especially when working with JSON data from APIs, reading metadata from files, or updating attributes of users or records.\n\n\nUse Cases\nDictionaries are incredibly versatile and can be found in many practical programming and data science scenarios:\n\nLookup tables: Use dictionaries to map inputs to outputs, such as converting state abbreviations to full names or translating category codes to descriptions.\nStructured data: Store rows of data as dictionaries where keys represent column names—this mirrors how data is stored in JSON format and is common when working with web APIs or data from files.\nFeature storage in machine learning models: Organize features (like ‘age’, ‘income’, or ‘region’) with their corresponding values for each individual observation, allowing for dynamic construction and retrieval of input data for models.\nConfiguration settings: Store user preferences or system parameters that can be easily accessed or updated using descriptive keys.\nData merging and deduplication: Use keys to uniquely identify records, making it easier to combine and clean data from different sources.\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry It!\n\n\n\n\nCreate a dictionary that stores a classmate’s nickname, phone number, and age.\njohn_doe = {\n    'nickname': _______,\n    'phone_number': __,\n    'age': __\n}\nCreate another dictionary called classmate2 with similar information for a different classmate.\nCombine these two dictionaries into a new dictionary called contacts, where each key is the classmate’s name and the value is their corresponding dictionary.\nAdd a new entry to the contacts dictionary for a third classmate, including their name, phone number, and age.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#summary",
    "href": "05-data-structures.html#summary",
    "title": "5  Introduction to Data Structures",
    "section": "5.5 Summary",
    "text": "5.5 Summary\nIn this chapter, we focused on three of Python’s most commonly used data structures: lists, tuples, and dictionaries. These structures provide the foundation for how data is organized and accessed in most Python programs, especially in data science workflows.\n\nLists are ordered and mutable, making them ideal when you need to maintain and modify a sequence of items.\nTuples are ordered but immutable, which is helpful for fixed sets of values where data integrity is important.\nDictionaries store labeled data as key-value pairs, offering a flexible way to organize and retrieve data by meaningful identifiers.\n\n\n\n\n\n\n\n\n\n\nData Structure\nType (via type())\nDescription & When to Use\nExample\n\n\n\n\nList\nlist\nOrdered, mutable collection. Use when you need to keep items in sequence and change them later.\nscores = [85, 90, 88, 92]\n\n\nTuple\ntuple\nOrdered, immutable collection. Ideal for fixed groupings like coordinates or dates.\ncoordinates = (39.76, -84.19)\n\n\nDictionary\ndict\nUnordered, mutable key-value pairs. Great for labeled data or fast lookups.\nstudent = {'name': 'Jordan', 'score': 95}\n\n\n\nAs we move forward in this book, we’ll explore more advanced data structures that build on these basics and help you perform data mining tasks more efficiently. For now, having a solid understanding of these core structures will serve as a crucial building block for your continued work in Python and data science.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "05-data-structures.html#exercise-student-records-management",
    "href": "05-data-structures.html#exercise-student-records-management",
    "title": "5  Introduction to Data Structures",
    "section": "5.6 Exercise: Student Records Management",
    "text": "5.6 Exercise: Student Records Management\nYou’ve been asked to build a simple data tracking system for a small classroom.\n\n\n\n\n\n\nNoneCreate a List\n\n\n\n\n\n\nMake a list called student_names that includes the names of 5 students.\nAdd a new student to the list.\nRemove the second student in the list.\nSort the list alphabetically and print the final list.\n\n\n\n\n\n\n\n\n\n\nNoneUse a Tuple\n\n\n\n\n\n\nCreate a tuple named classroom_location that stores the building name and room number, such as (\"Lindner Hall\", 315).\nUnpack the tuple into two variables: building and room, and print each one with a label.\n\n\n\n\n\n\n\n\n\n\nNoneBuild a Dictionary\n\n\n\n\n\n\nCreate a dictionary named student_info for one of the students, including their name, major, and graduation year.\nAdd a new key for GPA with a value of your choice.\nPrint out all the keys, all the values, and the full dictionary.\n\n\n\n\n\n\n\n\n\n\nNoneReflection\n\n\n\n\n\nThink through what you just did:\n\nWhat makes a list a good choice for student_names? Is there an alternative approach you could’ve taken?\nWhy would we use a tuple for classroom_location?\nWhat makes a dictionary a good choice for student_info?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Structures</span>"
    ]
  },
  {
    "objectID": "06-libraries.html",
    "href": "06-libraries.html",
    "title": "6  Packages, Libraries, and Modules",
    "section": "",
    "text": "6.1 Why Do We Care?\nAs a data scientist, you won’t be writing every piece of code from scratch. Python’s true power comes from its ecosystem of packages, libraries, and modules that extend its core functionality. These tools allow you to analyze data, build machine learning models, visualize patterns, and automate complex tasks with just a few lines of code.\nIn this chapter, we’ll explore what packages, libraries, and modules are, how to use them, and get hands-on with both built-in and third-party tools that will support you throughout this course.\nBy the end of this chapter, you will be able to:\nYou’ve probably heard that Python is one of the most popular languages in data science—but what makes it so powerful? One big reason is that Python gives you access to an enormous number of libraries that other people have written and shared.\nA library is a reusable collection of code—functions, tools, or entire frameworks—that someone else has written to make your life easier. Think of libraries as toolkits: instead of building a hammer every time you need to drive a nail, you just grab the hammer from your toolbox.\nPython libraries can help you:\nBy using the right library, you can accomplish in just a few lines of code what might otherwise take hours of work.\nLibraries are organized into modules, which are just files that group related code together. And when a collection of modules is bundled up and made shareable, we call that a package.\nThroughout this course, you’ll learn how to use some of the most powerful and popular Python libraries for data science. But first, let’s get familiar with the different types of libraries available and how to start using them.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#why-do-we-care",
    "href": "06-libraries.html#why-do-we-care",
    "title": "6  Packages, Libraries, and Modules",
    "section": "",
    "text": "Do complex math with a single function call.\nRead and clean messy data files.\nVisualize data with beautiful plots.\nTrain machine learning models.\n\n\n\n\n\n\n\n\n\n\nNoteTerminology: Modules, Libraries, and Packages\n\n\n\n\n\nThese three terms often get used interchangeably, but they have specific meanings in Python: * Module: A single .py file that contains Python code—functions, variables, classes—that you can reuse. For example, the math module lets you do mathematical calculations.\n\nModule: A single .py file that contains Python code—functions, variables, classes—that you can reuse. For example, the math module lets you do mathematical calculations.\nLibrary: A collection of related modules bundled together. For example, pandas is a library that includes several modules for data manipulation.\nPackage: A directory containing one or more modules or libraries, with an __init__.py file that tells Python it’s a package. You can think of a package as the container that holds libraries and modules.\n\nIn practice, you’ll often hear “library” and “package” used to refer to the same thing: something you install with pip and then import to use in your code. That’s okay! At this point in your learning, understanding the subtle differences between these terms is not critical. What’s more important is knowing that Python’s modular structure allows you to mix and match tools depending on your needs and that these tools make your work much more efficient.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#standard-library-vs-third-party-libraries",
    "href": "06-libraries.html#standard-library-vs-third-party-libraries",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.2 Standard Library vs Third-Party Libraries",
    "text": "6.2 Standard Library vs Third-Party Libraries\nOne of Python’s greatest strengths is its large collection of prebuilt tools. These tools fall into two broad categories: the standard library, which comes bundled with every Python installation, and third-party libraries, which you can download and install as needed.\n\nThe Standard Library\nThe standard library is like Python’s starter toolbox. It includes modules for doing math, generating random numbers, managing dates and times, reading and writing files, and even accessing the internet. Because it’s included with Python, you can use these modules right away—no installation required.\nFor example:\n\nWant to calculate square roots? Use the math module.\nNeed to simulate randomness? Try random.\nCurious where your code is running? The os module has answers.\n\nThese are great building blocks and perfect for learning foundational programming skills.\n\n\n\n\n\n\nTip📚 Want to Learn More?\n\n\n\nYou can explore the full list of available modules in Python’s standard library by visiting the official documentation here: https://docs.python.org/3/library/index.html\n\n\n\n\nThird-Party Libraries\nAs powerful as the standard library is, it doesn’t cover everything. That’s where third-party libraries come in. These are tools developed and maintained by the Python community to solve specific problems more efficiently. To use them, you’ll typically install them using a package manager called pip.\nFor example:\n\nNeed to work with large datasets? Use pandas.\nWant to make beautiful visualizations? Try matplotlib and seaborn.\nWant fast numerical computation? You’ll love numpy.\n\nThese libraries aren’t included by default, but they’re easy to install—and essential for doing real-world data science.\n\n\n\n\n\n\n\nStandard Library\nThird-Party Library\n\n\n\n\nAlready installed with Python\nInstalled manually (via pip)\n\n\nExamples: math, os, random, datetime\nExamples: pandas, numpy, matplotlib, seaborn\n\n\nNo internet needed to use\nRequires internet to install\n\n\n\nUnderstanding the difference between these two categories will help you know when to reach for built-in tools versus when to seek out more powerful external solutions.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneExploring Python Libraries\n\n\n\nVisit the following two documentation pages:\n\nPython math module (Standard Library)\nPandas library (Third-Party)\n\nThen answer the following questions:\n\nWhat is one function provided by the math module that you could use in one of your other classes? Briefly describe what it does.\nWhat is one feature or function of the pandas library that stands out to you? How might it help in data analysis?\nBased on your experience browsing both pages, what are some differences you notice between standard library documentation and third-party library documentation?\n\nTip: This exercise is not about memorizing everything. It’s about familiarizing yourself with how to explore documentation and recognizing the types of functionality different libraries provide.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#working-with-the-standard-library",
    "href": "06-libraries.html#working-with-the-standard-library",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.3 Working with the Standard Library",
    "text": "6.3 Working with the Standard Library\nThe standard library is like Python’s built-in Swiss Army knife. It includes dozens of modules for common programming tasks—and because it’s part of every Python installation, you can start using it immediately without needing to install anything.\n\nTo use a module from the standard library, you simply use the import statement. Once imported, you can access its functions and tools using dot notation.\nFor example, to use the math library to calculate the square root of a number we need to first import the math library and then we can access the square root function like below:\n\nimport math\n\nmath.sqrt(9)\n\n3.0\n\n\n\nThere are many useful standard libraries - here is an extremely incomplete list of some of the modules you might wish to explore and learn about:\n\nos and sys: Tools for interfacing with the operating system, including navigating file directory structures and executing shell commands\nmath and cmath: Mathematical functions and operations on real and complex numbers\nitertools: Tools for constructing and interacting with iterators and generators\nfunctools: Tools that assist with functional programming\nrandom: Tools for generating pseudorandom numbers\npickle: Tools for object persistence: saving objects to and loading objects from disk\njson and csv: Tools for reading JSON-formatted and CSV-formatted files.\nurllib: Tools for doing HTTP and other web requests.\ndatetime: Tools for working with dates, times, and time intervals\n\nLet’s see a few of these in action.\n\n🧮 math: Mathematical Functions\nThe math module gives you access to a wide variety of mathematical operations beyond basic arithmetic. These include square roots, exponentials, logarithms, trigonometric functions, and more.\nHere’s a simple example:\n\nimport math\n\nprint(math.ceil(9.2))        # Returns the smallest integer greater than or equal to 9.2 (i.e., 10)\nprint(math.factorial(6))     # Returns the factorial of 6 (i.e., 720)\nprint(math.sqrt(121))        # Returns the square root of 121 (i.e., 11.0)\n\n10\n720\n11.0\n\n\n\n\n📁 os: Interacting with the Operating System\nThe os module allows you to interact with your computer’s operating system. It’s helpful for navigating file paths, checking directories, and automating file-related tasks.\nExample:\n\nimport os\n\n# Get the current working directory\nprint(\"Current working directory:\", os.getcwd())\n\n# List the files and folders in that directory\nprint(\"Contents of the directory:\", os.listdir())\n\nCurrent working directory: /home/runner/work/uc-bana-4080/uc-bana-4080/book\nContents of the directory: ['references.bib', '10-manipulating-data.qmd', '.quarto', '15-data-viz-bokeh.qmd', '12-joining-data.qmd', '04-jupyter.qmd', '06-libraries.qmd', '_quarto.yml', '11_aggregating_data.quarto_ipynb', '03-python-basics.quarto_ipynb', '13-data-viz-pandas.qmd', '06-libraries.quarto_ipynb', '08-dataframes.quarto_ipynb', '17-iteration-statements.quarto_ipynb', '18-functions.qmd', 'index.quarto_ipynb', '16-control-statements.quarto_ipynb', '07-importing-data.qmd', '03-python-basics.html', '05-data-structures.html', '05-data-structures.qmd', '05-data-structures.quarto_ipynb', '09-subsetting.qmd', 'index.qmd', '16-control-statements.qmd', 'images', '14-data-viz-matplotlib.quarto_ipynb', '15-data-viz-bokeh.quarto_ipynb', '02-preparing-for-code.html', '08-dataframes.qmd', '01-intro-data-mining.html', '02-preparing-for-code.qmd', '17-iteration-statements.qmd', '07-importing-data.quarto_ipynb', '10-manipulating-data.quarto_ipynb', '12-joining-data.quarto_ipynb', 'references.qmd', '11_aggregating_data.qmd', '18-functions.quarto_ipynb', 'site_libs', 'summary.qmd', '_book', '.gitignore', '04-jupyter.html', '09-subsetting.quarto_ipynb', '01-intro-data-mining.qmd', '14-data-viz-matplotlib.qmd', '99-anaconda-install.qmd', 'index.html', '99-vscode-install.qmd', '13-data-viz-pandas.quarto_ipynb', 'cover.png', '03-python-basics.qmd']\n\n\n\n\n📅 datetime: Working with Dates and Times\nThe datetime module is essential for handling and manipulating dates and times. You can get the current date, format it in a specific way, or calculate time differences.\nExample:\n\nimport datetime\n\n# Get today's date\ntoday = datetime.date.today()\nprint(\"Today's date is:\", today)\n\n# Create a specific date\nbirthday = datetime.date(1980, 8, 24)\nprint(\"Birth date:\", birthday)\n\n# How many days have I been on earth\ndays_alive = (today-birthday).days\nprint(\"Days on earth:\", days_alive)\n\nToday's date is: 2025-09-01\nBirth date: 1980-08-24\nDays on earth: 16444\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneUsing the Standard Library\n\n\n\nStart a Jupyter notebook and write code that does the following using only the standard library:\n\nUses the datetime module to print today’s date.\nUses the math module to calculate the square root of 625.\nUses the random module to simulate rolling a 6-sided die five times.\nUses the os module to print your current working directory.\n\nTip: Try running each part separately and look up the documentation if you’re unsure how to use a module. This is great practice for solving problems using tools that are already built into Python! If you encounter any difficulties or have questions, don’t hesitate to ask ChatGPT or Copilot for assistance!",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#pythons-data-science-ecosystem-third-party-modules",
    "href": "06-libraries.html#pythons-data-science-ecosystem-third-party-modules",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.4 Python’s Data Science Ecosystem & Third-party Modules",
    "text": "6.4 Python’s Data Science Ecosystem & Third-party Modules\nThe standard library is powerful, but when you begin working on real-world data science tasks, you’ll quickly find yourself needing more specialized tools. This is where third-party libraries come in. One of the things that makes Python useful, especially within the world of data science, is its ecosystem of third-party modules. These are external packages developed by the Python community and are not included with Python by default.\nThese packages are typically hosted on a package manager and Python Package Index (PyPI for short) and Anaconda are the two primary public package managers for Python. As of June 2025 there was about 645,000 packages available through the Python Package Index (PyPI)! Usually, you can ask Google or ChatGPT about what you are trying to do, and there is often a third party module to help you do it.\nTo install packages from PyPI we can either type the following in our terminal:\n# install from PyPI\npip install pkg_name\nAlternatively, you can install Python packages directly from a code cell in Jupyter notebooks by prefixing the pip install command with an exclamation mark (!). For example:\n# install within Jupyter notebook cell\n!pip install pkg_name\nThis allows you to manage dependencies without leaving your notebook environment.\n\n\n\n\n\n\nNoteTry It!\n\n\n\nGo ahead and see if you can pip install the pandas library.\n\n\nOnce a package is installed, you can import the library using the import statement and optionally assign it an alias (a short nickname), which is a common convention:\n# install within Jupyter notebook cell\n!pip install pandas\n# import package using the `pd` alias\nimport pandas as pd\nThroughout this course we’ll use several third party libraries focused on data science – for example Numpy, SciPy, Pandas, Scikit-learn, among others. Let’s look at some examples of these third party packages to give you a flavor of what they do. Don’t worry, we’ll go into some of these more thoroughly in later lessons!\n\nNumPy\nNumPy provides an efficient way to store and manipulate multi-dimensional dense arrays in Python. The important features of NumPy are:\n\nIt provides an ndarray structure, which allows efficient storage and manipulation of vectors, matrices, and higher-dimensional datasets.\nIt provides a readable and efficient syntax for operating on this data, from simple element-wise arithmetic to more complicated linear algebraic operations.\n\n\n\n\n\n\n\nAlthough the package is officially spelled “NumPy” you will commonly see it referred to as Numpy and numpy across the Python ecosystem (and even within this course).\n\n\n\nIn the simplest case, NumPy arrays look a lot like Python lists. For example, here is an array containing the range of numbers 1 to 9:\n\nimport numpy as np\n\nx = np.arange(1, 10)\nx\n\narray([1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n\n\n\n\n\nStandard convention is to import numpy as the np alias.\n\n\n\nNumPy’s arrays offer both efficient storage of data, as well as efficient element-wise operations on the data. For example, to square each element of the array, we can apply the ** operator to the array directly:\n\nx ** 2\n\narray([ 1,  4,  9, 16, 25, 36, 49, 64, 81])\n\n\nThis element-wise operation capability (commonly referred to as vectorization) is extremely useful but is not available in base Python. In base Python, if you had a list of these same numbers you would have to loop through each element in the list and compute the square of each number:\n\nx_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n# not supported\nx_list ** 2\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[7], line 4\n      1 x_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n      3 # not supported\n----&gt; 4 x_list ** 2\n\nTypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'\n\n\n\nWe would need to use a non-vectorized approach that iterates through each element and computes the square. The below illustrates the much more verbose non-vectorized approach that produces the same result:\n\n\n\n\n\n\nDon’t worry about the syntax, you will learn about this in a later lesson. Just note how the above approach with Numpy is far more convenient!\n\n\n\n\nx_squared = [val ** 2 for val in x_list]\nx_squared\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nNumpy also provides a host of other vectorized arithmetic capabilities. For example, we can compute the mean of a list with the following:\n\nnp.mean([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\nnp.float64(5.5)\n\n\nUnlike Python lists (which are limited to one dimension), NumPy arrays can be multi-dimensional. For example, here we will reshape our x array into a 3x3 matrix:\n\nm = x.reshape((3, 3))\nm\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\nA two-dimensional array is one representation of a matrix, and NumPy knows how to efficiently do typical matrix operations. For example, you can compute the transpose using .T:\n\nm.T\n\narray([[1, 4, 7],\n       [2, 5, 8],\n       [3, 6, 9]])\n\n\nor a matrix-vector product using np.dot:\n\nnp.dot(m, [5, 6, 7])\n\narray([ 38,  92, 146])\n\n\nand even more sophisticated operations like eigenvalue decomposition:\n\nnp.linalg.eigvals(m)\n\narray([ 1.61168440e+01, -1.11684397e+00, -1.30367773e-15])\n\n\nSuch linear algebraic manipulation underpins much of modern data analysis, particularly when it comes to the fields of machine learning and data mining.\n\n\nPandas\nPandas is a much newer package than Numpy, and is in fact built on top of it. What Pandas provides is a labeled interface to multi-dimensional data, in the form of a DataFrame object that will feel very familiar to users of R and related languages. DataFrames in Pandas look something like the following.\n\n\n\n\n\n\nIt is a common convention to import Pandas with the pd alias.\n\n\n\n\nimport pandas as pd\n\ndf = pd.DataFrame({'label': ['A', 'B', 'C', 'A', 'B', 'C'],\n                   'value': [1, 2, 3, 4, 5, 6]})\n\nThe Pandas interface allows you to do things like select columns by name:\n\ndf['label']\n\n0    A\n1    B\n2    C\n3    A\n4    B\n5    C\nName: label, dtype: object\n\n\nApply string operations across string entries:\n\ndf['label'].str.lower()\n\n0    a\n1    b\n2    c\n3    a\n4    b\n5    c\nName: label, dtype: object\n\n\nCompute statistical aggregations for numerical columns:\n\ndf['value'].sum()\n\nnp.int64(21)\n\n\nAnd, perhaps most importantly, do efficient database-style joins and groupings:\n\ndf.groupby('label').sum()\n\n\n\n\n\n\n\n\nvalue\n\n\nlabel\n\n\n\n\n\nA\n5\n\n\nB\n7\n\n\nC\n9\n\n\n\n\n\n\n\nHere in one line we have computed the sum of all objects sharing the same label, something that is much more verbose (and much less efficient) using tools provided in Numpy and core Python.\n\n\n\n\n\n\nIn future lessons we will go much deeper into Pandas and you’ll also see a large dependency on using Pandas as we start exploring other parts of the statistical computing ecosystem (i.e. visualization, machine learning).\n\n\n\n\n\nMatplotlib\nMatplotlib is currently the most popular scientific visualization packages in Python. Even proponents admit that its interface is sometimes overly verbose, but it is a powerful library for creating a large range of plots.\n\n\n\n\n\n\nIt is a common convention to import Matplotlib with the plt alias.\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')  # make graphs in the style of R's ggplot\n\nNow let’s create some data and plot the results:\n\nx = np.linspace(0, 10)  # range of values from 0 to 10\ny = np.sin(x)           # sine of these values\nplt.plot(x, y);         # plot as a line\n\n\n\n\n\n\n\n\nThis is the simplest example of a Matplotlib plot; for ideas on the wide range of plot types available, see Matplotlib’s online gallery.\n\n\n\n\n\n\nAlthough you’ll be exposed to some Matplotlib throughout this course, we will tend to focus on other third-party visualization packages that are simpler to use.\n\n\n\n\n\nSciPy\nSciPy is a collection of scientific functionality that is built on Numpy. The package began as a set of Python wrappers to well-known Fortran libraries for numerical computing, and has grown from there. The package is arranged as a set of submodules, each implementing some class of numerical algorithms. Here is an incomplete sample of some of the more important ones for data science:\n\nscipy.fftpack: Fast Fourier transforms\nscipy.integrate: Numerical integration\nscipy.interpolate: Numerical interpolation\nscipy.linalg: Linear algebra routines\nscipy.optimize: Numerical optimization of functions\nscipy.sparse: Sparse matrix storage and linear algebra\nscipy.stats: Statistical analysis routines\n\nFor example, let’s take a look at interpolating a smooth curve between some data\n\nfrom scipy import interpolate\n\n# choose eight points between 0 and 10\nx = np.linspace(0, 10, 8)\ny = np.sin(x)\n\n# create a cubic interpolation function\nfunc = interpolate.interp1d(x, y, kind='cubic')\n\n# interpolate on a grid of 1,000 points\nx_interp = np.linspace(0, 10, 1000)\ny_interp = func(x_interp)\n\n# plot the results\nplt.figure()  # new figure\nplt.plot(x, y, 'o')\nplt.plot(x_interp, y_interp);\n\n\n\n\n\n\n\n\nWhat we see is a smooth interpolation between the points.\n\n\nOther Data Science Packages\nBuilt on top of these tools are a host of other data science packages, including general tools like Scikit-Learn for machine learning, Scikit-Image for image analysis, Seaborn for statistical visualization, and Statsmodels for statistical modeling; as well as more domain-specific packages like AstroPy for astronomy and astrophysics, NiPy for neuro-imaging, and many, many more.\nNo matter what type of scientific, numerical, or statistical problem you are facing, it’s likely there is a Python package out there that can help you solve it.\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneInstalling and Using Third-Party Libraries\n\n\n\n\nUse pip (or !pip in a Jupyter notebook) to install the following third-party libraries:\n\nnumpy\nbokeh\n\nOnce installed, copy and run the following code in a Jupyter notebook:\nimport numpy as np\nimport bokeh.plotting\nimport bokeh.io\n\nbokeh.io.output_notebook()\n\n# Generate plotting values\nt = np.linspace(0, 2*np.pi, 200)\nx = 16 * np.sin(t)**3\ny = 13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t)\n\np = bokeh.plotting.figure(height=250, width=275)\np.line(x, y, color='red', line_width=3)\ntext = bokeh.models.Label(x=0, y=0, text='BANA 4080', text_align='center')\np.add_layout(text)\n\nbokeh.io.show(p)\nAfter executing the code:\n\nWhat shape is created by this visualization?\nWhat part of the code controls the label “BANA 4080”?\nHow does numpy assist in preparing the data for visualization?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#summary",
    "href": "06-libraries.html#summary",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.5 Summary",
    "text": "6.5 Summary\nIn this chapter, you learned that one of Python’s greatest strengths is its rich ecosystem of reusable code—organized into modules, libraries, and packages. These tools allow you to write less code, solve complex problems more efficiently, and leverage the collective efforts of the Python community.\nWe began by discussing the difference between Python’s standard library and third-party libraries. You learned how to use built-in tools like math, os, and datetime for essential tasks, and how to install and import third-party packages using pip.\nWe then explored Python’s thriving data science ecosystem—highlighting libraries like NumPy, Pandas, Matplotlib, Seaborn, and SciPy. These libraries will be your go-to tools for data wrangling, statistical modeling, and visualization throughout this course and beyond.\nYou don’t need to memorize every function from every package right now. Instead, focus on building awareness of what kinds of tools exist and how to access them. The more you practice, the more fluent you’ll become in navigating and using Python’s expansive ecosystem.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "06-libraries.html#end-of-chapter-exercise-putting-python-libraries-to-work",
    "href": "06-libraries.html#end-of-chapter-exercise-putting-python-libraries-to-work",
    "title": "6  Packages, Libraries, and Modules",
    "section": "6.6 End-of-Chapter Exercise: Putting Python Libraries to Work",
    "text": "6.6 End-of-Chapter Exercise: Putting Python Libraries to Work\nCreate a new Jupyter notebook titled chapter-6-libraries-practice.ipynb. This notebook should include markdown cells to describe each section of your work and code cells to perform the tasks below. Be sure to run your code and document your findings or observations.\n\n\n\n\n\n\nNonePart 1: Standard Library Practice\n\n\n\n\n\nUse the following standard libraries: math, os, datetime, and random.\n\nMath Practice: Compute the square root, factorial, and log (base 10) of any number you choose using the math module.\nWorking with Files: Use the os module to print your current working directory and list all files in it.\nRandom Simulation: Use the random module to simulate flipping a coin 20 times. Count how many times you get heads vs. tails.\nDate Math: Use the datetime module to:\n\nPrint today’s date.\nCreate your birthday as a date object.\nCalculate how many days old you are.\n\n\n\n\n\n\n\n\n\n\n\nNonePart 2: Installing and Using Third-Party Libraries\n\n\n\n\n\n\nUse !pip install to install the following:\n\nnumpy\nbokeh\n\nCopy and run the following code to generate a heart-shaped plot:\nimport numpy as np\nimport bokeh.plotting\nimport bokeh.io\n\nbokeh.io.output_notebook()\n\nt = np.linspace(0, 2*np.pi, 200)\nx = 16 * np.sin(t)**3\ny = 13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t)\n\np = bokeh.plotting.figure(height=250, width=275)\np.line(x, y, color='red', line_width=3)\ntext = bokeh.models.Label(x=0, y=0, text='BANA 4080', text_align='center')\np.add_layout(text)\n\nbokeh.io.show(p)\nAdd a markdown cell answering the following:\n\nWhat shape is drawn?\nHow does numpy help in generating the plot data?\nWhat part of the code adds the label “BANA 4080”?\n\n\n\n\n\n\n\n\n\n\n\nNonePart 3: Summary Reflection\n\n\n\n\n\nIn a markdown cell, write 3–5 sentences reflecting on what you learned in this chapter. Consider:\n\nWhat surprised you about Python’s library ecosystem?\nWhich module or package do you think you’ll use the most in this course?\nDoes knowing about these tools change the way you think about programming?",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Packages, Libraries, and Modules</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html",
    "href": "07-importing-data.html",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "",
    "text": "7.1 From Disk to DataFrame: How Data Enters Python\nImagine you’re working as a summer intern for a real estate analytics firm. On your first day, your manager hands you a file: “Here’s the raw data for the Ames, Iowa housing market. Let’s start by pulling it into Python and taking a quick look around.”\nYou double-click the file — it’s filled with rows and rows of numbers, codes, and column headers you don’t quite understand. Where do you even begin?\nIn this chapter, you’ll walk through the exact steps you’d take in that situation. You’ll load real data, explore it using Python, and start to build your intuition for what’s inside a dataset. You won’t be doing full analysis yet — but you will learn how to get your bearings using one of Python’s most powerful tools: Pandas.\nLater chapters will teach you how to clean, transform, and analyze data — but first, you need to bring it into Python and take a look around.\nBy the end of this chapter, you will be able to:\nPython stores its data in memory - this makes it relatively quickly accessible but can cause size limitations in certain fields. In this class we will mainly work with small to moderate data sets, which means we should not run into any space limitations.\nPython memory is session-specific, so quitting Python (i.e. shutting down JupyterLab) removes the data from memory. A general way to conceptualize data import into and use within Python:\nHere is a visualization of this process:",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#from-disk-to-dataframe-how-data-enters-python",
    "href": "07-importing-data.html#from-disk-to-dataframe-how-data-enters-python",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "",
    "text": "Python does provide tooling that allows you to work with big data via distributed data (i.e. Pyspark) and relational databrases (i.e. SQL).\n\n\n\n\n\nData sits in on the computer/server - this is frequently called “disk”\nPython code can be used to copy a data file from disk to the Python session’s memory\nPython data then sits within Python’s memory ready to be used by other Python code\n\n\n\n\n\nPython memory",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#importing-delimited-files-with-read_csv",
    "href": "07-importing-data.html#importing-delimited-files-with-read_csv",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.2 Importing Delimited Files with read_csv()",
    "text": "7.2 Importing Delimited Files with read_csv()\nText files are a popular way to store and exchange tabular data. Nearly every data application supports exporting to CSV (Comma Separated Values) format or another type of text-based format. These files use a delimiter — such as a comma, tab, or pipe symbol — to separate elements within each line. Because of this consistent structure, importing text files into Python typically follows a straightforward process once the delimiter is identified.\nPandas provides a very efficient and simple way to load these types of files using its read_csv() function. While there are other approaches available (such as Python’s built-in csv module), Pandas is preferred for its ease of use and direct creation of a DataFrame — the primary tabular data structure used throughout this course.\n\nIn the example below, we use read_csv() to load a dataset listing some information on aircraft.\n\n\n\n\n\n\nPlease note that you must have internet access for this example to work.\nIn this first example, we will demonstrate how to import data directly from a URL. This approach is useful when your data is hosted online and you want to access it directly within your analysis.\nLater in this chapter, we will discuss how to import data that resides on your local computer.\n\n\n\n\nimport pandas as pd\n\nplanes = pd.read_csv('https://tinyurl.com/planes-data')\n\nWe see that our imported data is represented as a DataFrame:\n\ntype(planes)\n\npandas.core.frame.DataFrame\n\n\nWe can look at it in the Jupyter notebook, since Jupyter will display it in a well-organized, pretty way.\n\nplanes\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3317\nN997AT\n2002.0\nFixed wing multi engine\nBOEING\n717-200\n2\n100\nNaN\nTurbo-fan\n\n\n3318\nN997DL\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS AIRCRAFT CO\nMD-88\n2\n142\nNaN\nTurbo-fan\n\n\n3319\nN998AT\n2002.0\nFixed wing multi engine\nBOEING\n717-200\n2\n100\nNaN\nTurbo-fan\n\n\n3320\nN998DL\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS CORPORATION\nMD-88\n2\n142\nNaN\nTurbo-jet\n\n\n3321\nN999DN\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS CORPORATION\nMD-88\n2\n142\nNaN\nTurbo-jet\n\n\n\n\n3322 rows × 9 columns\n\n\n\nThis is a nice representation of the data, but we really do not need to display that many rows of the DataFrame in order to understand its structure. Instead, we can use the head() method of data frplanes to look at the first few rows. This is more manageable and gives us an overview of what the columns are. Note also the the missing data was populated with NaN.\n\nplanes.head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneGetting Started with Ames Housing Data\n\n\n\nUse the Pandas library to complete the following tasks.\n\nLoad the Data: Import the Ames housing dataset using the following URL - https://tinyurl.com/ames-raw\nVerify the Object Type: What type of Python object is the result of your import? Use the type() function to confirm.\nPreview the Data: Use the .head() method to print the first few rows of the dataset. Based on the output:\n\nWhat are some of the column names you see?\nWhat kinds of values are stored in these columns?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#file-paths",
    "href": "07-importing-data.html#file-paths",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.3 File Paths",
    "text": "7.3 File Paths\nIn the previous example, we imported data directly from a URL; however, datasets often reside on our computer and we need to specify the file path to read them from. For example, rather than import the planes.csv data from the URL we used above, I can read in that same dataset as follows.\n\nplanes = pd.read_csv('../data/planes.csv')\n\nplanes.head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nBut to understand why I use '../data/planes.csv' in the code above, we need to spend a little time talking about file paths.\n\nIt’s important to understand where files exist on your computer and how to reference those paths. There are two main approaches:\n\nAbsolute paths\nRelative paths\n\nAn absolute path always contains the root elements and the complete list of directories to locate the specific file or folder. For the planes.csv file, the absolute path on my computer is:\n\nimport os\n\nabsolute_path = os.path.abspath('../data/planes.csv')\nabsolute_path\n\n'/home/runner/work/uc-bana-4080/uc-bana-4080/data/planes.csv'\n\n\nI can always use this absolute path in pd.read_csv():\n\nplanes = pd.read_csv(absolute_path)\n\nIn contrast, a relative path is a path built starting from the current location. For example, say that I am operating in a directory called “Project A”. If I’m working in “my_notebook.ipynb” and I have a “my_data.csv” file in that same directory:\n# illustration of the directory layout\nProject A\n├── my_notebook.ipynb\n└── my_data.csv\nThen I can use this relative path to import this file: pd.read_csv('my_data.csv'). This just means to look for the ‘my_data.csv’ file relative to the current directory that I am in.\nOften, people store data in a “data” directory. If this directory is a subdirectory within my Project A directory:\n# illustration of the directory layout\nProject A\n├── my_notebook.ipynb\n└── data\n    └── my_data.csv\nThen I can use this relative path to import this file: pd.read_csv('data/my_data.csv'). This just means to look for the ‘data’ subdirectory relative to the current directory that I am in and then look for the ‘my_data.csv’ file.\nSometimes, the data directory may not be in the current directory. Sometimes a project directory will look the following where there is a subdirectory containing multiple notebooks and then another subdirectory containing data assets. If you are working in “notebook1.ipynb” within the notebooks subdirectory, you will need to tell Pandas to go up one directory relative to the notebook you are working in to the main Project A directory and then go down into the data directory.\n# illustration of the directory layout\nProject A\n├── notebooks\n│   ├── notebook1.ipynb\n│   ├── notebook2.ipynb\n│   └── notebook3.ipynb\n└── data\n    └── my_data.csv\nI can do this by using dot-notation in my relative path specification - here I use ‘..’ to imply “go up one directory relative to my current location”: pd.read_csv('../data/my_data.csv'). And this is why is used '../data/planes.csv' in the code at the beginning of this section, because my directory layout is:\nProject A\n├── notebooks\n│   ├── notebook1.ipynb\n│   ├── notebook2.ipynb\n│   └── notebook3.ipynb\n└── data\n    └── planes.csv\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry Different File Paths\n\n\n\nDownload the ames_raw.csv dataset and save the file in a folder structure like this:\nProject A\n├── notebooks\n│   └── import_demo.ipynb\n└── data\n    └── ames_raw.csv\nNow, do the following:\n\nImport using a relative path: In import_demo.ipynb, write the code to import the file using a relative path.\nImport using an absolute path: Use Python’s os.path.abspath() to determine the full absolute path of the file. Then use that path in pd.read_csv().\nReflection questions (write your answers in a Markdown cell):\n\nWhich method feels easier or more flexible to you?\nIn what situations might you prefer absolute over relative paths?\n\n\n\n\nGreat idea — adding an example using Google Colab will help students who are working in the cloud and need to upload files directly from their local machine. Here’s a clean, student-friendly example you can drop into your chapter:",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#importing-data-in-google-colab",
    "href": "07-importing-data.html#importing-data-in-google-colab",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.4 Importing Data in Google Colab",
    "text": "7.4 Importing Data in Google Colab\nIf you’re working in Google Colab, your files aren’t stored on your local machine — you’re running code on a cloud-based virtual machine. That means reading in local files (like a .csv on your desktop) works a little differently.\nHere’s how you can upload a file directly from your computer into Colab:\nfrom google.colab import files\n\n# This will open a file picker in Colab\nuploaded = files.upload()\nOnce you select the file you want to upload (e.g., planes.csv), Colab will store it temporarily in your session and make it available to use just like any other file:\nimport pandas as pd\n\n# Now you can load the file into a DataFrame\nplanes = pd.read_csv('planes.csv')\n\n\n\n\n\n\nFiles uploaded this way only persist for the current Colab session. If you close the browser or restart your runtime, you’ll need to re-upload the file.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#inspecting-your-dataframe",
    "href": "07-importing-data.html#inspecting-your-dataframe",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.5 Inspecting Your DataFrame",
    "text": "7.5 Inspecting Your DataFrame\nAfter importing, the data is stored as a DataFrame — the core data structure in Pandas. And with DataFrames, there are several ways to start understanding some basic, descriptive information about our data. For example, we can get the dimensions of our DataFrame. Here, we see that we have 3,322 rows and 9 columns.\n\nplanes.shape\n\n(3322, 9)\n\n\nWe can also see what type of data each column is. For example, we see that the tailnum column data type is object, the year column is a floating point (float64), and engines is an integer (int64).\n\nplanes.dtypes\n\ntailnum          object\nyear            float64\ntype             object\nmanufacturer     object\nmodel            object\nengines           int64\nseats             int64\nspeed           float64\nengine           object\ndtype: object\n\n\nThe following are the most common data types that appear frequently in DataFrames.\n\nboolean - only two possible values, True and False\ninteger - whole numbers without decimals\nfloat - numbers with decimals\nobject - typically strings, but may contain any object\ndatetime - a specific date and time with nanosecond precision\n\n\n\n\n\n\n\nBooleans, integers, floats, and datetimes all use a particular amount of memory for each of their values. The memory is measured in bits. The number of bits used for each value is the number appended to the end of the data type name. For instance, integers can be either 8, 16, 32, or 64 bits while floats can be 16, 32, 64, or 128. A 128-bit float column will show up as float128. Technically a float128 is a different data type than a float64 but generally you will not have to worry about such a distinction as the operations between different float columns will be the same.\n\n\n\nWe can also use the info() method, which provides output similar to dtypes, but also shows the number of non-missing values in each column along with more info such as:\n\nType of object (always a DataFrame)\nThe type of index and number of rows\nThe number of columns\nThe data types of each column and the number of non-missing (a.k.a non-null)\nThe frequency count of all data types\nThe total memory usage\n\n\nplanes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3322 entries, 0 to 3321\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   tailnum       3322 non-null   object \n 1   year          3252 non-null   float64\n 2   type          3322 non-null   object \n 3   manufacturer  3322 non-null   object \n 4   model         3322 non-null   object \n 5   engines       3322 non-null   int64  \n 6   seats         3322 non-null   int64  \n 7   speed         23 non-null     float64\n 8   engine        3322 non-null   object \ndtypes: float64(2), int64(2), object(5)\nmemory usage: 233.7+ KB\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneInspecting the Ames Housing Data\n\n\n\nIn the previous section, you imported the ames_raw.csv dataset. Now let’s explore its structure using the tools you just learned.\n\nHow big is the dataset?: Use .shape to find out how many rows and columns are in the dataset.\nWhat kinds of data are stored?: Use .dtypes to print out the data types of all columns.\n\nHow many columns are object type?\nHow many are float or int?\n\nDig deeper with .info()\n\nHow many non-null values are in each column?\nHow much memory does the dataset use?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#attributes-methods",
    "href": "07-importing-data.html#attributes-methods",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.6 Attributes & Methods",
    "text": "7.6 Attributes & Methods\n\nWe’ve seen that we can use the dot-notation to access functions in libraries (i.e. pd.read_csv()). We can use this same approach to access things inside of objects. What’s an object? Basically, a variable that contains other data or functionality inside of it that is exposed to users. Consequently, our DataFrame item is an object.\nIn the above code, we saw that we can make different calls with our DataFrame such as planes.shape and planes.head(). An observant reader probably noticed the difference between the two – one has parentheses and the other does not.\nAn attribute inside an object is simply a variable that is unique to that object and a method is just a function inside an object that is unique to that object.\n\n\n\n\n\n\nVariables inside an object are often called attributes and functions inside objects are called methods.\nattribute: A variable associated with an object and is referenced by name using dotted expressions. For example, if an object o has an attribute a it would be referenced as o.a\nmethod: A function associated with an object and is also referenced using dotted expressions but will include parentheses. For example, if an object o has a method m it would be called as o.m()\n\n\n\nEarlier, we saw the attributes shape and dtypes. Another attribute is columns, which will list all column names in our DataFrame.\n\nplanes.columns\n\nIndex(['tailnum', 'year', 'type', 'manufacturer', 'model', 'engines', 'seats',\n       'speed', 'engine'],\n      dtype='object')\n\n\nSimilar to regular functions, methods are called with parentheses and often take arguments. For example, we can use the tail() method to see the last n rows in our DataFrame:\n\nplanes.tail(3)\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n3319\nN998AT\n2002.0\nFixed wing multi engine\nBOEING\n717-200\n2\n100\nNaN\nTurbo-fan\n\n\n3320\nN998DL\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS CORPORATION\nMD-88\n2\n142\nNaN\nTurbo-jet\n\n\n3321\nN999DN\n1992.0\nFixed wing multi engine\nMCDONNELL DOUGLAS CORPORATION\nMD-88\n2\n142\nNaN\nTurbo-jet\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will be exposed to many of the available DataFrame methods throughout this course!\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneAttributes vs. Methods in the Ames Housing Data\n\n\n\nUsing the ames_raw DataFrame you’ve already imported:\n\nList all column names: Use the appropriate attribute to display all the column names in the dataset.\nPreview the last 5 rows: Use the appropriate method to display the last five rows of the dataset.\nIdentify each line of code: For each of the following, decide whether it’s a method or an attribute:\n\names.columns\names.head()\names.shape\names.tail(10)\names.dtypes\n\nReflect In a markdown cell, briefly explain in your own words:\n\nWhat’s the difference between an attribute and a method?\nHow can you tell which one you’re using?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#common-dataframe-errors",
    "href": "07-importing-data.html#common-dataframe-errors",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.7 Common DataFrame Errors",
    "text": "7.7 Common DataFrame Errors\nAs you’re learning to work with DataFrames in Pandas, you’ll likely encounter a few common errors. Don’t worry — these are normal and part of the learning process. This section introduces a few of the most frequent issues and how to fix them.\n\nForgetting Parentheses When Using a Method\nOne of the most common mistakes is confusing methods with attributes. Remember: methods require parentheses () — even if they don’t take arguments.\names.head     # 🚫 Returns the method itself, not the data\names.head()   # ✅ Correct — this returns the first few rows\n\n\nTypos in Column Names\nColumn names in a DataFrame must be typed exactly as they appear. They’re case-sensitive and must match spacing and punctuation.\names['SalePrice']     # ✅ Correct\names['saleprice']     # 🚫 KeyError: 'saleprice'\n\n\n\n\n\n\nTip: Use ames.columns to check exact column names.\n\n\n\n\n\nFileNotFoundError When Loading a File\nThis happens when the file path is incorrect or the file isn’t in the expected location.\n# 🚫 Incorrect: file doesn't exist at this path\npd.read_csv('data.csv')\n\n# ✅ Correct (based on your current working directory)\npd.read_csv('../data/ames_raw.csv')\n\n\n\n\n\n\nTip: Use os.getcwd() to check your working directory, and os.path.abspath() to confirm the full path.\n\n\n\n\n\nUsing Dot Notation with Column Names that Contain Spaces or Special Characters\nPandas allows you to access columns using dot notation only if the column name is a valid Python variable name.\names.SalePrice     # ✅ Works (if column is named 'SalePrice')\names.MS Zoning     # 🚫 SyntaxError\n\n# ✅ Use bracket notation instead\names['MS Zoning']\n\n\nConfusing Methods That Don’t Exist\nSometimes, learners assume there’s a method for something that doesn’t exist.\names.rows()    # 🚫 AttributeError: 'DataFrame' object has no attribute 'rows'\n\n\n\n\n\n\nTip: Use dir(ames) to see a list of available methods and attributes, or use tab-completion in Jupyter to explore.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#other-file-types",
    "href": "07-importing-data.html#other-file-types",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.8 Other File Types",
    "text": "7.8 Other File Types\nSo far, we’ve focused on CSVs — the most common format for tabular data. But Python, especially through the Pandas library, can import a wide variety of file types beyond just delimited text files.\nHere are a few common formats you might encounter:\n\nExcel spreadsheets (.xls, .xlsx)\nJSON files — often used for APIs and web data\nPickle files — Python’s own format for saving data structures\nSQL databases — such as SQLite, PostgreSQL, or MySQL\nParquet and Feather — efficient storage formats for big data workflows\n\nIn most cases, Pandas provides a convenient read_ function to handle the import process. For example, let’s look at how we can import an Excel file directly — without converting it to a CSV first.\n\nImporting Excel Files with read_excel()\nExcel is still one of the most widely used tools for storing and sharing data. And while many users convert Excel files into CSVs before importing them into Python, Pandas allows you to skip that step entirely.\nTo import data directly from an Excel workbook, you can use the read_excel() function. But first, you may need to install an additional dependency:\n# Run this in your terminal if you haven’t already\npip install openpyxl\nIn this example, we’ll import a mock dataset of grocery store products stored in a file called products.xlsx (download here).\n\n# Preview the available sheets in the workbook\nproducts_excel = pd.ExcelFile('../data/products.xlsx')\nproducts_excel.sheet_names\n\n['metadata', 'products data', 'grocery list']\n\n\nTo load a specific sheet from this workbook:\n\n\n\n\n\n\nIf you don’t explicitly specify a sheet name, Pandas will default to importing the first worksheet in the file.\n\n\n\n\nproducts = pd.read_excel('../data/products.xlsx', sheet_name='products data')\nproducts.head()\n\n\n\n\n\n\n\n\nproduct_num\ndepartment\ncommodity\nbrand_ty\nx5\n\n\n\n\n0\n92993\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n1\n93924\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n2\n94272\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n3\n94299\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n4\n94594\nNON-FOOD\nPET\nPRIVATE\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s an additional video provided by Corey Schafer that you might find useful. It covers importing and exporting data from multiple different sources.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#summary",
    "href": "07-importing-data.html#summary",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.9 Summary",
    "text": "7.9 Summary\nIn this chapter, you learned how to import real-world data into Python and begin exploring it using Pandas — one of the most important tools in the data science workflow.\nYou started with a realistic scenario: opening up a raw dataset and figuring out how to make sense of it. You learned how to read in delimited files (like CSVs), use relative and absolute file paths, and inspect your data using essential DataFrame attributes and methods like .shape, .dtypes, .info(), and .head().\nYou also saw how Pandas supports a wide variety of file formats beyond CSV, including Excel, JSON, Pickle, and SQL databases — making it a flexible tool for working with nearly any type of data source.\nThis chapter was focused on getting your data into Python and taking a first look around. In upcoming chapters, we’ll dig into the heart of data science: cleaning, wrangling, summarizing, and uncovering patterns in your data to support real-world decision making.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "07-importing-data.html#exercise-covid-19-cases-at-u.s.-colleges",
    "href": "07-importing-data.html#exercise-covid-19-cases-at-u.s.-colleges",
    "title": "7  Importing Data and Exploring Pandas DataFrames",
    "section": "7.10 Exercise: COVID-19 Cases at U.S. Colleges",
    "text": "7.10 Exercise: COVID-19 Cases at U.S. Colleges\nThe New York Times published a dataset tracking COVID-19 cases at colleges and universities across the United States. You can read about this dataset here. In this exercise, you’ll download and explore that dataset to practice the skills you’ve learned in this chapter.\n\n\n\n\n\n\nNoneStep 1: Download the Data\n\n\n\n\n\nDownload the dataset directly from this GitHub URL: colleges.csv\nSet up your project directory to look like this:\nBANA 4080\n├── notebooks\n│   └── covid_analysis.ipynb\n└── data\n    └── colleges.csv\n\n\n\n\n\n\n\n\n\nNoneStep 2: Import the Data\n\n\n\n\n\nIn your covid_analysis.ipynb notebook:\n\nImport Pandas\nLoad the dataset using a relative path\nUse the type() function to confirm that the data is stored as a DataFrame\n\n\n\n\n\n\n\n\n\n\nNoneStep 3: Explore the Structure\n\n\n\n\n\nUse the following DataFrame attributes and methods:\n\n.shape — how many rows and columns are in the dataset?\n.columns — what variables are included?\n.head() — what do the first few rows of the dataset look like?\n.info() — are there any missing values?\n\n\n\n\n\n\n\n\n\n\nNoneStep 4: Reflect\n\n\n\n\n\nIn a Markdown cell, write a short summary addressing the following:\n\nWhat is this dataset tracking?\nWhat are some variables you’d want to explore further?\nWhat kinds of questions could you answer with this data in future chapters?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Importing Data and Exploring Pandas DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html",
    "href": "08-dataframes.html",
    "title": "8  Deeper Dive on DataFrames",
    "section": "",
    "text": "8.1 Learning objectives\nImagine you’ve just imported a new airline dataset into Python. It includes hundreds of rows listing airline names and their carrier codes. Your manager asks, “Can you quickly pull a list of all airline names? And which carrier code has the longest name?” Before you can answer, you need to understand what’s inside that DataFrame.\nIn the previous chapter, we focused on how to import datasets into Python using Pandas — a crucial first step in any data analysis workflow. Now that we can get data into Python, we need to understand what we’re actually working with. This chapter takes a closer look at Pandas DataFrames and their building blocks, the Series.\nBy deepening your understanding of the DataFrame structure and how to access and manipulate data within it, you’ll build a foundation for future chapters focused on cleaning, transforming, and analyzing real-world data.\nAt the end of this lesson you should be able to:\nTo illustrate our points throughout this lesson, we’ll use the following airlines data which includes the name of the airline carrier and the airline carrier code:\nimport pandas as pd\n\ndf = pd.read_csv('../data/airlines.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#learning-objectives",
    "href": "08-dataframes.html#learning-objectives",
    "title": "8  Deeper Dive on DataFrames",
    "section": "",
    "text": "Explain the difference between DataFrames and Series\nAccess and manipulate data within DataFrames and Series\nSet and manipulate index values\n\n\n\n\n\n\n\nNote📓 Follow Along in Colab!\n\n\n\nAs you read through this chapter, we encourage you to follow along using the companion notebook in Google Colab (or other editor of choice). This interactive notebook lets you run code examples covered in the chapter—and experiment with your own ideas.\n👉 Open the Dataframes Notebook in Colab.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#understanding-the-dataframe-structure",
    "href": "08-dataframes.html#understanding-the-dataframe-structure",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.2 Understanding the DataFrame Structure",
    "text": "8.2 Understanding the DataFrame Structure\nA DataFrame is a two-dimensional, labeled data structure—similar to a table in Excel or a SQL database—that is used to store and manipulate structured data. You can think of it as a spreadsheet-like object where the data is organized into rows and columns.\nEach column in a DataFrame is actually a special type of object in Pandas called a Series. A Series is a one-dimensional array-like object that holds data and a corresponding index for labeling each entry. While the concept of a Series might be new to you, it’s fundamental to how Pandas works under the hood. Understanding how Series operate is essential because much of your interaction with DataFrames involves accessing and manipulating individual Series.\n\ndf.info()\ndf.shape\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 16 entries, 0 to 15\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   carrier  16 non-null     object\n 1   name     16 non-null     object\ndtypes: object(2)\nmemory usage: 388.0+ bytes\n\n\n(16, 2)\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneExploring Airport Data\n\n\n\nDownload and import the airports data into a DataFrame called airports.\n\nHow many rows and columns are in the dataset? Use .shape and .info() to help.\nWhat are the column names and data types? Are any surprising?\nTry printing the first 5 and last 5 rows using .head() and .tail(). What types of data does this dataset contain?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#series-the-building-blocks-of-dataframes",
    "href": "08-dataframes.html#series-the-building-blocks-of-dataframes",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.3 Series: The Building Blocks of DataFrames",
    "text": "8.3 Series: The Building Blocks of DataFrames\n\nColumns (and rows) of a DataFrame are actually Series objects. A Series is a one-dimensional labeled array capable of holding any data type — such as integers, strings, or even objects. You can think of it as a single column of data with a label on every entry. When you select a single column from a DataFrame, Pandas returns that column as a Series object.\n\n\n\n\n\n\nExample Series\n\n\n\n\nFigure 8.1: Source: Python Programming for Data Science\n\n\n\nYou can extract a Series using bracket notation with the column name:\n\ncarrier_column = df['carrier']\ncarrier_column\n\n0     9E\n1     AA\n2     AS\n3     B6\n4     DL\n5     EV\n6     F9\n7     FL\n8     HA\n9     MQ\n10    OO\n11    UA\n12    US\n13    VX\n14    WN\n15    YV\nName: carrier, dtype: object\n\n\nThis looks very much like a column of data — and it is — but under the hood, it’s a different type of object than the full DataFrame:\n\ntype(carrier_column)\n\npandas.core.series.Series\n\n\nThis confirms that what you extracted is a Series. You can verify its one-dimensional nature with:\n\ncarrier_column.shape  # One dimension (just the number of rows)\n\n(16,)\n\n\nCompare this with the shape of the full DataFrame, which has both rows and columns:\n\ndf.shape  # Two dimensions (rows, columns)\n\n(16, 2)\n\n\nIt’s important to be familiar with Series because they are fundamentally the core of DataFrames.\n\n\n\n\n\n\nExample DataFrame\n\n\n\n\nFigure 8.2: Source: Python Programming for Data Science\n\n\n\nUnderstanding this distinction is important, because many of the functions and behaviors available to Series differ from those of DataFrames. As we continue working with data, we’ll frequently switch between viewing data as Series and viewing it as part of a full DataFrame.\nInterestingly, when you extract a single row using .loc[], Pandas also returns a Series. In that case, the index of the Series becomes the column names of the original DataFrame:\n\n\n\n\n\n\n.loc[] is an accessor that allows us to retrieve rows and columns by labels. We’ll explore accessors more in the next chapter, but for now, just know we’re using it here to get the first row of the DataFrame.\n\n\n\n\nfirst_row = df.loc[0]  # Using .loc to access the first row of the DataFrame\ntype(first_row)        # pandas.core.series.Series\n\npandas.core.series.Series\n\n\nThis reinforces the idea that both columns and rows, when isolated, are treated as Series.\nAnother difference lies in the methods available to each. Series come with some specialized methods, such as .to_list(), which converts the data to a basic Python list. This method is not available on DataFrames:\n\ncarrier_column.to_list()  # works\n\n['9E',\n 'AA',\n 'AS',\n 'B6',\n 'DL',\n 'EV',\n 'F9',\n 'FL',\n 'HA',\n 'MQ',\n 'OO',\n 'UA',\n 'US',\n 'VX',\n 'WN',\n 'YV']\n\n\n\n# This will raise an error\ndf.to_list()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/tmp/ipykernel_12491/1197199109.py in ?()\n      1 # This will raise an error\n----&gt; 2 df.to_list()\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/generic.py in ?(self, name)\n   6314             and name not in self._accessors\n   6315             and self._info_axis._can_hold_identifiers_and_holds_name(name)\n   6316         ):\n   6317             return self[name]\n-&gt; 6318         return object.__getattribute__(self, name)\n\nAttributeError: 'DataFrame' object has no attribute 'to_list'\n\n\n\n\n\n\n\n\n\nAs you continue through this book, you’ll learn many methods that apply specifically to either Series or DataFrames, and some that work on both. Becoming familiar with the structure of each will help you develop the intuition to choose and apply the right methods in the right context.\n\n\n\nFinally, Series tend to print more compactly and are useful when you’re only interested in a single set of values — like a single variable or row. Their lightweight format makes them a convenient choice in many day-to-day data tasks. \n\nKnowledge Check\n\n\n\n\n\n\nNoneDigging into Columns and Rows\n\n\n\nUsing the airports DataFrame:\n\nSelect the alt column and assign it to a variable called altitudes. What type of object is altitudes?\nUse type() and .shape to confirm it’s a Series and check its dimensionality.\nExtract the first row using .loc[0]. What type of object is this? What is its index made up of?\nCompare the output of airports[\"alt\"] and airports[[\"alt\"]]. What do you think is the difference between the two?\nTry calling .to_list() on both the airports[\"alt\"] and airports[[\"alt\"]] objects. What happens?\nWhat happens if you try using dot notation like airports.alt? When does this work vs. when could you see this failing?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#creating-and-indexing-a-series",
    "href": "08-dataframes.html#creating-and-indexing-a-series",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.4 Creating and Indexing a Series",
    "text": "8.4 Creating and Indexing a Series\n\nFirst, let’s create our own Series object from scratch – they don’t always need to come from a DataFrame. Here, we pass a list in as an argument and it will be converted to a Series:\n\ns = pd.Series([10, 20, 30, 40, 50])\ns\n\n0    10\n1    20\n2    30\n3    40\n4    50\ndtype: int64\n\n\nThis gives us a one-dimensional structure that prints a little differently than a DataFrame. There are three main parts of the Series to pay attention to:\n\nThe values (10, 20, 30…)\nThe dtype, short for data type (in this case, int64)\nThe index (0, 1, 2… by default)\n\nValues are fairly self-explanatory — they’re the list elements we passed in. The data type describes what kind of values are being stored (numbers, strings, etc.). Series are often homogeneous — holding only one data type — though technically they can hold a mix (which we’ll avoid for clarity).\nThe index is where things get more interesting. Every Series has an index, which functions much like the keys in a dictionary — each label maps to a value. By default, Pandas assigns numeric labels starting at 0:\n\ns.index  # Default index (RangeIndex)\ns\n\n0    10\n1    20\n2    30\n3    40\n4    50\ndtype: int64\n\n\nBut we can change the index to be more meaningful. For example, we could relabel the values using letters:\n\ns.index = ['a', 'b', 'c', 'd', 'e']\ns\n\na    10\nb    20\nc    30\nd    40\ne    50\ndtype: int64\n\n\nNow, if we want to access the value 40, we can do so by label:\n\ns['d']\n\nnp.int64(40)\n\n\nThis flexibility is powerful. Recall how rows in a DataFrame are also Series. Let’s revisit our airline DataFrame:\n\ndf.head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\nIf we extract the first row using .loc[0], we get:\n\nfirst_row = df.loc[0]\nfirst_row\n\ncarrier                   9E\nname       Endeavor Air Inc.\nName: 0, dtype: object\n\n\nHere, the index labels of the Series are the original column names:\n\nfirst_row['carrier']  # returns '9E'\n\n'9E'\n\n\nThis demonstrates how Series behave consistently whether they’re extracted from columns, rows, or created from scratch. Understanding indexing will help you fluently navigate, reshape, and analyze your data as we move forward.\n\nKnowledge Check\n\n\n\n\n\n\nNoneBuild Your Own Series\n\n\n\n\nCreate a Series with the following values: [33, 64, 77, 22, 51]. Assign it to temps.\nWhat are the index labels? What is the data type?\nChange the index to letters: ['a', 'b', 'c', 'd', 'e'].\nWhat value is associated with the label 'c'?\nReassign the index back to numbers. What’s one reason you might prefer named indexes over numbers?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#indexing-in-dataframes",
    "href": "08-dataframes.html#indexing-in-dataframes",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.5 Indexing in DataFrames",
    "text": "8.5 Indexing in DataFrames\n\nIt’s not just Series that have indexes! DataFrames have them too. Take a look at the carrier DataFrame again and note the bold numbers on the left.\n\ndf.head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\nThese numbers are an index, just like the one we saw on our example Series. And DataFrame indexes support similar functionality.\n\n# Our index is a range from 0 (inclusive) to 16 (exclusive).\ndf.index\n\nRangeIndex(start=0, stop=16, step=1)\n\n\nWhen loading in a DataFrame, the default index will always be 0 to N-1, where N is the number of rows in your DataFrame. This is called a RangeIndex. Selecting individual rows by their index can also be done with the .loc accessor.\n\n# Get the row at index 4 (the fifth row).\ndf.loc[4]\n\ncarrier                      DL\nname       Delta Air Lines Inc.\nName: 4, dtype: object\n\n\nAs with Series, DataFrames support reassigning their index. However, with DataFrames it often makes sense to change one of your columns into the index. This is analogous to a primary key in relational databases: a way to rapidly look up rows within a table.\nIn our case, maybe we will often use the carrier code (carrier) to look up the full name of the airline. In that case, it would make sense to set the carrier column as our index.\n\ndf = df.set_index('carrier')\ndf.head()\n\n\n\n\n\n\n\n\nname\n\n\ncarrier\n\n\n\n\n\n9E\nEndeavor Air Inc.\n\n\nAA\nAmerican Airlines Inc.\n\n\nAS\nAlaska Airlines Inc.\n\n\nB6\nJetBlue Airways\n\n\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\nNow the RangeIndex has been replaced with a more meaningful index, and it’s possible to look up rows of the table by passing a carrier code to the .loc accessor.\n\ndf.loc['AA']\n\nname    American Airlines Inc.\nName: AA, dtype: object\n\n\n\n\n\n\n\n\nPandas does not require that indexes have unique values (that is, no duplicates) although many relational databases do have that requirement of a primary key. This means that it is possible to create a non-unique index, but highly inadvisable. Having duplicate values in your index can cause unexpected results when you refer to rows by index – but multiple rows have that index. Don’t do it if you can help it!\n\n\n\nWhen starting to work with a DataFrame, it’s often a good idea to determine what column makes sense as your index and to set it immediately. This will make your code nicer – by letting you directly look up values with the index – and also make your selections and filters faster, because Pandas is optimized for operations by index. If you want to change the index of your DataFrame later, you can always reset_index (and then assign a new one).\n\ndf.head() # DataFrame with carrier as the index\n\n\n\n\n\n\n\n\nname\n\n\ncarrier\n\n\n\n\n\n9E\nEndeavor Air Inc.\n\n\nAA\nAmerican Airlines Inc.\n\n\nAS\nAlaska Airlines Inc.\n\n\nB6\nJetBlue Airways\n\n\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\n\ndf = df.reset_index() # resetting the index to be 0:n-1\ndf.head()\n\n\n\n\n\n\n\n\ncarrier\nname\n\n\n\n\n0\n9E\nEndeavor Air Inc.\n\n\n1\nAA\nAmerican Airlines Inc.\n\n\n2\nAS\nAlaska Airlines Inc.\n\n\n3\nB6\nJetBlue Airways\n\n\n4\nDL\nDelta Air Lines Inc.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvoid non-unique indexes — they can lead to ambiguous behavior!\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneWorking with DataFrame Indexes\n\n\n\n\nWhat kind of index does the airports DataFrame currently use? Use .index to explore.\nIs this a good index? Why or why not?\nSet the faa column as the new index using .set_index().\nUse .loc to look up the row for FAA code '4G0'. What is the altitude of this airport?\nReset the index. What happens to the faa column after you reset it?",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#summary",
    "href": "08-dataframes.html#summary",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.6 Summary",
    "text": "8.6 Summary\nIn this chapter, you took your first deep dive into Pandas DataFrames — the central data structure for data analysis in Python. You learned that a DataFrame is essentially a table of data, where each column is a one-dimensional object called a Series. While Series and DataFrames are closely related, they behave differently and support different operations, and it’s important to recognize which you’re working with.\nYou also saw how to create Series objects manually and explored the concept of indexing — both in Series and in DataFrames. Indexing allows you to label and quickly access specific rows or columns, and gives structure to your data. We covered how to extract single rows and columns (which are treated as Series), how to interpret data types (dtype), and how to assign or reset index values.\nFinally, you learned that Pandas provides accessors like .loc for retrieving data by label, and that setting a meaningful index can make your code more readable and efficient. As we continue through the book, you’ll see more examples of these techniques in action, and you’ll build the intuition for when and how to apply them in your own data projects.\n\n\n\nConcept\nWhat it is\nExample\n\n\n\n\nDataFrame\n2D table of data\ndf.head()\n\n\nSeries\n1D column/row\ndf['carrier'], df.loc[0]\n\n\nIndex\nLabels for rows\ndf.set_index('carrier')\n\n\n\nIn the next chapter, you’ll begin working with real-world messy data — and the skills you learned here will make that much easier to manage.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "08-dataframes.html#exercise-exploring-covid-19-data-in-colleges",
    "href": "08-dataframes.html#exercise-exploring-covid-19-data-in-colleges",
    "title": "8  Deeper Dive on DataFrames",
    "section": "8.7 Exercise: Exploring COVID-19 Data in Colleges",
    "text": "8.7 Exercise: Exploring COVID-19 Data in Colleges\nIn the previous chapter’s exercise, you imported COVID-19 data related to U.S. colleges and universities using the New York Times dataset. Let’s now build on that by exploring and interacting with the structure of the DataFrame.\nYou can access the data again from this link: https://github.com/nytimes/covid-19-data/blob/master/colleges/colleges.csv\n\n\n\n\n\n\nNoneStep 1: Load the data\n\n\n\n\n\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/nytimes/covid-19-data/master/colleges/colleges.csv\"\ncovid = pd.read_csv(url)\ncovid.head()\n\n\n\n\n\n\n\n\ndate\nstate\ncounty\ncity\nipeds_id\ncollege\ncases\ncases_2021\nnotes\n\n\n\n\n0\n2021-05-26\nAlabama\nMadison\nHuntsville\n100654\nAlabama A&M University\n41\nNaN\nNaN\n\n\n1\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100724\nAlabama State University\n2\nNaN\nNaN\n\n\n2\n2021-05-26\nAlabama\nLimestone\nAthens\n100812\nAthens State University\n45\n10.0\nNaN\n\n\n3\n2021-05-26\nAlabama\nLee\nAuburn\n100858\nAuburn University\n2742\n567.0\nNaN\n\n\n4\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100830\nAuburn University at Montgomery\n220\n80.0\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneStep 2: Working with the data\n\n\n\n\n\n\nWhat is the current shape of the DataFrame? How many rows and columns does it have?\nUse .info() to inspect the column names and data types. Are any of them unexpected?\nSelect the column containing cumulative cases (cases) and check its type and structure. Is it a Series or a DataFrame?\nTry the following covid['cases'].sum(). What does this output represent? Try some other summary statistics. We’ll cover more summary statistics and aggregation methods in the next few chapters.\n\n\n\n\n\n\n\n\n\n\nNoneStep 3: Set and reset the index\n\n\n\n\n\n\nWhat kind of index does the full covid DataFrame currently use?\nWould any of the existing columns make a better index? If so, which one and why?\nSet the ipeds_id column as the new index.\nUse .loc[] to look up the row for where ipeds_id equals 201885 and report:\n\nWhich university is this?\nWhat is the total number of cases?\n\nReset the index back to its original form.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Deeper Dive on DataFrames</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html",
    "href": "09-subsetting.html",
    "title": "9  Subsetting Data",
    "section": "",
    "text": "Learning Objectives\nThis activity gives you a glimpse of a common task in data science: subsetting a dataset to focus on the most relevant information. Whether you’re analyzing flight records, home prices, or COVID case data, you’ll frequently need to extract specific rows and columns before you can analyze or visualize anything.\nIn this lesson, you’ll learn how to do this efficiently using Python and the pandas library — a skill that will save you time, reduce errors, and set the foundation for deeper analysis later.\nBy the end of this lesson, you’ll be able to:",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#prerequisites",
    "href": "09-subsetting.html#prerequisites",
    "title": "9  Subsetting Data",
    "section": "9.1 Prerequisites",
    "text": "9.1 Prerequisites\nTo illustrate selecting and filtering let’s go ahead and load the pandas library and import our planes data we’ve been using:\n\nimport pandas as pd\n\nplanes_df = pd.read_csv('../data/planes.csv')\nplanes_df.head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote📓 Follow Along in Colab!\n\n\n\nAs you read through this chapter, we encourage you to follow along using the companion notebook in Google Colab (or other editor of choice). This interactive notebook lets you run code examples covered in the chapter—and experiment with your own ideas.\n👉 Open the Subsetting Notebook in Colab.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#subsetting-dimensions",
    "href": "09-subsetting.html#subsetting-dimensions",
    "title": "9  Subsetting Data",
    "section": "9.2 Subsetting dimensions",
    "text": "9.2 Subsetting dimensions\n\nWe don’t always want all of the data in a DataFrame, so we need to take subsets of the DataFrame. In general, subsetting is extracting a small portion of a DataFrame – making the DataFrame smaller. Since the DataFrame is two-dimensional, there are two dimensions on which to subset.\nDimension 1: We may only want to consider certain variables. For example, we may only care about the year and engines variables:\n\nWe call this selecting columns/variables – this is similar to SQL’s SELECT or R’s dplyr package’s select().\nDimension 2: We may only want to consider certain cases. For example, we may only care about the cases where the manufacturer is Embraer.\n\nWe call this filtering or slicing – this is similar to SQL’s WHERE or R’s dplyr package’s filter() or slice(). And we can combine these two options to subset in both dimensions – the year and engines variables where the manufacturer is Embraer:\n\nIn the previous example, we want to do two things using planes_df:\n\nselect the year and engines variables\nfilter to cases where the manufacturer is Embraer\n\nBut we also want to return a new DataFrame – not just highlight certain cells. In other words, we want to turn this:\n\n\nCode\nplanes_df.head()\n\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nInto this:\n\n\nCode\nplanes_df.head().loc[planes_df['manufacturer'] == 'EMBRAER', ['year', 'engines']]\n\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n4\n2002.0\n2\n\n\n\n\n\n\n\nSo we really have a third need: return the resulting DataFrame so we can continue our analysis:\n\nselect the year and engines variables\nfilter to cases where the manufacturer is Embraer\nReturn a DataFrame to continue the analysis",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#subsetting-variables",
    "href": "09-subsetting.html#subsetting-variables",
    "title": "9  Subsetting Data",
    "section": "9.3 Subsetting variables",
    "text": "9.3 Subsetting variables\nRecall that the subsetting of variables/columns is called selecting variables/columns. In a simple example, we can select a single variable using bracket subsetting notation:\n\nplanes_df['year'].head()\n\n0    2004.0\n1    1998.0\n2    1999.0\n3    1999.0\n4    2002.0\nName: year, dtype: float64\n\n\nNotice the head() method also works on planes_df['year'] to return the first five elements.\n\n\n\n\n\n\nNonePop quiz!\n\n\n\nWhat is the data type of planes_df['year']?\n\n\nThis returns pandas.core.series.Series, referred to simply as a “Series”, rather than a DataFrame.\n\ntype(planes_df['year'])\n\npandas.core.series.Series\n\n\nThis is okay – the Series is a popular data structure in Python. Recall from a previous lesson:\n\nA Series is a one-dimensional data structure – this is similar to a Python list\nNote that all objects in a Series are usually of the same type (but this isn’t a strict requirement)\nEach DataFrame can be thought of as a list of equal-length Series (plus an Index)\n\n\n\n\n\n\nSeries can be useful, but for now, we are interested in returning a DataFrame rather than a series. We can select a single variable and return a DataFrame by still using bracket subsetting notation, but this time we will pass a list of variables names:\n\nplanes_df[['year']].head()\n\n\n\n\n\n\n\n\nyear\n\n\n\n\n0\n2004.0\n\n\n1\n1998.0\n\n\n2\n1999.0\n\n\n3\n1999.0\n\n\n4\n2002.0\n\n\n\n\n\n\n\nAnd we can see that we’ve returned a DataFrame:\n\ntype(planes_df[['year']].head())\n\npandas.core.frame.DataFrame\n\n\n\n\n\n\n\n\nNonePop quiz!\n\n\n\nWhat do you think is another advantage of passing a list?\n\n\nPassing a list into the bracket subsetting notation allows us to select multiple variables at once:\n\nplanes_df[['year', 'engines']].head()\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n1\n1998.0\n2\n\n\n2\n1999.0\n2\n\n\n3\n1999.0\n2\n\n\n4\n2002.0\n2\n\n\n\n\n\n\n\nIn another example, assume we are interested in the model of plane, number of seats and engine type:\n\nplanes_df[['model', 'seats', 'engine']].head()\n\n\n\n\n\n\n\n\nmodel\nseats\nengine\n\n\n\n\n0\nEMB-145XR\n55\nTurbo-fan\n\n\n1\nA320-214\n182\nTurbo-fan\n\n\n2\nA320-214\n182\nTurbo-fan\n\n\n3\nA320-214\n182\nTurbo-fan\n\n\n4\nEMB-145LR\n55\nTurbo-fan\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This\n\n\n\n\n______ is a common term for subsetting DataFrame variables.\nWhat type of object is a DataFrame column?\nWhat will be returned by the following code?\nplanes_df['type', 'model']",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#subsetting-rows",
    "href": "09-subsetting.html#subsetting-rows",
    "title": "9  Subsetting Data",
    "section": "9.4 Subsetting rows",
    "text": "9.4 Subsetting rows\nWhen we subset rows (aka cases, records, observations) we primarily use two names: slicing and filtering, but these are not the same:\n\nslicing, similar to row indexing, subsets observations by the value of the Index\nfiltering subsets observations using a conditional test\n\n\nSlicing rows\nRemember that all DataFrames have an Index:\n\nplanes_df.head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nWe can slice cases/rows using the values in the Index and bracket subsetting notation. It’s common practice to use .loc to slice cases/rows:\n\nplanes_df.loc[0:5]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n1\nN102UW\n1998.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n3\nN104UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n5\nN105UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that since this is not “indexing”, the last element is inclusive.\n\n\n\nWe can also pass a list of Index values:\n\nplanes_df.loc[[0, 2, 4, 6, 8]]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n2\nN103US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n6\nN107US\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n8\nN109UW\n1999.0\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\nFiltering rows\nWe can filter rows using a logical sequence equal in length to the number of rows in the DataFrame.\nContinuing our example, assume we want to determine whether each case’s manufacturer is Embraer. We can use the manufacturer Series and a logical equivalency test to find the result for each row:\n\nplanes_df['manufacturer'] == 'EMBRAER'\n\n0        True\n1       False\n2       False\n3       False\n4        True\n        ...  \n3317    False\n3318    False\n3319    False\n3320    False\n3321    False\nName: manufacturer, Length: 3322, dtype: bool\n\n\nWe can use this resulting logical sequence to test filter cases – rows that are True will be returned while those that are False will be removed:\n\nplanes_df[planes_df['manufacturer'] == 'EMBRAER'].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n10\nN11106\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n11\nN11107\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n12\nN11109\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nThis also works with .loc:\n\nplanes_df.loc[planes_df['manufacturer'] == 'EMBRAER'].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n4\nN10575\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNaN\nTurbo-fan\n\n\n10\nN11106\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n11\nN11107\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n12\nN11109\n2002.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nAny conditional test can be used to filter DataFrame rows:\n\n# Filter observations where year is greater than 2002\nplanes_df.loc[planes_df['year'] &gt; 2002].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n15\nN11121\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n16\nN11127\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n17\nN11137\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n18\nN11140\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nAnd multiple conditional tests can be combined using logical operators:\n\n# Filter observations where year is greater than 2002 and less than 2007\nplanes_df.loc[(planes_df['year'] &gt; 2002) & (planes_df['year'] &lt; 2007)].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n15\nN11121\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n16\nN11127\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n17\nN11137\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n18\nN11140\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that each condition is wrapped in parentheses – this is required.\n\n\n\nOften, as your condition gets more complex, it can be easier to read if you separate out the condition:\n\ncond = (planes_df['year'] &gt; 2002) & (planes_df['year'] &lt; 2004)\nplanes_df.loc[cond].head()\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n15\nN11121\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n16\nN11127\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n17\nN11137\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n18\nN11140\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n19\nN11150\n2003.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This\n\n\n\n\nWhat’s the difference between slicing cases and filtering cases?\nFill in the blanks to fix the following code to find planes that have more than three engines:\n planes_df.loc[______['______'] &gt; 3]",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#selecting-variables-and-filtering-rows",
    "href": "09-subsetting.html#selecting-variables-and-filtering-rows",
    "title": "9  Subsetting Data",
    "section": "9.5 Selecting variables and filtering rows",
    "text": "9.5 Selecting variables and filtering rows\nIf we want to select variables and filter cases at the same time, we have a few options:\n\nSequential operations\nSimultaneous operations\n\n\nSequential Operations\nWe can use what we’ve previously learned to select variables and filter cases in multiple steps:\n\nplanes_df_filtered = planes_df.loc[planes_df['manufacturer'] == 'EMBRAER']\nplanes_df_filtered_and_selected = planes_df_filtered[['year', 'engines']]\nplanes_df_filtered_and_selected.head()\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n4\n2002.0\n2\n\n\n10\n2002.0\n2\n\n\n11\n2002.0\n2\n\n\n12\n2002.0\n2\n\n\n\n\n\n\n\nThis is a good way to learn how to select and filter independently, and it also reads very clearly.\n\n\nSimultaneous operations\nHowever, we can also do both selecting and filtering in a single step with .loc:\n\nplanes_df.loc[planes_df['manufacturer'] == 'EMBRAER', ['year', 'engines']].head()\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n4\n2002.0\n2\n\n\n10\n2002.0\n2\n\n\n11\n2002.0\n2\n\n\n12\n2002.0\n2\n\n\n\n\n\n\n\nThis option is more succinct and also reduces programming time. As before, as your filtering and selecting conditions get longer and/or more complex, it can make it easier to read to break it up into separate lines:\n\nrows = planes_df['manufacturer'] == 'EMBRAER'\ncols = ['year', 'engines']\nplanes_df.loc[rows, cols].head()\n\n\n\n\n\n\n\n\nyear\nengines\n\n\n\n\n0\n2004.0\n2\n\n\n4\n2002.0\n2\n\n\n10\n2002.0\n2\n\n\n11\n2002.0\n2\n\n\n12\n2002.0\n2\n\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This\n\n\n\nSubset planes_df to only include planes made by Boeing and only return the seats and model variables.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#views-vs-copies",
    "href": "09-subsetting.html#views-vs-copies",
    "title": "9  Subsetting Data",
    "section": "9.6 Views vs copies",
    "text": "9.6 Views vs copies\nOne thing to be aware of, as you will likely experience it eventually, is the concept of returning a view (“looking” at a part of an existing object) versus a copy (making a new copy of the object in memory). This can be a bit abstract and even this section in the Pandas docs states “…it’s very hard to predict whether it will return a view or a copy.”\nThe main takeaway is that the most common warning you’ll encounter in Pandas is the SettingWithCopyWarning; Pandas raises it as a warning that you might not be doing what you think you’re doing or because the operation you are performing may behave unpredictably.\nLet’s look at an example. Say the number of seats on this particular plane was recorded incorrectly. Instead of 55 seats it should actually be 60 seats.\n\ntailnum_of_interest = planes_df['tailnum'] == 'N10156'\nplanes_df[tailnum_of_interest]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nInstead of using .iloc, we could actually filter and select this element in our DataFrame with the following bracket notation.\n\nplanes_df[tailnum_of_interest]['seats']\n\n0    55\nName: seats, dtype: int64\n\n\n\nplanes_df[tailnum_of_interest]['seats'] = 60\n\n/tmp/ipykernel_12522/2190037627.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  planes_df[tailnum_of_interest]['seats'] = 60\n\n\nSo what’s going on? Did our DataFrame get changed?\n\nplanes_df[tailnum_of_interest]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNaN\nTurbo-fan\n\n\n\n\n\n\n\nNo it didn’t, even though you probably thought it did. What happened above is that planes_df[tailnum_of_interest]['seats'] was executed first and returned a copy of the DataFrame, which is an entirely different object. We can confirm by using id():\n\nprint(f\"The id of the original dataframe is: {id(planes_df)}\")\nprint(f\" The id of the indexed dataframe is: {id(planes_df[tailnum_of_interest])}\")\n\nThe id of the original dataframe is: 140579686798672\n The id of the indexed dataframe is: 140579685619696\n\n\nWe then tried to set a value on this new object by appending ['seats'] = 60. Pandas is warning us that we are doing that operation on a copy of the original dataframe, which is probably not what we want. To fix this, you need to index in a single go, using .loc[] for example:\n\nplanes_df.loc[tailnum_of_interest, 'seats'] = 60\n\nNo error this time! And let’s confirm the change:\n\nplanes_df[tailnum_of_interest]\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\n0\nN10156\n2004.0\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n60\nNaN\nTurbo-fan\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe concept of views and copies is confusing and you can read more about it here.\nBut realize, this behavior is changing in pandas 3.0. A new system called Copy-on-Write will become the default, and it will prevent chained indexing from working at all — meaning instead of getting the SettingWithCopyWarning warning, pandas will simply raise an error.\nRegardless, always use .loc[] for combined filtering and selecting!",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#summary",
    "href": "09-subsetting.html#summary",
    "title": "9  Subsetting Data",
    "section": "9.7 Summary",
    "text": "9.7 Summary\nIn this chapter, you learned how to zoom in on the parts of a DataFrame that matter most. Whether you’re interested in just a few variables or a specific set of cases, being able to subset your data is a critical first step in any analysis.\nWe started by giving you a real-world task in Excel or Google Sheets — find aircraft built by Embraer in 2004 or later. That hands-on activity highlighted a common problem: manual filtering doesn’t scale. That’s where Python and pandas come in.\nIn this chapter, you learned how to use pandas for:\n\nSelecting columns using single or multiple variable names\nSlicing rows by index position\nFiltering rows using conditional logic\nCombining selection and filtering with .loc[] for efficient subsetting\n\n\n🧾 Quick Reference: Subsetting Techniques\n\n\n\n\n\n\n\n\nTask\nSyntax Example\nOutput Type\n\n\n\n\nSelect one column\ndf[\"col\"]\nSeries\n\n\nSelect multiple columns\ndf[[\"col1\", \"col2\"]]\nDataFrame\n\n\nSlice rows by index\ndf.loc[0:4]\nDataFrame\n\n\nFilter rows by condition\ndf[df[\"year\"] &gt; 2000]\nDataFrame\n\n\nCombine filter + select\ndf.loc[df[\"year\"] &gt; 2000, [\"col1\", \"col2\"]]\nDataFrame\n\n\nBest practice for assignment\ndf.loc[cond, \"col\"] = value\nSafe, avoids warnings\n\n\n\n\n\n🔁 Revisit the Challenge\nNow that you’ve learned how to subset data using pandas, go back to the original question:\n\nWhich aircraft were manufactured by Embraer in 2004 or later?\n\nThis time, solve it using Python instead of a spreadsheet. Use what you’ve learned in this chapter — filtering rows by conditions, and selecting only the columns you need — to create a clean, focused DataFrame.\nWhen you’re done, try to answer:\n\nHow many rows matched the condition?\nWhat columns did you choose to keep?\nCould you reuse your code later for different conditions?\n\nThis is how data scientists work: writing reusable, scalable code to extract insights from large datasets.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "09-subsetting.html#exercise-subsetting-covid-college-data",
    "href": "09-subsetting.html#exercise-subsetting-covid-college-data",
    "title": "9  Subsetting Data",
    "section": "9.8 Exercise: Subsetting COVID College Data",
    "text": "9.8 Exercise: Subsetting COVID College Data\nIn this exercise, you’ll apply what you learned to subset the New York Times college COVID-19 dataset. The dataset tracks COVID cases reported by colleges across the U.S.\n\n📂 Download the data from this GitHub link or load it directly from a local copy if provided.\n\n\n\n\n\n\n\nNoneStep 1: Load the Data\n\n\n\n\n\n\nimport pandas as pd\n\ndata_url = \"https://raw.githubusercontent.com/nytimes/covid-19-data/refs/heads/master/colleges/colleges.csv\"\ncollege_df = pd.read_csv(data_url)\ncollege_df.head()\n\n\n\n\n\n\n\n\ndate\nstate\ncounty\ncity\nipeds_id\ncollege\ncases\ncases_2021\nnotes\n\n\n\n\n0\n2021-05-26\nAlabama\nMadison\nHuntsville\n100654\nAlabama A&M University\n41\nNaN\nNaN\n\n\n1\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100724\nAlabama State University\n2\nNaN\nNaN\n\n\n2\n2021-05-26\nAlabama\nLimestone\nAthens\n100812\nAthens State University\n45\n10.0\nNaN\n\n\n3\n2021-05-26\nAlabama\nLee\nAuburn\n100858\nAuburn University\n2742\n567.0\nNaN\n\n\n4\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100830\nAuburn University at Montgomery\n220\n80.0\nNaN\n\n\n\n\n\n\n\nThis dataset includes many columns, but for this exercise, we’re going to focus only on:\n\nstate\ncity\ncollege\ncases\n\n\n\n\n\n\n\n\n\n\nNoneStep 2: Subsetting Practice\n\n\n\n\n\n\nSelect only the columns: state, city, college, and cases.\nFilter the dataset to show only colleges in Ohio (state == \"OH\") that reported more than 100 cases.\nHow many Ohio colleges reported more than 100 cases? (Hint: Use .shape or len())\n\n\n\n\n\n\n\n\n\n\nNoneStep 3: Dig Into a Specific School\n\n\n\n\n\n\nFilter the dataset to find records where the college is “University of Cincinnati” (case sensitive).\nHow many cases were reported by the University of Cincinnati?\n\n\n\n\n\n\n\n\n\n\nNoneMake It Dynamic\n\n\n\n\n\nTry making your filtering logic more flexible by defining parameters at the top of your notebook:\nmy_state = \"OH\"\nthreshold = 100\nThen write code that will:\n\nFilter for all colleges in my_state with cases greater than threshold\nReturn only the college and cases columns\n\nTest your code with a few different states and thresholds. Can you reuse it to answer different questions (i.e. How many colleges in California reported more than 500 Covid cases?)",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subsetting Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html",
    "href": "10-manipulating-data.html",
    "title": "10  Manipulating Data",
    "section": "",
    "text": "The Ames Housing Data\nIn our previous chapter, we explored how to access, index, and subset data from a pandas DataFrame. These are essential skills for understanding and navigating real-world datasets. Now we take the next step: learning how to manipulate data.\nAs a data analyst or scientist, you will spend much of your time transforming raw data into something clean, interpretable, and analysis-ready. This chapter provides the foundation for doing just that. You’ll learn to rename columns, create new variables, deal with missing values, and apply functions to your data — all of which are fundamental skills in the data science workflow.\nBy the end of this chapter, you will be able to:\nYou first encountered the Ames Housing dataset back in Chapter 7, where you were challenged with the task of analyzing raw data for the Ames, Iowa housing market. In this chapter, we’ll return to the Ames data as our primary example to learn how to manipulate and prepare real-world data for analysis.\nBefore diving into the new concepts, take a few minutes to reacquaint yourself with the data:\nLet’s begin by loading the dataset and inspecting the first few rows:\nimport pandas as pd\n\names = pd.read_csv('../data/ames_raw.csv')\names.head()\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 82 columns\nWe’ll use this dataset throughout the chapter to demonstrate how to rename columns, compute new variables, handle missing data, and apply transformations — all crucial steps in cleaning and preparing data for analysis and modeling.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#renaming-columns",
    "href": "10-manipulating-data.html#renaming-columns",
    "title": "10  Manipulating Data",
    "section": "10.1 Renaming Columns",
    "text": "10.1 Renaming Columns\nOne of the first things you’ll often do when working with a new dataset is clean up the column names. Column names might contain spaces, inconsistent capitalization, or other formatting quirks that make them harder to work with in code. In this section, we’ll walk through a few ways to rename columns in a DataFrame using the Ames Housing dataset.\nLet’s start by looking at the current column names:\n\names.columns\n\nIndex(['Order', 'PID', 'MS SubClass', 'MS Zoning', 'Lot Frontage', 'Lot Area',\n       'Street', 'Alley', 'Lot Shape', 'Land Contour', 'Utilities',\n       'Lot Config', 'Land Slope', 'Neighborhood', 'Condition 1',\n       'Condition 2', 'Bldg Type', 'House Style', 'Overall Qual',\n       'Overall Cond', 'Year Built', 'Year Remod/Add', 'Roof Style',\n       'Roof Matl', 'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type',\n       'Mas Vnr Area', 'Exter Qual', 'Exter Cond', 'Foundation', 'Bsmt Qual',\n       'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin SF 1',\n       'BsmtFin Type 2', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Total Bsmt SF',\n       'Heating', 'Heating QC', 'Central Air', 'Electrical', '1st Flr SF',\n       '2nd Flr SF', 'Low Qual Fin SF', 'Gr Liv Area', 'Bsmt Full Bath',\n       'Bsmt Half Bath', 'Full Bath', 'Half Bath', 'Bedroom AbvGr',\n       'Kitchen AbvGr', 'Kitchen Qual', 'TotRms AbvGrd', 'Functional',\n       'Fireplaces', 'Fireplace Qu', 'Garage Type', 'Garage Yr Blt',\n       'Garage Finish', 'Garage Cars', 'Garage Area', 'Garage Qual',\n       'Garage Cond', 'Paved Drive', 'Wood Deck SF', 'Open Porch SF',\n       'Enclosed Porch', '3Ssn Porch', 'Screen Porch', 'Pool Area', 'Pool QC',\n       'Fence', 'Misc Feature', 'Misc Val', 'Mo Sold', 'Yr Sold', 'Sale Type',\n       'Sale Condition', 'SalePrice'],\n      dtype='object')\n\n\nYou might notice that some of these column names contain spaces or uppercase letters, such as \"MS SubClass\" and \"MS Zoning\". These formatting issues can be inconvenient when writing code — especially if you’re trying to access a column using dot notation (e.g., df.column_name) or when using string methods.\n\nRenaming Specific Columns\nWe can rename one or more columns using the .rename() method. This method accepts a dictionary where the keys are the original column names and the values are the new names you’d like to assign.\n\names.rename(columns={'MS SubClass': 'ms_subclass', 'MS Zoning': 'ms_zoning'})\n\n\n\n\n\n\n\n\nOrder\nPID\nms_subclass\nms_zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\n2926\n923275080\n80\nRL\n37.0\n7937\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nGdPrv\nNaN\n0\n3\n2006\nWD\nNormal\n142500\n\n\n2926\n2927\n923276100\n20\nRL\nNaN\n8885\nPave\nNaN\nIR1\nLow\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2006\nWD\nNormal\n131000\n\n\n2927\n2928\n923400125\n85\nRL\n62.0\n10441\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nShed\n700\n7\n2006\nWD\nNormal\n132000\n\n\n2928\n2929\n924100070\n20\nRL\n77.0\n10010\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2006\nWD\nNormal\n170000\n\n\n2929\n2930\n924151050\n60\nRL\n74.0\n9627\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n11\n2006\nWD\nNormal\n188000\n\n\n\n\n2930 rows × 82 columns\n\n\n\nThis command runs without error; and if we check out our data (below) nothing seems different. Why?\n\names.head(3)\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n\n\n3 rows × 82 columns\n\n\n\nThat’s because .rename() returns a new DataFrame with the updated names, but it does not modify the original DataFrame unless you explicitly tell it to.\nThere are two common ways to make these changes permanent:\n\nUse the inplace=True argument\nReassign the modified DataFrame to the same variable\n\nThe Pandas development team recommends the second approach (reassigning) for most use cases, as it leads to clearer and more predictable code.\n\names = ames.rename(columns={'MS SubClass': 'ms_subclass', 'MS Zoning': 'ms_zoning'})\names.head()\n\n\n\n\n\n\n\n\nOrder\nPID\nms_subclass\nms_zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 82 columns\n\n\n\n\n\n\n\n\n\nAlways include columns= when using .rename(). If you don’t, Pandas will assume you are renaming index values instead of column names — and it won’t raise a warning or error if you make this mistake.\n\n\n\n\n\nRenaming Many Columns at Once\nUsing .rename() works well for renaming one or two columns. But if you want to rename many columns — such as applying a consistent format across the entire DataFrame — it’s more efficient to use the .columns attribute and vectorized string methods.\nPandas provides powerful tools for working with strings through the .str accessor. For example, we can convert all column names to lowercase like this:\n\names.columns.str.lower()\n\nIndex(['order', 'pid', 'ms_subclass', 'ms_zoning', 'lot frontage', 'lot area',\n       'street', 'alley', 'lot shape', 'land contour', 'utilities',\n       'lot config', 'land slope', 'neighborhood', 'condition 1',\n       'condition 2', 'bldg type', 'house style', 'overall qual',\n       'overall cond', 'year built', 'year remod/add', 'roof style',\n       'roof matl', 'exterior 1st', 'exterior 2nd', 'mas vnr type',\n       'mas vnr area', 'exter qual', 'exter cond', 'foundation', 'bsmt qual',\n       'bsmt cond', 'bsmt exposure', 'bsmtfin type 1', 'bsmtfin sf 1',\n       'bsmtfin type 2', 'bsmtfin sf 2', 'bsmt unf sf', 'total bsmt sf',\n       'heating', 'heating qc', 'central air', 'electrical', '1st flr sf',\n       '2nd flr sf', 'low qual fin sf', 'gr liv area', 'bsmt full bath',\n       'bsmt half bath', 'full bath', 'half bath', 'bedroom abvgr',\n       'kitchen abvgr', 'kitchen qual', 'totrms abvgrd', 'functional',\n       'fireplaces', 'fireplace qu', 'garage type', 'garage yr blt',\n       'garage finish', 'garage cars', 'garage area', 'garage qual',\n       'garage cond', 'paved drive', 'wood deck sf', 'open porch sf',\n       'enclosed porch', '3ssn porch', 'screen porch', 'pool area', 'pool qc',\n       'fence', 'misc feature', 'misc val', 'mo sold', 'yr sold', 'sale type',\n       'sale condition', 'saleprice'],\n      dtype='object')\n\n\nOr, we can chain multiple string methods together to standardize our column names — converting them to lowercase and replacing all spaces with underscores:\n\names.columns = ames.columns.str.lower().str.replace(\" \", \"_\")\names.head()\n\n\n\n\n\n\n\n\norder\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\n...\npool_area\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 82 columns\n\n\n\nThis makes your column names easier to type, more consistent, and less prone to errors in your code.\n\n\n\n\n\n\nYou can explore other string operations that work with .str by checking out the Pandas string methods documentation.\n\n\n\n\n\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry This!\n\n\n\nLet’s practice cleaning up messy column names using the techniques from this section. Below is a small example dataset with inconsistent column names:\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'First Name': ['Alice', 'Bob', 'Charlie'],\n    'Last-Name': ['Smith', 'Jones', 'Brown'],\n    'AGE ': [25, 30, 22],\n    'Email Address': ['alice@example.com', 'bob@example.com', 'charlie@example.com']\n})\nYour task:\n\nPrint the current column names. What formatting issues do you see?\nClean the column names by doing the following:\n\nConvert all column names to lowercase\nReplace any spaces or hyphens with underscores\nStrip any leading or trailing whitespace\n\nAssign the cleaned column names back to the DataFrame.\nPrint the updated column names and confirm the changes.\n\nHint: .columns.str.lower(), .str.replace(), and .str.strip() will come in handy here!",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#performing-calculations-with-columns",
    "href": "10-manipulating-data.html#performing-calculations-with-columns",
    "title": "10  Manipulating Data",
    "section": "10.2 Performing Calculations with Columns",
    "text": "10.2 Performing Calculations with Columns\nOnce your data is loaded and your columns are cleaned up, the next common task is to perform calculations on your data. This might include creating new variables, transforming existing values, or applying arithmetic operations across columns.\nLet’s begin by focusing on the saleprice column in the Ames Housing dataset. This column records the sale price of each home in dollars. For example:\n\nsale_price = ames['saleprice']\nsale_price\n\n0       215000\n1       105000\n2       172000\n3       244000\n4       189900\n         ...  \n2925    142500\n2926    131000\n2927    132000\n2928    170000\n2929    188000\nName: saleprice, Length: 2930, dtype: int64\n\n\nThese numbers are fairly large — often six digits long. In many analyses or visualizations, it can be helpful to express values in thousands of dollars instead of raw dollar amounts.\nTo convert the sale price to thousands, we simply divide each value by 1,000:\n\nsale_price_k = sale_price / 1000\nsale_price_k\n\n0       215.0\n1       105.0\n2       172.0\n3       244.0\n4       189.9\n        ...  \n2925    142.5\n2926    131.0\n2927    132.0\n2928    170.0\n2929    188.0\nName: saleprice, Length: 2930, dtype: float64\n\n\nThis results in a new Series where each home’s price is now shown in thousands. For instance, a home that originally sold for $215,000 is now shown as 215.0.\nAt this point, sale_price_k is a new object that exists separately from the ames DataFrame. In the next section, we’ll learn how to add this new variable as a column in our DataFrame so we can use it in further analysis.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#adding-and-removing-columns",
    "href": "10-manipulating-data.html#adding-and-removing-columns",
    "title": "10  Manipulating Data",
    "section": "10.3 Adding and Removing Columns",
    "text": "10.3 Adding and Removing Columns\nOnce you’ve created a new variable — like sale_price_k in the previous section — you’ll often want to add it to your existing DataFrame so it becomes part of the dataset you’re working with.\n\nAdding Columns\nIn pandas, you can add a new column to a DataFrame using assignment syntax:\n# example syntax\ndf['new_column_name'] = new_column_series\nLet’s add the sale_price_k series (which represents sale prices in thousands) to the ames DataFrame:\n\names['sale_price_k'] = sale_price_k\names.head()\n\n\n\n\n\n\n\n\norder\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\n...\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nsale_price_k\n\n\n\n\n0\n1\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\n...\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n215.0\n\n\n1\n2\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\n...\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n105.0\n\n\n2\n3\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\n...\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n172.0\n\n\n3\n4\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\n...\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n244.0\n\n\n4\n5\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\n...\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n189.9\n\n\n\n\n5 rows × 83 columns\n\n\n\nNow, you’ll see that a new column called \"sale_price_k\" appears at the end of the DataFrame.\n\n\n\n\n\n\nNotice how the column name ('sale_price_k') is placed in quotes inside the brackets on the left-hand side, while the Series providing the data goes on the right-hand side without quotes or brackets.\n\n\n\nThis entire process can be done in a single step, without creating an intermediate variable:\n\names['sale_price_k'] = ames['saleprice'] / 1000\n\nThis kind of operation is common in data science. What we’re doing here is applying vectorized math — performing arithmetic between a Series (a vector of values) and a scalar (a single constant value).\nHere are a few more examples:\n\n# Subtracting a scalar from a Series\n(ames['saleprice'] - 12).head()\n\n0    214988\n1    104988\n2    171988\n3    243988\n4    189888\nName: saleprice, dtype: int64\n\n\n\n# Multiplying a Series by a scalar\n(ames['saleprice'] * 10).head()\n\n0    2150000\n1    1050000\n2    1720000\n3    2440000\n4    1899000\nName: saleprice, dtype: int64\n\n\n\n# Raising a Series to a power\n(ames['saleprice'] ** 2).head()\n\n0    46225000000\n1    11025000000\n2    29584000000\n3    59536000000\n4    36062010000\nName: saleprice, dtype: int64\n\n\nVectorized operations like these are fast, efficient, and more readable than writing explicit loops.\n\n\nRemoving Columns\nJust as easily as we can add columns, we can also remove them. This is helpful when a column is no longer needed or was created only temporarily.\nTo drop one or more columns from a DataFrame, use the .drop() method with the columns= argument:\n\names = ames.drop(columns=['order', 'sale_price_k'])\names.head()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\npool_area\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31770\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n526350040\n20\nRH\n80.0\n11622\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n526351010\n20\nRL\n81.0\n14267\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n526353030\n20\nRL\n93.0\n11160\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n527105010\n60\nRL\n74.0\n13830\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 81 columns\n\n\n\nThis removes the \"order\" and \"sale_price_k\" columns from the DataFrame. Remember that most pandas methods return a new DataFrame by default, so you’ll need to reassign the result back to ames (or another variable) to make the change permanent.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneCreate a utility_space variable\n\n\n\n\nCreate a new column utility_space that is 1/5 of the above ground living space (gr_liv_area).\nYou will get fractional output with step #1. See if you can figure out how to round this output to the nearest integer.\nNow remove this column from your DataFrame",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#overwriting-columns",
    "href": "10-manipulating-data.html#overwriting-columns",
    "title": "10  Manipulating Data",
    "section": "10.4 Overwriting columns",
    "text": "10.4 Overwriting columns\nWhat if we discovered a systematic error in our data? Perhaps we find out that the “lot_area” column is not entirely accurate because the recording process includes an extra 50 square feet for every property. We could create a new column, “real_lot_area” but we’re not going to need the original “lot_area” column, and leaving it could cause confusion for others looking at our data.\nA better solution would be to replace the original column with the new, recalculated, values. We can do so using the same syntax as for creating a new column.\n\n# Subtract 50 from lot area, and then overwrite the original data.\names['lot_area'] = ames['lot_area'] - 50\names.head()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\npool_area\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31720\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n\n\n1\n526350040\n20\nRH\n80.0\n11572\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n\n\n2\n526351010\n20\nRL\n81.0\n14217\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n\n\n3\n526353030\n20\nRL\n93.0\n11110\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n\n\n4\n527105010\n60\nRL\n74.0\n13780\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n\n\n\n\n5 rows × 81 columns",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#calculating-with-multiple-columns",
    "href": "10-manipulating-data.html#calculating-with-multiple-columns",
    "title": "10  Manipulating Data",
    "section": "10.5 Calculating with Multiple Columns",
    "text": "10.5 Calculating with Multiple Columns\nUp to this point, we’ve focused on performing calculations between a column (Series) and a scalar — for example, dividing every value in saleprice by 1,000. But pandas also allows you to perform operations between columns — this is known as vector-vector arithmetic.\nLet’s look at an example where we calculate a new metric: price per square foot. We can compute this by dividing the sale price of each home by its above-ground living area (gr_liv_area):\n\nprice_per_sqft = ames['saleprice'] / ames['gr_liv_area']\nprice_per_sqft.head()\n\n0    129.830918\n1    117.187500\n2    129.420617\n3    115.639810\n4    116.574586\ndtype: float64\n\n\nNow that we’ve computed the new values, let’s add them to our DataFrame as a new column:\n\names['price_per_sqft'] = price_per_sqft\names.head()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\npool_qc\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31720\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n129.830918\n\n\n1\n526350040\n20\nRH\n80.0\n11572\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n117.187500\n\n\n2\n526351010\n20\nRL\n81.0\n14217\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n129.420617\n\n\n3\n526353030\n20\nRL\n93.0\n11110\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n115.639810\n\n\n4\n527105010\n60\nRL\n74.0\n13780\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n116.574586\n\n\n\n\n5 rows × 82 columns\n\n\n\nAs before, you could write this as a one-liner:\n\names['price_per_sqft'] = ames['saleprice'] / ames['gr_liv_area']\n\n\nCombining Multiple Operations\nYou can also combine multiple columns and scalars in more complex expressions. For example, the following line combines three columns and a constant:\n\names['nonsense'] = (ames['yr_sold'] + 12) * ames['gr_liv_area'] + ames['lot_area'] - 50\names.head()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\nnonsense\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31720\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n129.830918\n3380102\n\n\n1\n526350040\n20\nRH\n80.0\n11572\nPave\nNaN\nReg\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n117.187500\n1823234\n\n\n2\n526351010\n20\nRL\n81.0\n14217\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n129.420617\n2701405\n\n\n3\n526353030\n20\nRL\n93.0\n11110\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n115.639810\n4277480\n\n\n4\n527105010\n60\nRL\n74.0\n13780\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n116.574586\n3307568\n\n\n\n\n5 rows × 83 columns\n\n\n\nThis creates a column called \"nonsense\" using a mix of vector-vector and vector-scalar operations. While this particular example isn’t meaningful analytically, it shows how you can chain together multiple operations in a single expression.\nIn practice, you’ll often calculate new variables using a combination of existing columns — for example, calculating cost efficiency, total square footage, or ratios between two quantities. Being comfortable with these kinds of operations is essential for building features and preparing data for analysis or modeling.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneCreate a price_per_total_sqft variable\n\n\n\nCreate a new column price_per_total_sqft that is saleprice divided by the sum of gr_liv_area, total_bsmt_sf, wood_deck_sf, open_porch_sf.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#working-with-string-columns",
    "href": "10-manipulating-data.html#working-with-string-columns",
    "title": "10  Manipulating Data",
    "section": "10.6 Working with String Columns",
    "text": "10.6 Working with String Columns\nSo far, we’ve focused on numeric calculations — things like dividing, multiplying, and creating new variables based on numbers. But many datasets also contain non-numeric values, such as names, categories, or descriptive labels.\nIn pandas, string data is stored as object or string type columns, and you can perform operations on them just like you would with numbers. This is especially useful for cleaning, formatting, or combining text.\n\nString Concatenation\nA common operation with string data is concatenation — combining multiple strings together. For example, suppose we want to create a descriptive sentence using the neighborhood and sale condition of each home in the Ames dataset:\n\n'Home in ' + ames['neighborhood'] + ' neighborhood sold under ' + ames['sale_condition'] + ' condition'\n\n0       Home in NAmes neighborhood sold under Normal c...\n1       Home in NAmes neighborhood sold under Normal c...\n2       Home in NAmes neighborhood sold under Normal c...\n3       Home in NAmes neighborhood sold under Normal c...\n4       Home in Gilbert neighborhood sold under Normal...\n                              ...                        \n2925    Home in Mitchel neighborhood sold under Normal...\n2926    Home in Mitchel neighborhood sold under Normal...\n2927    Home in Mitchel neighborhood sold under Normal...\n2928    Home in Mitchel neighborhood sold under Normal...\n2929    Home in Mitchel neighborhood sold under Normal...\nLength: 2930, dtype: object\n\n\nThis works just like string addition in Python. Each piece of text is combined row by row across the DataFrame to generate a new sentence for each observation.\n\n\nString Methods with .str\nFor more advanced string operations, pandas provides a powerful set of tools through the .str accessor. This gives you access to many string-specific methods like .lower(), .replace(), .len(), and more.\nHere are a few examples:\n\n# Count the number of characters in each neighborhood name\names['neighborhood'].str.len()\n\n0       5\n1       5\n2       5\n3       5\n4       7\n       ..\n2925    7\n2926    7\n2927    7\n2928    7\n2929    7\nName: neighborhood, Length: 2930, dtype: int64\n\n\n\n# Standardize the format of garage type labels\names['garage_type'].str.lower().str.replace('tchd', 'tached')\n\n0       attached\n1       attached\n2       attached\n3       attached\n4       attached\n          ...   \n2925    detached\n2926    attached\n2927         NaN\n2928    attached\n2929    attached\nName: garage_type, Length: 2930, dtype: object\n\n\nThese methods are especially helpful when cleaning messy or inconsistent text data — for example, fixing capitalization, removing whitespace, or replacing substrings.\n\n\n\n\n\n\nIn this chapter, we’ve only scratched the surface of working with non-numeric data. In later chapters, we’ll take a deeper look at how to clean, transform, and analyze string values, as well as how to work with date and time data — including parsing timestamps, extracting components like month and day, and calculating time differences.\nFor now, if you want to dig into working with string columns some more, it’s worth exploring the official Pandas documentation on string methods to see the full range of capabilities.\n\n\n\nWhether you’re formatting text for a report, cleaning up inconsistent labels, or preparing inputs for machine learning models, working with string columns is a valuable part of your data wrangling skill set.\n\names\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\nnonsense\n\n\n\n\n0\n526301100\n20\nRL\n141.0\n31720\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\n0\n5\n2010\nWD\nNormal\n215000\n129.830918\n3380102\n\n\n1\n526350040\n20\nRH\n80.0\n11572\nPave\nNaN\nReg\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n6\n2010\nWD\nNormal\n105000\n117.187500\n1823234\n\n\n2\n526351010\n20\nRL\n81.0\n14217\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nGar2\n12500\n6\n2010\nWD\nNormal\n172000\n129.420617\n2701405\n\n\n3\n526353030\n20\nRL\n93.0\n11110\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n244000\n115.639810\n4277480\n\n\n4\n527105010\n60\nRL\n74.0\n13780\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n3\n2010\nWD\nNormal\n189900\n116.574586\n3307568\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\n923275080\n80\nRL\n37.0\n7887\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nGdPrv\nNaN\n0\n3\n2006\nWD\nNormal\n142500\n142.073779\n2031891\n\n\n2926\n923276100\n20\nRL\nNaN\n8835\nPave\nNaN\nIR1\nLow\nAllPub\n...\nMnPrv\nNaN\n0\n6\n2006\nWD\nNormal\n131000\n145.232816\n1829021\n\n\n2927\n923400125\n85\nRL\n62.0\n10391\nPave\nNaN\nReg\nLvl\nAllPub\n...\nMnPrv\nShed\n700\n7\n2006\nWD\nNormal\n132000\n136.082474\n1967801\n\n\n2928\n924100070\n20\nRL\n77.0\n9960\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2006\nWD\nNormal\n170000\n122.390209\n2812912\n\n\n2929\n924151050\n60\nRL\n74.0\n9577\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n11\n2006\nWD\nNormal\n188000\n94.000000\n4045527\n\n\n\n\n2930 rows × 83 columns",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#more-complex-column-manipulation",
    "href": "10-manipulating-data.html#more-complex-column-manipulation",
    "title": "10  Manipulating Data",
    "section": "10.7 More Complex Column Manipulation",
    "text": "10.7 More Complex Column Manipulation\nAs you become more comfortable working with individual columns, you’ll often find yourself needing to do more than basic math. In this section, we’ll cover a few additional, common column operations:\n\nReplacing values using a mapping\nIdentifying and handling missing values\nApplying custom functions\n\nThese are core techniques that will serve you in any data cleaning or feature engineering workflow.\n\nReplacing Values\nOne fairly common situation in data wrangling is needing to convert one set of values to another, where there is a one-to-one correspondence between the values currently in the column and the new values that should replace them. This operation can be described as “mapping one set of values to another”.\nLet’s look at an example of this. In our Ames data the month sold is represented numerically:\n\names['mo_sold'].head()\n\n0    5\n1    6\n2    6\n3    4\n4    3\nName: mo_sold, dtype: int64\n\n\nSuppose we want to change this so that values are represented by the month name:\n\n1 = ‘Jan’\n2 = ‘Feb’\n…\n12 = ‘Dec’\n\nWe can express this mapping of old values to new values using a Python dictionary.\n\n# Only specify the values we want to replace; don't include the ones that should stay the same.\nvalue_mapping = {\n    1: 'Jan',\n    2: 'Feb',\n    3: 'Mar',\n    4: 'Apr',\n    5: 'May',\n    6: 'Jun',\n    7: 'Jul',\n    8: 'Aug',\n    9: 'Sep',\n    10: 'Oct',\n    11: 'Nov',\n    12: 'Dec'\n    }\n\nPandas provides a handy method on Series, .replace, that accepts this value mapping and updates the Series accordingly. We can use it to recode our values.\n\names['mo_sold'].replace(value_mapping).head()\n\n0    May\n1    Jun\n2    Jun\n3    Apr\n4    Mar\nName: mo_sold, dtype: object\n\n\n\n\n\n\n\n\nIf you are a SQL user, this workflow may look familiar to you; it’s quite similar to a CASE WHEN statement in SQL.\n\n\n\n\n\nMissing values\nIn real-world datasets, missing values are common. In pandas, these are usually represented as NaN (Not a Number).\nTo detect missing values in a DataFrame, use .isnull(). This returns a DataFrame of the same shape with True where values are missing:\n\names.isnull()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\nnonsense\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2926\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2927\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2928\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2929\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n2930 rows × 83 columns\n\n\n\nWe can use this to easily compute the total number of missing values in each column:\n\names.isnull().sum()\n\npid                 0\nms_subclass         0\nms_zoning           0\nlot_frontage      490\nlot_area            0\n                 ... \nsale_type           0\nsale_condition      0\nsaleprice           0\nprice_per_sqft      0\nnonsense            0\nLength: 83, dtype: int64\n\n\nRecall we also get this information with .info(). Actually, we get the inverse as .info() tells us how many non-null values exist in each column.\n\names.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2930 entries, 0 to 2929\nData columns (total 83 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   pid              2930 non-null   int64  \n 1   ms_subclass      2930 non-null   int64  \n 2   ms_zoning        2930 non-null   object \n 3   lot_frontage     2440 non-null   float64\n 4   lot_area         2930 non-null   int64  \n 5   street           2930 non-null   object \n 6   alley            198 non-null    object \n 7   lot_shape        2930 non-null   object \n 8   land_contour     2930 non-null   object \n 9   utilities        2930 non-null   object \n 10  lot_config       2930 non-null   object \n 11  land_slope       2930 non-null   object \n 12  neighborhood     2930 non-null   object \n 13  condition_1      2930 non-null   object \n 14  condition_2      2930 non-null   object \n 15  bldg_type        2930 non-null   object \n 16  house_style      2930 non-null   object \n 17  overall_qual     2930 non-null   int64  \n 18  overall_cond     2930 non-null   int64  \n 19  year_built       2930 non-null   int64  \n 20  year_remod/add   2930 non-null   int64  \n 21  roof_style       2930 non-null   object \n 22  roof_matl        2930 non-null   object \n 23  exterior_1st     2930 non-null   object \n 24  exterior_2nd     2930 non-null   object \n 25  mas_vnr_type     1155 non-null   object \n 26  mas_vnr_area     2907 non-null   float64\n 27  exter_qual       2930 non-null   object \n 28  exter_cond       2930 non-null   object \n 29  foundation       2930 non-null   object \n 30  bsmt_qual        2850 non-null   object \n 31  bsmt_cond        2850 non-null   object \n 32  bsmt_exposure    2847 non-null   object \n 33  bsmtfin_type_1   2850 non-null   object \n 34  bsmtfin_sf_1     2929 non-null   float64\n 35  bsmtfin_type_2   2849 non-null   object \n 36  bsmtfin_sf_2     2929 non-null   float64\n 37  bsmt_unf_sf      2929 non-null   float64\n 38  total_bsmt_sf    2929 non-null   float64\n 39  heating          2930 non-null   object \n 40  heating_qc       2930 non-null   object \n 41  central_air      2930 non-null   object \n 42  electrical       2929 non-null   object \n 43  1st_flr_sf       2930 non-null   int64  \n 44  2nd_flr_sf       2930 non-null   int64  \n 45  low_qual_fin_sf  2930 non-null   int64  \n 46  gr_liv_area      2930 non-null   int64  \n 47  bsmt_full_bath   2928 non-null   float64\n 48  bsmt_half_bath   2928 non-null   float64\n 49  full_bath        2930 non-null   int64  \n 50  half_bath        2930 non-null   int64  \n 51  bedroom_abvgr    2930 non-null   int64  \n 52  kitchen_abvgr    2930 non-null   int64  \n 53  kitchen_qual     2930 non-null   object \n 54  totrms_abvgrd    2930 non-null   int64  \n 55  functional       2930 non-null   object \n 56  fireplaces       2930 non-null   int64  \n 57  fireplace_qu     1508 non-null   object \n 58  garage_type      2773 non-null   object \n 59  garage_yr_blt    2771 non-null   float64\n 60  garage_finish    2771 non-null   object \n 61  garage_cars      2929 non-null   float64\n 62  garage_area      2929 non-null   float64\n 63  garage_qual      2771 non-null   object \n 64  garage_cond      2771 non-null   object \n 65  paved_drive      2930 non-null   object \n 66  wood_deck_sf     2930 non-null   int64  \n 67  open_porch_sf    2930 non-null   int64  \n 68  enclosed_porch   2930 non-null   int64  \n 69  3ssn_porch       2930 non-null   int64  \n 70  screen_porch     2930 non-null   int64  \n 71  pool_area        2930 non-null   int64  \n 72  pool_qc          13 non-null     object \n 73  fence            572 non-null    object \n 74  misc_feature     106 non-null    object \n 75  misc_val         2930 non-null   int64  \n 76  mo_sold          2930 non-null   int64  \n 77  yr_sold          2930 non-null   int64  \n 78  sale_type        2930 non-null   object \n 79  sale_condition   2930 non-null   object \n 80  saleprice        2930 non-null   int64  \n 81  price_per_sqft   2930 non-null   float64\n 82  nonsense         2930 non-null   int64  \ndtypes: float64(12), int64(28), object(43)\nmemory usage: 1.9+ MB\n\n\nWe can use any() to identify which columns have missing values. We can use this information for various reasons such as subsetting for just those columns that have missing values.\n\nmissing = ames.isnull().any() # identify if missing values exist in each column\names[missing[missing].index]  # subset for just those columns that have missing values\n\n\n\n\n\n\n\n\nlot_frontage\nalley\nmas_vnr_type\nmas_vnr_area\nbsmt_qual\nbsmt_cond\nbsmt_exposure\nbsmtfin_type_1\nbsmtfin_sf_1\nbsmtfin_type_2\n...\ngarage_type\ngarage_yr_blt\ngarage_finish\ngarage_cars\ngarage_area\ngarage_qual\ngarage_cond\npool_qc\nfence\nmisc_feature\n\n\n\n\n0\n141.0\nNaN\nStone\n112.0\nTA\nGd\nGd\nBLQ\n639.0\nUnf\n...\nAttchd\n1960.0\nFin\n2.0\n528.0\nTA\nTA\nNaN\nNaN\nNaN\n\n\n1\n80.0\nNaN\nNaN\n0.0\nTA\nTA\nNo\nRec\n468.0\nLwQ\n...\nAttchd\n1961.0\nUnf\n1.0\n730.0\nTA\nTA\nNaN\nMnPrv\nNaN\n\n\n2\n81.0\nNaN\nBrkFace\n108.0\nTA\nTA\nNo\nALQ\n923.0\nUnf\n...\nAttchd\n1958.0\nUnf\n1.0\n312.0\nTA\nTA\nNaN\nNaN\nGar2\n\n\n3\n93.0\nNaN\nNaN\n0.0\nTA\nTA\nNo\nALQ\n1065.0\nUnf\n...\nAttchd\n1968.0\nFin\n2.0\n522.0\nTA\nTA\nNaN\nNaN\nNaN\n\n\n4\n74.0\nNaN\nNaN\n0.0\nGd\nTA\nNo\nGLQ\n791.0\nUnf\n...\nAttchd\n1997.0\nFin\n2.0\n482.0\nTA\nTA\nNaN\nMnPrv\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2925\n37.0\nNaN\nNaN\n0.0\nTA\nTA\nAv\nGLQ\n819.0\nUnf\n...\nDetchd\n1984.0\nUnf\n2.0\n588.0\nTA\nTA\nNaN\nGdPrv\nNaN\n\n\n2926\nNaN\nNaN\nNaN\n0.0\nGd\nTA\nAv\nBLQ\n301.0\nALQ\n...\nAttchd\n1983.0\nUnf\n2.0\n484.0\nTA\nTA\nNaN\nMnPrv\nNaN\n\n\n2927\n62.0\nNaN\nNaN\n0.0\nGd\nTA\nAv\nGLQ\n337.0\nUnf\n...\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nMnPrv\nShed\n\n\n2928\n77.0\nNaN\nNaN\n0.0\nGd\nTA\nAv\nALQ\n1071.0\nLwQ\n...\nAttchd\n1975.0\nRFn\n2.0\n418.0\nTA\nTA\nNaN\nNaN\nNaN\n\n\n2929\n74.0\nNaN\nBrkFace\n94.0\nGd\nTA\nAv\nLwQ\n758.0\nUnf\n...\nAttchd\n1993.0\nFin\n3.0\n650.0\nTA\nTA\nNaN\nNaN\nNaN\n\n\n\n\n2930 rows × 27 columns\n\n\n\n\nDropping Missing Values\nWhen you have missing values, we usually either drop them or impute them.You can drop missing values with .dropna():\n\names.dropna()\n\n\n\n\n\n\n\n\npid\nms_subclass\nms_zoning\nlot_frontage\nlot_area\nstreet\nalley\nlot_shape\nland_contour\nutilities\n...\nfence\nmisc_feature\nmisc_val\nmo_sold\nyr_sold\nsale_type\nsale_condition\nsaleprice\nprice_per_sqft\nnonsense\n\n\n\n\n\n\n0 rows × 83 columns\n\n\n\nWhoa! What just happened? Well, this data set actually has a missing value in every single row. .dropna() drops every row that contains a missing value so we end up dropping all observations. Consequently, we probably want to figure out what’s going on with these missing values and isolate the column causing the problem and imputing the values if possible.\n\n\n\n\n\n\nAnother “drop” method is .drop_duplcates() which will drop duplicated rows in your DataFrame.\n\n\n\n\n\nVisualizing Missingness\nSometimes visualizations help identify patterns in missing values. One thing I often do is print a heatmap of my dataframe to get a feel for where my missing values are. We’ll get into data visualization in future lessons but for now here is an example using the searborn library. We can see that several variables have a lot of missing values (alley, fireplace_qu, pool_qc, fence, misc_feature).\n\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(12, 8)})\n\n\names_missing = ames[missing[missing].index]\nsns.heatmap(ames_missing.isnull(), cmap='viridis', cbar=False);\n\n\n\n\n\n\n\n\n\n\nFilling Missing Values (Imputation)\nSince we can’t drop all missing values in this data set (since it leaves us with no rows), we need to impute (“fill”) them in. There are several approaches we can use to do this; one of which uses the .fillna() method. This method has various options for filling, you can use a fixed value, the mean of the column, the previous non-nan value, etc:\n\nimport numpy as np\n\n# example DataFrame with missing values\ndf = pd.DataFrame([[np.nan, 2, np.nan, 0],\n                   [3, 4, np.nan, 1],\n                   [np.nan, np.nan, np.nan, 5],\n                   [np.nan, 3, np.nan, 4]],\n                  columns=list('ABCD'))\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\nNaN\n2.0\nNaN\n0\n\n\n1\n3.0\n4.0\nNaN\n1\n\n\n2\nNaN\nNaN\nNaN\n5\n\n\n3\nNaN\n3.0\nNaN\n4\n\n\n\n\n\n\n\n\ndf.fillna(0)  # fill with 0\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n0.0\n2.0\n0.0\n0\n\n\n1\n3.0\n4.0\n0.0\n1\n\n\n2\n0.0\n0.0\n0.0\n5\n\n\n3\n0.0\n3.0\n0.0\n4\n\n\n\n\n\n\n\n\ndf.fillna(df.mean())  # fill with the mean\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n3.0\n2.0\nNaN\n0\n\n\n1\n3.0\n4.0\nNaN\n1\n\n\n2\n3.0\n3.0\nNaN\n5\n\n\n3\n3.0\n3.0\nNaN\n4\n\n\n\n\n\n\n\n\ndf.bfill()  # backward (upwards) fill from non-nan values\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n3.0\n2.0\nNaN\n0\n\n\n1\n3.0\n4.0\nNaN\n1\n\n\n2\nNaN\n3.0\nNaN\n5\n\n\n3\nNaN\n3.0\nNaN\n4\n\n\n\n\n\n\n\n\ndf.ffill()  # forward (downward) fill from non-nan values\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\nNaN\n2.0\nNaN\n0\n\n\n1\n3.0\n4.0\nNaN\n1\n\n\n2\n3.0\n4.0\nNaN\n5\n\n\n3\n3.0\n3.0\nNaN\n4\n\n\n\n\n\n\n\n\n\n\nApplying custom functions\nThere will be times when you want to apply a function that is not built-in to Pandas. For this, we have methods:\n\ndf.apply(), applies a function column-wise or row-wise across a dataframe (the function must be able to accept/return an array)\ndf.applymap(), applies a function element-wise (for functions that accept/return single values at a time)\nseries.apply()/series.map(), same as above but for Pandas series\n\nFor example, say you had the following custom function that defines if a home is considered a luxery home simply based on the price sold.\n\n\n\n\n\n\nDon’t worry, you’ll learn more about writing your own functions in future lessons!\n\n\n\n\ndef is_luxery_home(x):\n    if x &gt; 500000:\n        return 'Luxery'\n    else:\n        return 'Non-luxery'\n\names['saleprice'].apply(is_luxery_home)\n\n0       Non-luxery\n1       Non-luxery\n2       Non-luxery\n3       Non-luxery\n4       Non-luxery\n           ...    \n2925    Non-luxery\n2926    Non-luxery\n2927    Non-luxery\n2928    Non-luxery\n2929    Non-luxery\nName: saleprice, Length: 2930, dtype: object\n\n\nThis may have been better as a lambda function, which is just a shorter approach to writing functions. This may be a bit confusing but we’ll talk more about lambda functions in the writing functions lesson. For now, just think of it as being able to write a function for single use application on the fly.\n\names['saleprice'].apply(lambda x: 'Luxery' if x &gt; 500000 else 'Non-luxery')\n\n0       Non-luxery\n1       Non-luxery\n2       Non-luxery\n3       Non-luxery\n4       Non-luxery\n           ...    \n2925    Non-luxery\n2926    Non-luxery\n2927    Non-luxery\n2928    Non-luxery\n2929    Non-luxery\nName: saleprice, Length: 2930, dtype: object\n\n\nYou can even use functions that require additional arguments. Just specify the arguments in .apply():\n\ndef is_luxery_home(x, price):\n    if x &gt; price:\n        return 'Luxery'\n    else:\n        return 'Non-luxery'\n\names['saleprice'].apply(is_luxery_home, price=200000)\n\n0           Luxery\n1       Non-luxery\n2       Non-luxery\n3           Luxery\n4       Non-luxery\n           ...    \n2925    Non-luxery\n2926    Non-luxery\n2927    Non-luxery\n2928    Non-luxery\n2929    Non-luxery\nName: saleprice, Length: 2930, dtype: object\n\n\nSometimes we may have a function that we want to apply to every element across multiple columns. For example, say we wanted to convert several of the square footage variables to be represented as square meters. For this we can use the .applymap() method.\n\ndef convert_to_sq_meters(x):\n    return x*0.092903\n\names[['gr_liv_area', 'garage_area', 'lot_area']].map(convert_to_sq_meters)\n\n\n\n\n\n\n\n\ngr_liv_area\ngarage_area\nlot_area\n\n\n\n\n0\n153.847368\n49.052784\n2946.883160\n\n\n1\n83.241088\n67.819190\n1075.073516\n\n\n2\n123.468087\n28.985736\n1320.801951\n\n\n3\n196.025330\n48.495366\n1032.152330\n\n\n4\n151.338987\n44.779246\n1280.203340\n\n\n...\n...\n...\n...\n\n\n2925\n93.181709\n54.626964\n732.725961\n\n\n2926\n83.798506\n44.965052\n820.798005\n\n\n2927\n90.115910\n0.000000\n965.355073\n\n\n2928\n129.042267\n38.833454\n925.313880\n\n\n2929\n185.806000\n60.386950\n889.732031\n\n\n\n\n2930 rows × 3 columns",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#chapter-summary",
    "href": "10-manipulating-data.html#chapter-summary",
    "title": "10  Manipulating Data",
    "section": "10.8 Chapter Summary",
    "text": "10.8 Chapter Summary\nIn this chapter, you learned how to manipulate columns in a pandas DataFrame — a foundational skill for any kind of data analysis or modeling. You practiced working with both numeric and non-numeric data and explored common data wrangling tasks that analysts use every day.\nHere’s a recap of what you learned:\n\nHow to rename columns using .rename() or string methods like .str.lower() and .str.replace()\nHow to create new columns by performing arithmetic with scalars or other columns\nHow to remove columns using .drop()\nHow to work with text data, including string concatenation and using the .str accessor\nHow to replace values using a mapping (via .replace())\nHow to detect and handle missing values using .isnull(), .dropna(), and .fillna()\nHow to apply custom functions to transform your data using .apply() and .applymap()\n\nThese skills form the building blocks of effective data cleaning and transformation.\nIn the next few chapters, we’ll build on this foundation and introduce even more essential data wrangling techniques, including:\n\nComputing summary statistics and descriptive analytics\nGrouping and aggregating data\nJoining multiple datasets\nReshaping data with pivot tables and the .melt() and .pivot() methods\n\nBy the end of these upcoming lessons, you’ll be well-equipped to clean, prepare, and explore real-world datasets using pandas.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "10-manipulating-data.html#exercise-heart-disease-data",
    "href": "10-manipulating-data.html#exercise-heart-disease-data",
    "title": "10  Manipulating Data",
    "section": "10.9 Exercise: Heart Disease Data",
    "text": "10.9 Exercise: Heart Disease Data\nIn this exercise, you’ll apply what you’ve learned in this chapter to a new dataset on heart disease, which includes various patient health indicators that may be predictive of cardiovascular conditions.\nRead more about this dataset on Kaggle, and you can download copy of the heart.csv file here.\nYour task is to perform several data wrangling steps to start cleaning and transforming this dataset.\n\n\n\n\n\n\nNoneImport the Data\n\n\n\n\n\n\nLoad the dataset into a DataFrame using pd.read_csv().\nPreview the first few rows with .head().\n\n\n\n\n\n\n\n\n\n\nNoneClean the Column Names\n\n\n\n\n\nCheck out the column names and think how you would standardize these names. Use .columns and string methods to:\n\nConvert all column names to lowercase\nReplace any spaces or dashes with underscores\nRemove any trailing or leading whitespace\n\n\n\n\n\n\n\n\n\n\nNoneHandle Missing Values\n\n\n\n\n\n\nCheck for missing values using .isnull().sum().\nIf any columns contain missing values:\n\nIdentify the mode (most frequent value) for those columns\nFill the missing values using .fillna()\n\n\n\n\n\n\n\n\nThe mode can be accessed with .mode().iloc[0] to retrieve the most frequent value.\n\n\n\n\n\n\n\n\n\n\n\n\nNoneCreate a New Column\n\n\n\n\n\nCreate a new column called risk, calculated as:\n\\[\n     \\text{risk} = \\frac{\\text{age}}{\\text{rest\\_bp} + \\text{chol} + \\text{max\\_hr}}\n\\]\n\n\n\n\n\n\nBe sure to use parentheses in your formula to ensure proper order of operations.\n\n\n\n\n\n\n\n\n\n\n\n\nNoneReplace Values in a Categorical Column\n\n\n\n\n\nThe rest_ecg column contains several text categories. Recode the values using .replace() and the following mapping:\n\n\n\nOriginal Value\nNew Value\n\n\n\n\nnormal\nnormal\n\n\nleft ventricular hypertrophy\nlvh\n\n\nST-T wave abnormality\nstt_wav_abn\n\n\n\n\n\n\n\n\n\nMake sure to overwrite the existing column with the updated values.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html",
    "href": "11_aggregating_data.html",
    "title": "11  Summarizing Data",
    "section": "",
    "text": "11.1 Simple aggregation\nAs datasets grow in size and complexity, the ability to summarize information becomes an essential skill in any data scientist’s toolkit. Summary statistics—like averages, medians, and group-level comparisons—help us cut through the noise and uncover meaningful patterns. Whether you’re reporting results to stakeholders or exploring data to form new hypotheses, knowing how to aggregate data is fundamental.\nThese types of questions will guide your thinking as we explore tools in Pandas for summarizing and grouping data. And by the end of this lesson, you will be able to:\nIn the previous lesson, we learned how to manipulate data across columns, typically by performing operations on two or more Series within the same row. For example, we might calculate a total by adding two columns together:\nFor example, we can calculate a total by adding two columns together with code that looks like this:\nThis adds the values in column A to the corresponding values in column B, row by row. You could also use other operators (e.g., subtraction, multiplication) as long as the result returns one value per row.\nHowever, sometimes we want to shift our focus from individual rows to the entire column. Instead of computing values across columns within a row, we want to aggregate values across rows within a column. This is the foundation of many summary statistics—like computing the average, median, or maximum of a column.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html#simple-aggregation",
    "href": "11_aggregating_data.html#simple-aggregation",
    "title": "11  Summarizing Data",
    "section": "",
    "text": "These types of operations return the same number of rows as the original DataFrame. This is sometimes called a window function—but you can think of it as performing calculations at the row level.\n\n\n\n\nDataFrame['A'] + DataFrame['B']\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese types of operations return a single value that summarizes multiple values. This is often referred to as a summary function, but you can think of it as aggregating values across rows.\n\n\n\n\nSummarizing a Series\nOnce we’ve loaded a dataset into a Pandas DataFrame, one of the simplest ways to explore it is by summarizing individual columns—also known as Series. Pandas makes this easy with built-in methods like .sum(), .mean(), and .median().\nSuppose we want to compute the total of all home sale prices. We can do that by selecting the SalePrice column and using the .sum() method:\n\names['SalePrice'].sum()\n\nnp.int64(529732456)\n\n\nThis returns a single value, which makes this a summary operation—we’re aggregating values across all rows in the SalePrice column.\nPandas includes many other helpful summary methods for numerical data:\n\n# Average sale price\names['SalePrice'].mean()\n\nnp.float64(180796.0600682594)\n\n\n\n# Median sale price\names['SalePrice'].median()\n\nnp.float64(160000.0)\n\n\n\n# Standard deviation of sale prices\names['SalePrice'].std()   \n\nnp.float64(79886.692356665)\n\n\nThese methods are designed for quantitative variables and won’t work as expected on text-based columns.\n\n\n\n\n\n\nPandas will include the data type in the summary statistic output preceeding the actual summary stat (i.e. np.float64). If you want to not see that you can just wrap it with print():\n\nprint(ames['SalePrice'].sum())\n\n529732456\n\n\n\n\n\nHowever, Pandas also provides summary methods that are useful for categorical variables (like neighborhood names):\n\n# Number of unique neighborhoods\names['Neighborhood'].nunique()\n\n28\n\n\n\n# Most frequent neighborhood\names['Neighborhood'].mode()    \n\n0    NAmes\nName: Neighborhood, dtype: object\n\n\nThese allow us to summarize the structure and frequency of categorical data—just as we do with numbers.\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nWhat is the difference between a window operation and a summary operation?\nWhat are the mean, median, and standard deviation of the Gr Liv Area (above ground square footage)?\nHow many times does each neighborhood appear in the dataset? (Hint: Try using the GenAI code assistant to figure this out. Ask it how to “count value frequency in a Pandas Series.”). Then reflect: Would this count as a summary operation? Why or why not?\n\n\n\n\n\n\n\nThe describe() Method\nWhen you’re first getting familiar with a dataset, it’s helpful to quickly view a variety of summary statistics all at once. Pandas provides the .describe() method for exactly this purpose.\nFor numeric variables, .describe() returns common summary statistics such as the count, mean, standard deviation, min, and max values:\n\names['SalePrice'].describe()\n\ncount      2930.000000\nmean     180796.060068\nstd       79886.692357\nmin       12789.000000\n25%      129500.000000\n50%      160000.000000\n75%      213500.000000\nmax      755000.000000\nName: SalePrice, dtype: float64\n\n\nThis is especially useful during exploratory data analysis (EDA) when you’re trying to get a sense of a column’s range, distribution, and central tendency.\n\n\n\n\n\n\nThe behavior of .describe() changes based on the data type of the Series.\n\n\n\nFor categorical variables, .describe() provides a different summary, showing the number of non-null entries, number of unique values, most frequent value, and its frequency:\n\names['Neighborhood'].describe()\n\ncount      2930\nunique       28\ntop       NAmes\nfreq        443\nName: Neighborhood, dtype: object\n\n\nIn both cases, .describe() gives you a quick and informative overview—whether you’re working with numbers or categories.\n\n\nSummarizing a DataFrame\nSo far, we’ve focused on summarizing individual columns (Series). But in many cases, we want to explore multiple variables at once. Fortunately, Pandas allows us to apply the same summary methods to a subset of columns in a DataFrame.\nFirst, recall how to select multiple columns using double brackets and a list of column names:\n\names[['SalePrice', 'Gr Liv Area']]\n\n\n\n\n\n\n\n\nSalePrice\nGr Liv Area\n\n\n\n\n0\n215000\n1656\n\n\n1\n105000\n896\n\n\n2\n172000\n1329\n\n\n3\n244000\n2110\n\n\n4\n189900\n1629\n\n\n...\n...\n...\n\n\n2925\n142500\n1003\n\n\n2926\n131000\n902\n\n\n2927\n132000\n970\n\n\n2928\n170000\n1389\n\n\n2929\n188000\n2000\n\n\n\n\n2930 rows × 2 columns\n\n\n\nThis returns a new DataFrame with just the SalePrice and Gr Liv Area columns.\nWe can now apply summary methods—just like we did with a single Series:\n\names[['SalePrice', 'Gr Liv Area']].mean()\n\nSalePrice      180796.060068\nGr Liv Area      1499.690444\ndtype: float64\n\n\n\names[['SalePrice', 'Gr Liv Area']].median()\n\nSalePrice      160000.0\nGr Liv Area      1442.0\ndtype: float64\n\n\nThese methods return a Series object, where:\n\nThe index contains the column names, and\nThe values are the summary statistics (e.g., mean or median) for each column.\n\nThis approach is useful when you’re interested in comparing summary values across several numeric variables at once.\n\n\n\n\n\n\nEven though you’re summarizing multiple columns, each summary method still operates column-by-column under the hood.\n\n\n\n\n\nThe .agg() Method\nSo far, we’ve used built-in summary methods like .mean() and .median() to compute statistics on one or more columns. While this approach works well, it has a few important limitations when applied to DataFrames:\n\nYou can only apply one summary method at a time.\nThe same method gets applied to every selected column.\nThe result is returned as a Series, which can be harder to work with later.\n\nTo overcome these limitations, Pandas provides a more flexible tool: the .agg() method (short for .aggregate()).\nHere’s a basic example:\n\names.agg({\n    'SalePrice': ['mean']\n})\n\n\n\n\n\n\n\n\nSalePrice\n\n\n\n\nmean\n180796.060068\n\n\n\n\n\n\n\nThis returns a DataFrame rather than a Series—and gives us more control over how we summarize each column.\nLet’s break it down:\n\nWe pass a dictionary to .agg()\nThe keys are column names\nThe values are lists of summary functions we want to apply\n\n\n\n\n\n\n\nThe .agg() method is just shorthand for .aggregate(). You can use either version—they’re equivalent!\n\n\n\n\n# Verbose version\names.aggregate({\n    'SalePrice': ['mean']\n})\n\n# Concise version\names.agg({\n    'SalePrice': ['mean']\n})\n\n\n\n\n\n\n\n\nSalePrice\n\n\n\n\nmean\n180796.060068\n\n\n\n\n\n\n\nWe can easily extend this to include multiple columns:\n\names.agg({\n    'SalePrice': ['mean'],\n    'Gr Liv Area': ['mean']\n})\n\n\n\n\n\n\n\n\nSalePrice\nGr Liv Area\n\n\n\n\nmean\n180796.060068\n1499.690444\n\n\n\n\n\n\n\nAnd because the values in the dictionary are lists, we can apply multiple summary functions to each column:\n\names.agg({\n    'SalePrice': ['mean', 'median'],\n    'Gr Liv Area': ['mean', 'min']\n})\n\n\n\n\n\n\n\n\nSalePrice\nGr Liv Area\n\n\n\n\nmean\n180796.060068\n1499.690444\n\n\nmedian\n160000.000000\nNaN\n\n\nmin\nNaN\n334.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou don’t have to apply the same summary functions to every variable. If a summary function isn’t applied to a particular column, Pandas will return a NaN in that spot.\n\n\n\nThe .agg() method is especially useful when you want to apply different aggregations to different columns and get the results in a clean, tabular format.\n\nKnowledge Check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nFill in the blanks to compute the average number of rooms above ground (TotRms AbvGrd) and the average number of bedrooms above ground (Bedroom AbvGr). What type of object is returned?\names[['______', '______']].______()\nUse the .agg() method to complete the same computation as above. How does the output differ?\nFill in the blanks in the below code to calculate the minimum and maximum year built (Year Built) and the mean and median number of garage stalls (Garage Cars):\names.agg({\n    '_____': ['min', '_____'],\n    '_____': ['_____', 'median']\n})\n\n\n\n\n\n\n\nThe describe() Method\nWhile .agg() is a powerful and flexible tool for customized summaries, the .describe() method offers a quick and convenient overview of your entire DataFrame—making it especially useful during exploratory data analysis (EDA).\nTry running .describe() on the full Ames dataset:\n\names.describe()\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nLot Frontage\nLot Area\nOverall Qual\nOverall Cond\nYear Built\nYear Remod/Add\nMas Vnr Area\n...\nWood Deck SF\nOpen Porch SF\nEnclosed Porch\n3Ssn Porch\nScreen Porch\nPool Area\nMisc Val\nMo Sold\nYr Sold\nSalePrice\n\n\n\n\ncount\n2930.00000\n2.930000e+03\n2930.000000\n2440.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2907.000000\n...\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n2930.000000\n\n\nmean\n1465.50000\n7.144645e+08\n57.387372\n69.224590\n10147.921843\n6.094881\n5.563140\n1971.356314\n1984.266553\n101.896801\n...\n93.751877\n47.533447\n23.011604\n2.592491\n16.002048\n2.243345\n50.635154\n6.216041\n2007.790444\n180796.060068\n\n\nstd\n845.96247\n1.887308e+08\n42.638025\n23.365335\n7880.017759\n1.411026\n1.111537\n30.245361\n20.860286\n179.112611\n...\n126.361562\n67.483400\n64.139059\n25.141331\n56.087370\n35.597181\n566.344288\n2.714492\n1.316613\n79886.692357\n\n\nmin\n1.00000\n5.263011e+08\n20.000000\n21.000000\n1300.000000\n1.000000\n1.000000\n1872.000000\n1950.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n2006.000000\n12789.000000\n\n\n25%\n733.25000\n5.284770e+08\n20.000000\n58.000000\n7440.250000\n5.000000\n5.000000\n1954.000000\n1965.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n4.000000\n2007.000000\n129500.000000\n\n\n50%\n1465.50000\n5.354536e+08\n50.000000\n68.000000\n9436.500000\n6.000000\n5.000000\n1973.000000\n1993.000000\n0.000000\n...\n0.000000\n27.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n6.000000\n2008.000000\n160000.000000\n\n\n75%\n2197.75000\n9.071811e+08\n70.000000\n80.000000\n11555.250000\n7.000000\n6.000000\n2001.000000\n2004.000000\n164.000000\n...\n168.000000\n70.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n8.000000\n2009.000000\n213500.000000\n\n\nmax\n2930.00000\n1.007100e+09\n190.000000\n313.000000\n215245.000000\n10.000000\n9.000000\n2010.000000\n2010.000000\n1600.000000\n...\n1424.000000\n742.000000\n1012.000000\n508.000000\n576.000000\n800.000000\n17000.000000\n12.000000\n2010.000000\n755000.000000\n\n\n\n\n8 rows × 39 columns\n\n\n\nBy default, Pandas will summarize only the numeric columns, returning statistics like count, mean, standard deviation, min, max, and quartiles.\n\n\n\n\n\n\nBut wait, what’s missing from the output?\nString (object) columns — like Neighborhood — are excluded!\n\n\n\nIf you want to include other variable types, you can use the include parameter to tell Pandas what to summarize. For example:\n\names.describe(include=['int', 'float', 'object'])\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\ncount\n2930.00000\n2.930000e+03\n2930.000000\n2930\n2440.000000\n2930.000000\n2930\n198\n2930\n2930\n...\n2930.000000\n13\n572\n106\n2930.000000\n2930.000000\n2930.000000\n2930\n2930\n2930.000000\n\n\nunique\nNaN\nNaN\nNaN\n7\nNaN\nNaN\n2\n2\n4\n4\n...\nNaN\n4\n4\n5\nNaN\nNaN\nNaN\n10\n6\nNaN\n\n\ntop\nNaN\nNaN\nNaN\nRL\nNaN\nNaN\nPave\nGrvl\nReg\nLvl\n...\nNaN\nEx\nMnPrv\nShed\nNaN\nNaN\nNaN\nWD\nNormal\nNaN\n\n\nfreq\nNaN\nNaN\nNaN\n2273\nNaN\nNaN\n2918\n120\n1859\n2633\n...\nNaN\n4\n330\n95\nNaN\nNaN\nNaN\n2536\n2413\nNaN\n\n\nmean\n1465.50000\n7.144645e+08\n57.387372\nNaN\n69.224590\n10147.921843\nNaN\nNaN\nNaN\nNaN\n...\n2.243345\nNaN\nNaN\nNaN\n50.635154\n6.216041\n2007.790444\nNaN\nNaN\n180796.060068\n\n\nstd\n845.96247\n1.887308e+08\n42.638025\nNaN\n23.365335\n7880.017759\nNaN\nNaN\nNaN\nNaN\n...\n35.597181\nNaN\nNaN\nNaN\n566.344288\n2.714492\n1.316613\nNaN\nNaN\n79886.692357\n\n\nmin\n1.00000\n5.263011e+08\n20.000000\nNaN\n21.000000\n1300.000000\nNaN\nNaN\nNaN\nNaN\n...\n0.000000\nNaN\nNaN\nNaN\n0.000000\n1.000000\n2006.000000\nNaN\nNaN\n12789.000000\n\n\n25%\n733.25000\n5.284770e+08\n20.000000\nNaN\n58.000000\n7440.250000\nNaN\nNaN\nNaN\nNaN\n...\n0.000000\nNaN\nNaN\nNaN\n0.000000\n4.000000\n2007.000000\nNaN\nNaN\n129500.000000\n\n\n50%\n1465.50000\n5.354536e+08\n50.000000\nNaN\n68.000000\n9436.500000\nNaN\nNaN\nNaN\nNaN\n...\n0.000000\nNaN\nNaN\nNaN\n0.000000\n6.000000\n2008.000000\nNaN\nNaN\n160000.000000\n\n\n75%\n2197.75000\n9.071811e+08\n70.000000\nNaN\n80.000000\n11555.250000\nNaN\nNaN\nNaN\nNaN\n...\n0.000000\nNaN\nNaN\nNaN\n0.000000\n8.000000\n2009.000000\nNaN\nNaN\n213500.000000\n\n\nmax\n2930.00000\n1.007100e+09\n190.000000\nNaN\n313.000000\n215245.000000\nNaN\nNaN\nNaN\nNaN\n...\n800.000000\nNaN\nNaN\nNaN\n17000.000000\n12.000000\n2010.000000\nNaN\nNaN\n755000.000000\n\n\n\n\n11 rows × 82 columns\n\n\n\nThis will generate descriptive statistics for all numeric and categorical variables, including counts, unique values, top categories, and their frequencies.\n\n\n\n\n\n\nYou can also use include='all' to summarize every column, regardless of type. Just be aware that this can result in a mix of numeric and non-numeric statistics in the same table.\n\n\n\nThe .describe() method is a fast and effective way to get a high-level snapshot of your dataset—perfect for early-stage data exploration.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html#grouped-aggregation",
    "href": "11_aggregating_data.html#grouped-aggregation",
    "title": "11  Summarizing Data",
    "section": "11.2 Grouped Aggregation",
    "text": "11.2 Grouped Aggregation\nSo far, we’ve focused on summary operations that collapse an entire column—or a subset of columns—into a single result. But in many real-world analyses, we’re interested in summarizing within groups rather than across the whole dataset.\nThis is called a grouped aggregation. Instead of collapsing a DataFrame into a single row, we collapse it into one row per group.\nFor example, we might want to compute:\n\nTotal home sales by neighborhood\nAverage square footage by number of bedrooms\nMedian sale price by year\nMaximum temperature by month\n\nHere’s a simple illustration: suppose we want to compute the sum of column B for each unique category in column A:\n\n\n\n\n\n\nThe Groupby Model\nGrouped aggregation in Pandas always follows the same three-step process:\n\nGroup the data using groupby()\nApply a summary method like .sum(), .agg(), or .describe()\nReturn a DataFrame of group-level summaries\n\n\n\n\nCreating a Grouped Object\nWe use the groupby() method to define how we want to group the data. For example, to group homes by Neighborhood:\n\names_grp = ames.groupby('Neighborhood')\n\nThis creates a GroupBy object—it doesn’t return a DataFrame yet, but rather an internal structure that maps each group to its corresponding rows.\n\ntype(ames_grp)\n\npandas.core.groupby.generic.DataFrameGroupBy\n\n\nYou can inspect the structure of the groups:\n\names_grp.groups\n\n{'Blmngtn': [52, 53, 468, 469, 470, 471, 472, 473, 1080, 1081, 1082, 1083, 1084, 1741, 1742, 1743, 1744, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429], 'Blueste': [298, 299, 932, 933, 934, 935, 1542, 1543, 2225, 2227], 'BrDale': [29, 30, 31, 402, 403, 404, 405, 406, 407, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1675, 1676, 1677, 1678, 1679, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372], 'BrkSide': [129, 130, 191, 192, 193, 194, 195, 196, 197, 198, 199, 614, 615, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 1219, 1220, 1221, 1222, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1901, 1902, 1903, 1904, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2022, 2023, 2024, 2025, 2554, 2555, 2556, 2557, 2670, 2671, 2672, 2673, 2675, 2676, 2677, ...], 'ClearCr': [208, 209, 210, 228, 229, 232, 233, 255, 779, 781, 782, 785, 833, 834, 1374, 1392, 1395, 1398, 1399, 1400, 1401, 1402, 1406, 1429, 1430, 2045, 2071, 2072, 2073, 2077, 2115, 2116, 2117, 2118, 2701, 2725, 2726, 2727, 2730, 2731, 2764, 2765, 2766, 2767], 'CollgCr': [249, 250, 251, 252, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 1423, 1424, 1425, 1426, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, ...], 'Crawfor': [293, 294, 295, 296, 297, 300, 308, 912, 915, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 936, 937, 938, 944, 1523, 1524, 1525, 1526, 1528, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1559, 1560, 1561, 1562, 2196, 2197, 2198, 2199, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2226, 2228, 2229, 2230, 2245, 2246, 2850, 2853, 2854, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, ...], 'Edwards': [234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 287, 762, 763, 764, 765, 766, 767, 783, 784, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 1375, 1403, 1404, 1405, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, ...], 'Gilbert': [4, 5, 9, 10, 11, 12, 13, 16, 18, 51, 54, 55, 56, 57, 58, 344, 345, 346, 347, 348, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 369, 464, 465, 466, 467, 474, 475, 476, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 992, 993, 994, 995, 996, 997, 998, 1003, 1004, 1005, 1006, 1007, 1013, 1015, 1085, 1086, 1087, 1088, 1089, 1090, 1092, 1093, 1094, 1095, 1096, 1097, 1615, 1616, 1617, 1618, 1619, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1638, 1727, 1728, ...], 'Greens': [106, 107, 575, 1857, 2518, 2519, 2520, 2521], 'GrnHill': [2256, 2892], 'IDOTRR': [205, 206, 301, 302, 303, 304, 305, 306, 307, 726, 727, 754, 755, 758, 759, 760, 939, 940, 941, 942, 943, 945, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1368, 1369, 1370, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1610, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2043, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2669, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882], 'Landmrk': [2788], 'MeadowV': [326, 327, 328, 329, 330, 331, 332, 973, 975, 977, 978, 979, 1593, 1594, 1595, 1596, 1597, 1599, 1600, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2908, 2909, 2910, 2913, 2914, 2916, 2917, 2918, 2919, 2920], 'Mitchel': [309, 310, 311, 312, 313, 322, 323, 324, 325, 333, 334, 335, 336, 337, 338, 339, 340, 946, 947, 948, 949, 950, 951, 952, 953, 954, 969, 970, 971, 972, 974, 976, 980, 981, 982, 983, 984, 985, 986, 987, 988, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1588, 1589, 1590, 1591, 1592, 1598, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2277, 2278, 2279, 2280, 2281, 2282, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2885, 2886, 2887, 2888, 2889, 2890, 2903, 2904, 2905, ...], 'NAmes': [0, 1, 2, 3, 23, 24, 25, 26, 27, 28, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 341, 342, 343, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 418, 419, 593, 594, 595, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 616, 617, 618, 619, 620, 621, 622, 623, 624, ...], 'NPkVill': [32, 33, 34, 35, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 1046, 1047, 1048, 1680, 1681, 1682, 2373, 2376, 2377], 'NWAmes': [19, 20, 21, 110, 111, 112, 113, 114, 115, 116, 118, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 387, 388, 389, 390, 391, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 596, 597, 598, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1197, 1198, 1199, 1200, 1201, 1203, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1863, 1864, 1865, 1866, 1867, ...], 'NoRidge': [59, 60, 61, 62, 63, 64, 65, 90, 91, 92, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 564, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1157, 1158, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1832, 1833, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2499, 2500, 2501, 2502, 2503], 'NridgHt': [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 482, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1091, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, ...], 'OldTown': [158, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 200, 201, 202, 203, 204, 207, 650, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 749, 750, 751, 752, 753, 756, 757, 1254, 1255, 1257, 1258, 1259, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, ...], 'SWISU': [211, 212, 213, 214, 285, 286, 288, 289, 290, 291, 292, 905, 906, 907, 908, 909, 910, 911, 913, 914, 916, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1527, 1529, 2047, 2080, 2194, 2195, 2200, 2703, 2704, 2842, 2845, 2846, 2847, 2848, 2849, 2851, 2852, 2855, 2856], 'Sawyer': [83, 84, 85, 86, 87, 88, 89, 108, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 230, 231, 554, 555, 556, 557, 558, 559, 560, 561, 562, 578, 761, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 780, 1149, 1150, 1151, 1152, 1153, 1154, 1156, 1371, 1372, 1373, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1393, 1394, 1396, 1397, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1829, 1830, 1831, 1861, 2044, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, ...], 'SawyerW': [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 244, 245, 246, 247, 248, 253, 254, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 832, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1418, 1419, 1420, 1421, 1422, 1427, 1428, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 2089, 2090, 2091, 2092, ...], 'Somerst': [22, 66, 67, 68, 69, 70, 71, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 109, 383, 384, 385, 386, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 565, 566, 567, 568, 569, 570, 571, 572, 573, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, ...], 'StoneBr': [6, 7, 8, 14, 15, 17, 349, 350, 351, 352, 365, 366, 367, 368, 999, 1000, 1001, 1002, 1008, 1009, 1010, 1011, 1012, 1014, 1620, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1639, 1640, 1641, 1642, 2322, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2339, 2340, 2341], 'Timber': [314, 315, 316, 317, 318, 319, 320, 321, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 2255, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2891, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902], 'Veenker': [563, 574, 576, 577, 1155, 1178, 1179, 1180, 1181, 1182, 1183, 1827, 1828, 1853, 1854, 1855, 1856, 1858, 1859, 1860, 2498, 2516, 2517, 2522]}\n\n\nAnd access a specific group:\n\n# Get the Bloomington neighborhood group\names_grp.get_group('Blmngtn').head()\n\n\n\n\n\n\n\n\nOrder\nPID\nMS SubClass\nMS Zoning\nLot Frontage\nLot Area\nStreet\nAlley\nLot Shape\nLand Contour\n...\nPool Area\nPool QC\nFence\nMisc Feature\nMisc Val\nMo Sold\nYr Sold\nSale Type\nSale Condition\nSalePrice\n\n\n\n\n52\n53\n528228285\n120\nRL\n43.0\n3203\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n1\n2010\nWD\nNormal\n160000\n\n\n53\n54\n528228440\n120\nRL\n43.0\n3182\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n192000\n\n\n468\n469\n528228290\n120\nRL\n53.0\n3684\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n6\n2009\nWD\nNormal\n174000\n\n\n469\n470\n528228295\n120\nRL\n51.0\n3635\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2009\nWD\nNormal\n175900\n\n\n470\n471\n528228435\n120\nRL\n43.0\n3182\nPave\nNaN\nReg\nLvl\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2009\nWD\nNormal\n192500\n\n\n\n\n5 rows × 82 columns\n\n\n\n\n\nApplying Aggregations to Groups\nOnce a group is defined, you can apply the same aggregation methods we used earlier:\n\names.groupby('Neighborhood').agg({'SalePrice': ['mean', 'median']}).head()\n\n\n\n\n\n\n\n\nSalePrice\n\n\n\nmean\nmedian\n\n\nNeighborhood\n\n\n\n\n\n\nBlmngtn\n196661.678571\n191500.0\n\n\nBlueste\n143590.000000\n130500.0\n\n\nBrDale\n105608.333333\n106000.0\n\n\nBrkSide\n124756.250000\n126750.0\n\n\nClearCr\n208662.090909\n197500.0\n\n\n\n\n\n\n\nThis returns a DataFrame where each row corresponds to a different neighborhood, and the columns represent the aggregated values.\n\n\nGroups as Index vs. Variables\n\n\n\n\n\n\nBy default, the grouped variable becomes the index of the resulting DataFrame.\n\n\n\n\names.groupby('Neighborhood').agg({'SalePrice': ['mean', 'median']}).index\n\nIndex(['Blmngtn', 'Blueste', 'BrDale', 'BrkSide', 'ClearCr', 'CollgCr',\n       'Crawfor', 'Edwards', 'Gilbert', 'Greens', 'GrnHill', 'IDOTRR',\n       'Landmrk', 'MeadowV', 'Mitchel', 'NAmes', 'NPkVill', 'NWAmes',\n       'NoRidge', 'NridgHt', 'OldTown', 'SWISU', 'Sawyer', 'SawyerW',\n       'Somerst', 'StoneBr', 'Timber', 'Veenker'],\n      dtype='object', name='Neighborhood')\n\n\nThis is the most efficient default behavior in Pandas. However, if you’d prefer the group column to remain a regular column instead of becoming the index, you can set as_index=False:\n\names.groupby('Neighborhood', as_index=False).agg({'SalePrice': ['mean', 'median']}).head()\n\n\n\n\n\n\n\n\nNeighborhood\nSalePrice\n\n\n\n\nmean\nmedian\n\n\n\n\n0\nBlmngtn\n196661.678571\n191500.0\n\n\n1\nBlueste\n143590.000000\n130500.0\n\n\n2\nBrDale\n105608.333333\n106000.0\n\n\n3\nBrkSide\n124756.250000\n126750.0\n\n\n4\nClearCr\n208662.090909\n197500.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing as_index=False can make your results easier to merge with other DataFrames or write to CSV later.\n\n\n\n\n\nGrouping by Multiple Variables\nYou can also group by more than one variable. For example, to compute the average sale price by both Neighborhood and Yr Sold:\n\names.groupby(['Neighborhood', 'Yr Sold'], as_index=False).agg({'SalePrice': 'mean'})\n\n\n\n\n\n\n\n\nNeighborhood\nYr Sold\nSalePrice\n\n\n\n\n0\nBlmngtn\n2006\n214424.454545\n\n\n1\nBlmngtn\n2007\n194671.500000\n\n\n2\nBlmngtn\n2008\n190714.400000\n\n\n3\nBlmngtn\n2009\n177266.666667\n\n\n4\nBlmngtn\n2010\n176000.000000\n\n\n...\n...\n...\n...\n\n\n125\nTimber\n2010\n224947.625000\n\n\n126\nVeenker\n2006\n270000.000000\n\n\n127\nVeenker\n2007\n253577.777778\n\n\n128\nVeenker\n2008\n225928.571429\n\n\n129\nVeenker\n2009\n253962.500000\n\n\n\n\n130 rows × 3 columns\n\n\n\nThis returns one row for each combination of Neighborhood and Yr Sold, giving you more granular insights.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nReframe the following question using Pandas’ grouped aggregation syntax. Which Pandas functions will you need?\n\nWhat is the average above-ground square footage of homes, grouped by neighborhood and number of bedrooms?\n\nNow compute the result using the Ames Housing dataset. 🔍 Hint…\n\nGr Liv Area = above-ground square footage\n\nNeighborhood = neighborhood name\n\nBedroom AbvGr = number of bedrooms\n\nUsing your results from #2, identify any neighborhoods where 1-bedroom homes have an average of more than 1500 square feet above ground. How many neighborhoods meet this condition?",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html#summary",
    "href": "11_aggregating_data.html#summary",
    "title": "11  Summarizing Data",
    "section": "11.3 Summary",
    "text": "11.3 Summary\nIn this chapter, we explored how to summarize and aggregate data using Pandas—a foundational skill in any data analysis workflow. You learned how to compute summary statistics on individual columns (Series), multiple columns in a DataFrame, and across groups using the powerful groupby() method. We introduced tools like .mean(), .median(), .describe(), and .agg() to help extract key insights from both numerical and categorical variables. These techniques allow us to make sense of large datasets by reducing complexity and identifying trends, patterns, and outliers.\nIn the chapters ahead, we’ll continue building on these data wrangling skills. You’ll learn how to combine datasets using joins, reshape data through pivoting and tidying, and prepare data for modeling and visualization. Mastering these techniques will give you the ability to transform raw data into a clean, structured form ready for deeper analysis.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "11_aggregating_data.html#exercise-aggregating-covid-college-data",
    "href": "11_aggregating_data.html#exercise-aggregating-covid-college-data",
    "title": "11  Summarizing Data",
    "section": "11.4 Exercise: Aggregating COVID College Data",
    "text": "11.4 Exercise: Aggregating COVID College Data\nNow that you’ve practiced subsetting and filtering, let’s dig deeper by computing some summary statistics from the college COVID-19 dataset.\n\n\n\n\n\n\nNoneStep 1: Load the Data\n\n\n\n\n\n\nimport pandas as pd\n\ndata_url = \"https://raw.githubusercontent.com/nytimes/covid-19-data/refs/heads/master/colleges/colleges.csv\"\ncollege_df = pd.read_csv(data_url)\ncollege_df.head()\n\n\n\n\n\n\n\n\ndate\nstate\ncounty\ncity\nipeds_id\ncollege\ncases\ncases_2021\nnotes\n\n\n\n\n0\n2021-05-26\nAlabama\nMadison\nHuntsville\n100654\nAlabama A&M University\n41\nNaN\nNaN\n\n\n1\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100724\nAlabama State University\n2\nNaN\nNaN\n\n\n2\n2021-05-26\nAlabama\nLimestone\nAthens\n100812\nAthens State University\n45\n10.0\nNaN\n\n\n3\n2021-05-26\nAlabama\nLee\nAuburn\n100858\nAuburn University\n2742\n567.0\nNaN\n\n\n4\n2021-05-26\nAlabama\nMontgomery\nMontgomery\n100830\nAuburn University at Montgomery\n220\n80.0\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneStep 2: Compute the average number of cases reported across all colleges\n\n\n\n\n\n\nWhat function will help you find the average (mean) value of a column?\nWhat is the average value of cases?\n\ncollege_df['cases'].____()\n\n\n\n\n\n\n\n\n\nNoneStep 3: Compute the total number of cases and total number of cases_2021\n\n\n\n\n\n\nWhat is the average value of cases?\nHow do these two numbers compare? Why might they be different? Hint: Read the variable definitions in the NYT GitHub repo. Are these columns measuring the same thing?\n\ncollege_df[['cases', 'cases_2021']].____()\n\n\n\n\n\n\n\n\n\nNoneStep 4: Compute the total number of cases by state\n\n\n\n\n\n\nUse groupby() and sum() to find the total cases reported by colleges in each state.\nWhich state had the most cases?\nWhich had the fewest?\n\ncollege_df.groupby('state')[['cases']].____()\n\n\n\n\n\n\n\n\n\nNoneStep 5: Focus on colleges in Ohio\n\n\n\n\n\nFilter the data to show only colleges in Ohio. Then compute:\n\nThe total number of cases\nThe average (mean) number of cases across Ohio colleges\n\nohio_df = college_df[college_df['state'] == '____']\n\n# Your aggregations go here",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html",
    "href": "12-joining-data.html",
    "title": "12  Relational data",
    "section": "",
    "text": "12.1 Prerequisites\nIn many data projects, we work with more than one table. A product’s description might be in one table, customer demographics in another, and transaction records in yet another. To make sense of these interconnected datasets, we need to combine them in ways that preserve the relationships between them.\nThis is the world of relational data—data stored in multiple related tables. Each table gives us a piece of the story, but we only get a full picture when we connect the pieces together.\nTo do that, we use joins, which allow us to merge tables using key variables they have in common. In this chapter, we’ll build up your understanding of joins in pandas and give you hands-on experience combining data from multiple sources.\nBy the end of this lesson you’ll be able to:\nBefore we dive into code, watch this short video for a practical overview of how to join DataFrames in pandas. It walks through key concepts like different types of joins and when to use them. After watching, you’ll be ready to roll up your sleeves and apply these techniques in the hands-on examples that follow.\nBefore we dive into real-world data, let’s start by loading the pandas library. This will give us access to the join functions we’ll be using throughout the chapter.\nimport pandas as pd\nTo build your intuition around joins, we’ll begin with two very simple DataFrames, x and y. In these examples:\nHere’s how we’ll create them in code:\nx = pd.DataFrame({'id': [1, 2, 3], 'val_x': ['x1', 'x2', 'x3']})\ny = pd.DataFrame({'id': [1, 2, 4], 'val_y': ['y1', 'y2', 'y4']})\nThese examples will help you understand what joins do and how they behave before we move on to working with larger, more complex tables.\nHowever, we will also build upon the simple examples by using various data sets from the completejourney_py library. This library provides access to data sets characterizing household level transactions over one year from a group of 2,469 households who are frequent shoppers at a grocery store.\nThere are eight built-in data sets available in this library. The data sets include:\nThis is a Python equivalent of the R package completejourney. The R package has a full guide to get you acquainted with the various data set schemas, which you can read here.\nfrom completejourney_py import get_data\n\n# get_data() provides a dictionary of several DataFrames\ncj_data = get_data()\ncj_data.keys()\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/completejourney_py/get_data.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.\n  from pkg_resources import resource_filename\n\n\ndict_keys(['campaign_descriptions', 'coupons', 'promotions', 'campaigns', 'demographics', 'transactions', 'coupon_redemptions', 'products'])\n# We can check out the transactions data with the following\ncj_data['transactions'].head()\n\n\n\n\n\n\n\n\nhousehold_id\nstore_id\nbasket_id\nproduct_id\nquantity\nsales_value\nretail_disc\ncoupon_disc\ncoupon_match_disc\nweek\ntransaction_timestamp\n\n\n\n\n0\n900\n330\n31198570044\n1095275\n1\n0.50\n0.00\n0.0\n0.0\n1\n2017-01-01 11:53:26\n\n\n1\n900\n330\n31198570047\n9878513\n1\n0.99\n0.10\n0.0\n0.0\n1\n2017-01-01 12:10:28\n\n\n2\n1228\n406\n31198655051\n1041453\n1\n1.43\n0.15\n0.0\n0.0\n1\n2017-01-01 12:26:30\n\n\n3\n906\n319\n31198705046\n1020156\n1\n1.50\n0.29\n0.0\n0.0\n1\n2017-01-01 12:30:27\n\n\n4\n906\n319\n31198705046\n1053875\n2\n2.78\n0.80\n0.0\n0.0\n1\n2017-01-01 12:30:27",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#prerequisites",
    "href": "12-joining-data.html#prerequisites",
    "title": "12  Relational data",
    "section": "",
    "text": "The colored column represents the key variable used to match rows between tables.\nThe gray column represents the value column—this is the information that gets carried along in the merge.\n\n\n\n\n\n\n\nSimple Example DFs\n\n\n\n\nFigure 12.1: Two simple DataFrames with a shared key column.\n\n\n\n\n\n\n\n\n\ntransactions: item-level purchases made by households at a retail grocery store\ndemographics: household demographic data (age, income, family size, etc.)\nproducts: product metadata (brand, description, etc.)\ncampaigns: campaigns received by each household\ncampaign_descriptions: campaign metadata (length of time active)\ncoupons: coupon metadata (UPC code, campaign, etc.)\ncoupon_redemptions: coupon redemptions (household, day, UPC code, campaign)\n\n\n\n\n\n\n\n\n\n\nNoneTODO\n\n\n\nTake some time to read about the completejourney data set schema here.\n\nWhat different data sets are available and what do they represent?\nWhat are the common variables between each table?",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#understanding-keys",
    "href": "12-joining-data.html#understanding-keys",
    "title": "12  Relational data",
    "section": "12.2 🔑 Understanding Keys",
    "text": "12.2 🔑 Understanding Keys\nTo combine two tables, we need a way to tell pandas how rows in one table relate to rows in another. That’s where keys come in.\nA key is one or more columns used to match rows between two tables. These columns typically contain identifiers that link observations—like customer IDs, product codes, or dates.\nThere are two main types of keys you’ll encounter:\n\nA primary key uniquely identifies each row within its own table.\nA foreign key connects to a primary key in another table, creating a relationship between the two.\n\n\n\n\n\n\n\nFor example:\n\nIn the transactions table, household_id acts as a foreign key—it tells us which household made the purchase.\nIn the demographics table, household_id is a primary key—each household appears only once.\n\n\n\n\nTogether, these keys form a relation. In most cases, the relationship is one-to-many: one household can have many transactions, but each transaction belongs to only one household. Occasionally, you’ll encounter one-to-one relationships, where each row in one table maps to exactly one row in another.\nWhen data is cleaned appropriately the keys used to match two tables will be commonly named. For example, the variable that can link our x and y data sets is named id:\n\nx.columns.intersection(y.columns)\n\nIndex(['id'], dtype='object')\n\n\nWe can easily see this by looking at the x and y data but when working with larger data sets this becomes more appropriate than just viewing the data. For example, we can easily identify the common columns in the completejourney_py transactions and demographics data:\n\ntransactions = cj_data['transactions']\ndemographics = cj_data['demographics']\n\ntransactions.columns.intersection(demographics.columns)\n\nIndex(['household_id'], dtype='object')\n\n\n\n\n\n\n\n\nNoteA Note on Column Names\n\n\n\nWhile it’s common for keys to have the same name in both tables (like id, household_id, or product_id), that’s not always the case. For example, our household identifier could be named household_id in the transaction data but be hshd_id in the demographics table. Although the names differ, they represent the same information. When column names differ, you can still join the tables—you just need to tell pandas which columns to use, which we will discuss later.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#mutating-joins",
    "href": "12-joining-data.html#mutating-joins",
    "title": "12  Relational data",
    "section": "12.3 Mutating Joins",
    "text": "12.3 Mutating Joins\nWhen working with multiple DataFrames, we often want to combine information from different sources based on a shared key. A mutating join allows us to do exactly that—it matches rows across two tables based on a key and brings in additional columns from one table to the other.\nIn this section, you’ll learn how to use pandas to perform various types of joins that are essential for working with relational data.\n\nTypes of Joins\nThere are several types of joins, each serving a different purpose:\n\nInner join: Keeps only the rows with keys that match in both tables.\nLeft join: Keeps all rows from the left table and brings in matching rows from the right.\nRight join: Keeps all rows from the right table and brings in matching rows from the left.\nFull outer join: Keeps all rows from both tables.\n\n\n\n\n\n\n\nIn pandas, you can join DataFrames using either .join() or .merge(). While .join() is designed for joining on indexes, .merge() is more flexible and allows joining on one or more columns. We’ll use .merge() throughout this chapter.\n\n\n\n\n\nInner Join\nAn inner join returns only the rows where the key exists in both DataFrames. This is the most restrictive type of join.\n\n\n\n\n\n\nInner join\n\n\n\n\nFigure 12.2: Inner join (source).\n\n\n\n\nx.merge(y, on=\"id\", how=\"inner\")\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nx1\ny1\n\n\n1\n2\nx2\ny2\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnly rows with matching values in both x and y are retained. In our example, only id values 1 and 2 appear in both tables.\n\n\n\n\n\nOuter Joins\nAn inner join keeps observations that appear in both tables. However, we often want to retain all observations in at least one of the tables. Consequently, we can apply various outer joins to retain observations that appear in at least one of the tables. There are three main types of outer joins:\n\nA left join keeps all observations in x.\nA right join keeps all observations in y.\nA full join keeps all observations in x and y.\n\nThese joins work by adding NaN in rows where non-matching information exists:\n\n\n\n\n\n\nExamples of outer joins\n\n\n\n\nFigure 12.3: Difference in left join, right join, and outer join procedures (source).\n\n\n\n\nLeft Join\nA left join keeps all rows from the left DataFrame (x) and adds matching rows from the right DataFrame (y). If no match is found, the result will contain NaN for the missing values.\n\nx.merge(y, on=\"id\", how=\"left\")\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nx1\ny1\n\n\n1\n2\nx2\ny2\n\n\n2\n3\nx3\nNaN\n\n\n\n\n\n\n\n\n\nRight Join\nA right join is similar to a left join, but it retains all rows from the right DataFrame (y).\n\nx.merge(y, on=\"id\", how=\"right\")\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nx1\ny1\n\n\n1\n2\nx2\ny2\n\n\n2\n4\nNaN\ny4\n\n\n\n\n\n\n\n\n\n\n\n\n\nShould I use a right join, or a left join? To answer this, ask yourself “which DataFrame should retain all of its rows?” - and use this one as the baseline. A left join keeps all the rows in the first (leftside) DataFrame written in the command, whereas a right join keeps all the rows in the second (rightside) DataFrame.\n\n\n\n\n\nFull Outer Join\nA full outer join retains all rows from both DataFrames. Where there are no matches, it fills in NaN for missing values.\n\nx.merge(y, on=\"id\", how=\"outer\")\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nx1\ny1\n\n\n1\n2\nx2\ny2\n\n\n2\n3\nx3\nNaN\n\n\n3\n4\nNaN\ny4\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is the most inclusive join. It’s useful when you don’t want to lose any data.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#working-with-differently-named-keys",
    "href": "12-joining-data.html#working-with-differently-named-keys",
    "title": "12  Relational data",
    "section": "12.4 Working with Differently Named Keys",
    "text": "12.4 Working with Differently Named Keys\nSo far, the keys we’ve used to join two DataFrames have had the same name. This was encoded by using on='id'. However, having keys with the same name is not a requirement. But what happens we our common key variable is named differently in each DataFrame?\nFor example:\n\na = pd.DataFrame({'id_a': [1, 2, 3], 'val_a': ['x1', 'x2', 'x3']})\nb = pd.DataFrame({'id_b': [1, 2, 4], 'val_b': ['y1', 'y2', 'y4']})\n\nIn this case, since our common key variable has different names in each table (id_a in a and id_b in b), our inner join function doesn’t know how to join these two DataFrames and an error results.\n\n\n\n\n\n\nWarningExample merge error\n\n\n\n\n\n\na.merge(b)\n\n\n---------------------------------------------------------------------------\nMergeError                                Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 a.merge(b)\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/frame.py:10839, in DataFrame.merge(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\n  10820 @Substitution(\"\")\n  10821 @Appender(_merge_doc, indents=2)\n  10822 def merge(\n   (...)\n  10835     validate: MergeValidate | None = None,\n  10836 ) -&gt; DataFrame:\n  10837     from pandas.core.reshape.merge import merge\n&gt; 10839     return merge(\n  10840         self,\n  10841         right,\n  10842         how=how,\n  10843         on=on,\n  10844         left_on=left_on,\n  10845         right_on=right_on,\n  10846         left_index=left_index,\n  10847         right_index=right_index,\n  10848         sort=sort,\n  10849         suffixes=suffixes,\n  10850         copy=copy,\n  10851         indicator=indicator,\n  10852         validate=validate,\n  10853     )\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/reshape/merge.py:170, in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\n    155     return _cross_merge(\n    156         left_df,\n    157         right_df,\n   (...)\n    167         copy=copy,\n    168     )\n    169 else:\n--&gt; 170     op = _MergeOperation(\n    171         left_df,\n    172         right_df,\n    173         how=how,\n    174         on=on,\n    175         left_on=left_on,\n    176         right_on=right_on,\n    177         left_index=left_index,\n    178         right_index=right_index,\n    179         sort=sort,\n    180         suffixes=suffixes,\n    181         indicator=indicator,\n    182         validate=validate,\n    183     )\n    184     return op.get_result(copy=copy)\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/reshape/merge.py:786, in _MergeOperation.__init__(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\n    779     msg = (\n    780         \"Not allowed to merge between different levels. \"\n    781         f\"({_left.columns.nlevels} levels on the left, \"\n    782         f\"{_right.columns.nlevels} on the right)\"\n    783     )\n    784     raise MergeError(msg)\n--&gt; 786 self.left_on, self.right_on = self._validate_left_right_on(left_on, right_on)\n    788 (\n    789     self.left_join_keys,\n    790     self.right_join_keys,\n   (...)\n    793     right_drop,\n    794 ) = self._get_merge_keys()\n    796 if left_drop:\n\nFile /opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/pandas/core/reshape/merge.py:1572, in _MergeOperation._validate_left_right_on(self, left_on, right_on)\n   1570 common_cols = left_cols.intersection(right_cols)\n   1571 if len(common_cols) == 0:\n-&gt; 1572     raise MergeError(\n   1573         \"No common columns to perform merge on. \"\n   1574         f\"Merge options: left_on={left_on}, \"\n   1575         f\"right_on={right_on}, \"\n   1576         f\"left_index={self.left_index}, \"\n   1577         f\"right_index={self.right_index}\"\n   1578     )\n   1579 if (\n   1580     not left_cols.join(common_cols, how=\"inner\").is_unique\n   1581     or not right_cols.join(common_cols, how=\"inner\").is_unique\n   1582 ):\n   1583     raise MergeError(f\"Data columns not unique: {repr(common_cols)}\")\n\nMergeError: No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False\n\n\n\n\n\n\nWhen this happens, we can explicitly tell our join function to use unique key names in each DataFrame as a common key with the left_on and right_on arguments:\n\na.merge(b, left_on=\"id_a\", right_on=\"id_b\")\n\n\n\n\n\n\n\n\nid_a\nval_a\nid_b\nval_b\n\n\n\n\n0\n1\nx1\n1\ny1\n\n\n1\n2\nx2\n2\ny2",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#a-larger-example-with-complete-journey-data",
    "href": "12-joining-data.html#a-larger-example-with-complete-journey-data",
    "title": "12  Relational data",
    "section": "12.5 A Larger Example with Complete Journey Data",
    "text": "12.5 A Larger Example with Complete Journey Data\nLet’s apply what we’ve learned to real data from the completejourney_py package.\nSuppose we want to add product details to each transaction. That means we’ll join the transactions and products DataFrames. Because we want to retain all transaction records, even if product details are missing, we’ll use a left join.\nFirst, check the column names:\n\ncj_data = get_data()\ntransactions = cj_data[\"transactions\"]\nproducts = cj_data[\"products\"]\n\nprint(f'transactions columns: {transactions.columns}')\nprint(f'products columns: {products.columns}')\n\ntransactions columns: Index(['household_id', 'store_id', 'basket_id', 'product_id', 'quantity',\n       'sales_value', 'retail_disc', 'coupon_disc', 'coupon_match_disc',\n       'week', 'transaction_timestamp'],\n      dtype='object')\nproducts columns: Index(['product_id', 'manufacturer_id', 'department', 'brand',\n       'product_category', 'product_type', 'package_size'],\n      dtype='object')\n\n\nAnd we can find if a common column name exists:\n\ntransactions.columns.intersection(products.columns)\n\nIndex(['product_id'], dtype='object')\n\n\nWe see that both DataFrames share the product_id column. This aligns to the data dictionary so we can trust this is the accurate common key. We can now perform a left join usingproduct_id as the common key.\n\n\n\n\n\n\nJoins add new variables to the far right of the resulting DataFrame. If you’re working in a wide table, you may need to scroll to see the added columns.\n\n\n\n\ntransactions.merge(products, on=\"product_id\", how=\"left\").head()\n\n\n\n\n\n\n\n\nhousehold_id\nstore_id\nbasket_id\nproduct_id\nquantity\nsales_value\nretail_disc\ncoupon_disc\ncoupon_match_disc\nweek\ntransaction_timestamp\nmanufacturer_id\ndepartment\nbrand\nproduct_category\nproduct_type\npackage_size\n\n\n\n\n0\n900\n330\n31198570044\n1095275\n1\n0.50\n0.00\n0.0\n0.0\n1\n2017-01-01 11:53:26\n2.0\nPASTRY\nNational\nROLLS\nROLLS: BAGELS\n4 OZ\n\n\n1\n900\n330\n31198570047\n9878513\n1\n0.99\n0.10\n0.0\n0.0\n1\n2017-01-01 12:10:28\n69.0\nGROCERY\nPrivate\nFACIAL TISS/DNR NAPKIN\nFACIAL TISSUE & PAPER HANDKE\n85 CT\n\n\n2\n1228\n406\n31198655051\n1041453\n1\n1.43\n0.15\n0.0\n0.0\n1\n2017-01-01 12:26:30\n69.0\nGROCERY\nPrivate\nBAG SNACKS\nPOTATO CHIPS\n11.5 OZ\n\n\n3\n906\n319\n31198705046\n1020156\n1\n1.50\n0.29\n0.0\n0.0\n1\n2017-01-01 12:30:27\n2142.0\nGROCERY\nNational\nREFRGRATD DOUGH PRODUCTS\nREFRIGERATED BAGELS\n17.1 OZ\n\n\n4\n906\n319\n31198705046\n1053875\n2\n2.78\n0.80\n0.0\n0.0\n1\n2017-01-01 12:30:27\n2326.0\nGROCERY\nNational\nSEAFOOD - SHELF STABLE\nTUNA\n5.0 OZ\n\n\n\n\n\n\n\nThis has now added product information to each transaction. Consequently, if we wanted to get the total sales across the meat department but summarized at the product_category level so that we can identify which products generate the greatest sales we could follow this joining procedure with additional skills we learned in previous lessons:\n\n(\n    transactions\n    .merge(products, how='left', on='product_id')\n    .query(\"department == 'MEAT'\")\n    .groupby('product_category', as_index=False)\n    .agg({'sales_value': 'sum'})\n    .sort_values(by='sales_value', ascending=False)\n)\n\n\n\n\n\n\n\n\nproduct_category\nsales_value\n\n\n\n\n1\nBEEF\n176614.54\n\n\n2\nCHICKEN\n52703.51\n\n\n10\nPORK\n50809.31\n\n\n12\nSMOKED MEATS\n15324.22\n\n\n13\nTURKEY\n11128.95\n\n\n4\nEXOTIC GAME/FOWL\n860.42\n\n\n5\nLAMB\n829.27\n\n\n14\nVEAL\n167.13\n\n\n8\nMEAT SUPPLIES\n57.03\n\n\n7\nMEAT - MISC\n39.66\n\n\n11\nRW FRESH PROCESSED MEAT\n30.84\n\n\n3\nCOUPON\n7.00\n\n\n6\nLUNCHMEAT\n2.20\n\n\n9\nMISCELLANEOUS\n0.95\n\n\n0\nBACON\n0.30\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nJoin the transactions and demographics data so that you have household demographics for each transaction. Now compute the total sales by age category to identify which age group generates the most sales.\nUse successive joins to join transactions with coupons and then with coupon_redemptions. Use the proper join that will only retain those transactions that have coupon and coupon redemption data.\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#merge-indicator",
    "href": "12-joining-data.html#merge-indicator",
    "title": "12  Relational data",
    "section": "12.6 Merge Indicator",
    "text": "12.6 Merge Indicator\nYou can use the indicator argument to add a special column called _merge that shows where each row in the joined table came from. The values in _merge will be:\n\n'left_only' — the row came only from the left table\n'right_only' — the row came only from the right table\n'both' — the row had a match in both tables\n\nThis is helpful when you’re trying to understand or debug the result of a join:\n\nx.merge(y, how='outer', indicator=True)\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n_merge\n\n\n\n\n0\n1\nx1\ny1\nboth\n\n\n1\n2\nx2\ny2\nboth\n\n\n2\n3\nx3\nNaN\nleft_only\n\n\n3\n4\nNaN\ny4\nright_only\n\n\n\n\n\n\n\nThis feature is also useful when filtering rows based on whether they had a match. For example:\n\n\n\n\n\n\nNoneScenario: Your manager asks,\n\n\n\n\n“Of all our transactions, how many are from households that we don’t have demographic information for?”\n\n\n\nYou can answer this by performing an outer join between transactions and demographics, then filtering for rows with _merge == 'left_only':\n\n# Total number of transactions\ntransactions.shape\n\n(1469307, 11)\n\n\n\n# Transactions without matching demographic info\n(\n    transactions\n    .merge(demographics, how='outer', indicator=True)\n    .query(\"_merge == 'left_only'\")\n).shape\n\n(640457, 19)\n\n\nIn this case, 640,457 transactions (about 43%) come from households without demographic information.\n\n🔍 Knowledge Check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nUsing the products and transactions tables, how many products have been sold?\nHow many products are in inventory but have not appeared in any transaction?\nUsing demographics and transactions, which income group buys the highest total quantity of goods?\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#summary",
    "href": "12-joining-data.html#summary",
    "title": "12  Relational data",
    "section": "12.7 Summary",
    "text": "12.7 Summary\nIn this chapter, you explored how to work with relational data—data that lives across multiple, related tables. You learned how to use joins in pandas to combine these tables using shared key variables, allowing you to answer more complex and meaningful questions.\nYou practiced:\n\nIdentifying and working with keys (primary and foreign).\nPerforming mutating joins including inner, left, right, and full outer joins.\nHandling differently named key columns using left_on and right_on.\nUsing the merge indicator to understand which rows matched or didn’t match between tables.\nApplying these techniques to real retail transaction data using the completejourney_py package.\n\nRelational data techniques give you the power to piece together a fuller picture of what’s happening in your data. But once you’ve wrangled the data into shape, the next challenge is making your findings clear and compelling.\nIn the next chapter, we’ll shift gears and explore data visualization—learning how to communicate insights effectively using Python plotting libraries like matplotlib and seaborn.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "12-joining-data.html#exercise-understanding-shopping-behavior",
    "href": "12-joining-data.html#exercise-understanding-shopping-behavior",
    "title": "12  Relational data",
    "section": "12.8 Exercise: Understanding Shopping Behavior",
    "text": "12.8 Exercise: Understanding Shopping Behavior\nUse the datasets provided by the completejourney_py package to complete the following exercises. These tasks will help you practice joining tables, filtering data, and computing summary statistics. If you’re unfamiliar with the structure of these datasets, take a few minutes to review the Complete Journey data dictionary to understand how the tables relate to one another.\n\n\n\n\n\n\nNone1. Total Sales by Age Group (With a Join)\n\n\n\n\n\nJoin the transactions and demographics tables. Then compute the total sales_value by age group.\n\nWhich age group generates the most total sales?\nBonus: How does this change if you only consider transactions over $5?\n\n\n\n\n\n\n\n\n\n\nNone2. What Are Families Buying?\n\n\n\n\n\nUsing the transactions, products, and demographics tables:\n\nFocus only on households with 3 or more members (use household_size from demographics).\nIdentify the top 5 most frequently purchased product categories among these households.\nWhat percent of their purchases come from the MEAT department?\n\n\n\n\n\n\n\n\n\n\nNone3. Coupon-Driven Purchases\n\n\n\n\n\nJoin transactions, coupon_redemptions, and coupons together. Then:\n\nFind the total number of coupon redemptions by campaign.\nWhich campaign resulted in the highest total sales_value?\n\n\n\n\n\n\n\n\n\n\nNone4. High Spenders Without Demographics\n\n\n\n\n\nFrom your existing prompt:\n\nIdentify households with total sales ≥ $100.\nAmong them, how many do not appear in the demographics table?\nWhat percent of all $100+ spenders are missing demographic info?\n\nTip: Use the indicator=True argument to help with this.\n\n\n\n\n\n\n\n\n\nNone5. Front Display Effectiveness\n\n\n\n\n\nJoin promotions with transactions and products.\n\nFilter to products that were part of a front-of-store display (display_location == 1).\nFor those products, compute:\n\nTotal sales_value\nTotal quantity sold\nTop 3 departments (by sales)\n\n\nHow do these compare to products not on display?",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html",
    "href": "13-data-viz-pandas.html",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "",
    "text": "13.1 Prerequisites\nData visualization is a core skill in any data analyst’s toolkit. While numbers and statistics tell part of the story, charts and graphs help us understand patterns, uncover insights, and communicate findings more effectively. There are two primary use cases for visualization in data science:\nIn this chapter, we focus on exploratory visualization using Pandas, a quick and powerful way to visually explore data and uncover insights — all within the same environment where your analysis lives. In future chapters, we’ll dive deeper into refining visualizations for explanatory purposes, showing how libraries like Matplotlib* and Bokeh can help you tell compelling, professional-grade data stories.\nBy the end of this chapter, you will be able to:\nWe’ll use the completejourney_py data to analyze household shopping behavior. First, import the libraries and prepare your data:\nimport pandas as pd\nfrom completejourney_py import get_data\n\ncj_data = get_data()\ntransactions = cj_data['transactions']\nproducts = cj_data['products']\ndemographics = cj_data['demographics']\n\n# Merge datasets to enrich transaction data\ndf = (transactions\n      .merge(products, on='product_id', how='left')\n      .merge(demographics, on='household_id', how='left'))\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/completejourney_py/get_data.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.\n  from pkg_resources import resource_filename",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html#the-.plot-attribute",
    "href": "13-data-viz-pandas.html#the-.plot-attribute",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "13.2 The .plot Attribute",
    "text": "13.2 The .plot Attribute\nOne of the most convenient aspects of using Pandas for data visualization is that Series and DataFrame objects come equipped with a built-in .plot attribute. This attribute serves as a simple wrapper around the powerful Matplotlib library (which we’ll discuss in the next chapter), enabling you to generate a wide range of plots with minimal code.\nThis .plot attribute is a little unique as you can call it as a method and specify the plot of interest using an argument:\ndf.plot(kind='scatter', ...)\nOr you can use it to access sub-methods:\ndf.plot.scatter(...)\nThere are several plotting options via .plot, including but not limited to:\n\nline: (default) for time series or continuous data\nbar / barh: for categorical comparisons\nhist: for distributions of continuous variables\nbox: for spotting outliers and summary statistics\nscatter: for examining relationships between two numeric variables\n\nHere’s a simple example of a histogram created from a Series:\n\ndf['sales_value'].plot.hist(bins=20, log=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneVideo Primer\n\n\n\nHere’s a nice introductory video to watch before we dig into the details that follow.\n\n\n\n\n🔍 Knowledge Check\n\n\n\n\n\n\nNoteDo This!\n\n\n\nCheck out the .plot documentation for Series and DataFrames.\n\nWhat parameter would you use to control the figure size?\nWhat parameter would you use to add a title?\nWhat parameter(s) would you use to log scale an x and/or y axis?",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html#visualizing-single-variables-univariate-plots",
    "href": "13-data-viz-pandas.html#visualizing-single-variables-univariate-plots",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "13.3 Visualizing Single Variables (Univariate Plots)",
    "text": "13.3 Visualizing Single Variables (Univariate Plots)\nUnivariate plots help us understand the distribution, range, and key characteristics of a single variable. These visualizations are especially useful during exploratory data analysis, where you want to:\n\nSpot patterns, such as skewness or multimodality\nIdentify outliers or unusual values\nCompare distributions across different groups (later, with grouping)\n\nCommon types of univariate plots include:\n\nHistograms: Show the distribution of a numerical variable by dividing values into bins.\nBoxplots: Display the median, quartiles, and potential outliers in a compact format.\nBar plots (for categorical variables): Visualize frequency or total values across discrete categories.\n\nThese plots are quick to generate and provide immediate insight into the shape and spread of your data. Let’s look at a couple of examples using the sales_value variable. We’ve already looked at some summary stats of our sales_value distribution…\n\ndf['sales_value'].describe()\n\ncount    1.469307e+06\nmean     3.128032e+00\nstd      4.290385e+00\nmin      0.000000e+00\n25%      1.290000e+00\n50%      2.000000e+00\n75%      3.490000e+00\nmax      8.400000e+02\nName: sales_value, dtype: float64\n\n\nbut we can understand more by visualizing this variable. But if we look at a basic histogram it doesn’t tell us a whole lot because we have several zero dollar transactions (i.e. returns) in our data plus this feature is heavily skewed towards low value transactions (measured by cents or single dollar values).\n\ndf['sales_value'].plot.hist()\n\n\n\n\n\n\n\n\nWe can make some adjustments such as remove any zero dollar transactions, log transform our axis, and increase the number of bins. This helps to pull out additional insights in our sales_value distribution. For example, we see we have a lot of very low dollar transactions and the frequency decreases as the transaction dollar amount increases. However, we also see an increase in transactions right at the $200 mark but that decreases quickly. There are also a few outlier transaction values that are around the $600 and $800 value marks.\n\n(\n    df.loc[df['sales_value'] &gt; 0, 'sales_value']\n    .plot.hist(log=True, bins=30, title='Distribution of Sales Values')\n);\n\n\n\n\n\n\n\n\nBox plots and kernel density estimation (KDE) plots are an alternative way to view univariate distribtions. For example, let’s compute the total sales_value across all stores. The resulting sales_by_store object is a Series. A boxplot provides a lot of information (read about them here). We can see that the median (red line) is around \\(10^2 = 100\\) and the interquartile range is within the blue box. We also see we have some outliers on the upper end.\n\nsales_by_store = df.groupby('store_id')['sales_value'].sum()\n\n# boxplot\nsales_by_store.plot.box(logy=True, title='Distribution of total sales across all stores');\n\n\n\n\n\n\n\n\nWe can quickly compare our boxplot with our numeric distribution and we see our they are similar (median: 96, interquartile range: 25-2966).\n\nsales_by_store.describe()\n\ncount       457.000000\nmean      10056.979387\nstd       20671.239346\nmin           0.500000\n25%          25.390000\n50%          95.590000\n75%        2965.560000\nmax      148169.670000\nName: sales_value, dtype: float64\n\n\nThe KDE plot (which is also produced with .plot.density()) provides a smoothed histogram.\n\nsales_by_store.plot.kde(title='Distribution of total sales across all stores');\n\n\n\n\n\n\n\n\nThe .plot sub-methods work exceptionally well with time series data. To illustrate, let’s create a Series that contains the sales_value of each transaction with the transaction_timestamp as the index.\n\nsales = df.set_index('transaction_timestamp')['sales_value']\nsales.head()\n\ntransaction_timestamp\n2017-01-01 11:53:26    0.50\n2017-01-01 12:10:28    0.99\n2017-01-01 12:26:30    1.43\n2017-01-01 12:30:27    1.50\n2017-01-01 12:30:27    2.78\nName: sales_value, dtype: float64\n\n\nA handy method we have not talked about is resample() which allows us to easily convert time series data. For example, if we wanted to sum all sales_values by the hour we can use .resample('h') followed by .sum().\n\nsales.resample('h').sum()\n\ntransaction_timestamp\n2017-01-01 11:00:00      0.50\n2017-01-01 12:00:00     19.76\n2017-01-01 13:00:00     41.92\n2017-01-01 14:00:00     97.31\n2017-01-01 15:00:00    769.62\n                        ...  \n2018-01-01 00:00:00    607.66\n2018-01-01 01:00:00    524.89\n2018-01-01 02:00:00    493.45\n2018-01-01 03:00:00    256.74\n2018-01-01 04:00:00     10.52\nFreq: h, Name: sales_value, Length: 8754, dtype: float64\n\n\nIf we followed this sequence of code with plot.line() we get a line plot of the total sales values on the y-axis and the date-time on the x-axis.\n\n(\n    sales\n    .resample('h')\n    .sum()\n    .plot.line(figsize=(10,4))\n);\n\n\n\n\n\n\n\n\nThe above plot is a bit busy since we’re plotting values for every hour over the course of a year. Let’s reduce the frequency and, instead, sum the sales_values by day (.resample('D')). Now we see a bit more of a descriptive pattern. It looks like there is routinely higher sales transactions on particular days (probably certain days of the week such as weekends).\n\n(\n    sales\n    .resample('D')\n    .sum()\n    .plot.line(figsize=(10,4))\n);\n\n\n\n\n\n\n\n\nLet’s validate our assumption above regarding the weekly shopping pattern. The below code chunk performs the same as above where we compute total daily sales across all days of the year but then we follow that up by extracting the name of the weekday from the date-timestamp and then grouping by the day of week and computing the median and interquartile range (IQR) for all daily sales for the year.\n\n\n\n\n\n\nIf you have not yet seen code that looks like lambda idx: idx.day_name() do not worry. These are called lambda (anonymous) functions and we’ll discuss them more in a future chapter.\n\n\n\nWe definitely see that Saturday and Sunday are the weekdays with the heaviest sales value transactions.\n\nday_order = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\ntotal_sales_by_weekday = (\n    sales\n    .resample('D')                        # resample by day\n    .sum()                                # compute total daily sales\n    .rename(lambda idx: idx.day_name())   # extract week day name from date-timestamp\n    .groupby('transaction_timestamp')     # group by day of week\n    .quantile([.25, .5, .75])             # compute median and IQR of sales values\n    .unstack()                            # flatten output (results in a DataFrame)\n    .reindex(day_order)                   # force index to follow weekday order         \n)\n\ntotal_sales_by_weekday.plot.line(title='Median and IQR of total sales by weekday', figsize=(10,4));\n\n\n\n\n\n\n\n\nAnother common plot for Series data is the bar plot. Let’s look at the median values from the analysis above. If we peak at the result we see we have a Series that contains the median total sales values for each weekday.\n\nmedian_sales_by_weekday = total_sales_by_weekday[0.50]\nmedian_sales_by_weekday\n\ntransaction_timestamp\nMonday       12129.070\nTuesday      11191.945\nWednesday    10558.995\nThursday     10852.630\nFriday       11959.995\nSaturday     14741.720\nSunday       15745.450\nName: 0.5, dtype: float64\n\n\nRather than plot this as a line chart as we did above, we can use .plot.bar() to create a bar plot:\n\nmedian_sales_by_weekday.plot.bar(title='Median total sales by weekday', figsize=(8,4));\n\n\n\n\n\n\n\n\nA common pattern you’ll use is to follow a .value_counts() method call with a bar plot. For example, say we want to assess the number of transactions in our data by department. We could easily get this with the following:\n\n(\n    df['department']\n    .value_counts(ascending=True)\n    .plot.barh(title='Total transactions by department', figsize=(6,8))\n);\n\n\n\n\n\n\n\n\nUnfortunately, we see a lot of very small values that overcrowds the plot. We can make some small adjustments to our code to leave all department values for those departments in the top 10 as is but for all departments not in the top 10 we can condense them down to an ‘Other’ category.\n\n\n\n\n\n\nWe will discuss the .where() method in a later module. For now just realize its a way to apply an if-else condition to a Series.\n\n\n\n\ntop10 = df['department'].value_counts().index[:10]\nisin_top10 = df['department'].isin(top10)\n\n(\n    df['department']\n    .where(isin_top10, 'Other')\n    .value_counts(ascending=True)\n    .plot.barh(title='Total transaction sales by department', figsize=(6,6))\n);\n\n\n\n\n\n\n\n\n\n🔍 Summary: Exploring Univariate Distributions Through Iteration\nThis section demonstrates how we can iteratively filter, aggregate, and visualize data to better understand the distribution and characteristics of a single variable — in this case, sales_value. Starting with basic summary statistics, we build deeper insights by:\n\nFiltering out uninformative records (e.g., $0 transactions)\nAggregating data at relevant levels (e.g., by basket or store)\nApplying different types of univariate plots (histograms, boxplots, KDEs) to reveal patterns, outliers, and skew\n\nThrough this iterative approach, we move from basic descriptive stats to more nuanced visual understanding of sales behavior — helping us identify patterns like high-frequency low-dollar transactions or weekly sales cycles. The section also illustrates how to apply .plot sub-methods for time series and categorical visualizations, reinforcing how visual exploration and data transformation go hand-in-hand in exploratory data analysis.\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This!\n\n\n\n\nCreate a histogram for the quantity column. Remove any zero quantities and/or adjust the axis to make the plot more informative.\nCompute the sum of quantity for each store_id. Now create density plot and box plot. Compare these plots to the summary statistics provided by .describe().\nUse .resample() to compute the sum of quantity for each day. Plot the results to assess if there is similar pattern as we saw with sales_value.\nUse a bar plot to plot the total transaction quantities by department. Are the results similar to what we saw with total sales_value by department?",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html#visualizing-relationships-between-variables-bivariate-plots",
    "href": "13-data-viz-pandas.html#visualizing-relationships-between-variables-bivariate-plots",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "13.4 Visualizing Relationships Between Variables (Bivariate Plots)",
    "text": "13.4 Visualizing Relationships Between Variables (Bivariate Plots)\nIn the previous section, we focused on understanding the distribution of a single variable — sales_value — using univariate plots. But as we explored patterns like how sales varied by day of the week or department, we were already stepping into the world of bivariate analysis: looking at how one variable (sales) changes in relation to another (like day or department).\nThis section builds on that by introducing formal approaches to bivariate visualization — using Pandas’ .plot() method with DataFrames to uncover relationships between two variables. These visualizations help answer questions like:\n\nHow do total sales compare across demographic groups?\nAre some product categories driving more revenue than others?\nHow does purchasing behavior change over time?\n\nLet’s look at some examples of bar plots, line plots, and scatter plots to explore bivariate comparisons and trends — reinforcing how visualizing relationships is key to discovering insights in your data.\n\nScatter Plots: Continuous Relationships\nTo visualize the relationship between two continuous variables, such as sales_value and quantity, we can use a scatter plot. This requires specifying the x and y arguments:\n\ndf.plot.scatter(x='quantity', y='sales_value', title='Sales versus quantity', figsize=(8,4))\n\n\n\n\n\n\n\n\nThis plot helps us see whether higher quantities tend to drive higher sales values (they do — but with a lot of low-spend noise). Scatter plots are ideal for spotting correlations, clusters, and outliers.\n\n\nBar Charts: Group Comparisons\nAlthough we previously used bar plots in a univariate context (e.g., frequency of departments), they’re just as valuable for bivariate visualizations — especially when summarizing a numeric variable across categories.\nLet’s say we want to view the top 10 departments by total sales:\n\ndept_sales = (\n    df\n    .groupby('department', as_index=False)\n    .agg({'sales_value': 'sum'})\n    .nlargest(10, 'sales_value')\n    .reset_index(drop=True)\n)\n\ndept_sales\n\n\n\n\n\n\n\n\ndepartment\nsales_value\n\n\n\n\n0\nGROCERY\n2316393.89\n\n\n1\nDRUG GM\n596827.45\n\n\n2\nFUEL\n329594.45\n\n\n3\nPRODUCE\n322858.82\n\n\n4\nMEAT\n308575.33\n\n\n5\nMEAT-PCKGD\n232282.53\n\n\n6\nDELI\n148344.06\n\n\n7\nMISCELLANEOUS\n78858.67\n\n\n8\nPASTRY\n69116.68\n\n\n9\nNUTRITION\n57261.22\n\n\n\n\n\n\n\nTo visualize this, we plot department on the x-axis and sales_value on the y-axis:\n\n(\n    dept_sales\n    .sort_values('sales_value')\n    .plot.barh(x='department', y='sales_value', color='red')\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s common to sort values before bar plotting to produce a more readable left-to-right visual.\n\n\n\n\n\nMulti-Series Plots\nOne benefit of plotting from a DataFrame is that we can visualize multiple numeric columns at once. Suppose we want to compare total sales_value and quantity per department:\n\ndept_totals = (\n    df\n    .query(\"department != 'FUEL' & department != 'MISCELLANEOUS'\")\n    .groupby('department', as_index=False)\n    .agg({'sales_value': 'sum', 'quantity': 'sum'})\n    .nlargest(10, 'sales_value')\n    .reset_index(drop=True)\n)\n\ndept_totals\n\n\n\n\n\n\n\n\ndepartment\nsales_value\nquantity\n\n\n\n\n0\nGROCERY\n2316393.89\n1242944\n\n\n1\nDRUG GM\n596827.45\n198635\n\n\n2\nPRODUCE\n322858.82\n185444\n\n\n3\nMEAT\n308575.33\n67526\n\n\n4\nMEAT-PCKGD\n232282.53\n83423\n\n\n5\nDELI\n148344.06\n37954\n\n\n6\nPASTRY\n69116.68\n28132\n\n\n7\nNUTRITION\n57261.22\n25024\n\n\n8\nSEAFOOD-PCKGD\n35977.46\n8028\n\n\n9\nFLORAL\n22303.18\n3160\n\n\n\n\n\n\n\nWe can pass both columns to the y argument to produce a grouped bar chart:\n\n(\n    dept_totals\n    .sort_values('sales_value')\n    .plot.barh(x='department', y=['sales_value', 'quantity'])\n    .legend(loc='lower right')\n)\n\n\n\n\n\n\n\n\nThis is especially useful when comparing two related metrics across the same category.\n\n\nTime Series with Multiple Columns\nWhen your DataFrame includes a datetime index, the .plot() methods become even more powerful. For example, let’s look at daily total discounts for the GROCERY department across three types of discounts:\n\ntotal_daily_discounts = (\n    df\n    .query(\"department == 'GROCERY'\")\n    .set_index('transaction_timestamp')\n    .loc[:, ['retail_disc', 'coupon_disc', 'coupon_match_disc']]\n    .resample('D')\n    .sum()\n)\n\ntotal_daily_discounts.head()\n\n\n\n\n\n\n\n\nretail_disc\ncoupon_disc\ncoupon_match_disc\n\n\ntransaction_timestamp\n\n\n\n\n\n\n\n2017-01-01\n925.84\n15.75\n1.90\n\n\n2017-01-02\n1158.67\n13.14\n4.55\n\n\n2017-01-03\n1153.42\n13.75\n5.84\n\n\n2017-01-04\n1266.71\n6.10\n2.10\n\n\n2017-01-05\n1359.38\n30.74\n5.64\n\n\n\n\n\n\n\nUsing .plot.line() will automatically generate a multi-line plot for all three columns:\n\ntotal_daily_discounts.plot.line(logy=True, figsize=(10, 4));\n\n\n\n\n\n\n\n\nYou can apply this same logic to other plot types like KDE, histogram, or box plots:\n\ntotal_daily_discounts.plot.kde(logx=True);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote📚 Want to See More Pandas Plot Examples?\n\n\n\nIf you’re looking to go beyond the examples you’ve seen thus far, check out these high-quality resources:\n\nPandas Official Visualization Docs The most authoritative guide for all things .plot(), with clear examples and customization options.\nPython Graph Gallery – Pandas Section A collection of simple, well-formatted Pandas plotting examples with explanations and visuals.\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry This!\n\n\n\n\nCompute the average sales_value and quantity by household_id. Create a density plot that visualizes both columns together.\nUse a bar plot to assess whether married versus unmarried customers produce more transactions. Then do the same for age groups.\nUse .resample() to compute the monthly totals of quantity and sales_value. Plot the results to explore which months are the busiest.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html#under-the-hood---matplotlib",
    "href": "13-data-viz-pandas.html#under-the-hood---matplotlib",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "13.5 Under the hood - Matplotlib",
    "text": "13.5 Under the hood - Matplotlib\nUnderneath the hood Pandas is using Matplotlib to create the plots. Matplotlib is the most tried-and-true, mature plotting library in Python; however, its a bit more difficult to digest Matplotlib which is why I first introduce plotting with Pandas.\nIn the next lesson we will dig into Matplotlib because, with it being the most popular plotting library in the Python ecosystem, it is important for you to have a baseline understanding of its capabilities. But one thing I want to point out here is, since Pandas builds plots based on Matplotlib, we can actually use Matplotlib in conjunction with Pandas to advance our plots.\nFor example, Matplotlib provides many style options that can be used to beautify our plots. If you are familiar with fivethirtyeight.com you’ll know that most of their visualizations have a consistent theme. We can use Matplotlib to change the style of our plots to look like fivethirtyeight plots.\n\nimport matplotlib.pyplot as plt\n\nplt.style.use('fivethirtyeight')\n\nmedian_sales_by_weekday.plot.bar(title='Median total sales by weekday', figsize=(8,4))\n\n\n\n\n\n\n\n\nWe may also want to refine our tick marks so that they are formatted in the units of interest. For example, below we use Matplotlib’s ticker module to format our y-axis to be in dollar and comma formatted units:\n\nimport matplotlib.ticker as mtick\n\ntick_format = mtick.StrMethodFormatter('${x:,.0f}')\n\n(\n    median_sales_by_weekday\n    .plot.bar(title='Median total sales by weekday', xlabel='', figsize=(8,4))\n    .yaxis.set_major_formatter(tick_format)\n)\n\n\n\n\n\n\n\n\nWe’ll explore more Matplotlib capabilities in the next lesson but for now, happy Pandas plotting!",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html#summary",
    "href": "13-data-viz-pandas.html#summary",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "13.6 Summary",
    "text": "13.6 Summary\nIn this chapter, we explored how to use Pandas’ built-in .plot() functionality to quickly and effectively visualize data. Starting with univariate plots, we examined how to understand the distribution of a single variable — like sales_value — using histograms, boxplots, and KDE plots. We saw how filtering and transforming data before plotting helps surface key insights, such as frequent low-dollar transactions and outlier behavior.\nWe then moved into bivariate visualizations, where we analyzed how one variable changes in relation to another. We used scatter plots to assess continuous relationships, bar charts to compare grouped values (like total sales by department), and time series plots to understand trends over days, weeks, and months. Along the way, we learned how .plot() works with both Series and DataFrames, and how to create multi-series plots for deeper comparative insight.\nWhile Pandas plots are perfect for fast exploratory analysis, they also serve as a gentle introduction to the broader Python visualization ecosystem.\n\nWhat’s Next?\nIn the next chapter, we’ll dive deeper into the core engine behind Pandas plots — the Matplotlib library. You’ll learn how to build highly customized visualizations from scratch using Matplotlib’s object-oriented API. This will give you full control over plot styling, layout, and annotations — crucial for producing professional-grade charts.\nThen, in the final chapter of this visualization module, we’ll explore Bokeh, a more advanced plotting library for building interactive, web-ready charts. These tools are especially powerful when communicating results to stakeholders, allowing you to go beyond static images to create interactive dashboards and storytelling experiences.\nSo far, you’ve built a strong foundation in fast, exploratory plotting with Pandas. Up next: mastering the tools that help you refine and present your story with clarity and impact.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "13-data-viz-pandas.html#exercise-visualizing-pizza-shopping-behavior",
    "href": "13-data-viz-pandas.html#exercise-visualizing-pizza-shopping-behavior",
    "title": "13  Intro to Data Visualization with Pandas",
    "section": "13.7 Exercise: Visualizing Pizza Shopping Behavior",
    "text": "13.7 Exercise: Visualizing Pizza Shopping Behavior\nUse the datasets provided by the completejourney_py package to complete the following exercises. These tasks will help you practice filtering, grouping, and visualizing data using .plot() for both univariate and bivariate analysis.\n\n\n\n\n\n\nNone1. Identifying Pizza Products\n\n\n\n\n\nUsing the products table:\n\nIdentify all unique products where the product_type contains the word \"pizza\" (case-insensitive).\nHow many distinct pizza products are sold?\n\nTip: Use .str.contains() to search the column.\n\n\n\n\n\n\n\n\n\nNone2. Pizza Purchases by Marital Status\n\n\n\n\n\nJoin the transactions, products, and demographics tables.\n\nFilter to transactions where the product_type contains \"pizza\".\nUse a bar plot to compare the total quantity of pizza items purchased by married vs. unmarried households.\n\nBonus: Are there any differences in total sales_value between the groups?\n\n\n\n\n\n\n\n\n\nNone3. Quantity vs. Sales Value of Pizza\n\n\n\n\n\nUse a scatter plot to visualize the relationship between quantity and sales_value for pizza product transactions.\n\nWhat patterns do you observe?\nAre there any outliers in either dimension?\n\nTip: Use .plot.scatter() on the filtered DataFrame.\n\n\n\n\n\n\n\n\n\nNone4. Daily Pizza Sales Trends\n\n\n\n\n\nUse .resample() to analyze time-based patterns:\n\nFilter to pizza products as before.\nCompute the total quantity of pizza products purchased per day.\nUse a line plot to visualize the daily trend across the year.\n\nBonus: Do you notice any recurring spikes? Can you hypothesize what might cause them?",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Intro to Data Visualization with Pandas</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html",
    "href": "14-data-viz-matplotlib.html",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "",
    "text": "14.1 Anatomy of a Figure\nMatplotlib is one of the most widely used libraries for creating visualizations in Python. While it’s more low-level than the plotting tools you used with Pandas, it gives you much more control over how your charts look and behave.\nIn fact, many other visualization libraries—like Pandas, Seaborn, and Altair—are built on top of Matplotlib. So, by learning the basics of Matplotlib, you’re building a strong foundation that will transfer to many other tools in the Python ecosystem.\nIn this chapter, you’ll get hands-on experience with Matplotlib and learn how to create and customize common plot types from scratch. By the end of this lesson you will be able to:\nThere is a hierarchy you must understand when plotting with Matplotlib. The highest and outermost part of a plot is the Figure. The Figure contains all the other plotting elements. Typically, you do not interact with it much. Inside the Figure is the Axes. This is the actual plotting surface that you normally would refer to as a ‘plot’.\nA Figure may contain any number of these Axes. The Axes is a container for all of the other physical pixels that get drawn onto your screen. This includes the x and y axis, lines, text, points, legends, images, etc.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#anatomy-of-a-figure",
    "href": "14-data-viz-matplotlib.html#anatomy-of-a-figure",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "",
    "text": "Figure 14.1: Anatomy of Matplotlib figures.\n\n\n\n\n\n\n\n\n\nWithin Matplotlib the term Axes is not actually plural and does not mean more than one axis. It literally stands for a single ‘plot’. It’s unfortunate that this fundamental element has a name that is so confusing.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#importing-the-pyplot-module",
    "href": "14-data-viz-matplotlib.html#importing-the-pyplot-module",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "14.2 Importing the pyplot module",
    "text": "14.2 Importing the pyplot module\nImporting matplotlib into your workspace is done a little differently than NumPy or Pandas. You rarely will import matplotlib itself directly like this:\n\nimport matplotlib\n\nThe above is perfectly valid code, but the matplotlib developers decided not to put all the main functionality in the top level module.\nWhen you import pandas as pd, you get access to nearly all of the available functions and classes of the Pandas library. This isn’t true with Matplotlib. Instead, much of the functionality for quickly plotting is found in the pyplot module. If you navigate to the matplotlib source directory, found in your site-packages directory, you will see a pyplot.py file. This is the module that you are importing into your workspace.\n\n\n\n\n\n\nFigure 14.2: pyplot submodule within matplotlib library.\n\n\n\n\n\n\n\n\n\nThere is some functionality in other matplotlib submodules; however, vast majority of what you will use starting out is isolated to the pyplot submodule.\n\n\n\nLet’s import the pyplot module now and alias it as plt, which is commonly done by convention:\n\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#figures-and-axes",
    "href": "14-data-viz-matplotlib.html#figures-and-axes",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "14.3 Figures and axes",
    "text": "14.3 Figures and axes\npyplot does provide lots of useful functions, one of which creates a Figure and any number of Axes that you desire. You can do this without pyplot, but it involves more syntax. It’s also quite standard to begin the object-oriented approach by laying out your Figure and Axes first with pyplot and then proceed by calling methods from these objects.\nThe pyplot subplots() function creates a single Figure and any number of Axes. If you call it with the default parameters it will create a single Axes within a Figure.\n\n\n\n\n\n\nThe subplots function returns a two-item tuple containing the Figure and the Axes. Recall from our earlier lesson where we learned about tuples that we can unpack each of these objects as their own variable.\n\n\n\n\nfig, ax = plt.subplots()\n\n\n\n\n\n\n\n\nLet’s verify that we indeed have a Figure and Axes.\n\ntype(fig)\n\nmatplotlib.figure.Figure\n\n\n\ntype(ax)\n\nmatplotlib.axes._axes.Axes\n\n\n\nDistinguishing the Figure from the Axes\nIt’s not obvious, from looking at the plot which part is the Figure and which is the Axes. We will call our first method, set_facecolor in an object-oriented fashion from both the Figure and Axes objects. We pass it a name of a color (more on colors later).\n\n# set figure and axes colors\nfig.set_facecolor('skyblue')\nax.set_facecolor('sandybrown')\n\n# show result\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice, that the two calls above to the set_facecolor method were made without an assignment statement. Both of these operations happened in-place. The calling Figure and Axes objects were updated without a new one getting created.\n\n\n\n\n\nSetting the size of the Figure upon creation\nThe default Figure is fairly small. We can change this when creating it with the figsize parameter. Pass a two-item tuple to configure the height and width of the figure as we did in the previous lesson with Pandas plotting. By default, these dimensions are 6 by 4. They represent inches and are literally the inches that your figure would be if you printed out on paper.\nBelow, we create a new Figure that is 8 inches in width by 4 inches in height. We also color the faces of the both the Figure and Axes again.\n\nfig, ax = plt.subplots(figsize=(8, 4))\nfig.set_facecolor('skyblue')\nax.set_facecolor('sandybrown')\n\n\n\n\n\n\n\n\nEverything on our plot is a separate object. Each of these objects may be explicitly referenced by a variable. Once we have a reference to a particular object, we can then modify it by calling methods on it.\nThus far we have two references, fig and ax. There are many other objects on our Axes that we can reference such as the x and y axis, tick marks, tick labels, and others. We do not yet have references to these objects.\n\n\nCalling Axes methods - get_ and set_ methods\nBefore we start referencing these other objects, let’s change some of the properties of our Axes by calling some methods on it. Many methods begin with either get_ or set_ followed by the part of the Axes that will get retrieved or modified. The following list shows several of the most common properties that can be set on our Axes. We will see examples of each one below.\n\ntitle\nxlabel/ylabel\nxlim/ylim\nxticks/yticks\nxticklabels/yticklabels\n\n\nGetting and setting the title of the Axes\nThe get_title method will return the title of the Axes as a string. There is no title at this moment so it returns an empty string.\n\nax.get_title()\n\n''\n\n\nThe set_title method will place a centered title on our Axes when passing it a string. Notice that a matplotlib Text object has been returned. More on this later.\n\nax.set_title('My First Matplotlib Graph')\n\nText(0.5, 1.0, 'My First Matplotlib Graph')\n\n\nAgain, we must place our Figure variable name as the last line in our cell to show it in our notebook.\n\nfig\n\n\n\n\n\n\n\n\nNow, if we run the get_title method again, we will get the string that was used as the title.\n\nax.get_title()\n\n'My First Matplotlib Graph'\n\n\n\n\nGetting and setting the x and y limits\nBy default, the limits of both the x and y axis are 0 to 1. We can change this with the set_xlim and set_ylim methods. Pass these methods a new left and right boundary to change the limits. These methods actually return a tuple of the limits.\n\nax.get_xlim()\n\n(np.float64(0.0), np.float64(1.0))\n\n\n\nax.get_ylim()\n\n(np.float64(0.0), np.float64(1.0))\n\n\n\nax.set_xlim(0, 5)\nax.set_ylim(-10, 50)\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice, that the size of the figure remains the same. Only the limits of the x and y axis have changed.\n\n\n\n\n\nGetting and setting the location of the x and y ticks\nIn the graph above, it has chosen to place ticks every 1 unit for the x and every 10 units for the y. Matplotlib chooses reasonable default values. Let’s see the location of these ticks with the get_xticks and get_yticks methods.\n\nax.get_xticks()\n\narray([0., 1., 2., 3., 4., 5.])\n\n\n\nax.get_yticks()\n\narray([-10.,   0.,  10.,  20.,  30.,  40.,  50.])\n\n\nWe can specify the exact location of the x and y ticks with the set_xticks and set_ticks methods. We pass them a list of numbers indicating where we want the ticks.\n\n\n\n\n\n\nIf we set the ticks outside of the current bounds of the axis, this forces matplotlib to change the limits. For example, below we set the lower bound ytick (-99) beyond the lower bound of the y-axis limit (-10).\n\n\n\n\nax.set_xticks([1.8, 3.99, 4.4])\nax.set_yticks([-99, -9, -1, 22, 44])\nfig\n\n\n\n\n\n\n\n\n\n\nGetting and setting the x and y tick labels\nThe current tick labels for the x-axis and y-axis are the same as the tick locations. Let’s first view the current tick labels. The output displays a list of Text objects that contain Text(xaxis_location, yaxis_location, tick_label).\n\nax.get_xticklabels()\n\n[Text(1.8, 0, '1.80'), Text(3.99, 0, '3.99'), Text(4.4, 0, '4.40')]\n\n\nWe can pass the set_xticklabels and set_yticklabels methods a list of strings to use as the new labels.\n\nax.set_xticklabels(['dog', 'cat', 'snake'])\nax.set_yticklabels(['Boehmke', 'D', 'A', 'R', 'B'])\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe tick locations are a completely separate concept than the tick labels. The tick locations are always numeric and determine where on the plot the tick mark will appear. The tick labels on the other hand are the strings that are used on the graph.\nThe tick labels are defaulted to be a string of the tick location, but you can set them to be any string you want, as we did above. But use this wisely otherwise you may lead viewers of your plots astray!\n\n\n\n\n\n\nSetting text styles\nAll the text we placed on our plot was plain. We can add styling to it by changing the text properties. See the documentation for a list of all the text properties.\nCommon text properties:\n\nsize - Number in “points” where 1 point is defaulted to 1/72nd of an inch\ncolor - One of the named colors. See the colors API for more.\nbackgroundcolor - Same as above\nfontname - Name of font as a string\nrotation - Degree of rotation\n\n\nax.set_title(\n    'Tests',\n    size=20, \n    color='firebrick',\n    backgroundcolor='steelblue',\n    fontname='Courier New',\n    rotation=70\n    )\nfig\n\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Courier New' not found.\n\n\n\n\n\n\n\n\n\nAny other text may be stylized with those same parameters. Below we do so with the x labels.\n\nax.set_xlabel(\n    'New and Imporved X-Axis Stylized Label',\n    size=15,\n    color='indigo', \n    fontname='Times New Roman', \n    rotation=15\n    )\nfig\n\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Times New Roman' not found.\nfindfont: Font family 'Times New Roman' not found.\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Courier New' not found.\nfindfont: Font family 'Times New Roman' not found.\nfindfont: Font family 'Times New Roman' not found.\nfindfont: Font family 'Times New Roman' not found.\nfindfont: Font family 'Courier New' not found.\n\n\n\n\n\n\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoteDo This!\n\n\n\nCreate a Figure with a single Axes. Modify the Axes by using all of the methods in this notebook.\n\ntitle: set as your name and change the font size, color, and name.\nxlabel/ylabel: set as your two favorite colors.\nxlim/ylim: set x-axis to be 10-100 and y-axis to be -25-25\nxticks/yticks: set the xticks to be in increments of 10 and yticks to be in increments of 5.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#plotting-data",
    "href": "14-data-viz-matplotlib.html#plotting-data",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "14.4 Plotting Data",
    "text": "14.4 Plotting Data\nIn previous section, we created Figure and Axes objects, and proceeded to change their properties without plotting any actual data. In this section, we will learn how to create some of the same plots that we created in the Plotting with Pandas lesson.\nThe matplotlib documentation has a nice layout of the Axes API. There are around 300 different calls you make with an Axes object. The API page categorizes and groups each method by its functionality. The first third (approximately) of the categories in the API are used to create plots.\nThe simplest and most common plots are found in the Basics category and include plot, scatter, bar, pie, and others.\nLet’s go ahead and import Pandas along with our Complete Journey data:\n\nimport pandas as pd\nfrom completejourney_py import get_data\n\ncj_data = get_data()\ndf = (\n    cj_data['transactions']\n    .merge(cj_data['products'], how='inner', on='product_id')\n    .merge(cj_data['demographics'], how='inner', on='household_id')\n)\n\ndf.head()\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/completejourney_py/get_data.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.\n  from pkg_resources import resource_filename\n\n\n\n\n\n\n\n\n\nhousehold_id\nstore_id\nbasket_id\nproduct_id\nquantity\nsales_value\nretail_disc\ncoupon_disc\ncoupon_match_disc\nweek\n...\nproduct_category\nproduct_type\npackage_size\nage\nincome\nhome_ownership\nmarital_status\nhousehold_size\nhousehold_comp\nkids_count\n\n\n\n\n0\n900\n330\n31198570044\n1095275\n1\n0.50\n0.00\n0.0\n0.0\n1\n...\nROLLS\nROLLS: BAGELS\n4 OZ\n35-44\n35-49K\nHomeowner\nMarried\n2\n2 Adults No Kids\n0\n\n\n1\n900\n330\n31198570047\n9878513\n1\n0.99\n0.10\n0.0\n0.0\n1\n...\nFACIAL TISS/DNR NAPKIN\nFACIAL TISSUE & PAPER HANDKE\n85 CT\n35-44\n35-49K\nHomeowner\nMarried\n2\n2 Adults No Kids\n0\n\n\n2\n1228\n406\n31198655051\n1041453\n1\n1.43\n0.15\n0.0\n0.0\n1\n...\nBAG SNACKS\nPOTATO CHIPS\n11.5 OZ\n45-54\n100-124K\nNone\nUnmarried\n1\n1 Adult No Kids\n0\n\n\n3\n906\n319\n31198705046\n1020156\n1\n1.50\n0.29\n0.0\n0.0\n1\n...\nREFRGRATD DOUGH PRODUCTS\nREFRIGERATED BAGELS\n17.1 OZ\n55-64\nUnder 15K\nHomeowner\nMarried\n2\n1 Adult Kids\n1\n\n\n4\n906\n319\n31198705046\n1053875\n2\n2.78\n0.80\n0.0\n0.0\n1\n...\nSEAFOOD - SHELF STABLE\nTUNA\n5.0 OZ\n55-64\nUnder 15K\nHomeowner\nMarried\n2\n1 Adult Kids\n1\n\n\n\n\n5 rows × 24 columns\n\n\n\n\nLine plots\nThe plot method’s primary purpose is to create line plots. It does have the ability to create scatter plots as well, but that task is best reserved for scatter. The plot method is very flexible and can take a variety of different inputs (i.e. lists, numpy arrays, Pandas Series) but our examples will focus on using Matplotlib with a DataFrame.\nLet’s compute the total daily sales and create a line plot:\n\ndaily_sales = (\n    df\n    .set_index('transaction_timestamp')['sales_value']\n    .resample('D')\n    .sum()\n    .to_frame()\n    .reset_index()\n)\ndaily_sales.head()\n\n\n\n\n\n\n\n\ntransaction_timestamp\nsales_value\n\n\n\n\n0\n2017-01-01\n4604.39\n\n\n1\n2017-01-02\n6488.94\n\n\n2\n2017-01-03\n6856.84\n\n\n3\n2017-01-04\n7087.92\n\n\n4\n2017-01-05\n6894.67\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 4))\nax.plot('transaction_timestamp', 'sales_value', data=daily_sales);\n\n\n\n\n\n\n\n\nWe can change several properties of this plot. For example, we can change the line style and color of our line directly in the ax.plot call along with adding plot a\n\n\n\n\n\n\nYou can find all possible parameters to modify the line plot here. You can also find all the different color options and ways to specify them here.\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 4))\n\n# create/modify line plot\nax.plot(\n    'transaction_timestamp', \n    'sales_value', \n    data=daily_sales, \n    linestyle=':', \n    color='gray',\n    linewidth=2\n    )\n\n# add additional context\nax.set_title('Total daily sales across all stores', size=20)\nax.set_ylabel('Total sales ($)');\n\n\n\n\n\n\n\n\nWe can even add additional features such as gridlines and text/arrows to call out certain parts of the plot.\n\n\n\n\n\n\nSince our x-axis is a date object we need to specify a date location within ax.annotate. The datetime module comes as part of the Standard Library and is the defacto approach to creating and manipulating dates and times outside of Pandas.\n\n\n\n\nfrom datetime import date as dt\n\nfig, ax = plt.subplots(figsize=(10, 4))\n\n# create/modify line plot\nax.plot(\n    'transaction_timestamp', \n    'sales_value', \n    data=daily_sales, \n    linestyle=':', \n    color='gray',\n    linewidth=2\n    )\n\n# add additional context\nax.set_title('Total daily sales across all stores', size=20)\nax.set_ylabel('Total sales ($)');\n\n# add gridlines, arrow, and text\nax.grid(linestyle='dashed')\nax.annotate(\n    'Christmas Eve', \n    xy=([dt(2017, 12, 20), 15900]), \n    xytext=([dt(2017, 9, 1), 15500]), \n    arrowprops={'color':'blue', 'width':2},\n    size=10\n    );\n\n\n\n\n\n\n\n\n\n\nOther plots\nWe can create many of the other forms of plots that we saw in the previous lesson. The following are just a few examples that recreate the histogram, boxplot, and scatter plots that we made in Chapter 13.\n\ntotals_by_store = df.groupby('store_id').agg({'sales_value': 'sum', 'quantity': 'sum'})\n\n# histogram\nfig, ax = plt.subplots(figsize=(8, 4))\nax.hist('sales_value', data=totals_by_store, bins=30);\n\n\n\n\n\n\n\n\n\n# boxplot\nfig, ax = plt.subplots(figsize=(8, 4))\nax.boxplot('sales_value', data=totals_by_store, vert=False)\n\n# adjust axes\nax.set_xscale('log')\nax.set_yticklabels('')\nax.set_yticks([]);\n\n\n\n\n\n\n\n\n\n# scatter plot\nfig, ax = plt.subplots(figsize=(8, 4))\nax.scatter('quantity', 'sales_value', data=totals_by_store, c='gray', s=5);\n\n\n\n\n\n\n\n\n\n\nAdding more dimensions\nIn some cases we can even add additional dimensions to our data. For example, say we have the following data set that has total quantity and sales_value plus a third variable that indicates the number of transactions for each store.\n\nstore_count = (\n    df\n    .groupby('store_id', as_index=False)\n    .size()\n    .rename(columns={'size': 'n'})\n)\n\ntotals_by_store = (\n    df\n    .groupby('store_id', as_index=False)\n    .agg({'sales_value': 'sum', 'quantity': 'sum'})\n    .merge(store_count)\n)\n\ntotals_by_store.head()\n\n\n\n\n\n\n\n\nstore_id\nsales_value\nquantity\nn\n\n\n\n\n0\n2\n13.99\n1\n1\n\n\n1\n27\n443.97\n186\n150\n\n\n2\n37\n7.27\n3\n3\n\n\n3\n42\n22.78\n7\n7\n\n\n4\n45\n13.01\n9\n8\n\n\n\n\n\n\n\nWe can actually have the color (c) and size (s) of our points based on data. For example, here we create a Series that is based on the number of store observations and another Series to indicate if the store count was greater than the 95 percentile.\n\n\n\n\n\n\nWe could literally use s='n' to reference our count column within the DataFrame; however, since there is such a wide dispersion of count values the size of the points explode. Here, we adjust the dispersion to be smaller with a fractional exponentiation.\n\n\n\n\nsize_adj = totals_by_store['n']**0.4\nn_outliers = totals_by_store['n'] &gt; totals_by_store['n'].quantile(0.95)\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.scatter('quantity', 'sales_value', data=totals_by_store, c=n_outliers, s=size_adj);\n\n\n\n\n\n\n\n\n\n\nMultiple plots\nWe can create multiple plots (“Axes”) within our figure. For example, the following will create 4 plots (2 rows x 2 colums).\n\nfig, ax_array = plt.subplots(2, 2, figsize=(8, 6), constrained_layout=True)\n\n\n\n\n\n\n\n\nWhenever you create multiple Axes on a figure with subplots, you will be returned a NumPy array of Axes objects. Let’s verify that the type and shape of this array.\n\ntype(ax_array)\n\nnumpy.ndarray\n\n\n\nax_array.shape\n\n(2, 2)\n\n\nIf we simply output the array, we will see 4 different Axes objects. Let’s extract each of these Axes into their own variable. We need to index for both the row and column to get the respective plot (remember that we need to use zero-based indexing!)\n\nax_array\n\narray([[&lt;Axes: &gt;, &lt;Axes: &gt;],\n       [&lt;Axes: &gt;, &lt;Axes: &gt;]], dtype=object)\n\n\n\nax1 = ax_array[0, 0]  # row 0, col 0\nax2 = ax_array[0, 1]  # row 0, col 1\nax3 = ax_array[1, 0]  # row 1, col 0\nax4 = ax_array[1, 1]  # row 1, col 1\n\nWe can now customize our individual plots:\n\n# plot 1\nax1.plot(\n    'transaction_timestamp', \n    'sales_value', \n    data=daily_sales, \n    linestyle=':', \n    color='gray',\n    linewidth=2\n    )\n\n# add additional context\nax1.set_title('Total daily sales across all stores', size=12)\nax1.set_ylabel('Total sales ($)');\n\n# add gridlines, arrow, and text\nax1.grid(linestyle='dashed')\nax1.tick_params(axis='x', which='major', labelsize=8, labelrotation=45)\nax1.annotate(\n    'Christmas Eve', \n    xy=([dt(2017, 12, 20), 15900]), \n    xytext=([dt(2017, 7, 1), 15500]), \n    arrowprops={'color':'blue', 'width':0.5},\n    size=8\n    )\n    \n# plot 2\nax2.scatter('quantity', 'sales_value', data=totals_by_store, c=n_outliers, s=size_adj)\nax2.set_title('Total store-level sales vs quantity.', size=12)\n\n# plot 3\nax3.hist('sales_value', data=totals_by_store, bins=30)\nax3.set_title('Histogram of total store-level sales.', size=12)\n\n# plot 4\nax4.boxplot('quantity', data=totals_by_store, vert=False)\nax4.set_xscale('log')\nax4.set_yticklabels('')\nax4.set_yticks([])\nax4.set_title('Histogram of total store-level quantity.', size=12);\n\n# final plot title\nfig.suptitle('Total store-level sales and quantities (2017)', fontsize=20)\nfig",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#video-tutorials",
    "href": "14-data-viz-matplotlib.html#video-tutorials",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "14.5 Video Tutorials",
    "text": "14.5 Video Tutorials\n\n\n\n\n\n\nNoneVideo 🎥: Anatomy of Matplotlib\n\n\n\n\n\nThe following video is from the SciPy 2018 conference. It is an extended (3 hours) introduction to Matplotlib and has been a very popular training offered at SciPy conferences over the years.\n\n\n\n\n\n\n\n\n\n\nNoneVideo 🎥: Corey Shafer series on Matplotlib\n\n\n\n\n\nYou can also check out this series of Matplotlib tutorials provided by Corey Schafer. They are excellent! Here’s the first video:",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#summary",
    "href": "14-data-viz-matplotlib.html#summary",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "14.6 Summary",
    "text": "14.6 Summary\nIn this chapter, you explored the fundamentals of Matplotlib, Python’s core plotting library. While it may feel more manual compared to Pandas’ built-in plotting, understanding how Matplotlib works gives you the power to fully customize your visualizations—from titles and labels to tick marks, color schemes, and layout. This level of control is key to creating polished, professional-looking plots suitable for presentations, publications, and stakeholder reports.\nBy building your skills with Matplotlib, you’ve laid the groundwork for working with many other Python visualization libraries that use it under the hood.\nIn the next chapter, we’ll explore Bokeh, a more advanced library designed for creating interactive, web-ready visualizations. These tools allow you to move beyond static charts and into the world of dynamic dashboards and visual storytelling—ideal for engaging stakeholders and sharing results in more impactful ways.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "14-data-viz-matplotlib.html#exercise-customizing-pizza-visualizations-with-matplotlib",
    "href": "14-data-viz-matplotlib.html#exercise-customizing-pizza-visualizations-with-matplotlib",
    "title": "14  Fundamentals of Plotting with Matplotlib",
    "section": "14.7 Exercise: Customizing Pizza Visualizations with Matplotlib",
    "text": "14.7 Exercise: Customizing Pizza Visualizations with Matplotlib\nIn this exercise set, you’ll revisit the pizza-related analyses from the previous chapter. However, this time you’ll build the plots using Matplotlib directly, allowing you to practice creating visualizations from scratch and customizing them for improved clarity and presentation quality.\nUse the datasets from the completejourney_py package and import Matplotlib as needed:\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom completejourney_py import get_data\n\ncj_data = get_data()\ndf = (\n    cj_data['transactions']\n    .merge(cj_data['products'], how='inner', on='product_id')\n    .merge(cj_data['demographics'], how='inner', on='household_id')\n)\n\n\n\n\n\n\nTip💡 Stuck or unsure how to do something?\n\n\n\nDon’t hesitate to use AI tools like ChatGPT or GitHub Copilot to help you troubleshoot, format plots, or discover new ways to customize your charts. Part of learning data visualization is knowing how to find and adapt solutions.\n\n\n\n\n\n\n\n\nNone1. Identifying Pizza Products\n\n\n\n\n\nAs before, use the products table to:\n\nIdentify all products where the product_type contains \"pizza\" (case-insensitive).\nCount how many distinct pizza products are sold.\n\nThere’s no plot needed here—but this filtered data will be reused in later tasks, so keep it handy!\n\n\n\n\n\n\n\n\n\nNone2. Pizza Purchases by Marital Status (Bar Chart)\n\n\n\n\n\nUsing transactions, products, and demographics, create a Matplotlib bar chart comparing the total quantity of pizza purchased by married vs. unmarried households.\n\nAdd a descriptive title and axis labels.\nCustomize the bar colors and axis tick labels to make the chart more readable.\nBonus: Add the value labels directly on top of the bars.\n\n\n\n\n\n\n\n\n\n\nNone3. Quantity vs. Sales Value (Scatter Plot)\n\n\n\n\n\nCreate a scatter plot using Matplotlib to visualize the relationship between quantity and sales_value for pizza purchases.\n\nUse transparency (alpha) to handle overplotting.\nAdd a title and axis labels.\nBonus: Use color or marker size to represent another dimension (e.g., household income level or day of the week).\n\nOptional: Try annotating or highlighting outliers if they seem interesting.\n\n\n\n\n\n\n\n\n\nNone4. Daily Pizza Sales Trends (Line Plot)\n\n\n\n\n\nUsing resample() on the pizza transactions:\n\nCompute total quantity of pizza products purchased per day.\nCreate a line plot with Matplotlib to visualize the daily trend.\nAdd titles, axis labels, and format the date axis so it doesn’t appear cluttered.\n\nBonus: Add gridlines, a rolling average, or vertical lines to mark major holidays.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fundamentals of Plotting with Matplotlib</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html",
    "href": "15-data-viz-bokeh.html",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "",
    "text": "15.1 Prerequisites\nIn the previous two lessons, you learned how to visualize data using pandas’ high-level plotting tools for quick insights, and matplotlib for more detailed and customized charting.\nIn this lesson, you’ll take your visualization skills to the next level with Bokeh, a Python library designed for building interactive visualizations in modern web browsers. With Bokeh, you can create everything from simple scatterplots to fully-featured, dynamic dashboards — all in Python, no JavaScript required.\nTools like Bokeh allow you to move beyond static charts and into the world of interactive, responsive data experiences. This shift enables visual storytelling, where users can explore data themselves, and helps you deliver more engaging, stakeholder-friendly output. Whether you’re sharing insights with a non-technical audience or designing a data dashboard, Bokeh gives you the flexibility and power to communicate results in more impactful ways.\nAlthough Bokeh is considered a lower-level visualization API than pandas or seaborn, it strikes a great balance between ease of use and customizability. You’ll find it intuitive to get started with and incredibly powerful as you go deeper.\nIn this chapter, you’ll get hands-on experience with Bokeh and learn how to create and customize interactive visualizations for the web. By the end of this lesson you will be able to:\nMost of the functionality of Bokeh is accessed through submodules such as bokeh.plotting and bokeh.models. Also, when using Bokeh in a notebook we need to run bokeh.io.output_notebook() to make our plots viewable and interactive.\nimport pandas as pd\n\n# Our main plotting package (must have explicit import of submodules)\nimport bokeh.io\nimport bokeh.models\nimport bokeh.plotting\nimport bokeh.transform\n\n# Enable viewing Bokeh plots in the notebook\nbokeh.io.output_notebook()\n\n    \n    \n        \n        Loading BokehJS ...\nWe’ll use a cleaned up version of the Ames, IA housing data for illustration purposes:\ndf = pd.read_csv('../data/ames_clean.csv')\ndf.head()\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html#bokehs-grammar-and-our-first-plot-with-bokeh",
    "href": "15-data-viz-bokeh.html#bokehs-grammar-and-our-first-plot-with-bokeh",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "15.2 Bokeh’s grammar and our first plot with Bokeh",
    "text": "15.2 Bokeh’s grammar and our first plot with Bokeh\nConstructing a plot with Bokeh consists of four main steps.\n\nCreating a figure on which to populate glyphs (symbols that represent data, e.g., dots for a scatter plot). Think of this figure as a “canvas” which sets the space on which you will “paint” your glyphs.\nDefining a data source that is the reference used to place the glyphs.\nChoose the kind of glyph you would like.\nRefining the plot by adding titles, formatted axis labels, or even interactive components.\n\nAfter completing these steps, you need to render the graphic.\nLet’s go through these steps to generate an interactive scatter plot of home sales price and total living area. So you have the concrete example in mind, the final graphic will look like this:\n\n\n\n  \n\n\n\n\n\n1. Our first step is creating a figure, our “canvas.” In creating the figure, we are implicitly thinking about what kind of representation for our data we want. That is, we have to specify axes and their labels. We might also want to specify the title of the figure, whether or not to have grid lines, and all sorts of other customizations. Naturally, we also want to specify the size of the figure.\n(Almost) all of this is accomplished in Bokeh by making a call to bokeh.plotting.figure() with the appropriate keyword arguments.\n\n# Create the figure, stored in variable `p`\np = bokeh.plotting.figure(\n    frame_width=700,\n    frame_height=350,\n    title='Relationship between home sale price and living area \\nAmes, Iowa (2006-2010)',\n    x_axis_label='Living Area (Square feet)',\n    y_axis_label='Sale Price'\n)\n\nThere are many more keyword attributes you can assign, including all of those listed in the Bokeh Plot class and the additional ones listed in the Bokeh Figure class.\n2. Now that we have set up our canvas, we can decide on the data source. It is convenient to create a ColumnDataSource, a special Bokeh object that holds data to be displayed in a plot. (Later on we will see that we can change the data in a ColumnDataSource and the plot will automatically update!) Conveniently, we can instantiate a ColumnDataSource directly from a Pandas data frame.\n\nsource = bokeh.models.ColumnDataSource(df)\n\n\n\n\n\n\n\nWe could also instantiate a data source using a dictionary of arrays, like…\nsource = bokeh.models.ColumnDataSource(dict(x=[1, 2, 3, 4], y=[1, 4, 9, 16]))\n\n\n\n3. Since we are creating a scatter plot we will choose scatter as our glyph. This kind of glyph requires that we specify which column of the data source will serve to place the glyphs along the \\(x\\)-axis and which will serve to place the glyphs along the \\(y\\)-axis. We choose the 'GrLivArea' column to specify the \\(x\\)-coordinate of the glyph and the 'SalePrice' column to specify the \\(y\\)-coordinate. Since there are a lot of observations clustered together we can control overplotting by adjusting the transparency with alpha.\nWe accomplish step 3 by calling one of the glyph methods of the Bokeh Figure instance, p. Since we are choosing a scatter plot, the appropriate method is p.scatter(), and we use the source, x, and y kwargs to specify the positions of the glyphs.\n\np.scatter(\n    source=source,\n    x='GrLivArea',\n    y='SalePrice',\n    alpha=0.25\n);\n\n4. Lastly, we can refine the plot in various ways. In this example we make the x and y-axis labels comma and dollar formatted respectively. We can also add interactive components to our visuals. Here, I add a hover tool so that sale price and total living area is displayed when my mouse hovers over a point.\n\n\n\n\n\n\nWe can specify these features (axis configuration and tooltips) when we instantiate the figure or afterwards by assigning attribute values to an already instantiated figure.\n\n\n\nThe syntax for a tooltip is a list of 2-tuples, where each tuple represents the tooltip you want. The first entry in the tuple is the label and the second is the column from the data source that has the values. The second entry must be preceded with an @ symbol signifying that it is a field in the data source and not field that is intrinsic to the plot, which is preceded with a $ sign. If there are spaces in the column heading, enclose the column name in braces (i.e. {name with spaces}). (See the documentation for tooltip specification for more information.)\n\np.yaxis.formatter = bokeh.models.NumeralTickFormatter(format=\"$,\")\np.xaxis.formatter = bokeh.models.NumeralTickFormatter(format=\",\")\n\ntooltips = [(\"Sale Price\",\"@SalePrice\"),(\"SqFt\",\"@GrLivArea\")]\nhover = bokeh.models.HoverTool(tooltips=tooltips, mode='mouse')\np.add_tools(hover)\n\nNow that we have built the plot, we can render it in the notebook using bokeh.io.show().\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nIn looking at the plot, notice a toolbar to right of the plot that enables you to zoom and pan within the plot.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html#coloring-with-other-dimensions",
    "href": "15-data-viz-bokeh.html#coloring-with-other-dimensions",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "15.3 Coloring with other dimensions",
    "text": "15.3 Coloring with other dimensions\nLet’s say we wanted to make the same plot, but we wanted to color the points based on another feature such as whether the home has central air or not (CentralAir). To do this, we take advantage of two features of Bokeh.\n\nWe create a color mapping using factor_cmap() that assigns colors to the discrete levels of a given factor (CentralAir in this example). Here, we simply assign red and blue colors; however, Bokeh has many color palettes to choose from.\nWe can then use the scatter method to assign the glyph of choice and pass the color_mapper object to fill_color and/or fill_line. I also add the legend field so it shows up in the plot and we can format our legend as necessary (i.e. add title, change font).\n\n\n# Create the figure, stored in variable `p`\np = bokeh.plotting.figure(\n    frame_width=700,\n    frame_height=350,\n    title='Relationship between home sale price and living area \\nAmes, Iowa (2006-2010)',\n    x_axis_label='Living Area (Square feet)',\n    y_axis_label='Sale Price'\n)\n\nsource = bokeh.models.ColumnDataSource(df)\n\n# create color mapper\ncolor_mapper = bokeh.transform.factor_cmap(\n    'CentralAir',\n    palette=['red', 'blue'],\n    factors=df['CentralAir'].unique()\n    )\n\np.scatter(\n    source=source,\n    x='GrLivArea',\n    y='SalePrice',\n    marker='circle',\n    alpha=0.25,\n    fill_color=color_mapper,\n    line_color=color_mapper,\n    legend_field='CentralAir'\n)\n\np.legend.title = \"Has central air\"\n\np.yaxis.formatter = bokeh.models.NumeralTickFormatter(format=\"$,\")\np.xaxis.formatter = bokeh.models.NumeralTickFormatter(format=\",\")\n\ntooltips = [(\"Sale Price\",\"@SalePrice\"),(\"SqFt\",\"@GrLivArea\")]\nhover = bokeh.models.HoverTool(tooltips=tooltips, mode='mouse')\np.add_tools(hover)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html#saving-bokeh-plots",
    "href": "15-data-viz-bokeh.html#saving-bokeh-plots",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "15.4 Saving Bokeh plots",
    "text": "15.4 Saving Bokeh plots\nAfter you create your plot, you can save it to a variety of formats. Most commonly you would save them as PNG (for presentations), SVG (for publications in the paper of the past), and HTML (for the paper of the future or sharing with colleagues).\nTo save as a PNG for quick use, you can click the disk icon in the tool bar.\nTo save to SVG, you first change the output backend to 'svg' and then you can click the disk icon again, and you will get an SVG rendering of the plot. After saving the SVG, you should change the output backend back to 'canvas' because it has much better in-browser performance.\n\np.output_backend = 'svg'\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nNow, click the disk icon in the plot above to save it.\nAfter saving, we should switch back to canvas.\n\np.output_backend = 'canvas'\n\nYou can also save the figure programmatically using the bokeh.io.export_svgs() function. This requires additional installations, so we will not do it here, but show the code to do it. Again, this will only work if the output backed is 'svg'.\np.output_backend = 'svg'\nbokeh.io.export_svgs(p, filename='ames_sale_price_vs_living_area.svg')\np.output_backend = 'canvas'\nFinally, to save as HTML, you can use the bokeh.io.save() function. This saves your plot as a standalone HTML page. Note that the title kwarg is not the title of the plot, but the title of the web page that will appear on your Browser tab.\nbokeh.io.save(\n    p,\n    filename='ames_sale_price_vs_living_area.html',\n    title='Bokeh plot'\n);",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html#video-tutorial",
    "href": "15-data-viz-bokeh.html#video-tutorial",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "15.5 Video Tutorial",
    "text": "15.5 Video Tutorial\nThis lesson only scratches the surface of what Bokeh can do. From interactive widgets and streaming data to fully responsive dashboards, Bokeh offers a wide range of advanced capabilities. To see more examples and explore what’s possible, visit the official Bokeh gallery at https://demo.bokeh.org/.\n\n\n\n\n\n\nNoneVideo 🎥\n\n\n\nThe following video provides an overview of Bokeh and will also expose you to other types of plots you can create (i.e. line charts, histograms, area plots).",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html#summary",
    "href": "15-data-viz-bokeh.html#summary",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "15.6 Summary",
    "text": "15.6 Summary\nIn this chapter, you explored the fundamentals of creating interactive data visualizations using Bokeh, a powerful Python library designed for the web. You learned how to use Bokeh’s figure interface to build plots from scratch, enhance them with interactive tools like hovers and legends, and combine multiple charts into dashboard-style layouts.\nBokeh opens the door to interactive, engaging visual storytelling—ideal for exploratory analysis and sharing insights with stakeholders in a more dynamic format. While this chapter focused on the core building blocks, Bokeh supports much more, including advanced widgets, real-time data streaming, and fully customizable layouts.\nTo explore Bokeh’s full capabilities, check out the official demo gallery.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "15-data-viz-bokeh.html#exercise-exploring-housing-trends-with-bokeh",
    "href": "15-data-viz-bokeh.html#exercise-exploring-housing-trends-with-bokeh",
    "title": "15  Interactive Data Visualization with Bokeh",
    "section": "15.7 Exercise: Exploring Housing Trends with Bokeh",
    "text": "15.7 Exercise: Exploring Housing Trends with Bokeh\nIn this exercise set, you’ll revisit the Ames Housing dataset and apply what you’ve learned about Bokeh to build interactive plots from scratch. These visualizations will give you hands-on practice with building charts, customizing tooltips, applying categorical coloring, and creating simple layouts.\nUse the Ames Housing dataset we used in this chapter, and import the necessary Bokeh modules as needed:\nimport pandas as pd\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\nfrom bokeh.transform import factor_cmap\nfrom bokeh.layouts import row, column\nfrom bokeh.models import HoverTool\n\noutput_notebook()\n\ndf = pd.read_csv(\"data/ames_clean.csv\")  # adjust path as needed\n\n\n\n\n\n\nTip💡 Stuck or unsure how to do something?\n\n\n\nUse tools like ChatGPT, GitHub Copilot, or the Bokeh documentation to help troubleshoot, format your plots, or discover ways to customize interactivity. Learning how to adapt examples is part of becoming a confident data visualizer.\n\n\n\n\n\n\n\n\nNone1. Explore the Bokeh Gallery\n\n\n\n\n\nVisit https://demo.bokeh.org and spend a few minutes exploring different chart types.\n\nWhat kinds of visualizations stand out to you?\nCan you imagine how these might be used in a data science or business setting?\n\nThere’s no coding for this part—just use it for inspiration before diving into the next tasks.\n\n\n\n\n\n\n\n\n\nNone2. Bar Chart: Housing Count by Neighborhood\n\n\n\n\n\nCreate a Bokeh bar chart that shows the number of houses sold per Neighborhood.\n\nUse groupby() and value_counts() to prepare your data.\nCreate a vertical bar chart with vbar().\nAdd a title, axis labels, and hover tooltips that show the neighborhood name and house count.\nBonus: Sort the bars in descending order.\n\n\n\n\n\n\n\n\n\n\nNone3. Scatter Plot: Living Area vs. Sale Price\n\n\n\n\n\nVisualize the relationship between GrLivArea (above ground living area) and SalePrice.\n\nCreate a scatter plot using figure().circle().\nAdd a HoverTool that shows the address (or another interesting feature), living area, and sale price when you hover over points.\nBonus: Add color or marker size based on a third variable like OverallQual.\n\n\n\n\n\n\n\n\n\n\nNone4. Add Categorical Color to Your Scatter Plot\n\n\n\n\n\nEnhance your scatter plot by mapping color to a categorical feature such as CentralAir, BldgType, or a binned version of OverallQual.\n\nUse factor_cmap to map categories to colors.\nInclude a legend and adjust the plot size or layout if needed.\n\nBonus: Use ColumnDataSource to simplify your interactivity.\n\n\n\n\n\n\n\n\n\nNone5. Combine Multiple Plots into a Layout\n\n\n\n\n\nCreate a second plot of your choice (e.g., a histogram of SalePrice or another scatter plot).\n\nUse row() or column() to combine it with your earlier scatter plot.\nAdd a shared title or explanatory text.\nReflect: How might this layout be useful as a small dashboard for exploring housing data?",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Interactive Data Visualization with Bokeh</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html",
    "href": "16-control-statements.html",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "",
    "text": "16.1 Prerequisites\nAs your programs become more complex, they need to be able to respond to different situations — choosing different paths depending on the data. Conditional statements are how Python makes those decisions.\nIn this chapter, you’ll learn how to:\nLet’s explore how conditional logic helps you write smarter, more dynamic code.\nBy the end of this lesson you will be able to:\nMost of the examples in this lesson use base Python code without any modules; however, we will illustrate some examples of integrating control statements within Pandas. We also illustrate a couple examples that use Numpy.\nimport pandas as pd\nimport numpy as np\nAlso, most of the examples use toy data; however, when illustrating concepts integrated with Pandas we will use the Complete Journey transaction data:\nfrom completejourney_py import get_data\n\ndf = get_data()['transactions']\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/completejourney_py/get_data.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.\n  from pkg_resources import resource_filename",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html#if-statement",
    "href": "16-control-statements.html#if-statement",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "16.2 if statement",
    "text": "16.2 if statement\nThe conditional if statement is used to test an expression. Below is some psuedo code illustrating what this looks like. If the test_expression is True, the statement gets executed. But if it’s False, nothing happens.\nif test_expression:\n    statement\nThe following is an example that tests if a particular object is negative. Notice that there are no braces or “begin/end” delimiters around the block of code. Python uses the indentation to determine blocks of code. Consequently, you can include multiple lines in the if statement as long as they have the same indentation.\n\nx = -8\n\nif x &lt; 0:\n    print('x contains a negative number')\n\nx contains a negative number\n\n\n\n# multiple lines in the statement are fine as long as they have the\n# same indentation\nif x &lt; 0:\n    new_value = abs(x)\n    print(f'absolute value of x is {new_value}')\n\nabsolute value of x is 8\n\n\nIt is possible to write very short if statements on one line. This can be useful in limited situations but as soon as your resulting statement become more verbose it is best practice to switch to a multi-line approach.\n\n# single line approach\nif x &lt; 0: print('x contains a negative number')\n\nx contains a negative number\n\n\nIts helpful to remember that everything in Python has some form of truthiness. In fact, any nonzero number or nonempty object is True. This allows you to evaluate the object directly:\n\n# a conditional statement on an empty object is equivalent to False\nempty_list = []\nif empty_list:\n    print(\"since empty_list is False this won't exectute\")\n\n\n# a conditional statement on a non-empty object is equivalent to True\nnon_empty_list = ['not', 'empty']\nif non_empty_list:\n    print(\"This list is not empty\")\n\nThis list is not empty\n\n\nPython uses and and or operators to evaluate multiple expressions. They always return a single True or False. Moreover, Python will stop evaluating multiple expressions as soon as the result is known.\n\nx = -1\ny = 4\nif x &lt; 0 or y &lt; 0:\n    print('At least one of these objects are less than zero.')\n\nAt least one of these objects are less than zero.\n\n\n\nif x &lt; 0 and y &lt; 0:\n    print('Both x and y or less than zero')\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nFill in the following code chunk so that:\n\nIf month has value 1-9 the file name printed out will be “data/Month-0X.csv”\nWhat happens if the month value is 10-12?\nmonth = 4\n\nif month ________ :\n    print(f'data/Month-0{month}.csv')\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html#multiway-branching",
    "href": "16-control-statements.html#multiway-branching",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "16.3 Multiway branching",
    "text": "16.3 Multiway branching\nMultiway branching is when we want to have multiple return statement options based on the input conditions. The general form of multiway branch if statements is as follows.\n\n\n\n\n\n\nThe elif block is not always necessary. If you want only two output branches then just use if followed by else. However, if you have many branches, you can use as many elif statements as necessary.\n\n\n\nif test_1:\n  statement_1\nelif test2:\n  statement_2\nelse:\n  statement_3\nThe following illustrates with a simple example. Python will perform this code in sequence and execute the statements nested under the first test that is True or the else if all tests are False.\n\nx = 22.50\n\nif 0 &lt;= x &lt; 10:\n    print('low')\nelif 10 &lt;= x &lt; 20:\n    print('medium-low')\nelif 20 &lt;= x &lt; 30:\n    print('medium')\nelse:\n    print('preferred')\n\nmedium\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nFill in the following code chunk so that:\n\nif month has value 1-9 the file name printed out will be “data/month-0X.csv”\nif month has value 10-12 the file name printed out will be “data/month-1X.csv”\nif month is an invalid month number (not 1-12), the result printed out is “Invalid month”\ntest it out for when month equals 6, 10, & 13\nmonth = 4\n\nif month ________:\n    print(f'data/Month-0{month}.csv')\n_____:\n    print(f'data/Month-{month}.csv')\n_____:\n    print('________')\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html#switch-statements",
    "href": "16-control-statements.html#switch-statements",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "16.4 Switch statements",
    "text": "16.4 Switch statements\nMany other languages have a switch or case statement that allows you to evaluate an expression and return the statement that aligns with the value. For example, in R, the switch statement looks like the following:\nchoice &lt;- 'ham'\n\nswitch(choice,\n       'spam'  = 1.25,\n       'ham'   = 1.99,\n       'eggs'  = 0.99,\n       'bacon' = 1.10,\n)\n## [1] 1.99\nPython does not have switch statement but has some handy alternatives. In the most basic approach, you could just use a multiway branch if statement:\n\nchoice = 'ham'\n\nif choice == 'spam':\n    print(1.25)\nelif choice == 'ham':\n    print(1.99)\nelif choice == 'eggs':\n    print(0.99)\nelif choice == 'bacon':\n    print(1.10)\nelse:\n    print('Bad choice')\n\n1.99\n\n\nHowever, this approach is a bit verbose. An efficient alternative is to use a dictionary that provides the same key-value matching as a switch statement.\n\noptions = {'spam': 1.25, 'ham': 1.99, 'eggs': 0.99, 'bacon': 1.10}\n\nYou can either index this dictionary for the matching key:\n\noptions[choice]\n\n1.99\n\n\nOr, a more trustworthy approach is to use the get() method. This allows you to provide a default response in the case that the key you are looking for is not in the dictionary\n\n\n\n\n\n\nUsing the get() method allows you to supply a value to provide if there is no matching key (or as in if statements if there are no other conditions that equate to True).\n\n\n\n\noptions.get(choice, 'Bad choice')\n\n1.99\n\n\n\nchoice = 'broccoli'\noptions.get(choice, 'Bad choice')\n\n'Bad choice'\n\n\nDictionaries are good for associating values with keys, but what about the more complicated actions you can code in the statement blocks associated with if statements? Fortunately, dictionaries can also hold functions (both named functions and lambda functions) which can allow you to perform more sophisticated switch-like execution.\n\n\n\n\n\n\nDon’t worry, you will learn more about functions in an upcoming lesson.\n\n\n\n\ndef produce_revenue(sqft, visits, trend):\n    total = 9.91 * sqft * visits * trend\n    return round(total, 2)\n\ndef frozen_revenue(sqft, visits, trend):\n    prod = produce_revenue(sqft, visits, trend)\n    total = 3.28 * sqft * visits * trend - prod * .005\n    return round(total, 2)\n\nexpected_annual_revenue = {\n    'produce':    produce_revenue,\n    'frozen':     frozen_revenue,\n    'pharmacy':   lambda: 16.11 * visits * trend\n    }\n\nchoice = 'frozen'\nexpected_annual_revenue.get(choice, 'Bad choice')(sqft=937, visits=465, trend=0.98)\n\n1379372.75\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nConvert the following multi-branch if-else statement into a dict where you get the month path file with path_files.get(month). In this case, which approach seems more reasonable?\nmonth = 4\n\nif month &lt;= 9:\n    print(f'data/Month-0{month}.csv')\nelif month &gt;= 10 and month &lt;= 12:\n    print(f'data/Month-{month}.csv')\nelse:\n    print('Invalid month')\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html#applying-in-pandas",
    "href": "16-control-statements.html#applying-in-pandas",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "16.5 Applying in Pandas",
    "text": "16.5 Applying in Pandas\nWhen data mining, we often want to perform conditional statements to not only filter observations, but also to create new variables. For example, say we want to create a new variable that classifies transactions above $10 as “high value” otherwise they are “low value”. There are several methods we can use to perform this but a simple one is to use the apply method:\n\ndf['value'] = df['sales_value'].apply(lambda x: 'high value' if x &gt; 10 else 'low value')\ndf.head()\n\n\n\n\n\n\n\n\nhousehold_id\nstore_id\nbasket_id\nproduct_id\nquantity\nsales_value\nretail_disc\ncoupon_disc\ncoupon_match_disc\nweek\ntransaction_timestamp\nvalue\n\n\n\n\n0\n900\n330\n31198570044\n1095275\n1\n0.50\n0.00\n0.0\n0.0\n1\n2017-01-01 11:53:26\nlow value\n\n\n1\n900\n330\n31198570047\n9878513\n1\n0.99\n0.10\n0.0\n0.0\n1\n2017-01-01 12:10:28\nlow value\n\n\n2\n1228\n406\n31198655051\n1041453\n1\n1.43\n0.15\n0.0\n0.0\n1\n2017-01-01 12:26:30\nlow value\n\n\n3\n906\n319\n31198705046\n1020156\n1\n1.50\n0.29\n0.0\n0.0\n1\n2017-01-01 12:30:27\nlow value\n\n\n4\n906\n319\n31198705046\n1053875\n2\n2.78\n0.80\n0.0\n0.0\n1\n2017-01-01 12:30:27\nlow value\n\n\n\n\n\n\n\n\ndf.groupby('value').size()\n\nvalue\nhigh value      45265\nlow value     1424042\ndtype: int64\n\n\nAn alternative, and much faster approach is to use np.where(), which requires numpy to be loaded. np.where has been show to be over 2.5 times faster than apply():\n\ndf['value'] = np.where(df['sales_value'] &gt; 10, 'high value', 'low value')\ndf.head()\n\n\n\n\n\n\n\n\nhousehold_id\nstore_id\nbasket_id\nproduct_id\nquantity\nsales_value\nretail_disc\ncoupon_disc\ncoupon_match_disc\nweek\ntransaction_timestamp\nvalue\n\n\n\n\n0\n900\n330\n31198570044\n1095275\n1\n0.50\n0.00\n0.0\n0.0\n1\n2017-01-01 11:53:26\nlow value\n\n\n1\n900\n330\n31198570047\n9878513\n1\n0.99\n0.10\n0.0\n0.0\n1\n2017-01-01 12:10:28\nlow value\n\n\n2\n1228\n406\n31198655051\n1041453\n1\n1.43\n0.15\n0.0\n0.0\n1\n2017-01-01 12:26:30\nlow value\n\n\n3\n906\n319\n31198705046\n1020156\n1\n1.50\n0.29\n0.0\n0.0\n1\n2017-01-01 12:30:27\nlow value\n\n\n4\n906\n319\n31198705046\n1053875\n2\n2.78\n0.80\n0.0\n0.0\n1\n2017-01-01 12:30:27\nlow value\n\n\n\n\n\n\n\nAs our conditions get more complex, it often becomes useful to create a separate function and use apply. This approach is probably the most legible; however, not always the fastest approach if you are working with significantly large data.\n\ndef flag(df):\n    if (df['quantity'] &gt; 20) or (df['sales_value'] &gt; 10):\n        return 'Large purchase'\n    elif (df['quantity'] &gt; 10) or (df['sales_value'] &gt; 5):\n        return 'Medium purchase'\n    elif (df['quantity'] &gt; 0) or (df['sales_value'] &gt; 0):\n        return 'Small purchase'\n    else:\n        return 'Alternative transaction'\n\ndf['purchase_flag'] = df.apply(flag, axis = 1)\ndf.head()\n\n\n\n\n\n\n\n\nhousehold_id\nstore_id\nbasket_id\nproduct_id\nquantity\nsales_value\nretail_disc\ncoupon_disc\ncoupon_match_disc\nweek\ntransaction_timestamp\nvalue\npurchase_flag\n\n\n\n\n0\n900\n330\n31198570044\n1095275\n1\n0.50\n0.00\n0.0\n0.0\n1\n2017-01-01 11:53:26\nlow value\nSmall purchase\n\n\n1\n900\n330\n31198570047\n9878513\n1\n0.99\n0.10\n0.0\n0.0\n1\n2017-01-01 12:10:28\nlow value\nSmall purchase\n\n\n2\n1228\n406\n31198655051\n1041453\n1\n1.43\n0.15\n0.0\n0.0\n1\n2017-01-01 12:26:30\nlow value\nSmall purchase\n\n\n3\n906\n319\n31198705046\n1020156\n1\n1.50\n0.29\n0.0\n0.0\n1\n2017-01-01 12:30:27\nlow value\nSmall purchase\n\n\n4\n906\n319\n31198705046\n1053875\n2\n2.78\n0.80\n0.0\n0.0\n1\n2017-01-01 12:30:27\nlow value\nSmall purchase\n\n\n\n\n\n\n\n\ndf.groupby('purchase_flag').size()\n\npurchase_flag\nAlternative transaction       8820\nLarge purchase               46730\nMedium purchase             128361\nSmall purchase             1285396\ndtype: int64\n\n\n\n\n\n\n\n\nTipVideo 🎥:\n\n\n\nHere is a more thorough introduction to the apply method; plus, you’ll also be introduced to the map and applymap methods.\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nFill in the blanks below to assign each transaction to a power rating of 1, 2, 3, or 4 based on the sales_value variable:\n\npower_rating = 1: if sales_value &lt; 25th percentile\npower_rating = 2: if sales_value &lt; 50th percentile\npower_rating = 3: if sales_value &lt; 75th percentile\npower_rating = 4: if sales_value &gt;= 75th percentile\n\nHint: use the .quantile(perc_value)\nlow = df['sales_value'].quantile(____)\nmed = df['sales_value'].quantile(____) \nhig = df['sales_value'].quantile(____)\n\ndef power_rater(df):\n   if (df['sales_value'] &lt; _____):\n      return ___\n   elif (df['sales_value'] &lt; _____):\n      return ___\n   elif (df['sales_value'] &lt; _____):\n      return ___\n   else:\n      return ___\n\ndf['power_rating'] = df.apply(power_rater, axis = 1)\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html#summary",
    "href": "16-control-statements.html#summary",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "16.6 Summary",
    "text": "16.6 Summary\nConditional statements are the foundation for making decisions in Python programs. In this chapter, you learned how to control the flow of your code by selectively executing blocks of logic depending on whether certain conditions are met.\nYou began by exploring simple if statements and saw how Python uses indentation, not brackets, to define code blocks. From there, you learned how to extend decision logic using elif and else to handle multiple branching paths.\nYou also saw how to:\n\nUse logical operators like and and or to combine conditions\nSimulate switch-like behavior using dictionaries and the .get() method\nEmbed functions within dictionaries to build more dynamic logic maps\n\nImportantly, you practiced applying conditional logic within Pandas DataFrames using methods like .apply() and np.where()—a key skill when working with real-world data. These tools allow you to classify, flag, or transform rows based on business rules or data thresholds.\nWhether you’re categorizing transactions, building smart logic into a script, or adapting decisions based on user input, mastering conditional statements sets the stage for writing more intelligent, flexible programs.\nYou’re now ready to take on iteration—using loops to automate repeated tasks—covered in the next chapter.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "16-control-statements.html#exercise-classifying-transaction-discounts-with-conditionals",
    "href": "16-control-statements.html#exercise-classifying-transaction-discounts-with-conditionals",
    "title": "16  Controlling Program Flow with Conditional Statements",
    "section": "16.7 Exercise: Classifying Transaction Discounts with Conditionals",
    "text": "16.7 Exercise: Classifying Transaction Discounts with Conditionals\nIn this exercise set, you’ll work with the Complete Journey transactions dataset and apply what you’ve learned about conditional statements to classify and analyze retail discount patterns. These tasks will help you get comfortable with conditional logic in Python and how to apply it to Pandas DataFrames.\nUse the transactions dataset from the completejourney_py package, and import the necessary libraries as needed:\nimport pandas as pd\nimport numpy as np\nfrom completejourney_py import get_data\n\ndf = get_data()['transactions']\n\n\n\n\n\n\nTip💡 Need a refresher?\n\n\n\nReview how to use np.where(), the .apply() method, and conditional logic blocks with if-elif-else. Try using ChatGPT or Pandas documentation to explore options for vectorized logic and grouping.\n\n\n\n\n\n\n\n\nNone1. Create a total_disc Column\n\n\n\n\n\nStart by calculating the total discount applied to each transaction.\n\nCreate a new column called total_disc.\nThis should be the sum of retail_disc and coupon_disc.\n\n\n\n\n\n\n\n\n\n\nNone2. Classify Discount Levels\n\n\n\n\n\nNext, classify each transaction based on the level of discount received.\n\nCreate a new column disc_rating using the following logic:\n\n'none': if total_disc == 0\n'low': if total_disc &gt; 0 but less than the 25th percentile\n'medium': if total_disc ≥ 25th percentile and &lt; 75th percentile\n'high': if total_disc ≥ 75th percentile\n'other': for any row that doesn’t meet the above conditions\n\n\n💡 Use .quantile(0.25) and .quantile(0.75) to calculate the thresholds.\n\n\n\n\n\n\n\n\n\nNone3. Summarize Discount Ratings\n\n\n\n\n\n\nUse groupby() and size() (or .value_counts()) to determine how many transactions fall into each disc_rating category.\nAre most transactions high-discount or low-discount?\n\n\n\n\n\n\n\n\n\n\nNone4. (Bonus) Compare with sales_value\n\n\n\n\n\nAs an extension, explore whether high-discount transactions are associated with higher sales_value.\n\nUse groupby('disc_rating')['sales_value'].mean() to compare average spending by discount group.\nWhat patterns do you notice?",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Controlling Program Flow with Conditional Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html",
    "href": "17-iteration-statements.html",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "",
    "text": "17.1 Prerequisites\nRepetition is common in data science tasks—from looping through rows of a dataset to applying transformations across multiple features or files. Fortunately, programming languages like Python provide iteration statements to handle these repetitive tasks efficiently and clearly.\nIn this chapter, you’ll learn how to use for and while loops to perform repetition in your code. You’ll also learn how to control loop behavior using break and continue, explore the concept of iterables, and practice using list comprehensions—a powerful and Pythonic way to iterate and transform data collections.\nThese tools are foundational in data mining and data science work, where we often need to process large amounts of data, automate repetitive operations, and build reusable code structures.\nBy the end of this lesson you will be able to:\nimport glob\nimport os\nimport random\nimport pandas as pd",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#for-loop",
    "href": "17-iteration-statements.html#for-loop",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.2 for loop",
    "text": "17.2 for loop\nThe for loop is used to execute repetitive code statements for a particular number of times. The general syntax is provided below where i is the counter and as i assumes each sequential value the code in the body will be performed for that ith value.\n# syntax of for loop\n# !! this code won't run but, rather, gives you an idea of what the syntax looks like !!\nfor i in sequence:\n    &lt;do stuff here with i&gt;\nThere are three main components of a for loop to consider:\n\nSequence: The sequence represents each element in a list or tuple, each key-value pair in a dictionary, or each column in a DataFrame.\nBody: apply some function(s) to the object we are iterating over.\nOutput: You must specify what to do with the result. This may include printing out a result or modifying the object in place.\n\nFor example, say we want to iterate N times, we can perform a for loop using the range() function:\n\nfor number in range(10):\n    print(number)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nWe can add multiple lines to our for loop; we just need to ensure that each line follows the same indentation patter:\n\nfor number in range(10):\n    squared = number * number\n    print(f'{number} squared = {squared}')\n\n0 squared = 0\n1 squared = 1\n2 squared = 4\n3 squared = 9\n4 squared = 16\n5 squared = 25\n6 squared = 36\n7 squared = 49\n8 squared = 64\n9 squared = 81\n\n\nRather than just print out some result, we can also assign the computation to an object. For example, say we wanted to assign the squared result in the previous for loop to a dictionary where the key is the original number and the value is the squared value.\n\nsquared_values = {}\n\nfor number in range(10):\n    squared = number * number\n    squared_values[number] = squared\n\nsquared_values\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81}\n\n\n\nKnowledge check\nWe can see all data sets that we have in the “data/monthly_data” folder with glob.glob:\n\nmonthly_data_files = sorted(glob.glob(\"../data/monthly_data/*\"))\nmonthly_data_files\n\n['../data/monthly_data/Month-01.csv',\n '../data/monthly_data/Month-02.csv',\n '../data/monthly_data/Month-03.csv',\n '../data/monthly_data/Month-04.csv',\n '../data/monthly_data/Month-05.csv',\n '../data/monthly_data/Month-06.csv',\n '../data/monthly_data/Month-07.csv',\n '../data/monthly_data/Month-08.csv',\n '../data/monthly_data/Month-09.csv',\n '../data/monthly_data/Month-10.csv',\n '../data/monthly_data/Month-11.csv']\n\n\nIf you wanted to get just the file name from the string path we can use os.path.basename:\n\nfile_name = os.path.basename(monthly_data_files[0])\nfile_name\n\n'Month-01.csv'\n\n\nAnd if we wanted to just get the name minus the file extension we can apply some simple string indexing to remove the last four characters (.csv):\n\nfile_name[:-4]\n\n'Month-01'\n\n\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nUse this knowledge to:\n\nCreate an empty dictionary called monthly_data.\nLoop over monthly_data_files and assign the file name as the dictionary key and assign the file path as the value.\nLoop over monthly_data_files and assign the file name as the dictionary key, import the data with pd.read_csv() and assign the imported DataFrame as the value in the dictionary.\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#controlling-sequences",
    "href": "17-iteration-statements.html#controlling-sequences",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.3 Controlling sequences",
    "text": "17.3 Controlling sequences\nThere are two ways to control the progression of a loop:\n\ncontinue: terminates the current iteration and advances to the next.\nbreak: exits the entire for loop.\n\nBoth are used in conjunction with if statements. For example, this for loop will iterate for each element in year; however, when it gets to the element that equals the year of covid (2020) it will break out and end the for loop process.\n\n# range will produce numbers starting at 2018 and up to but not include 2023\nyears = range(2018, 2023)\nlist(years)\n\n[2018, 2019, 2020, 2021, 2022]\n\n\n\ncovid = 2020\n\nfor year in years:\n    if year == covid: break\n    print(year)\n\n2018\n2019\n\n\nThe continue argument is useful when we want to skip the current iteration of a loop without terminating it. On encountering continue, the Python parser skips further evaluation and starts the next iteration of the loop. In this example, the for loop will iterate for each element in year; however, when it gets to the element that equals covid it will skip the rest of the code execution simply jump to the next iteration.\n\nfor year in years:\n    if year == covid: continue\n    print(year)\n\n2018\n2019\n2021\n2022\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nModify the following for loop with a continue or break statement to:\n\nonly import Month-01 through Month-07\nonly import Month-08 through Month-10\n\nmonthly_data_files = glob.glob(\"../data/monthly_data/*\")\nmonthly_data = {}\n\nfor file in monthly_data_files:\n    file_name = os.path.basename(file)[:-4]\n    monthly_data[file_name] = pd.read_csv(file)\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#list-comprehensions",
    "href": "17-iteration-statements.html#list-comprehensions",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.4 List comprehensions",
    "text": "17.4 List comprehensions\nList comprehensions offer a shorthand syntax for for loops and are very common in the Python community. Although a little odd at first, the way to think of list comprehensions is as a backward for loop where we state the expression first, and then the sequence.\n# !! this code won't run but, rather, gives you an idea of what the syntax looks like !!\n\n# syntax of for loop\nfor i in sequence:\n    expression\n  \n# syntax for a list comprehension\n[expression for i in sequence]\nOften, we’ll see a pattern like the following where we:\n\ncreate an empty object (list in this example)\nloop over an object and perform some computation\nsave the result to the empty object\n\n\nsquared_values = []\nfor number in range(5):\n    squared = number * number\n    squared_values.append(squared)\n\nsquared_values\n\n[0, 1, 4, 9, 16]\n\n\nA list comprehension allows us to condense this pattern to a single line:\n\nsquared_values = [number * number for number in range(5)]\nsquared_values\n\n[0, 1, 4, 9, 16]\n\n\nList comprehensions even allow us to add conditional statements. For example, here we use a conditional statement to skip even numbers:\n\nsquared_odd_values = [number * number for number in range(10) if number % 2 != 0]\nsquared_odd_values\n\n[1, 9, 25, 49, 81]\n\n\nFor more complex conditional statements, or if the list comprehension gets a bit long, we can use multiple lines to make it easier to digest:\n\nsquared_certain_values = [\n    number * number for number in range(10)\n    if number % 2 != 0 and number != 5\n    ]\n\nsquared_certain_values\n\n[1, 9, 49, 81]\n\n\nThere are other forms of comprehensions as well. For example, we can perform a dictionary comprehension where we follow the same patter; however, we use dict brackets ({) instead of list brackets ([):\n\nsquared_values_dict = {number: number*number for number in range(10)}\nsquared_values_dict\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81}\n\n\n\n\n\n\n\n\nTipVideo 🎥:\n\n\n\nCheck out this video that provides more discussion and examples of using comprehensions.\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nRe-write the following for loop using a dictionary comprehension:\nmonthly_data_files = glob.glob(\"../data/monthly_data/*\")\nmonthly_data = {}\n\nfor file in monthly_data_files:\n    file_name = os.path.basename(file)[:-4]\n    monthly_data[file_name] = pd.read_csv(file)\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#while-loop",
    "href": "17-iteration-statements.html#while-loop",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.5 while loop",
    "text": "17.5 while loop\nWe may not always know how many iterations we need to make. Rather, we simply want to perform some task while a particular condition exists. This is the job of a while loop. A while loop follows the same logic as a for loop, except, rather than specify a sequence we want to specify a condition that will determine how many iterations.\n# syntax of for loop\nwhile condition_holds:\n    &lt;do stuff here with i&gt;\nFor example, the probability of flipping 10 coins and getting all heads or tails is \\((\\frac{1}{2})^{10} = 0.0009765625\\) (1 in 1024 tries). Let’s implement this and see how many times it’ll take to accomplish this feat.\nThe following while statement will check if the number of unique values for 10 flips are 1, which implies that we flipped all heads or tails. If it is not equal to 1 then we repeat the process of flipping 10 coins and incrementing the number of tries. When our condition statement ten_of_a_kind == True then our while loop will stop.\n\n# create a coin\ncoin = ['heads', 'tails']\n\n# we'll use this to track how many tries it takes to get 10 heads or 10 tails\nn_tries = 0\n\n# signals if we got 10 heads or 10 tails\nten_of_a_kind = False\n\nwhile not ten_of_a_kind:\n    # flip coin 10 times\n    ten_coin_flips = [random.choice(coin) for flip in range(11)]\n\n    # check if there\n    ten_of_a_kind = len(set(ten_coin_flips)) == 1\n\n    # add iteration to counter\n    n_tries += 1\n\n\nprint(f'After {n_tries} flips: {ten_coin_flips}')\n\nAfter 569 flips: ['heads', 'heads', 'heads', 'heads', 'heads', 'heads', 'heads', 'heads', 'heads', 'heads', 'heads']\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nAn elementary example of a random walk is the random walk on the integer number line, \\(Z\\), which starts at 0 and at each step moves +1 or −1 with equal probability.\nFill in the incomplete code chunk below to perform a random walk starting at value 0, with each step either adding or subtracting 1. Have your random walk stop if the value it exceeds 100 or if the number of steps taken exceeds 10,000.\nvalue = 0\nn_tries = 0\nexceeds_100 = False\n\nwhile not exceeds_100 or _______:\n    # randomly add or subtract 1\n    random_value = random.choice([-1, 1])\n    value += _____\n\n    # check if value exceeds 100\n    exceeds_100 = ______\n\n    # add iteration to counter\n    n_tries += _____\n\n  \nprint(f'The final value was {value} after {n_tries} iterations.')\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#iterables",
    "href": "17-iteration-statements.html#iterables",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.6 Iterables",
    "text": "17.6 Iterables\nPython strongly leverages the concept of iterable objects. An object is considered iterable if it is either a physically stored sequence, or an object that produces one result at a time in the context of an interation tool like a for loop. Up to this point, our example looping structures have primarily iterated over a DataFrame or a list.\nWhen our for loop iterates over a DataFrame, underneath the hood it is first accessing the iterable object, and then iterating over each item. As the following illustrates, the default iterable components of a DataFrame are the columns:\n\ndf = pd.DataFrame({'col1': [1, 2, 3], 'col2': [3, 4, 5], 'col3': [6, 6, 6]})\n\nI = df.__iter__() # access iterable object\nprint(next(I))    # first iteration\nprint(next(I))    # second iteration\nprint(next(I))    # third iteration\n\ncol1\ncol2\ncol3\n\n\nWhen our for loop iterates over a list, the same procedure unfolds. Note that when no more items are available to iterate over, a StopIteration is thrown which signals to our for loop that no more itertions should be performed.\n\nnames = ['Robert', 'Sandy', 'John', 'Patrick']\n\nI = names.__iter__() # access iterable object\nprint(next(I))       # first iteration\nprint(next(I))       # second iteration\nprint(next(I))       # third iteration\nprint(next(I))       # fourth iteration\nprint(next(I))       # no more items\n\nRobert\nSandy\nJohn\nPatrick\n\n\n\n---------------------------------------------------------------------------\nStopIteration                             Traceback (most recent call last)\nCell In[18], line 8\n      6 print(next(I))       # third iteration\n      7 print(next(I))       # fourth iteration\n----&gt; 8 print(next(I))       # no more items\n\nStopIteration: \n\n\n\nDictionaries and tuples are also iterable objects. Iterating over dictionary automatically returns one key at a time, which allows us to have the key and index for that key at the same time:\n\nD = {'a':1, 'b':2, 'c':3}\n\nI = D.__iter__()  # access iterable object\nprint(next(I))    # first iteration\nprint(next(I))    # second iteration\nprint(next(I))    # third iteration\n\na\nb\nc\n\n\n\nfor key in D:\n    print(key, D[key])\n\na 1\nb 2\nc 3\n\n\nAlthough using these iterables in a for loop is quite common, you will often see two other approaches which include the iterables range() and enumerate(). range is often used to generate indexes in a for loop but you can use it anywhere you need a series of integers. However, range is an iterable that generates items on demand:\n\nvalues = range(5)\n\nI = values.__iter__()\nprint(next(I))\nprint(next(I))\nprint(next(I))\n\n0\n1\n2\n\n\nSo if you wanted to iterate over each column in our DataFrame, an alternative is to use range. In this example, range produces the numeric index for each column so we simply use that value to index for the column within the for loop:\n\nunique_values = []\nfor col in range(len(df.columns)):\n  value = df.iloc[:, col].nunique()\n  unique_values.append(value)\n\nunique_values\n\n[3, 3, 1]\n\n\nAnother common iterator you will see is enumerate. Actually, the enumerate function returns a generator object, which also supports this iterator concept. The benefit of enumerate is that it returns a (index, value) tuple each time through the loop:\n\nE = enumerate(df) # access iterable object\nprint(next(E))    # first iteration\nprint(next(E))    # second iteration\nprint(next(E))    # third iteration\n\n(0, 'col1')\n(1, 'col2')\n(2, 'col3')\n\n\nThe for loop steps through these tuples automatically and allows us to unpack their values with tuple assignment in the header of the for loop. In the following example, we unpack the tuples into the variables index and col and we can now use both of these values however necessary in a for loop.\n\nfor index, col in enumerate(df):\n    print(f'{index} - {col}')\n\n0 - col1\n1 - col2\n2 - col3\n\n\nThere are additional iterable objects that can be used in looping structures (i.e. zip, map); however, the ones discussed here are the most common you will come across and likely use.\n\n\n\n\n\n\nTipVideo 🎥:\n\n\n\nLearn more about iterables and a similar, yet different concept – ‘iterators’ with this video.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#summary",
    "href": "17-iteration-statements.html#summary",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.7 Summary",
    "text": "17.7 Summary\nIn this chapter, you learned how to use iteration statements to write more efficient and powerful Python code. These tools are essential for any data scientist or analyst, especially when working with large datasets or needing to automate repetitive tasks.\nYou explored how:\n\nThe for loop allows you to iterate over sequences like lists, dictionaries, and DataFrames.\nThe while loop executes code repeatedly until a specified condition is no longer true.\nbreak and continue give you more control over loop execution.\nList and dictionary comprehensions provide a compact and readable way to create new collections.\nIterables and iterator objects, such as range() and enumerate(), form the foundation of Python looping behavior and data traversal.\n\nUnderstanding these concepts sets you up for more advanced programming patterns, where repetition, transformation, and control flow are crucial.\nBut iteration isn’t the only way to make your code more concise and reusable. In the next chapter, you’ll take your skills a step further by learning how to write your own functions. Functions allow you to encapsulate logic into clean, modular blocks of code—another key capability for data scientists who want to write readable, efficient, and maintainable analysis pipelines.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "17-iteration-statements.html#exercise-practicing-looping-and-iteration-patterns",
    "href": "17-iteration-statements.html#exercise-practicing-looping-and-iteration-patterns",
    "title": "17  Controlling Repetition with Iteration Statements",
    "section": "17.8 Exercise: Practicing Looping and Iteration Patterns",
    "text": "17.8 Exercise: Practicing Looping and Iteration Patterns\nIn this exercise set, you’ll practice using for loops, while loops, conditional logic, and comprehensions. These tasks will help you build fluency with the iteration patterns that show up frequently in data wrangling and automation tasks.\nYou can run these exercises in your own Python editor or in the companion notebook.\n\n\n\n\n\n\nTip💡 Stuck or Unsure?\n\n\n\nDon’t hesitate to ask for help! Use ChatGPT, GitHub Copilot, or any other AI coding assistant to get guidance or debug your code. It’s a great way to reinforce learning and explore alternate solutions.\n\n\n\n\n\n\n\n\nNone1. Filter Capitalized Names with a Comprehension\n\n\n\n\n\nUse the list of names below to write a list comprehension that returns only the values that start with a capital letter (i.e. a “title case” word).\nnames = ['Steve Irwin', 'koala', 'kangaroo', 'Australia', 'Sydney', 'desert']\nHint: Try using the .istitle() method.\nWhich names are included in the result?\n\n\n\n\n\n\n\n\n\nNone2. Generate the Fibonacci Sequence\n\n\n\n\n\nThe Fibonacci Sequence starts with the numbers 0 and 1, and each subsequent number is the sum of the two previous numbers. For example: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...]\nWrite a for loop that generates the first 25 Fibonacci numbers and stores them in a list.\n\n\n\n\n\n\n\n\n\nNone3. Sum with Conditional Skip\n\n\n\n\n\nWrite a for loop that computes the sum of all numbers from 0 through 100, excluding the numbers in the list below:\nskip_these_numbers = [8, 29, 43, 68, 98]\nUse a continue statement to skip over those values. What is the resulting sum?",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Controlling Repetition with Iteration Statements</span>"
    ]
  },
  {
    "objectID": "18-functions.html",
    "href": "18-functions.html",
    "title": "18  Writing Your Own Functions",
    "section": "",
    "text": "18.1 Prerequisites\nIn data science and data mining, writing your own functions is critical. As your analyses grow in complexity, so do your code bases. Functions allow you to:\nThis chapter will show you how to write your own functions—from the basics of syntax to more advanced ideas like type hints and anonymous functions. Whether you’re preparing data, building machine learning models, or conducting statistical analyses, writing custom functions will make you a more effective data scientist.\nBy the end of this lesson you will be able to:\nimport numpy as np\nimport pandas as pd\nfrom completejourney_py import get_data\n\ndf = get_data()['transactions']\n\n/opt/hostedtoolcache/Python/3.13.7/x64/lib/python3.13/site-packages/completejourney_py/get_data.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.\n  from pkg_resources import resource_filename",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#when-to-write-functions",
    "href": "18-functions.html#when-to-write-functions",
    "title": "18  Writing Your Own Functions",
    "section": "18.2 When to write functions",
    "text": "18.2 When to write functions\nYou should consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code). For example, take a look at this code. What does it do?\n\n# array containing 4 sets of 10 random numbers\nx = np.random.random_sample((4, 10))\n\nx[0] = (x[0] - x[0].min()) / (x[0].max() - x[0].min())\nx[1] = (x[1] - x[1].min()) / (x[1].max() - x[1].min())\nx[2] = (x[2] - x[2].min()) / (x[1].max() - x[2].min())\nx[3] = (x[3] - x[3].min()) / (x[3].max() - x[3].min())\n\nYou might be able to puzzle out that this rescales each array to have a range from 0 to 1. But did you spot the mistake? I made an error when copying-and-pasting the code for the third array: I forgot to change an x[1] to an x[2]. Writing a function reduces the chance of this error and makes it more explicit regarding our intentions since we can name our action (rescale) that we want to perform:\n\nx = np.random.random_sample((4, 10))\n\ndef rescale(array):\n    for index, vector in enumerate(array):\n        array[index] = (vector - vector.min()) / (vector.max() - vector.min())\n\n    return(array)\n\nrescale(x)\n\narray([[0.18671603, 0.86306023, 1.        , 0.        , 0.75826259,\n        0.88977032, 0.96623175, 0.59961373, 0.03426687, 0.40745092],\n       [0.16184841, 0.17323808, 0.05852793, 1.        , 0.08795447,\n        0.38394026, 0.62244628, 0.        , 0.60964869, 0.5288501 ],\n       [0.80487493, 0.        , 0.10956488, 0.50669911, 0.55617685,\n        0.61704458, 0.92275747, 0.71375595, 1.        , 0.61294382],\n       [0.        , 0.57458424, 0.23716534, 0.93152213, 0.5119379 ,\n        0.49836653, 0.60681435, 0.20691826, 0.24938548, 1.        ]])",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#functions-vs-methods",
    "href": "18-functions.html#functions-vs-methods",
    "title": "18  Writing Your Own Functions",
    "section": "18.3 Functions vs methods",
    "text": "18.3 Functions vs methods\nFunctions and methods are very similar, and beginners to object oriented programming languages such as Python often refer to them synonymously. Although they are very similar, it is always good to distinguish between the two as it will help you explain code situations more explicitly.\nA method refers to a function which is part of a class. You access it with an instance or object of the class. A function doesn’t have this restriction: it just refers to a standalone function. This means that all methods are functions, but not all functions are methods.\n\n# stand alone function\nsum(x)\n\narray([1.15343938, 1.61088255, 1.40525816, 2.43822124, 1.91433182,\n       2.38912169, 3.11824986, 1.52028794, 1.89330103, 2.54924484])\n\n\n\n# method\nx.sum(axis = 0)\n\narray([1.15343938, 1.61088255, 1.40525816, 2.43822124, 1.91433182,\n       2.38912169, 3.11824986, 1.52028794, 1.89330103, 2.54924484])\n\n\nAs illustrated above, there can be functions and methods that behave in a similar manner. However, often, methods have special parameters that allow them to behave uniquely to the object they are attached to. For example, the sum() method for the Numpy array allows you to get the overall sum, sum of each column, and sum of each row. This would take more effort to compute with the stand alone function.\n\n# overall sum\nx.sum()\n\nnp.float64(19.99233850260639)\n\n\n\n# sum of each column\nx.sum(axis = 0)\n\narray([1.15343938, 1.61088255, 1.40525816, 2.43822124, 1.91433182,\n       2.38912169, 3.11824986, 1.52028794, 1.89330103, 2.54924484])\n\n\n\n# sum of each row\nx.sum(axis = 1)\n\narray([5.70537243, 3.62645422, 5.84381761, 4.81669424])\n\n\nNonetheless, being able to create stand alone functions is a critical task. And, if you delve into building your own object classes down the road, defining methods is a piece of cake if you know how to define stand alone functions.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#defining-functions",
    "href": "18-functions.html#defining-functions",
    "title": "18  Writing Your Own Functions",
    "section": "18.4 Defining functions",
    "text": "18.4 Defining functions\nThere are four main steps to defining a function:\n\nUse the keyword def to declare the function and follow this up with the function name.\nAdd parameters to the function: they should be within the parentheses of the function. End your line with a colon.\nAdd statements that the functions should execute.\nEnd your function with a return statement if the function should output something. Without the return statement, your function will return an object None.\n\n\ndef yell(text):\n    new_text = text.upper()\n    return new_text\n\nyell('hello world!')\n\n'HELLO WORLD!'\n\n\nOf course, your functions will get more complex as you go along: you can add for loops, flow control, and more to it to make it more finegrained. Let’s build a function that finds the total sales for a store, for a given day. The parameters required for the function are the data frame being analyzed, the store_id, and the specific week for which we need the sales.\n\ndef store_sales(data, store, week):\n    filt = (data['store_id'] == store) & (data['week'] == week)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\nstore_sales(data=df, store=309, week=48)\n\nnp.float64(395.6)\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\n\nDefine a function titled ratio that takes arguments x and y and returns their ratio, \\(\\frac{x}{y}\\).\nNow add a third variable digits that allows you to round the output to a specified decimal.\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#parameters-vs-arguments",
    "href": "18-functions.html#parameters-vs-arguments",
    "title": "18  Writing Your Own Functions",
    "section": "18.5 Parameters vs arguments",
    "text": "18.5 Parameters vs arguments\nPeople often refer to function parameters and arguments synonymously but, technically, they represent two different things. Parameters are the names used when defining a function or a method, and into which arguments will be mapped. In other words, arguments are the things which are supplied to any function or method call, while the function or method code refers to the arguments by their parameter names.\n\n\n\n\n\n\nBeing specific about the term ‘parameters’ versus ‘arguments’ can be helpful when discussing specific details regarding a function or inputs, especially during debugging.\n\n\n\nPython accepts a variety of parameter-argument calls. The following discusses some of the more common implementations you’ll see.\n\nKeyword arguments\nKeyword arguments are simply named arguments in the function call. Consider our store_sales() function, we can specify parameter arguments in two different ways. The first is positional, in which we supply our arguments in the same position as the parameter. However, this is less explicit and if we happen to switch our store and week argument placement then we may get different results then anticipated.\n\n# implicitly computing store sales for store 46 during week 43\nstore_sales(df, 46, 43)\n\nnp.float64(60.39)\n\n\n\n# implicitly computing store sales for store 43 (does not exist) during week 46\nstore_sales(df, 43, 46)\n\nnp.float64(0.0)\n\n\nUsing keyword, rather than positional, arguments makes your function call more explicit and allows you to specify them in different orders.\n\n\n\n\n\n\nPer the Zen of Python, ‘explicit is better than implicit’.\n\n\n\n\n# explicitly computing store sales for store 46 during week 43\nstore_sales(data=df, week=43, store=46)\n\nnp.float64(60.39)\n\n\n\n\nDefault arguments\nDefault arguments are those that take a default value if no argument value is passed during the function call. You can assign this default value by with the assignment operator =, just like in the below example. For example, if your analysis routinely analyzes transactions for all quantities but sometimes you only focus on sales for when quantity purchased meets some threshold, you could add a new parameter with a default value.\n\n\n\n\n\n\nDefault arguments should be placed after parameters with no defaults assigned in the function call.\n\n\n\n\ndef store_sales(data, store, week, qty_greater_than=0):\n    filt = (data['store_id'] == store) & (data['week'] == week) & (data['quantity'] &gt; qty_greater_than)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\n# you do not need to specify an input for qty_greater_than\nstore_sales(data=df, store=309, week=48)\n\nnp.float64(395.6)\n\n\n\n# but you can if you want to change it from the default\nstore_sales(data=df, store=309, week=48, qty_greater_than=2)\n\nnp.float64(92.73)\n\n\n\n\n*args and **kwargs\n*args and **kwargs allow you to pass a variable number of arguments to a function. This can be usefule when you do not know before hand how many arguments will be passed to your function by the user or if you want to allow users to pass arguments through to an embedded function.\n\n\n\n\n\n\nJust an FYI that it is not necessary to write *args or **kwargs. Only the * (asterisk) is necessary. You could have also written *var and **vars. Writing *args and **kwargs is just a convention established in the Python community.\n\n\n\n*args is used to send a non-keyworded variable length argument list to the function. When the function is called, Python collects the *args values as a tuple, which can be used in a similar manner as any other tuple functionality (i.e. indexing, for looped) For example, we can let the user supply as many string inputs to the very useful yell() function by using *args.\n\ndef yell(*args):\n    new_text = ' '.join(args).upper()\n    return new_text\n\nyell('hello world!', 'I', 'love', 'Python!!')\n\n'HELLO WORLD! I LOVE PYTHON!!'\n\n\nThe special syntax **kwargs in function definitions in python is used to pass a keyworded, variable-length argument list. The reason is because the double star allows us to pass through keyword arguments (and any number of them). Python collects the **kwargs into a new dictionary, which can be used for any normal dictionary functionality.\n\n# **kwargs just creates a dictionary\ndef students(**kwargs):\n    print(kwargs)\n\nstudents(student1='John', student2='Robert', student3='Sally')\n\n{'student1': 'John', 'student2': 'Robert', 'student3': 'Sally'}\n\n\n\n# we can use this dictionary however necessary\ndef print_student_names(**kwargs):\n    for key, value in kwargs.items():\n        print(f'{key} = {value}')\n\nprint_student_names(student1='John', student2='Robert', student3='Sally')\n\nstudent1 = John\nstudent2 = Robert\nstudent3 = Sally\n\n\n\n\nType hints\nPython 3.5 added a new library called typing that adds type hinting to Python. Type hinting is part of a larger functionality called Type Checking (see this tutorial for a comprehensive Type Checking guide). Type hinting provides a way to help users better understand what type of arguments a function takes and the expected output.\nFor example, let’s look at a simple function:\n\ndef some_function(name, age):\n    return f'{name} is {age} years old'\n\nsome_function('Tom', 27)\n\n'Tom is 27 years old'\n\n\nIn the above function we simply expect the user to insert the proper type of arguments and to realize the output will be a string. Now, this isn’t that challenging because its a small and simple function but as our functions get more complex it is not always easy to decipher what type of arguments should be passed and what the output will be.\nWe can hint to the user this information with type hints by re-writing the function as below. Here, we are hinting that the name argument should be a string, age should be an integer, and the output will be a string (-&gt; str).\n\ndef some_function(name: str, age: int) -&gt; str:\n    return f'{name} is {age} years old'\n\nsome_function('Tom', 27)\n\n'Tom is 27 years old'\n\n\nNow, when you look at the function documentation, you can quickly see the type hints (we’ll see additional ways to improve function documentation in the Docstrings section).\n\nhelp(some_function)\n\nHelp on function some_function in module __main__:\n\nsome_function(name: str, age: int) -&gt; str\n\n\n\nEven better, most IDEs support type hints so that they can give you feedback as you are typing:\n\n\n\n\n\n\nFigure 18.1: Type hints often show up when you hover over the function in most IDEs.\n\n\n\nWhen declaring types, you can use any of the built-in Python data types. This includes, but is not limited to:\n\nint, float, str, bool\ntuple, list, set, dict, array\nfrozenset\nOptional\nIterable, Iterator\nand many more!\n\nYou can also declare non-built-in data types such as Pandas and Numpy object types. For example, we can re-write our store_sales() function with type hints where we hint that data should be a pd.DataFrame.\n\n\n\n\n\n\nNote that pd.DataFrame assumes that you imported the Pandas module with the pd alias.\n\n\n\n\ndef store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n    filt = (data['store_id'] == store) & (data['week'] == week)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\nstore_sales(data=df, store=309, week=48)\n\nnp.float64(395.6)\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nBuild on to your ratio() function by:\n\nSetting the digits parameter to default to 2.\nAdd type hints that specify x and y to be numeric, digits to be int and the output returned to be float.\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#docstrings",
    "href": "18-functions.html#docstrings",
    "title": "18  Writing Your Own Functions",
    "section": "18.6 Docstrings",
    "text": "18.6 Docstrings\nType hints are great for guiding users; however, they only provide small hints. Python docstrings are a much more robust way to document a Python function (docstrings can also be used to document modules, classes, and methods). Docstrings are used to provide users with descriptive information about the function, required inputs, expected outputs, example usage and more. Also, it is a common practice to generate online (html) documentation automatically from docstrings using tools such as Sphinx.\n\n\n\n\n\n\nSee the Pandas API reference for the various Pandas functions/methods/class for example Sphinx documentation that is built from docstrings.\n\n\n\nDocstrings typically consist of a multi-line character string description that follows the header line of a function definition. The following provides an example for a simple addition function. Note how the docstring provides a summary of what the function does, information regarding expected parameter inputs, what the returned output will be, a “see also” section that lets users know about related functions/methods (this can be skipped if not applicable), and some examples of implementing the function.\n\ndef store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n    \"\"\"\n    Compute total store sales.\n\n    This function computes the total sales for a given\n    store and week based on a user supplied DataFrame that\n    contains sales in a column named `sales_value`.\n\n    Parameters\n    ----------\n    data : DataFrame\n        Pandas DataFrame\n    store : int\n        Integer value representing store number\n    week : int\n        Integer value representing week of year\n\n    Returns\n    -------\n    float\n        A float object representing total store sales\n\n    See Also\n    --------\n    store_visits : Computes total store visits\n\n    Examples\n    --------\n    &gt;&gt;&gt; store_sales(data=df, store=309, week=48)\n    395.6\n    &gt;&gt;&gt; store_sales(data=df, store=46, week=43)\n    60.39\n    \"\"\"\n    filt = (data['store_id'] == store) & (data['week'] == week)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\n\n\n\n\n\n\nThere are some typical Pythonic conventions used when writing docstrings. Rather than reiterate them here, we highly suggest using the Pandas and Numpy docstring guide standards when writing your docstrings.\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nGo back to the ratio function you created in the last knowledge check and add docstrings. Be sure to include a general description of what the function does, documentation on the parameters and return output, along with including examples.\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#errors-and-exceptions",
    "href": "18-functions.html#errors-and-exceptions",
    "title": "18  Writing Your Own Functions",
    "section": "18.7 Errors and exceptions",
    "text": "18.7 Errors and exceptions\nFor functions that will be used over and over again, and especially for those used by someone other than the creator of the function, it is good to include procedures to check for errors that may derail function execution. This may include ensuring the user supplies proper argument types, the computation can perform with the values supplied, or even that execution can be performed in a reasonable amount of time.\nThis falls under the umbrella of exception handling and is actually far broader than what we can cover here. In this section, we’ll demonstrate some of the basics.\n\nValidating arguments\nA common problem is when the user supplies invalid argument types or values. Although we have seen how to use type hints and docstrings to inform users, we can also include useful exception calls for when users supply improper argument types and values. For example, the following will check if the arguments of the correct type by using isinstance and, if they are not, we raise an Exception and include an informative error.\n\ndef store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n     # argument validation\n    if not isinstance(data, pd.DataFrame): raise Exception('`data` should be a Pandas DataFrame')\n    if not isinstance(store, int): raise Exception('`store` should be an integer')\n    if not isinstance(week, int): raise Exception('`week` should be an integer')\n\n    # computation\n    filt = (data['store_id'] == store) & (data['week'] == week)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\nstore_sales(data=df, store='309', week=48)\n\n\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\nCell In[24], line 12\n      9     total_sales = data['sales_value'][filt].sum()\n     10     return total_sales\n---&gt; 12 store_sales(data=df, store='309', week=48)\n\nCell In[24], line 4, in store_sales(data, store, week)\n      1 def store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n      2      # argument validation\n      3     if not isinstance(data, pd.DataFrame): raise Exception('`data` should be a Pandas DataFrame')\n----&gt; 4     if not isinstance(store, int): raise Exception('`store` should be an integer')\n      5     if not isinstance(week, int): raise Exception('`week` should be an integer')\n      7     # computation\n\nException: `store` should be an integer\n\n\n\nNote that Exception() is used to create a generic exception object/output. Python has many built-in exception types that can be used to indicate a specific error has occured. For example, we could replace Exception() with TypeError() in the previous example to make it more specific that the error is due to an invalid argument type.\n\ndef store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n    # argument validation\n    if not isinstance(data, pd.DataFrame): raise TypeError('`data` should be a Pandas DataFrame')\n    if not isinstance(store, int): raise TypeError('`store` should be an integer')\n    if not isinstance(week, int): raise TypeError('`week` should be an integer')\n\n    # computation\n    filt = (data['store_id'] == store) & (data['week'] == week)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\nstore_sales(data=df, store='309', week=48)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[25], line 12\n      9     total_sales = data['sales_value'][filt].sum()\n     10     return total_sales\n---&gt; 12 store_sales(data=df, store='309', week=48)\n\nCell In[25], line 4, in store_sales(data, store, week)\n      1 def store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n      2     # argument validation\n      3     if not isinstance(data, pd.DataFrame): raise TypeError('`data` should be a Pandas DataFrame')\n----&gt; 4     if not isinstance(store, int): raise TypeError('`store` should be an integer')\n      5     if not isinstance(week, int): raise TypeError('`week` should be an integer')\n      7     # computation\n\nTypeError: `store` should be an integer\n\n\n\nWe can expand this as much as necessary. For example, say we want to ensure users only use a store value that exists in our data. Currently, if the user supplies a store value that does not exist (i.e. 35), they simply get a return value of 0.\n\nstore_sales(data=df, store=35, week=48)\n\nnp.float64(0.0)\n\n\nWe can add an if statement to check if the store number exists and supply an error message to the user if it does not:\n\ndef store_sales(data: pd.DataFrame, store: int, week: int) -&gt; float:\n    # argument validation\n    if not isinstance(data, pd.DataFrame): raise TypeError('`data` should be a Pandas DataFrame')\n    if not isinstance(store, int): raise TypeError('`store` should be an integer')\n    if not isinstance(week, int): raise TypeError('`week` should be an integer')\n    if store not in data.store_id.unique():\n        raise ValueError(f'`store` {store} does not exist in the supplied DataFrame')\n\n\n    # computation\n    filt = (data['store_id'] == store) & (data['week'] == week)\n    total_sales = data['sales_value'][filt].sum()\n    return total_sales\n\nstore_sales(data=df, store=35, week=48)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[27], line 15\n     12     total_sales = data['sales_value'][filt].sum()\n     13     return total_sales\n---&gt; 15 store_sales(data=df, store=35, week=48)\n\nCell In[27], line 7, in store_sales(data, store, week)\n      5 if not isinstance(week, int): raise TypeError('`week` should be an integer')\n      6 if store not in data.store_id.unique():\n----&gt; 7     raise ValueError(f'`store` {store} does not exist in the supplied DataFrame')\n     10 # computation\n     11 filt = (data['store_id'] == store) & (data['week'] == week)\n\nValueError: `store` 35 does not exist in the supplied DataFrame\n\n\n\n\n\nAssert statements\nPython’s assert statement is a debugging aid that tests a condition. If the condition is true, it does nothing and your program just continues to execute. But if the assert condition evaluates to false, it raises an AssertionError exception with an optional error message. This seems very similar to the example in the last section where we checked if the supplied store ID exists. However, the proper use of assertions is to inform users about unrecoverable errors in a program. They’re not intended to signal expected error conditions, like “store ID not found”, where a user can take corrective action or just try again.\n\n\n\n\n\n\nAnother way to look at it is to say that assertions are internal self-checks for your program. They work by declaring some conditions as impossible in your code. If one of these conditions doesn’t hold that means there’s a bug in the program.\n\n\n\nFor example, say you have a program that automatically applies a discount to product and you eventually write the following apply_discount function:\n\ndef apply_discount(product, discount):\n    price = round(product['price'] * (1.0 - discount), 2)\n    assert 0 &lt;= price &lt;= product['price']\n    return price\n\nNotice the assert statement in there? It will guarantee that, no matter what, discounted prices cannot be lower than $0 and they cannot be higher than the original price of the product.\nLet’s make sure this actually works as intended. You can see that the second example throws an AssertionError\n\n# 25% off 3.50 should equal 2.62\nmilk = {'name': 'Chocolate Milk', 'price': 3.50}\napply_discount(milk, 0.25)\n\n2.62\n\n\n\n# 200% discount is not allowed\napply_discount(milk, 2.00)\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[30], line 2\n      1 # 200% discount is not allowed\n----&gt; 2 apply_discount(milk, 2.00)\n\nCell In[28], line 3, in apply_discount(product, discount)\n      1 def apply_discount(product, discount):\n      2     price = round(product['price'] * (1.0 - discount), 2)\n----&gt; 3     assert 0 &lt;= price &lt;= product['price']\n      4     return price\n\nAssertionError: \n\n\n\nIn the above example, we simply got an AssertionError but no message to help us understand what caused the error. The assert statement follows the syntax of assert condition, message so we can add an informative message at the end of our assert statement.\n\ndef apply_discount(product, discount):\n    price = round(product['price'] * (1.0 - discount), 2)\n    assert 0 &lt;= price &lt;= product['price'], 'Invalid discount applied'\n    return price\n\napply_discount(milk, 2.00)\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[31], line 6\n      3     assert 0 &lt;= price &lt;= product['price'], 'Invalid discount applied'\n      4     return price\n----&gt; 6 apply_discount(milk, 2.00)\n\nCell In[31], line 3, in apply_discount(product, discount)\n      1 def apply_discount(product, discount):\n      2     price = round(product['price'] * (1.0 - discount), 2)\n----&gt; 3     assert 0 &lt;= price &lt;= product['price'], 'Invalid discount applied'\n      4     return price\n\nAssertionError: Invalid discount applied\n\n\n\n\n\nTry and except\nIn the prior sections, we looked at ways to identify errors that may occur. The result of our exception handling is to signal an error occurred and stop the program. However, there are often times when we do not want to stop the program. For example, if the program has a database connection that should be closed after execution then we need our program to continue running even if an error occurs to ensure the connection is closed.\nThe try except procedure allows us to try execute code. If it works, great! If not, rather than just throw an exception error, we define what the program should continue to do. For example, the following tries the apply_discount() function with a pre-defined discount value. If it throws and exception then we adjust the discount value (if discount is greater than 100% of product price we adjust it to the largest acceptable value, if its less than 0 we just set it to 0%).\n\n# this discount is created somewhere else in the program\ndiscount = 2\n\n# if discount causes an error adjust it\ntry:\n    apply_discount(milk, discount)\nexcept Exception:\n    if discount &gt; 1: discount = 0.99\n    if discount &lt; 0: discount = 0\n    apply_discount(milk, discount)\n\nThe try except procedure allows us to include as many except statements as necessary for different types of errors (think of them like elifs). For example, the following illustrates how we could have different types of code execution for a TypeError, a ValueError, and then the final else statement captures all other errors.\n\n\n\n\n\n\nThis procedure will run in order, consequently you want to have the most specific exception handlers first followed by more general exception handlers later on.\n\n\n\n\ntry:\n    store_sales(data=df, store=35, week=48)\nexcept TypeError:\n    print('do something specific for a `TypeError`')\nexcept ValueError:\n    print('do something specific for a `ValueError`')\nelse:\n    print('do something specific for all other errors')\n\ndo something specific for a `ValueError`\n\n\nLastly, we may have a certain piece of code that we always want to ensure gets ran in a program. For example, we may need to ensure a database connection or file is closed before an error is thrown. The following illustrates how we can add a finally at the end of our try except procedure. This finally will always be ran. If the code in the try clause runs without error, the finally code chunk will run after the try block. If an error occurse, the finally code chunk will run before the relevant except code chunk.\n\ntry:\n    store_sales(data=df, store=35, week=48)\nexcept TypeError:\n    raise\nexcept ValueError:\n    raise\nfinally:\n    print('Code to close database connection')\n\nCode to close database connection\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[34], line 2\n      1 try:\n----&gt; 2     store_sales(data=df, store=35, week=48)\n      3 except TypeError:\n      4     raise\n\nCell In[27], line 7, in store_sales(data, store, week)\n      5 if not isinstance(week, int): raise TypeError('`week` should be an integer')\n      6 if store not in data.store_id.unique():\n----&gt; 7     raise ValueError(f'`store` {store} does not exist in the supplied DataFrame')\n     10 # computation\n     11 filt = (data['store_id'] == store) & (data['week'] == week)\n\nValueError: `store` 35 does not exist in the supplied DataFrame\n\n\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nGoing back to the ratio function:\n\nAdd a procdure to validate that x and y inputs are numeric and digits is an integer.\nAdd a try and except procedure where if a TypeError is thrown for digits because it is a float then the digits value is rounded to the nearest integer and applied.\n\nTry the above yourself and then see my approach here:",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#scoping",
    "href": "18-functions.html#scoping",
    "title": "18  Writing Your Own Functions",
    "section": "18.8 Scoping",
    "text": "18.8 Scoping\nScoping refers to the set of rules a programming language uses to lookup the value to variables and/or symbols. The following illustrates the basic concept behind the lexical scoping rules that Python follows. In short, Python follows a nested environment structure and uses what is commonly referred to as the LEGB search rule:\n\nSearch local scope first,\nthen the local scopes of enclosing functions,\nthen the global scope,\nand finally the built-in scope\n\n\nSearching for variables\nWhat exactly does this mean? A function has its own environment and when you assign an argument in the def header of the function, the function creates a separate environment that keeps that variable separate from any variable in the global environment. This is why you can have an x variable in the global environment that won’t be confused with an x variable in your function:\n\nx = 84\n\ndef func(x):\n  return x + 1\n\nfunc(x = 50)\n\n51\n\n\nHowever, the function environment is only active when called and all function variables are removed from memory after being called. Consequently, you can continue using x that is contained in the global environment.\n\nx\n\n84\n\n\nHowever, if a variable does not exist within the function, Python will look one level up to see if the variable exists. In this case, since y is not supplied in the function header, Python will look in the next environment up (global environment in this case) for that variable:\n\ny = 'Boehmke'\n\ndef func(x):\n  return x + ' ' + y\n\nfunc(x = 'Brad')\n\n'Brad Boehmke'\n\n\nThe same will happen if we have nested functions. Python will search in enclosing functions in a hierarchical fashion until it finds the necessary variables. In this convoluted procedure…\n\ny is a global variable,\nx & sep are local variables to the my_name function,\nand my_paste has no local variables,\nthe my_name function gets y from the global environment,\nand the my_paste function gets all its inputs from the my_name environment.\n\n\n\n\n\n\n\nWe do not recommend that you write functions like this. This is primarily only for explaining how Python searches for information, which can be helpful for debugging.\n\n\n\n\ny = 'Boehmke'\n\ndef my_name(sep):\n    x = 'Brad'\n    def my_paste():\n        return x + sep + y\n    return my_paste()\n\nmy_name(sep=' ')\n\n'Brad Boehmke'\n\n\n\n\nChanging variables\nIt is possible to change variable values that are outside of the active environment. This is rarely necessary, and is usually not good practice, but it is good to know about. For example, the following changes the global variable y by including the keyword-variable statement global y prior to making the assignment of the new value for y.\n\n\n\n\n\n\nThe same can be done with changing values in nested functions; however, you would use the keyword nonlocal to change the value of an enclosing function’s local variable.\n\n\n\n\ny = 8451\n\ndef convert(x):\n    x = str(x)\n    firstpart, secondpart = x[:len(x)//2], x[len(x)//2:]\n    global y\n    y = firstpart + '.' + secondpart\n    return y\n\nconvert(8451)\n\n'84.51'\n\n\n\ny\n\n'84.51'",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#anonymous-functions",
    "href": "18-functions.html#anonymous-functions",
    "title": "18  Writing Your Own Functions",
    "section": "18.9 Anonymous functions",
    "text": "18.9 Anonymous functions\nSo far we have been discussing defined functions; however, Python allows you to generate anonymous functions on the fly. These are often referred to as lambdas. Lambdas allow for an alternative approach when creating short and simple functions that are only used once or twice.\nFor example, the following two functions are equivalent:\n# defined function\ndef func(x, y, z):\n    return x + y + z\n  \n# lambda function  \nlambda x, y, z: x + y + z  \nNote how the lambda’s body is a single expression and not a block statement. This is a requirement, which typically restricts lambdas to very short, concise function calls. The best use case for lambda functions are for when you want a simple function to be anonymously embedded within a larger expressions. For example, say we wanted to loop over each item in a list and apply a simple square function, we could accomplish this by supplying a lambda function to the map function. map just iterates over each item in an object and applies a given function (you could accomplish the exact same with a list comprehension).\n\nnums = [48, 6, 9, 21, 1]\n\nlist(map(lambda x: x ** 2, nums))\n\n[2304, 36, 81, 441, 1]\n\n\nAnother good example is the following one you already lesson 6a where we apply a lambda function to assign ‘high value’ for each transaction where sales_value is greater than 10 and ‘low value’ for all other transactions.\n\n(\n    df['sales_value']\n    .apply(lambda x: 'high value' if x &gt; 10 else 'low value')\n)\n\n0          low value\n1          low value\n2          low value\n3          low value\n4          low value\n             ...    \n1469302    low value\n1469303    low value\n1469304    low value\n1469305    low value\n1469306    low value\nName: sales_value, Length: 1469307, dtype: object\n\n\nHere is another example where we group by basket_id and then apply a lambda function to compute the average cost per item in each basket.\n\n(\n    df[['basket_id', 'sales_value', 'quantity']]\n    .groupby('basket_id')\n    .apply(lambda x: (x['sales_value'] / x['quantity']).mean())\n)\n\n/tmp/ipykernel_12834/1550772873.py:4: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda x: (x['sales_value'] / x['quantity']).mean())\n\n\nbasket_id\n31198437603    1.951207\n31198445400    1.250000\n31198445429    2.045000\n31198445465    1.680000\n31198452527    4.048333\n                 ...   \n41480008448    1.533810\n41480013048    1.206667\n41480018446    1.773210\n41481252623    1.285556\n41481282915    8.495000\nLength: 155848, dtype: float64\n\n\n\nKnowledge check\n\n\n\n\n\n\nNoneTry it yourself:\n\n\n\nLet’s focus on the transaction timestamp for each transaction (df['transaction_timestamp']). Use the .apply() method along with a lambda function to assign each transaction to ‘weekday’ or ‘weekend’. To do this check out the .day_name() method (i.e. df['transaction_timestamp'][0].day_name())",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#summary",
    "href": "18-functions.html#summary",
    "title": "18  Writing Your Own Functions",
    "section": "18.10 Summary",
    "text": "18.10 Summary\nWriting your own functions is one of the most essential skills for any data scientist. In this chapter, you learned how to encapsulate your logic, reduce repetition, and build cleaner, more modular code through custom function definitions.\nWe began by discussing when and why to write functions, especially when working on complex data analysis tasks that benefit from reusability and clarity. You saw how functions differ from methods and how they support both simple and advanced programming constructs.\nYou learned how to:\n\nDefine functions using the def keyword, including parameters and return values.\nUse keyword arguments and default arguments to make your functions more flexible and explicit.\nLeverage *args and **kwargs to support variable-length argument lists for more general-purpose tools.\nAdd type hints to clarify what types of inputs a function expects and what it returns—great for documentation and collaboration.\nDocument your functions with docstrings, following common Python conventions that enhance readability and support automated documentation tools.\nApply error handling techniques using raise, try-except, assert, and custom error messages to make your functions more robust and user-friendly.\nUnderstand Python’s variable scoping rules, including the LEGB rule (Local, Enclosing, Global, Built-in), and how variables are resolved when functions are nested.\nCreate concise anonymous functions (a.k.a. lambda functions) and apply them within higher-order functions like map, apply, and groupby.\n\nThroughout the chapter, you saw realistic examples related to store sales and transaction data, reinforcing the importance of custom functions in real-world data mining and analysis workflows. Whether you’re cleaning data, transforming values, calculating metrics, or building more advanced analytics pipelines, your ability to write well-structured functions will directly improve the quality and maintainability of your work.\nIn short, functions are foundational to writing professional, efficient, and reusable Python code—skills that will serve you across any domain of data science.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "18-functions.html#exercise-practicing-function-writing-and-application",
    "href": "18-functions.html#exercise-practicing-function-writing-and-application",
    "title": "18  Writing Your Own Functions",
    "section": "18.11 Exercise: Practicing Function Writing and Application",
    "text": "18.11 Exercise: Practicing Function Writing and Application\nIn this exercise set, you’ll practice defining and applying custom Python functions, using type hints and docstrings, and leveraging methods like .apply() to work with real-world data. These tasks will help solidify your understanding of functions and how to use them in data cleaning, feature engineering, and exploratory analysis workflows.\nYou can run these exercises in your own Python editor or in the companion notebook.\n\n\n\n\n\n\nTip💡 Stuck or Unsure?\n\n\n\nUse ChatGPT, GitHub Copilot, or any other AI coding assistant to debug your code or talk through your logic. It’s a great way to reinforce concepts and practice problem solving.\n\n\n\n\n\n\n\n\nNone1. Load and Inspect the Data\n\n\n\n\n\nDownload the companies.csv dataset and load it into a DataFrame. This dataset contains company names and financial attributes.\nInspect the first few rows. What columns are available?\n\n\n\n\n\n\n\n\n\nNone2. Define the is_incorporated() Function\n\n\n\n\n\nWrite a function is_incorporated(name) that checks whether the input string name contains the substring \"inc\" or \"Inc\". If either appears in the name, return True; otherwise return False.\nTest it using a few sample strings like:\nis_incorporated(\"Acme Inc.\")\nis_incorporated(\"Global Tech\")\n\n\n\n\n\n\n\n\n\nNone3. Add Type Hints and a Docstring\n\n\n\n\n\nNow update your is_incorporated() function to include:\n\nA type hint for the name parameter and the return type\nA docstring describing what the function does, the input parameter, and the return value\n\nUse the help() function or hover in your IDE to verify the documentation.\n\n\n\n\n\n\n\n\n\nNone4. Apply the Function with a Loop\n\n\n\n\n\nUse a for loop to iterate through the Name column of the companies DataFrame. For each value, call your is_incorporated() function and print the company name along with whether it’s incorporated.\nYour output might look like:\nAcme Inc. → True  \nGlobal Tech → False  \nBright Inc. → True\n\n\n\n\n\n\n\n\n\nNone5. Apply the Function with .apply()\n\n\n\n\n\nNow rewrite your logic using the .apply() method instead of a for loop.\n\nApply is_incorporated() to the Name column\nStore the result in a new column called \"is_incorporated\"\nPrint the updated DataFrame to verify the new column",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Writing Your Own Functions</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html",
    "href": "99-anaconda-install.html",
    "title": "19  Anaconda Installation",
    "section": "",
    "text": "19.1 Installing Anaconda\nAnaconda Distribution is one of the most popular platforms for doing data science with Python. It’s designed to make it easier for beginners and professionals alike to set up their Python environment with all the essential tools for analysis, visualization, and machine learning.\nWhen you install Anaconda, you get much more than just Python, you get:\nHere’s why Anaconda is a great option for beginners:",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#installing-anaconda",
    "href": "99-anaconda-install.html#installing-anaconda",
    "title": "19  Anaconda Installation",
    "section": "",
    "text": "CautionWhat about Miniconda?\n\n\n\n\n\nYou may hear about a tool called Miniconda, which is a lightweight alternative to Anaconda. While Miniconda gives you more control by starting with a barebones Python environment, it requires you to install everything manually. In this course, we recommend Anaconda because it comes preloaded with everything you need for data science—and it’s much easier to get started with. If you’re new to Python, stick with Anaconda.\n\n\n\n\nBefore You Begin\nYou’ll need:\n\nA stable internet connection\nMake sure your operating system is current enough for the Anaconda install. See requirements here.\n~3–4 GB of free disk space\nA bit of patience—it’s a large install!\n\n\n\nFor Windows Users\n\nGo to https://www.anaconda.com/products/distribution\nClick “Download” and choose the Windows installer (64-bit) \nOnce downloaded, open the installer\nFollow the prompts:\n\nChoose “Just Me” (recommended)\nLeave default install location\nIMPORTANT: When asked about adding Anaconda to your PATH: Keep it unchecked (recommended)\n\nClick Install and wait (~5–10 mins)\nAfter installation, click “Finish”\nVerify your installation: The Anaconda Navigator should automatically open after successful installation of Anaconda Distribution. If it does not, you should be able to search for “Anaconda Navigator” in the taskbar search.\n\nFor more installation instructions you can find the Anaconda documentation for Windows installation here. You can also view the following video:\n\n\n\nFor Mac Users\n\nGo to https://www.anaconda.com/products/distribution\nDownload the macOS (Intel or M1/M2) installer that matches your hardware \nOpen the .pkg file and follow the installation instructions\n\nGrant permission when prompted\nLeave default settings\n\nVerify your installation: The Anaconda Navigator should automatically open after successful installation of Anaconda Distribution. If it does not, you should be able to search for “Anaconda Navigator” using Spotlight (Cmd + Space → “Anaconda Navigator”)\n\nFor more installation instructions you can find the Anaconda documentation for Windows installation here. You can also view the following video:",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#anaconda-navigator",
    "href": "99-anaconda-install.html#anaconda-navigator",
    "title": "19  Anaconda Installation",
    "section": "19.2 Anaconda Navigator",
    "text": "19.2 Anaconda Navigator\nAnaconda Navigator is the graphical interface included with Anaconda that makes it easier to access your development tools without needing to use the command line.\nWhen you launch Navigator, you’ll see a dashboard of available applications, including but not limited to:\n\nJupyter Notebook – A lightweight, web-based interface to write and run Python code interactively.\nJupyterLab – A more advanced interface for working with notebooks, code files, terminals, and data.\nSpyder – A scientific development environment similar to RStudio (not used in this course).\nVS Code – If installed, you may see a launcher for Visual Studio Code.\n\n\n\n\nAnaconda Navigator\n\n\nYou can also manage environments and packages through Navigator, but for now, your main focus will be on using it to launch Jupyter Notebook or JupyterLab.\nThis simple interface removes a lot of the friction of getting started with Python and is part of what makes Anaconda such a great option for beginners.\n\n\n\n\n\n\nTipLearn more about Anaconda Navigator\n\n\n\n\n\nAnaconda Navigator is a powerful application that can do much more than what we’ve covered here. If you’re curious and want to explore its full functionality, you can read more at here or watch this helpful overview video:\n\nThat said, don’t feel pressured to learn it all at once—some of the functionality may feel overwhelming at first, and that’s perfectly okay. We’ll ease into the tools you need as the course progresses.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#launching-jupyter-and-running-your-first-notebook",
    "href": "99-anaconda-install.html#launching-jupyter-and-running-your-first-notebook",
    "title": "19  Anaconda Installation",
    "section": "19.3 Launching Jupyter and Running Your First Notebook",
    "text": "19.3 Launching Jupyter and Running Your First Notebook\nNow that you’ve successfully installed Anaconda and explored the Navigator interface, it’s time to put it to use. One of the most common tools you’ll use as a data scientist is a Jupyter Notebook—an interactive coding environment that runs in your web browser and lets you combine code, text, and output in one place. In this section, we’ll walk you through how to launch a notebook and run your very first line of code using your newly installed Anaconda environment.\n\n\n\n\n\n\nNoteJupyter Notebooks vs. JupyterLab\n\n\n\nYou may notice that both Jupyter Notebook and JupyterLab are available in Anaconda Navigator. While they both allow you to write and execute code in the Jupyter Notebook format, JupyterLab is the newer, more modern interface. It provides a more flexible workspace, supports multiple tabs and file types, and integrates better with other tools. In this course, we’ll be using JupyterLab by default because of these added capabilities—but feel free to explore both if you’re curious!\n\n\nOnce you’ve installed Anaconda:\n\nOpen Anaconda Navigator: Start by opening the Anaconda Navigator application from your system’s start menu or application launcher.\nLocate JupyterLab: Within the Navigator interface, you’ll see a list of available applications. Find the “JupyterLab” icon. \nLaunch JupyterLab: Click the launch button at the bottom of the JupyterLab icon.\n\nThis will open a new tab in your default web browser at a local address like http://localhost:8888.\nThis web-based interface acts as your coding workspace. It allows you to create and run notebooks, write scripts, view files, and manage your data science projects—all in one place. And you interact with this interface through your browser.\n\nCreate a new notebook: In the Jupyter interface click New → Python 3 Notebook and this will launch a new Jupyter notebook. \nWrite Python code: In the first cell, type the following and press Shift + Enter to run the code:\nprint(\"Hello Anaconda\")\nYou should see:\nHello Anaconda\n\nThat’s it! You’ve set up your local Python data science environment.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#troubleshooting-tips",
    "href": "99-anaconda-install.html#troubleshooting-tips",
    "title": "19  Anaconda Installation",
    "section": "19.4 Troubleshooting Tips",
    "text": "19.4 Troubleshooting Tips\nIf you run into issues during installation or while launching tools like Anaconda Navigator or JupyterLab, don’t worry—it’s common when working with new software. Below are some tips that can help you troubleshoot the most common problems:\n\nAnaconda Installation Issues\n\nThe installer won’t open or crashes: Make sure your system meets the requirements and try re-downloading the installer from the official site.\nInstallation is stuck or taking too long: This process can take several minutes. If it seems frozen for more than 15–20 minutes, cancel the installation, restart your machine, and try again.\nAccidentally added Anaconda to PATH and things broke: Try uninstalling and reinstalling Anaconda, and leave the PATH option unchecked this time (recommended). You can follow the steps in this official guide to uninstall Anaconda properly:\n\nHow to uninstall Anaconda (official guide)\n\n\n\n\nIssues Launching Anaconda Navigator\n\nNavigator doesn’t launch:\n\nRestart your computer and try again.\nOn Windows, try launching the Anaconda Prompt and typing anaconda-navigator to launch it manually.\n\nNavigator opens but buttons are unresponsive or blank: This could be a display issue. Try updating your graphics drivers or restarting the app.\n\n\n\nProblems Starting JupyterLab\n\nClicking “Launch” in Navigator does nothing:\n\nRestart Navigator or your computer.\nTry launching JupyterLab via the command line:\n\nOn Windows: open Anaconda Prompt and type jupyter lab\nOn Mac: open Terminal and type jupyter lab\n\n\nBrowser doesn’t open automatically:\n\nCopy the URL shown in the terminal (e.g., http://localhost:8888) and paste it into your browser manually.\n\nJupyterLab opens but shows an error or doesn’t load:\n\nTry clearing your browser cache or switching to a different browser.\nMake sure no firewall or antivirus program is blocking access to localhost.\n\n\nIf all else fails, feel free to reach out to your instructor or teaching assistant for help!",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-anaconda-install.html#additional-resources",
    "href": "99-anaconda-install.html#additional-resources",
    "title": "19  Anaconda Installation",
    "section": "19.5 Additional Resources",
    "text": "19.5 Additional Resources\nWant to explore more about Anaconda and JupyterLab beyond the basics? Here are some helpful resources if you’re curious to dig deeper:\n\nAnaconda Navigator Documentation: Learn about all the features available through Navigator, including managing environments and installing packages.\n\nhttps://www.anaconda.com/docs/tools/anaconda-navigator/main\n\nAnaconda Navigator Overview Video: A quick video tour of Navigator’s capabilities.\n\nhttps://youtu.be/PxMPl1x-qng\n\nJupyterLab Documentation: Explore JupyterLab’s interface, features, and how to extend its functionality.\n\nhttps://jupyterlab.readthedocs.io/en/stable/\n\n\nThese resources go into far more detail than we’ll need in this course—so don’t feel pressured to master everything right away. Use them as a reference when you’re ready to explore more.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Anaconda Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html",
    "href": "99-vscode-install.html",
    "title": "20  VS Code Installation",
    "section": "",
    "text": "20.1 Installing VS Code\nVisual Studio Code (VS Code) is a free, lightweight, and powerful code editor used by developers and data scientists around the world. It’s highly customizable and supports many programming languages and tools—including Python.\nVS Code is more than just a text editor. With the right extensions, it can run code, work with Jupyter notebooks, help you manage projects, and even connect to Git for version control. It’s one of the most widely used environments in the industry and is likely the tool you’ll encounter in internships or full-time roles.\nWhile it may feel more advanced than Anaconda Navigator or JupyterLab, VS Code offers more flexibility and control as your projects grow. In this guide, we’ll walk you through getting started with VS Code for Python programming and data science.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#installing-vs-code",
    "href": "99-vscode-install.html#installing-vs-code",
    "title": "20  VS Code Installation",
    "section": "",
    "text": "Visit the official VS Code website: https://code.visualstudio.com\nClick Download and select the installer for your operating system (Windows, macOS, or Linux). \nRun the installer and follow the default prompts.\n\n\nWindows-Specific Notes:\n\nAllow the installer to add VS Code to your system PATH (this helps you launch it from the command line).\nYou can also enable options to open VS Code from the right-click context menu for added convenience.\n\n\n\nmacOS-Specific Notes:\n\nDrag and drop the VS Code application into your Applications folder.\nOpen it via Spotlight Search (Cmd + Space → “Visual Studio Code”).\n\nOnce installed, you should be able to open VS Code and access its interface.\n\nOn Windows: Click the Start menu, type “Visual Studio Code,” and press Enter. If you checked the box to add VS Code to the system PATH, you can also open a Command Prompt or PowerShell window and type code to launch it.\nOn macOS: Open Spotlight Search by pressing Cmd + Space, type “Visual Studio Code,” and press Enter. If you installed the command line tool, you can also open a Terminal window and type code.\n\nThis should open the VS Code editor which will look like:\n\n\n\nVS Code Editor\n\n\n\n\n\n\n\n\nIf you have any issues during setup, check out the VS Code setup docs for details around system requirements, more detailed setups for each operating system, along with common questions.\n\n\n\nIn the next section, we’ll help you install the necessary extensions to begin working with Python inside VS Code.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#installing-the-python-extension",
    "href": "99-vscode-install.html#installing-the-python-extension",
    "title": "20  VS Code Installation",
    "section": "20.2 Installing the Python Extension",
    "text": "20.2 Installing the Python Extension\nTo use Python inside VS Code, you’ll need two things:\n\nA Python interpreter: This is the program that actually runs your Python code. If you’ve already installed Anaconda, you’re all set—Anaconda includes a Python interpreter. If you haven’t (for example, if you’re coming straight from using Colab), you’ll need to install Python separately. You can download the latest version from https://www.python.org/downloads/.\nThe official Python extension provided by Microsoft: This extension adds support for writing, running, and debugging Python code in VS Code, and helps you work with virtual environments. To install this extension:\n\nOpen VS Code.\nClick on the Extensions icon on the left sidebar (or press Ctrl+Shift+X / Cmd+Shift+X).\nIn the search bar, type “Python” and look for the extension authored by Microsoft.\nClick Install. \n\n\nOnce installed, the Python extension will automatically detect Python interpreters on your system and provide features such as syntax highlighting, auto-complete, linting, and integrated terminal support.\n\nWhile You’re at It: Install These Additional Extensions\nTo get the most out of VS Code for Python and Jupyter work, we recommend installing the following extensions now:\n\nJupyter: Enables Jupyter Notebook support within VS Code. \nPylance: Improves code intelligence and type checking. \n\nAfter installing these extensions, you’re ready to start writing and running Python code in VS Code!\n\n\nUnderstanding Extensions in VS Code\nExtensions are small add-ons that enhance the functionality of VS Code. Think of them like plugins—they allow you to customize your coding environment based on your needs.\nSome extensions help you write and run code in specific languages, while others add support for tools like Git, Jupyter notebooks, or data visualization libraries.\nHere are a few common extensions for data science workflows:\n\nPython (by Microsoft) – Adds Python language support\nJupyter – Enables Jupyter Notebook support\nPylance – Adds fast, accurate code intelligence and type checking\nGitLens – Enhances Git capabilities directly in VS Code\nCode Runner – Allows you to run snippets of code from many languages\n\nTo explore more extensions:\n\nClick the Extensions icon in the sidebar.\nBrowse featured or recommended extensions.\nUse the search bar to find tools that match your interests.\n\n\n\n\n\n\n\nYou don’t need to install a lot of extensions to get started. As you become more experienced, you’ll naturally add tools that support your workflow and projects.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#creating-your-first-jupyter-notebook",
    "href": "99-vscode-install.html#creating-your-first-jupyter-notebook",
    "title": "20  VS Code Installation",
    "section": "20.3 Creating Your First Jupyter Notebook",
    "text": "20.3 Creating Your First Jupyter Notebook\nNow that VS Code is set up with Python and Jupyter support, let’s walk through creating your first Jupyter notebook and running a simple Python command.\nSteps to Create and Run a Notebook:\n\nOpen VS Code.\nClick File → New File → select Jupyter Notebook \nLet’s go ahead and save this file. Go to File → Save As and you can select where you want to save this file to and also rename it (e.g., hello.ipynb).\nVS Code will recognize this as a Jupyter notebook and open an interactive notebook interface.\nIn the first cell, type:\n\nprint(\"Hello VS Code\")\n\nClick the Run Cell button (the small ▶️ icon) to the left of the cell, or press Shift + Enter. You should see the output Hello VS Code as below.\n\n\n\n\nHello VS Code\n\n\nCongratulations! You’ve just run your first Jupyter notebook in VS Code. You’re now ready to use this environment for more advanced analysis and coding throughout the course.\n\n\n\n\n\n\nHere’s a great video to get you up and running with Jupyter notebooks in VS Code!",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#exploring-the-vs-code-interface",
    "href": "99-vscode-install.html#exploring-the-vs-code-interface",
    "title": "20  VS Code Installation",
    "section": "20.4 Exploring the VS Code Interface",
    "text": "20.4 Exploring the VS Code Interface\nVS Code is packed with features, but you don’t need to use them all right away. As you become more comfortable with VS Code and your data science skills grow, you’ll naturally start using more of its capabilities. Here are some of the features you’ll find yourself using most often:\n\nExplorer (📁 icon): View and manage your project files.\nRun and Debug (▶️ icon): Run code or set up debugging tools.\nExtensions (🔌 icon): Install and manage extensions.\nIntegrated Terminal: Open a terminal inside VS Code using Ctrl + `` orCmd + `` (backtick).\nNotebook Interface: When working in .ipynb files, you can use cells to write and run code interactively.\n\nIf the interface feels overwhelming at first, don’t worry. Start by focusing on writing and running Python code—over time, you’ll naturally grow into the more advanced features.\n\n\n\n\n\n\nHere is a nice video that will introduce you to some of VS Code’s features. Don’t worry about understanding the code shown or what all these features mean right now—just watch to get a sense of what VS Code can do!",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#troubleshooting-tips",
    "href": "99-vscode-install.html#troubleshooting-tips",
    "title": "20  VS Code Installation",
    "section": "20.5 Troubleshooting Tips",
    "text": "20.5 Troubleshooting Tips\nIf you run into issues while setting up or using VS Code, here are some tips:\n\nVS Code doesn’t install:: If you have any issues during setup, check out the VS Code setup docs for details around system requirements, more detailed setups for each operating system, along with common questions.\nVS Code installs but doesn’t launch:\n\nMake sure the installation finished successfully and you can find the application on your computer\n\nOn Windows: Click the Start menu, type “Visual Studio Code,” and press Enter. If you checked the box to add VS Code to the system PATH, you can also open a Command Prompt or PowerShell window and type code to launch it.\nOn macOS: Open Spotlight Search by pressing Cmd + Space, type “Visual Studio Code,” and press Enter. If you installed the command line tool, you can also open a Terminal window and type code.\n\nTry restarting your machine.\nReinstall VS Code from the official website.\n\nPython extension isn’t working or missing:\n\nMake sure it’s installed and enabled.\nReload the window (Cmd/Ctrl + Shift + P → type Reload Window).\n\nPython interpreter not found:\n\nPress Ctrl+Shift+P / Cmd+Shift+P and search for “Python: Select Interpreter”. Choose the appropriate Python version (often one installed with Anaconda).\nIf no Python options come up then you probably don’t have a Python interpreter installed. Go to https://www.python.org/downloads/ and download the latest version.\n\nNotebook cells not running:\n\nMake sure the Jupyter extension is installed.\nMake sure the file is saved with a .ipynb extension.\n\n\nIf all else fails, try searching the error message online or ask your instructor or teaching assistant for help.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  },
  {
    "objectID": "99-vscode-install.html#additional-resources",
    "href": "99-vscode-install.html#additional-resources",
    "title": "20  VS Code Installation",
    "section": "20.6 Additional Resources",
    "text": "20.6 Additional Resources\nIf you want to learn more about using VS Code for Python and data science, here are some helpful links:\n\nTutorial: Get started with Visual Studio Code\nVS Code Python Docs\nVS Code for Data Science Docs\nJupyter Notebooks in VS Code\nVarious VS Code Introductory Videos\n\nYou don’t need to master all of this at once—use these resources when you’re ready to explore more advanced features or troubleshoot issues.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>VS Code Installation</span>"
    ]
  }
]