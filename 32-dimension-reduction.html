<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.9.12">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>32&nbsp; Dimension Reduction with PCA – BANA 4080: Data Mining</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./33-modern-ml-algorithms.html" rel="next">
<link href="./31-clustering.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-cdaacfc258cb6f151192107f105ac881.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-a555615fda6662b620e1ae2330e503bf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-cdaacfc258cb6f151192107f105ac881.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-45c1b2e5a2b0567ccfb99e4dfc03f650.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-54ba87e1857bbaa32a381632a2aab8bf.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-45c1b2e5a2b0567ccfb99e4dfc03f650.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="nav-sidebar docked quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./31-clustering.html">Module 12</a></li><li class="breadcrumb-item"><a href="./32-dimension-reduction.html"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Dimension Reduction with PCA</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">BANA 4080: Data Mining</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/bradleyboehmke/uc-bana-4080" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./BANA-4080--Data-Mining.epub" title="Download ePub" class="quarto-navigation-tool px-1" aria-label="Download ePub"><i class="bi bi-journal"></i></a>
    <a href="https://twitter.com/intent/tweet?url=|url|" title="Twitter" class="quarto-navigation-tool px-1" aria-label="Twitter"><i class="bi bi-twitter"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro-data-mining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-preparing-for-code.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Setting Up Your Python Environment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-python-basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Python Basics – Working with Data and Variables</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 2</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-jupyter.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Getting Started with Jupyter Notebooks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-data-structures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction to Data Structures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-libraries.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Packages, Libraries, and Modules</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-importing-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Importing Data and Exploring Pandas DataFrames</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-dataframes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Deeper Dive on DataFrames</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-subsetting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Subsetting Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 4</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-manipulating-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Manipulating Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_aggregating_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Summarizing Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-joining-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Relational data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 5</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-data-viz-pandas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Intro to Data Visualization with Pandas</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-data-viz-matplotlib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Fundamentals of Plotting with Matplotlib</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-data-viz-bokeh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Interactive Data Visualization with Bokeh</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 6</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-control-statements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Controlling Program Flow with Conditional Statements</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-iteration-statements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Controlling Repetition with Iteration Statements</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Writing Your Own Functions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 7</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-intro-ml-ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Introduction to Machine Learning and Artificial Intelligence</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-before-we-build.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Before You Build: Key Considerations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 8</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-correlation-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Correlation and Linear Regression Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-regression-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Evaluating Regression Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 9</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Introduction to Logistic Regression for Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-classification-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Evaluating Classification Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 10</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-decision-trees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Decision Trees: Foundations and Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-random-forests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Random Forests: Ensemble Power and Robustness</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./27-feature-importance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Understanding Feature Importance: Peeking Inside the Black Box</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 11</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./28-cross-validation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Cross-validation: Reliable model evaluation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./29-hyperparameter-tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning: Finding Optimal Model Configurations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./30-feature-engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Feature Engineering</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 12</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./31-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Unsupervised Learning and Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./32-dimension-reduction.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Dimension Reduction with PCA</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 13</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./33-modern-ml-algorithms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Beyond the Basics: Modern Machine Learning Algorithms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./34-ml-roadmap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">The Machine Learning Roadmap: Where to Go Next</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendix</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./99-anaconda-install.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Anaconda Installation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./99-vscode-install.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">VS Code Installation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#from-clustering-to-dimension-reduction" id="toc-from-clustering-to-dimension-reduction" class="nav-link active" data-scroll-target="#from-clustering-to-dimension-reduction"><span class="header-section-number">32.1</span> From Clustering to Dimension Reduction</a>
  <ul class="collapse">
  <li><a href="#quick-recap-two-main-types-of-unsupervised-learning" id="toc-quick-recap-two-main-types-of-unsupervised-learning" class="nav-link" data-scroll-target="#quick-recap-two-main-types-of-unsupervised-learning">Quick Recap: Two Main Types of Unsupervised Learning</a></li>
  <li><a href="#why-we-need-dimension-reduction" id="toc-why-we-need-dimension-reduction" class="nav-link" data-scroll-target="#why-we-need-dimension-reduction">Why We Need Dimension Reduction</a></li>
  </ul></li>
  <li><a href="#what-is-dimension-reduction" id="toc-what-is-dimension-reduction" class="nav-link" data-scroll-target="#what-is-dimension-reduction"><span class="header-section-number">32.2</span> What Is Dimension Reduction?</a>
  <ul class="collapse">
  <li><a href="#conceptual-overview" id="toc-conceptual-overview" class="nav-link" data-scroll-target="#conceptual-overview">Conceptual Overview</a></li>
  <li><a href="#feature-selection-vs.-feature-extraction" id="toc-feature-selection-vs.-feature-extraction" class="nav-link" data-scroll-target="#feature-selection-vs.-feature-extraction">Feature Selection vs.&nbsp;Feature Extraction</a></li>
  <li><a href="#real-world-applications" id="toc-real-world-applications" class="nav-link" data-scroll-target="#real-world-applications">Real-World Applications</a></li>
  </ul></li>
  <li><a href="#introducing-principal-component-analysis-pca" id="toc-introducing-principal-component-analysis-pca" class="nav-link" data-scroll-target="#introducing-principal-component-analysis-pca"><span class="header-section-number">32.3</span> Introducing Principal Component Analysis (PCA)</a>
  <ul class="collapse">
  <li><a href="#intuitive-explanation-finding-new-axes-that-capture-the-most-variance" id="toc-intuitive-explanation-finding-new-axes-that-capture-the-most-variance" class="nav-link" data-scroll-target="#intuitive-explanation-finding-new-axes-that-capture-the-most-variance">Intuitive Explanation: Finding New Axes That Capture the Most Variance</a></li>
  <li><a href="#visual-intuition-projecting-data-onto-a-new-coordinate-system" id="toc-visual-intuition-projecting-data-onto-a-new-coordinate-system" class="nav-link" data-scroll-target="#visual-intuition-projecting-data-onto-a-new-coordinate-system">Visual Intuition: Projecting Data onto a New Coordinate System</a></li>
  <li><a href="#core-terms-components-loadings-explained-variance" id="toc-core-terms-components-loadings-explained-variance" class="nav-link" data-scroll-target="#core-terms-components-loadings-explained-variance">Core Terms: Components, Loadings, Explained Variance</a></li>
  </ul></li>
  <li><a href="#how-pca-works-step-by-step" id="toc-how-pca-works-step-by-step" class="nav-link" data-scroll-target="#how-pca-works-step-by-step"><span class="header-section-number">32.4</span> How PCA Works (Step-by-Step)</a>
  <ul class="collapse">
  <li><a href="#step-1-standardize-the-data" id="toc-step-1-standardize-the-data" class="nav-link" data-scroll-target="#step-1-standardize-the-data">Step 1: Standardize the Data</a></li>
  <li><a href="#step-2-compute-the-covariance-matrix" id="toc-step-2-compute-the-covariance-matrix" class="nav-link" data-scroll-target="#step-2-compute-the-covariance-matrix">Step 2: Compute the Covariance Matrix</a></li>
  <li><a href="#step-3-find-eigenvectors-and-eigenvalues" id="toc-step-3-find-eigenvectors-and-eigenvalues" class="nav-link" data-scroll-target="#step-3-find-eigenvectors-and-eigenvalues">Step 3: Find Eigenvectors and Eigenvalues</a></li>
  <li><a href="#step-4-choose-principal-components" id="toc-step-4-choose-principal-components" class="nav-link" data-scroll-target="#step-4-choose-principal-components">Step 4: Choose Principal Components</a></li>
  <li><a href="#step-5-transform-the-data" id="toc-step-5-transform-the-data" class="nav-link" data-scroll-target="#step-5-transform-the-data">Step 5: Transform the Data</a></li>
  </ul></li>
  <li><a href="#case-study-pca-for-breast-cancer-classification" id="toc-case-study-pca-for-breast-cancer-classification" class="nav-link" data-scroll-target="#case-study-pca-for-breast-cancer-classification"><span class="header-section-number">32.5</span> Case Study: PCA for Breast Cancer Classification</a>
  <ul class="collapse">
  <li><a href="#step-1-load-and-standardize-the-data" id="toc-step-1-load-and-standardize-the-data" class="nav-link" data-scroll-target="#step-1-load-and-standardize-the-data">Step 1: Load and Standardize the Data</a></li>
  <li><a href="#step-2-fit-pca" id="toc-step-2-fit-pca" class="nav-link" data-scroll-target="#step-2-fit-pca">Step 2: Fit PCA</a></li>
  <li><a href="#step-3-choose-number-of-components-scree-plot-elbow-method" id="toc-step-3-choose-number-of-components-scree-plot-elbow-method" class="nav-link" data-scroll-target="#step-3-choose-number-of-components-scree-plot-elbow-method">Step 3: Choose Number of Components (Scree Plot &amp; Elbow Method)</a></li>
  <li><a href="#step-4-examine-principal-components" id="toc-step-4-examine-principal-components" class="nav-link" data-scroll-target="#step-4-examine-principal-components">Step 4: Examine Principal Components</a></li>
  <li><a href="#step-5-transform-the-data-1" id="toc-step-5-transform-the-data-1" class="nav-link" data-scroll-target="#step-5-transform-the-data-1">Step 5: Transform the Data</a></li>
  <li><a href="#step-6-using-pca-features-in-machine-learning" id="toc-step-6-using-pca-features-in-machine-learning" class="nav-link" data-scroll-target="#step-6-using-pca-features-in-machine-learning">Step 6: Using PCA Features in Machine Learning</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">32.6</span> Summary</a></li>
  <li><a href="#end-of-chapter-exercises" id="toc-end-of-chapter-exercises" class="nav-link" data-scroll-target="#end-of-chapter-exercises"><span class="header-section-number">32.7</span> End of Chapter Exercises</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/bradleyboehmke/uc-bana-4080/edit/main/32-dimension-reduction.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/bradleyboehmke/uc-bana-4080/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./31-clustering.html">Module 12</a></li><li class="breadcrumb-item"><a href="./32-dimension-reduction.html"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Dimension Reduction with PCA</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-dimension-reduction" class="quarto-section-identifier"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Dimension Reduction with PCA</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Imagine you’re analyzing customer behavior using 50 different features: purchase frequency across product categories, browsing patterns, demographic information, social media engagement, and more. Each feature captures something about your customers, but do you really need all 50? What if most of the meaningful variation in customer behavior could be captured by just 5-10 carefully constructed combinations of these features?</p>
<p>This is the promise of <strong>dimension reduction</strong>—transforming high-dimensional data into a lower-dimensional representation that preserves the most important information. It’s like creating a highlight reel from a full movie: you lose some detail, but you capture the essential story.</p>
<p>In the previous chapter, you explored clustering—one of the two main branches of unsupervised learning. Clustering groups similar observations together. Dimension reduction, the second major branch, focuses on reducing the number of features while preserving as much information as possible. Both techniques discover hidden structure in data without relying on labeled outcomes.</p>
<p>This chapter introduces you to <strong>Principal Component Analysis (PCA)</strong>—the most widely-used dimension reduction technique in data science. You’ll learn how PCA transforms correlated features into a smaller set of uncorrelated “principal components” that capture the most important patterns in your data. Through hands-on examples, you’ll master when to use PCA, how to interpret its results, and how to integrate it into your machine learning pipelines to improve model performance and computational efficiency.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li>Explain the difference between dimension reduction and clustering as two types of unsupervised learning</li>
<li>Understand why high-dimensional data poses challenges and when dimension reduction helps</li>
<li>Describe how PCA finds directions of maximum variance using eigenvectors and eigenvalues</li>
<li>Explain the difference between feature extraction (like PCA) and feature selection</li>
<li>Execute the complete PCA workflow: standardize data, fit PCA, choose components, examine loadings, and transform data</li>
<li>Use scree plots and the elbow method to determine the optimal number of components</li>
<li>Interpret component loadings to understand what each principal component represents</li>
<li>Apply PCA to real datasets (like breast cancer classification) to reduce dimensionality</li>
<li>Compare machine learning models using original features vs.&nbsp;PCA-transformed features</li>
<li>Evaluate the tradeoffs between dimensionality reduction and model performance</li>
<li>Recognize PCA’s limitations and when not to use it</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Follow along in Colab
</div>
</div>
<div class="callout-body-container callout-body">
<p>As you read through this chapter, we encourage you to follow along using the <a href="https://github.com/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/32_dimension_reduction.ipynb">companion notebook</a> in Google Colab (or another editor of your choice). This interactive notebook lets you run all the code examples covered hereand experiment with your own ideas.</p>
<p>▶ Open the <a href="https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/32_dimension_reduction.ipynb">Dimension Reduction Notebook in Colab</a>.</p>
</div>
</div>
<section id="from-clustering-to-dimension-reduction" class="level2" data-number="32.1">
<h2 data-number="32.1" class="anchored" data-anchor-id="from-clustering-to-dimension-reduction"><span class="header-section-number">32.1</span> From Clustering to Dimension Reduction</h2>
<section id="quick-recap-two-main-types-of-unsupervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="quick-recap-two-main-types-of-unsupervised-learning">Quick Recap: Two Main Types of Unsupervised Learning</h3>
<p>In the previous chapter, you learned about <strong>clustering</strong>—algorithms that automatically group similar observations together. Clustering answers the question: “Can we organize our observations into meaningful groups?”</p>
<p>Dimension reduction addresses a different question: “Can we represent our data using fewer features while preserving the most important information?”</p>
<p>Both are forms of <strong>unsupervised learning</strong> because they discover patterns in data without relying on labeled outcomes. But they serve different purposes:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Clustering</strong></th>
<th><strong>Dimension Reduction</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Groups similar <strong>observations</strong></td>
<td>Combines correlated <strong>features</strong></td>
</tr>
<tr class="even">
<td>Reduces rows conceptually (creates segments)</td>
<td>Reduces columns (creates new features)</td>
</tr>
<tr class="odd">
<td>“These customers behave similarly”</td>
<td>“These features capture similar information”</td>
</tr>
<tr class="even">
<td>Output: Cluster labels for each observation</td>
<td>Output: Transformed dataset with fewer features</td>
</tr>
<tr class="odd">
<td>Example: Segment customers into 5 groups</td>
<td>Example: Reduce 50 features to 10 components</td>
</tr>
</tbody>
</table>
<p>Think of it this way: clustering helps you understand how your observations relate to each other, while dimension reduction helps you understand how your features relate to each other.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>They Work Great Together
</div>
</div>
<div class="callout-body-container callout-body">
<p>Clustering and dimension reduction are often used in sequence! You might first use PCA to reduce 100 features to 10 principal components, then apply K-Means clustering to those 10 components. This approach reduces noise, improves clustering quality, and makes results easier to visualize.</p>
</div>
</div>
</section>
<section id="why-we-need-dimension-reduction" class="level3">
<h3 class="anchored" data-anchor-id="why-we-need-dimension-reduction">Why We Need Dimension Reduction</h3>
<p>Modern datasets are getting wider and wider. It’s not uncommon to work with datasets that have hundreds or thousands of features. While more information seems better, high-dimensional data creates several practical challenges:</p>
<section id="the-curse-of-dimensionality" class="level4">
<h4 class="anchored" data-anchor-id="the-curse-of-dimensionality">The Curse of Dimensionality</h4>
<p>As the number of features increases, strange things happen:</p>
<p><strong>1. Sparse Data Space</strong>: With 2 features, you need relatively few observations to cover the feature space reasonably well. But with 100 features, the data becomes incredibly sparseobservations are far apart from each other in high-dimensional space. This makes it difficult for algorithms to find patterns or make reliable predictions.</p>
<p>Imagine you have 100 observations in a dataset:</p>
<ul>
<li>With 2 features, those 100 points can adequately cover a 2D plane</li>
<li>With 10 features, they’re spread thin across 10-dimensional space</li>
<li>With 100 features, they’re isolated dots in a vast 100-dimensional void</li>
</ul>
<p><strong>2. Computational Cost</strong>: More features mean more calculations. Training time, memory requirements, and prediction time all increase with dimensionality. An algorithm that runs in seconds with 10 features might take hours with 1,000 features.</p>
<p><strong>3. Overfitting Risk</strong>: More features give your model more opportunities to find spurious patterns that don’t generalize. Your model might achieve perfect training accuracy by memorizing noise rather than learning true patterns.</p>
<p><strong>4. Multicollinearity</strong>: Many real-world features are correlated. If you’re predicting house prices, square footage and number of rooms are likely correlated. These redundant features add complexity without adding proportional information.</p>
<div class="callout callout-style-simple callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>When Does Dimensionality Become a Problem?
</div>
</div>
<div class="callout-body-container callout-body">
<p>There’s no magic number, but watch for these warning signs:</p>
<ul>
<li>You have more features than observations (or close to it)</li>
<li>Many features are highly correlated with each other</li>
<li>Your model trains slowly or runs out of memory</li>
<li>Your model performs well on training data but poorly on test data</li>
<li>You struggle to interpret which features matter most</li>
</ul>
</div>
</div>
</section>
<section id="benefits-of-dimension-reduction" class="level4">
<h4 class="anchored" data-anchor-id="benefits-of-dimension-reduction">Benefits of Dimension Reduction</h4>
<p>When applied appropriately, dimension reduction offers several advantages:</p>
<p><strong>1. Visualization</strong>: We humans can visualize 2D and 3D spaces easily but struggle beyond that. Reducing high-dimensional data to 2-3 dimensions enables powerful exploratory visualizations. You might reduce customer behavior from 50 features to 2 principal components and plot them to reveal natural customer segments.</p>
<p><strong>2. Noise Reduction</strong>: Real-world data contains measurement errors and irrelevant variation. By focusing on the dimensions that capture the most variance, PCA effectively filters out noise that lives in the minor dimensions.</p>
<p><strong>3. Improved Model Performance</strong>: Counterintuitively, removing features can sometimes improve model accuracy by reducing overfitting. A model with fewer, more meaningful features may generalize better than one overwhelmed by noisy, redundant features.</p>
<p><strong>4. Computational Efficiency</strong>: Fewer features mean faster training, faster predictions, and lower memory requirements. This matters especially when deploying models in production or working with very large datasets.</p>
<p><strong>5. Addressing Multicollinearity</strong>: Some algorithms (like linear regression) struggle when features are highly correlated. PCA creates uncorrelated components, which can stabilize these algorithms and improve their performance.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>The Information-Complexity Tradeoff
</div>
</div>
<div class="callout-body-container callout-body">
<p>Dimension reduction is fundamentally about tradeoffs. You’re trading some information (detail) for reduced complexity (simplicity). The art is finding the sweet spot where you retain enough information to solve your problem while gaining meaningful benefits in interpretability, speed, or performance.</p>
</div>
</div>
</section>
</section>
</section>
<section id="what-is-dimension-reduction" class="level2" data-number="32.2">
<h2 data-number="32.2" class="anchored" data-anchor-id="what-is-dimension-reduction"><span class="header-section-number">32.2</span> What Is Dimension Reduction?</h2>
<section id="conceptual-overview" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-overview">Conceptual Overview</h3>
<p>At its core, dimension reduction transforms your data from a high-dimensional space to a lower-dimensional space. Instead of representing each observation using 50 original features, you represent it using 10 new features that capture most of the important variation.</p>
<p>Think of a photograph. A high-resolution image might have millions of pixels (dimensions), but you can compress it to a much smaller file size that still looks nearly identical. The compression algorithm identifies redundancies and preserves the essential visual information while discarding less important details. Dimension reduction does something similar with tabular data.</p>
<div id="fig-compression-analogy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-compression-analogy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/31-image-compression-analogy.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compression-analogy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32.1: Image compression as an analogy for dimension reduction. The original high-resolution image contains millions of pixels (high dimensionality), while the compressed version uses far fewer bytes to represent essentially the same visual information (lower dimensionality). Similarly, PCA reduces many correlated features into fewer principal components while preserving the essential patterns in the data.
</figcaption>
</figure>
</div>
<p>There are two main philosophical approaches to dimension reduction:</p>
</section>
<section id="feature-selection-vs.-feature-extraction" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection-vs.-feature-extraction">Feature Selection vs.&nbsp;Feature Extraction</h3>
<p><strong>Feature Selection</strong> chooses a subset of the original features to keep. It’s like picking your favorite songs from an albumyou’re selecting from what already exists.</p>
<ul>
<li><strong>Approach</strong>: Keep features with highest importance, lowest correlation, or best predictive power</li>
<li><strong>Advantages</strong>: Results are directly interpretable (you’re still using the original features)</li>
<li><strong>Disadvantages</strong>: Might discard features that contain useful information</li>
<li><strong>Example</strong>: Using correlation analysis or feature importance scores to select the 10 most relevant features from 50</li>
</ul>
<p><strong>Feature Extraction</strong> creates new features by combining the original features mathematically. It’s like creating a playlist of mashup songsyou’re blending existing content into something new.</p>
<ul>
<li><strong>Approach</strong>: Transform original features into new composite features</li>
<li><strong>Advantages</strong>: Can capture more information than simple feature selection</li>
<li><strong>Disadvantages</strong>: New features are less interpretable (they’re mathematical combinations)</li>
<li><strong>Example</strong>: PCA creates new features that are weighted combinations of all original features</li>
</ul>
<p>This chapter focuses on <strong>feature extraction</strong> through PCA, though both approaches have their place in a data scientist’s toolkit.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Why Learn Feature Extraction First?
</div>
</div>
<div class="callout-body-container callout-body">
<p>While feature selection is more intuitive, feature extraction (particularly PCA) is more commonly used in professional practice because it:</p>
<ol type="1">
<li>Captures more information than selecting individual features</li>
<li>Removes redundancy by creating uncorrelated components</li>
<li>Works well as a preprocessing step for many machine learning algorithms</li>
<li>Has become the standard approach in fields like computer vision and genomics</li>
</ol>
<p>Once you understand PCA, feature selection techniques will feel straightforward by comparison.</p>
</div>
</div>
</section>
<section id="real-world-applications" class="level3">
<h3 class="anchored" data-anchor-id="real-world-applications">Real-World Applications</h3>
<p>Dimension reduction, particularly PCA, appears across many domains:</p>
<p><strong>Image Compression and Computer Vision</strong></p>
<ul>
<li>Facial recognition systems use PCA to reduce thousands of pixel values to key “eigenfaces”</li>
<li>Image compression algorithms reduce file sizes while preserving visual quality</li>
<li>Computer vision systems reduce high-dimensional image data before object detection</li>
</ul>
<p><strong>Text Analysis and Natural Language Processing</strong></p>
<ul>
<li>Document analysis uses techniques like Latent Semantic Analysis (related to PCA) to reduce documents from thousands of word counts to meaningful topics</li>
<li>Sentiment analysis systems reduce high-dimensional text features before classification</li>
</ul>
<p><strong>Genomics and Bioinformatics</strong></p>
<ul>
<li>Gene expression studies might measure 20,000+ genes per patient, but PCA can reduce this to a handful of components capturing the main biological patterns</li>
<li>Population genetics uses PCA to visualize genetic relationships between populations</li>
</ul>
<p><strong>Finance and Economics</strong></p>
<ul>
<li>Stock market analysis reduces hundreds of stock prices to a few principal factors</li>
<li>Economic indices combine multiple economic indicators into composite measures</li>
<li>Risk management systems use PCA to identify common risk factors across assets</li>
</ul>
<p><strong>Recommendation Systems</strong></p>
<ul>
<li>Netflix and Spotify use matrix factorization (similar to PCA) to reduce user-item matrices</li>
<li>These systems identify latent factors like “action movie preference” or “likes indie rock”</li>
</ul>
<p><strong>Sensor Data and IoT</strong></p>
<ul>
<li>Industrial systems with hundreds of sensors use PCA to monitor for anomalies</li>
<li>Reducing sensor data makes real-time monitoring computationally feasible</li>
</ul>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Master PCA, Understand Its Cousins
</div>
</div>
<div class="callout-body-container callout-body">
<p>While this chapter focuses on PCA, the same core concepts apply to related techniques you encountered above like Latent Semantic Analysis (for text), matrix factorization (for recommendation systems), and other dimension reduction methods. Once you understand how PCA finds patterns by combining features and reducing dimensionality, these alternative approaches will make intuitive sense—they’re essentially PCA adapted for specific data types or domains. Think of PCA as learning to ride a bicycle; the other techniques are like riding a mountain bike or road bike—the fundamentals transfer directly.</p>
</div>
</div>
</section>
</section>
<section id="introducing-principal-component-analysis-pca" class="level2" data-number="32.3">
<h2 data-number="32.3" class="anchored" data-anchor-id="introducing-principal-component-analysis-pca"><span class="header-section-number">32.3</span> Introducing Principal Component Analysis (PCA)</h2>
<section id="intuitive-explanation-finding-new-axes-that-capture-the-most-variance" class="level3">
<h3 class="anchored" data-anchor-id="intuitive-explanation-finding-new-axes-that-capture-the-most-variance">Intuitive Explanation: Finding New Axes That Capture the Most Variance</h3>
<p>Imagine you’re at a crowded concert, trying to take a photo of the stage. You could stand directly in front (one perspective), but if you move to the side at an angle, you might get a much better view that captures more of the action. PCA does something similarit rotates your data to find the best “viewing angles” that reveal the most variation.</p>
<p>More precisely, PCA finds new coordinate axes (called <strong>principal components</strong>) such that:</p>
<ol type="1">
<li>The <strong>first principal component</strong> points in the direction where the data varies the most</li>
<li>The <strong>second principal component</strong> points in the direction of the next-most variation, perpendicular to the first</li>
<li>The <strong>third principal component</strong> points in the direction of the next-most variation, perpendicular to both the first and second</li>
<li>And so on…</li>
</ol>
<p>The beauty of this approach is that the first few principal components often capture most of the variation in your data. If 5 components explain 90% of the variance in your original 50 features, you can use just those 5 components instead of all 50 featureswith minimal information loss.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>The Apartment-Finding Analogy
</div>
</div>
<div class="callout-body-container callout-body">
<p>Imagine you’re helping friends find apartments and tracking many features: rent, square footage, distance from downtown, number of rooms, building age, floor number, etc.</p>
<p>You might notice that many features are correlated:</p>
<ul>
<li>Larger apartments tend to have more rooms</li>
<li>Newer buildings tend to be farther from downtown</li>
<li>Higher floors tend to cost more</li>
</ul>
<p>PCA would identify underlying factors like:</p>
<ul>
<li><strong>Component 1</strong>: “Size and space” (combining square footage, rooms, closet space)</li>
<li><strong>Component 2</strong>: “Location and age” (combining distance, building age, neighborhood)</li>
<li><strong>Component 3</strong>: “Luxury and amenities” (combining floor number, finishes, building features)</li>
</ul>
<p>Instead of tracking 10 correlated features, you now have 3 uncorrelated components that capture the essential variation in apartments.</p>
</div>
</div>
</section>
<section id="visual-intuition-projecting-data-onto-a-new-coordinate-system" class="level3">
<h3 class="anchored" data-anchor-id="visual-intuition-projecting-data-onto-a-new-coordinate-system">Visual Intuition: Projecting Data onto a New Coordinate System</h3>
<p>Let’s build intuition with a simple 2D example. Imagine you have height and weight measurements for 100 people. When you plot this data, you’ll see a diagonal cloudtaller people tend to weigh more.</p>
<p>The original axes are “height” and “weight”, but PCA finds new axes:</p>
<ul>
<li><strong>PC1</strong> (First Principal Component) points along the main diagonal of the cloud, capturing the combined “size” of a person</li>
<li><strong>PC2</strong> (Second Principal Component) points perpendicular to PC1, capturing whether someone is “heavier for their height” or “lighter for their height”</li>
</ul>
<div id="cell-fig-pca-2d-intuition" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="fig-pca-2d-intuition" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-2d-intuition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="32-dimension-reduction_files/figure-html/fig-pca-2d-intuition-output-1.png" width="902" height="422" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-2d-intuition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32.2: PCA finds new coordinate axes. Left: Original height/weight data with PC1 (red) pointing along maximum variance and PC2 (orange) perpendicular to it. Right: Same data rotated to align with principal components, making it easy to see that PC1 captures ‘overall size’ while PC2 captures ‘body type’.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-pca-2d-intuition" class="quarto-xref">Figure&nbsp;<span>32.2</span></a> illustrates the core idea behind PCA. In the <strong>left panel</strong>, you see the original data plotted using height and weight as axes. The blue points form a diagonal cloud because these variables are correlated—taller people generally weigh more. The red arrow (PC1) points along the direction of maximum variance, capturing the main trend: overall size. The orange arrow (PC2) points perpendicular to PC1, capturing the secondary pattern: whether someone is stocky or slim for their height.</p>
<p>In the <strong>right panel</strong>, we’ve rotated the same data so that the principal components become our new axes. Notice how the data now aligns with the horizontal axis (PC1)—this is the direction where the data spreads out the most. The vertical spread (PC2) is much smaller. The text box shows that PC1 explains about 85-86% of the variance in the data, while PC2 explains only about 14%. This is the essence of PCA: most of the information is concentrated in the first few components.</p>
<p>Now imagine you only kept PC1 and discarded PC2. You’d lose the ability to distinguish whether someone is stocky versus slim, but you’d preserve the main patternoverall size. This is dimension reduction: trading some detail for simplicity.</p>
<p>In higher dimensions, the same idea applies. With 50 features, PCA finds 50 principal components. But typically, the first 10-20 components capture 80-95% of the variance, so you can safely discard the rest.</p>
</section>
<section id="core-terms-components-loadings-explained-variance" class="level3">
<h3 class="anchored" data-anchor-id="core-terms-components-loadings-explained-variance">Core Terms: Components, Loadings, Explained Variance</h3>
<p>Before we dive into how PCA works, let’s define the key terms you’ll encounter:</p>
<p><strong>Principal Components</strong> (PCs): The new features created by PCA. Each component is a linear combination (weighted sum) of the original features. PCs are ordered by importance: PC1 captures the most variance, PC2 captures the second-most, etc.</p>
<p><strong>Loadings</strong>: The weights that define how original features combine to create each principal component. If PC1 has high loadings on “square footage” and “number of rooms,” these features strongly influence that component. Loadings help you interpret what each component represents.</p>
<p><strong>Explained Variance</strong>: The amount of information (variance) captured by each principal component, usually expressed as a percentage. If PC1 explains 40% of variance and PC2 explains 25%, together they capture 65% of the total variation in your data.</p>
<p><strong>Cumulative Explained Variance</strong>: The sum of explained variance as you add more components. This tells you how much total information you’ve retained. For example, “5 components explain 85% of cumulative variance” means those 5 components capture 85% of the information in your original data.</p>
<p><strong>Eigenvectors and Eigenvalues</strong>: The mathematical machinery behind PCA. Eigenvectors define the direction of each principal component, while eigenvalues indicate the amount of variance along that direction. You don’t need to calculate these by hand (scikit-learn does it for you), but they’re what makes PCA work mathematically.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Don’t Worry About the Math (Yet)
</div>
</div>
<div class="callout-body-container callout-body">
<p>PCA involves linear algebra concepts like eigenvectors and covariance matrices. While understanding the math deepens your intuition, you can effectively apply PCA by understanding the concepts and interpreting the results. Focus first on the “what” and “why,” then explore the “how” if you’re curious about the mathematical foundations.</p>
</div>
</div>
</section>
</section>
<section id="how-pca-works-step-by-step" class="level2" data-number="32.4">
<h2 data-number="32.4" class="anchored" data-anchor-id="how-pca-works-step-by-step"><span class="header-section-number">32.4</span> How PCA Works (Step-by-Step)</h2>
<p>Understanding the PCA algorithm helps you apply it more effectively, even though scikit-learn handles the implementation. Let’s walk through the five key steps using a simple, concrete example.</p>
<p>We’ll use a small dataset of 6 students with 4 features: hours studied per week, practice problems completed, class attendance (%), and average quiz score:</p>
<div id="246ff866" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Example data generation</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a simple student study habits dataset</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Student'</span>: [<span class="st">'Alice'</span>, <span class="st">'Bob'</span>, <span class="st">'Carol'</span>, <span class="st">'David'</span>, <span class="st">'Emma'</span>, <span class="st">'Frank'</span>],</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Hours_Studied'</span>: [<span class="dv">10</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">12</span>, <span class="dv">6</span>, <span class="dv">9</span>],</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Practice_Problems'</span>: [<span class="dv">50</span>, <span class="dv">20</span>, <span class="dv">40</span>, <span class="dv">60</span>, <span class="dv">25</span>, <span class="dv">45</span>],</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Attendance_Pct'</span>: [<span class="dv">95</span>, <span class="dv">70</span>, <span class="dv">85</span>, <span class="dv">98</span>, <span class="dv">75</span>, <span class="dv">90</span>],</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Quiz_Score'</span>: [<span class="dv">88</span>, <span class="dv">65</span>, <span class="dv">78</span>, <span class="dv">92</span>, <span class="dv">68</span>, <span class="dv">82</span>]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Student</th>
<th data-quarto-table-cell-role="th">Hours_Studied</th>
<th data-quarto-table-cell-role="th">Practice_Problems</th>
<th data-quarto-table-cell-role="th">Attendance_Pct</th>
<th data-quarto-table-cell-role="th">Quiz_Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>Alice</td>
<td>10</td>
<td>50</td>
<td>95</td>
<td>88</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>Bob</td>
<td>5</td>
<td>20</td>
<td>70</td>
<td>65</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>Carol</td>
<td>8</td>
<td>40</td>
<td>85</td>
<td>78</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>David</td>
<td>12</td>
<td>60</td>
<td>98</td>
<td>92</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>Emma</td>
<td>6</td>
<td>25</td>
<td>75</td>
<td>68</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>This small dataset makes it easy to see what’s happening at each step. Notice that the features are correlated—students who study more tend to complete more problems and score higher on quizzes.</p>
<section id="step-1-standardize-the-data" class="level3">
<h3 class="anchored" data-anchor-id="step-1-standardize-the-data">Step 1: Standardize the Data</h3>
<p>PCA is sensitive to the scale of your features. If one feature ranges from 0-1000 while another ranges from 0-1, the larger-scale feature will dominate the principal components simply because it has larger valuesnot because it’s actually more important.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Recall…
</div>
</div>
<div class="callout-body-container callout-body">
<p>from our feature engineering chapter, <strong>standardization</strong> transforms each feature to have mean=0 and standard deviation=1.</p>
</div>
</div>
<p>Let’s standardize our student data:</p>
<div id="81f2b939" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract features for PCA (exclude student names)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">'Hours_Studied'</span>, <span class="st">'Practice_Problems'</span>, <span class="st">'Attendance_Pct'</span>, <span class="st">'Quiz_Score'</span>]].values</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> [<span class="st">'Hours_Studied'</span>, <span class="st">'Practice_Problems'</span>, <span class="st">'Attendance_Pct'</span>, <span class="st">'Quiz_Score'</span>]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize the features</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Show Standardized data</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    np.<span class="bu">round</span>(X_scaled[:<span class="dv">5</span>], <span class="dv">2</span>),</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>feature_names,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span>df[<span class="st">'Student'</span>][:<span class="dv">5</span>]</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Hours_Studied</th>
<th data-quarto-table-cell-role="th">Practice_Problems</th>
<th data-quarto-table-cell-role="th">Attendance_Pct</th>
<th data-quarto-table-cell-role="th">Quiz_Score</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">Student</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">Alice</th>
<td>0.71</td>
<td>0.72</td>
<td>0.94</td>
<td>0.93</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">Bob</th>
<td>-1.41</td>
<td>-1.44</td>
<td>-1.53</td>
<td>-1.41</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">Carol</th>
<td>-0.14</td>
<td>0.00</td>
<td>-0.05</td>
<td>-0.08</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">David</th>
<td>1.56</td>
<td>1.44</td>
<td>1.23</td>
<td>1.34</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">Emma</th>
<td>-0.99</td>
<td>-1.08</td>
<td>-1.04</td>
<td>-1.10</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>After standardization:</p>
<ul>
<li>A value of 0 means “average”</li>
<li>A value of 1 means “one standard deviation above average”</li>
<li>A value of -2 means “two standard deviations below average”</li>
</ul>
<p>Notice how Alice’s 10 hours studied becomes approximately 0.71 (above average), Bob’s 5 hours became -1.41 (more than one standard deviation below average), and David’s 12 hours becomes approximately 1.56 (more than one standard deviation above average). This puts all features on a comparable scale before PCA analyzes them.</p>
<div class="callout callout-style-simple callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Always Standardize Before PCA
</div>
</div>
<div class="callout-body-container callout-body">
<p>Unless you have a specific reason not to (rare cases where the original scale matters), you should always standardize features before PCA. This is one of the most common mistakes in applying PCAforgetting to scale your data first.</p>
<p>The exception is when all features are already on the same scale (e.g., all are percentages from 0-100 or all are already standardized).</p>
</div>
</div>
</section>
<section id="step-2-compute-the-covariance-matrix" class="level3">
<h3 class="anchored" data-anchor-id="step-2-compute-the-covariance-matrix">Step 2: Compute the Covariance Matrix</h3>
<p>The <strong>covariance matrix</strong> captures how features vary together:</p>
<ul>
<li><strong>Diagonal elements</strong> show how much each feature varies on its own (variance)</li>
<li><strong>Off-diagonal elements</strong> show how pairs of features vary together (covariance)</li>
<li>Positive covariance means features tend to increase together</li>
<li>Negative covariance means when one increases, the other tends to decrease</li>
<li>Near-zero covariance means features vary independently</li>
</ul>
<p>The covariance matrix is the input for finding principal components. PCA essentially analyzes this matrix to find directions where the data varies most.</p>
<p>You don’t typically compute this manually—scikit-learn’s PCA does it internally—but let’s peek at it for our student data to build intuition:</p>
<div id="19144471" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute covariance matrix (what PCA does behind the scenes)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>cov_matrix <span class="op">=</span> np.cov(X_scaled.T)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Covariance Matrix (4x4 for our 4 features):"</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">round</span>(cov_matrix, <span class="dv">2</span>))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a labeled version for easier interpretation</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>cov_df <span class="op">=</span> pd.DataFrame(cov_matrix,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>                      columns<span class="op">=</span>feature_names,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>                      index<span class="op">=</span>feature_names)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Labeled Covariance Matrix:"</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>cov_df.<span class="bu">round</span>(<span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Covariance Matrix (4x4 for our 4 features):
[[1.2  1.2  1.18 1.19]
 [1.2  1.2  1.19 1.19]
 [1.18 1.19 1.2  1.2 ]
 [1.19 1.19 1.2  1.2 ]]

Labeled Covariance Matrix:</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Hours_Studied</th>
<th data-quarto-table-cell-role="th">Practice_Problems</th>
<th data-quarto-table-cell-role="th">Attendance_Pct</th>
<th data-quarto-table-cell-role="th">Quiz_Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">Hours_Studied</th>
<td>1.20</td>
<td>1.20</td>
<td>1.18</td>
<td>1.19</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">Practice_Problems</th>
<td>1.20</td>
<td>1.20</td>
<td>1.19</td>
<td>1.19</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">Attendance_Pct</th>
<td>1.18</td>
<td>1.19</td>
<td>1.20</td>
<td>1.20</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">Quiz_Score</th>
<td>1.19</td>
<td>1.19</td>
<td>1.20</td>
<td>1.20</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Notice several important patterns in this matrix:</p>
<p><strong>Diagonal elements</strong> (all ≈ 1.20): These represent the variance of each feature after standardization. They’re all similar, confirming that standardization worked properly.</p>
<p><strong>Off-diagonal elements</strong> (all ≈ 1.18-1.20): These show the covariances between pairs of features. The consistently high positive values tell us that all four variables are strongly correlated—students who study more hours also tend to complete more practice problems, attend more classes, and score higher on quizzes. This strong correlation is exactly why PCA will be effective: it can capture this shared variation in fewer components.</p>
<p>For example, the covariance between Hours_Studied and Practice_Problems is 1.20, indicating these variables increase together almost perfectly. Similarly, Hours_Studied and Quiz_Score have a covariance of 1.19, showing students who study more tend to score higher.</p>
<p>This high correlation across all features suggests that PC1 will likely capture the majority of variance by representing an “overall academic engagement/performance” dimension.</p>
<p>For a dataset with 50 features, this would produce a 50×50 covariance matrix showing all pairwise relationships.</p>
</section>
<section id="step-3-find-eigenvectors-and-eigenvalues" class="level3">
<h3 class="anchored" data-anchor-id="step-3-find-eigenvectors-and-eigenvalues">Step 3: Find Eigenvectors and Eigenvalues</h3>
<p>This is where the magic (and the linear algebra) happens. PCA finds the <strong>eigenvectors</strong> and <strong>eigenvalues</strong> of the covariance matrix:</p>
<ul>
<li><strong>Eigenvectors</strong> define the directions of the principal components (the new axes)</li>
<li><strong>Eigenvalues</strong> indicate how much variance exists along each direction (the importance of each component)</li>
</ul>
<p>The eigenvector with the largest eigenvalue becomes PC1 (first principal component), the eigenvector with the second-largest eigenvalue becomes PC2, and so on.</p>
<p>To make this concrete, let’s revisit our height/weight example and visualize how eigenvectors and eigenvalues work together:</p>
<div id="cell-fig-eigenvector-eigenvalue" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div id="fig-eigenvector-eigenvalue" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-eigenvector-eigenvalue-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="32-dimension-reduction_files/figure-html/fig-eigenvector-eigenvalue-output-1.png" width="681" height="662" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-eigenvector-eigenvalue-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32.3: Eigenvectors show the directions of maximum variance (arrows), while eigenvalues determine their length (importance). PC1 (red) has a much larger eigenvalue than PC2 (orange), shown by the dramatically different arrow lengths. This demonstrates that PC1 captures far more variance in the height/weight relationship.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In this visualization:</p>
<ul>
<li><strong>Eigenvector 1 (red arrow direction)</strong>: Points along the main diagonal where height and weight increase together—this is the direction of maximum variance in the data</li>
<li><strong>Eigenvalue 1 (red arrow length)</strong>: Large value (≈1.71) shown by the long arrow, indicating PC1 captures most of the variance (~86% of total variation)</li>
<li><strong>Eigenvector 2 (orange arrow direction)</strong>: Points perpendicular to PC1, capturing the secondary pattern (stocky vs.&nbsp;slim for a given overall size)</li>
<li><strong>Eigenvalue 2 (orange arrow length)</strong>: Small value (≈0.29) shown by the short arrow, indicating PC2 captures much less variance (~14% of total variation)</li>
</ul>
<p>The dramatic difference in arrow lengths visually demonstrates why PC1 is far more important than PC2. If we kept only PC1 (the long red arrow) and discarded PC2 (the short orange arrow), we’d retain about 86% of the information while reducing from 2 dimensions to 1.</p>
<p>Fortunately, you don’t need to compute eigendecomposition manually. Scikit-learn does this when you call <code>fit()</code>:</p>
<div id="7e3ff6f4" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit PCA on our student data (keep all 4 components initially)</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>pca.fit(X_scaled)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA has computed eigenvectors and eigenvalues internally</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  display: none;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  display: block;
  width: 100%;
  overflow: visible;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}

.estimator-table summary {
    padding: .5rem;
    font-family: monospace;
    cursor: pointer;
}

.estimator-table details[open] {
    padding-left: 0.1rem;
    padding-right: 0.1rem;
    padding-bottom: 0.3rem;
}

.estimator-table .parameters-table {
    margin-left: auto !important;
    margin-right: auto !important;
}

.estimator-table .parameters-table tr:nth-child(odd) {
    background-color: #fff;
}

.estimator-table .parameters-table tr:nth-child(even) {
    background-color: #f6f6f6;
}

.estimator-table .parameters-table tr:hover {
    background-color: #e0e0e0;
}

.estimator-table table td {
    border: 1px solid rgba(106, 105, 104, 0.232);
}

.user-set td {
    color:rgb(255, 94, 0);
    text-align: left;
}

.user-set td.value pre {
    color:rgb(255, 94, 0) !important;
    background-color: transparent !important;
}

.default td {
    color: black;
    text-align: left;
}

.user-set td i,
.default td i {
    color: black;
}

.copy-paste-icon {
    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);
    background-repeat: no-repeat;
    background-size: 14px 14px;
    background-position: 0;
    display: inline-block;
    width: 14px;
    height: 14px;
    cursor: pointer;
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>PCA(n_components=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>PCA</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.7/modules/generated/sklearn.decomposition.PCA.html">?<span>Documentation for PCA</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted" data-param-prefix="">
        <div class="estimator-table">
            <details>
                <summary>Parameters</summary>
                
<table class="parameters-table caption-top table table-sm table-striped small">
<tbody>
<tr class="user-set odd">
<td><em></em></td>
<td class="param">n_components&nbsp;</td>
<td class="value">4</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">copy&nbsp;</td>
<td class="value">True</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">whiten&nbsp;</td>
<td class="value">False</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">svd_solver&nbsp;</td>
<td class="value">'auto'</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">tol&nbsp;</td>
<td class="value">0.0</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">iterated_power&nbsp;</td>
<td class="value">'auto'</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">n_oversamples&nbsp;</td>
<td class="value">10</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">power_iteration_normalizer&nbsp;</td>
<td class="value">'auto'</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">random_state&nbsp;</td>
<td class="value">None</td>
</tr>
</tbody>
</table>

            </details>
        </div>
    </div></div></div></div></div><script>function copyToClipboard(text, element) {
    // Get the parameter prefix from the closest toggleable content
    const toggleableContent = element.closest('.sk-toggleable__content');
    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';
    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;

    const originalStyle = element.style;
    const computedStyle = window.getComputedStyle(element);
    const originalWidth = computedStyle.width;
    const originalHTML = element.innerHTML.replace('Copied!', '');

    navigator.clipboard.writeText(fullParamName)
        .then(() => {
            element.style.width = originalWidth;
            element.style.color = 'green';
            element.innerHTML = "Copied!";

            setTimeout(() => {
                element.innerHTML = originalHTML;
                element.style = originalStyle;
            }, 2000);
        })
        .catch(err => {
            console.error('Failed to copy:', err);
            element.style.color = 'red';
            element.innerHTML = "Failed!";
            setTimeout(() => {
                element.innerHTML = originalHTML;
                element.style = originalStyle;
            }, 2000);
        });
    return false;
}

document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {
    const toggleableContent = element.closest('.sk-toggleable__content');
    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';
    const paramName = element.parentElement.nextElementSibling.textContent.trim();
    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;

    element.setAttribute('title', fullParamName);
});
</script>
</div>
</div>
<p>Let’s check out the eigenvector and eigenvalue results.</p>
<p><strong>Eigenvectors (Feature Weights):</strong></p>
<p>First, let’s get the eigenvectors for the 4 PCs we created:</p>
<div id="56d9b452" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>components_df <span class="op">=</span> pd.DataFrame(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    pca.components_,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>feature_names,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span>[<span class="st">'PC1'</span>, <span class="st">'PC2'</span>, <span class="st">'PC3'</span>, <span class="st">'PC4'</span>]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>components_df.<span class="bu">round</span>(<span class="dv">3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Hours_Studied</th>
<th data-quarto-table-cell-role="th">Practice_Problems</th>
<th data-quarto-table-cell-role="th">Attendance_Pct</th>
<th data-quarto-table-cell-role="th">Quiz_Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">PC1</th>
<td>0.499</td>
<td>0.501</td>
<td>0.499</td>
<td>0.501</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">PC2</th>
<td>0.666</td>
<td>0.250</td>
<td>-0.651</td>
<td>-0.264</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">PC3</th>
<td>0.247</td>
<td>-0.648</td>
<td>-0.268</td>
<td>0.669</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">PC4</th>
<td>-0.497</td>
<td>0.517</td>
<td>-0.504</td>
<td>0.481</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Look at <strong>PC1</strong> (first row): All weights are approximately equal and positive (≈0.50 for all features). This tells us that PC1 is a balanced combination of all four variables—it represents overall academic engagement. Students who score high on PC1 study more, complete more problems, attend more classes, AND score higher on quizzes. This is the “general performance” dimension.</p>
<p>Now look at <strong>PC2</strong> (second row): The weights have different signs! Hours_Studied (+0.666) and Practice_Problems (+0.250) are positive, while Attendance_Pct (−0.651) and Quiz_Score (−0.264) are negative. This creates a contrast: PC2 captures students who study/practice a lot but don’t attend class as much or score as highly. This might represent “independent learners” who study on their own rather than attending lectures.</p>
<p><strong>PC3</strong> and <strong>PC4</strong> have more complex patterns with mixed signs, but we likely won’t use them since they capture very little variance (see eigenvalues below).</p>
<p><strong>Eigenvalues (Importance):</strong></p>
<p>Now let’s check out the eigenvalues. Recall that eigenvalues represent the <strong><em>explained variance.</em></strong></p>
<div id="8b8f767e" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Raw Eigenvalues"</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pca.explained_variance_)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Nicely formatted Eigenvalues:"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PC1: </span><span class="sc">{</span>pca<span class="sc">.</span>explained_variance_[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PC2: </span><span class="sc">{</span>pca<span class="sc">.</span>explained_variance_[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PC3: </span><span class="sc">{</span>pca<span class="sc">.</span>explained_variance_[<span class="dv">2</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PC4: </span><span class="sc">{</span>pca<span class="sc">.</span>explained_variance_[<span class="dv">3</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Raw Eigenvalues
[4.77040922e+00 2.31866701e-02 4.05194419e-03 2.35216276e-03]

Nicely formatted Eigenvalues:
PC1: 4.770
PC2: 0.023
PC3: 0.004
PC4: 0.002</code></pre>
</div>
</div>
<p>Notice the dramatic drop: <strong>PC1 has an eigenvalue of 4.770</strong>, while <strong>PC2 is only 0.023</strong>—that’s a 200× difference! This means PC1 captures almost all the meaningful variation in student performance. PC3 (0.004) and PC4 (0.002) are even smaller and represent mostly noise.</p>
<p>This pattern tells us we can safely reduce from 4 features down to just 1 or 2 principal components without losing much information.</p>
</section>
<section id="step-4-choose-principal-components" class="level3">
<h3 class="anchored" data-anchor-id="step-4-choose-principal-components">Step 4: Choose Principal Components</h3>
<p>Not all principal components are equally useful. You need to decide how many to keep.</p>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>We can extract the percent variance explained by each PCA using <code>pca.explained_variance_ratio_</code>.</p>
</div>
</div>
</div>
<p>Let’s look at the explained variance to help us decide:</p>
<div id="7658f135" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Look at variance explained by each component</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Variance Explained by Each PC:"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, var <span class="kw">in</span> <span class="bu">enumerate</span>(pca.explained_variance_ratio_, <span class="dv">1</span>):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  PC</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>var<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Cumulative Variance Explained:"</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>cumsum <span class="op">=</span> np.cumsum(pca.explained_variance_ratio_)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, var <span class="kw">in</span> <span class="bu">enumerate</span>(cumsum, <span class="dv">1</span>):</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  First </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> PC(s): </span><span class="sc">{</span>var<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Variance Explained by Each PC:
  PC1: 99.4%
  PC2: 0.5%
  PC3: 0.1%
  PC4: 0.0%

Cumulative Variance Explained:
  First 1 PC(s): 99.4%
  First 2 PC(s): 99.9%
  First 3 PC(s): 100.0%
  First 4 PC(s): 100.0%</code></pre>
</div>
</div>
<p>For our student data, you might find that PC1 and PC2 together explain 85-90% of the variance. This means we could reduce from 4 features to 2 principal components while retaining most of the information.</p>
<p>Common approaches for choosing how many components to keep:</p>
<p><strong>Option 1: Keep components explaining X% of variance</strong> (e.g., 90%)</p>
<div id="aa99b347" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Keep enough components to explain 90% of variance</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>pca_90 <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="fl">0.90</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>pca_90.fit(X_scaled)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">To explain 90% of variance, keep </span><span class="sc">{</span>pca_90<span class="sc">.</span>n_components_<span class="sc">}</span><span class="ss"> components"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
To explain 90% of variance, keep 1 components</code></pre>
</div>
</div>
<p><strong>Option 2: Keep a specific number of components</strong> (e.g., reduce to 2)</p>
<div id="f5668d84" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Keep exactly 2 components for visualization</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>pca_2 <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>pca_2.fit(X_scaled)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Keeping 2 components explains </span><span class="sc">{</span>pca_2<span class="sc">.</span>explained_variance_ratio_<span class="sc">.</span><span class="bu">sum</span>()<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">% of variance"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Keeping 2 components explains 99.9% of variance</code></pre>
</div>
</div>
<p><strong>Option 3: Use the “elbow method”</strong> (plot variance explained and look for the elbow)</p>
<p>We’ll explore these decision strategies in detail in Section 31.6.</p>
</section>
<section id="step-5-transform-the-data" class="level3">
<h3 class="anchored" data-anchor-id="step-5-transform-the-data">Step 5: Transform the Data</h3>
<p>Finally, project your original data onto the principal components. This creates your new, lower-dimensional representation:</p>
<div id="ae3788bc" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform the original data into principal component space (using 2 components)</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>X_pca <span class="op">=</span> pca_2.transform(X_scaled)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Original Data Shape: "</span>, X_scaled.shape, <span class="st">"(6 students × 4 features)"</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Transformed Data Shape:"</span>, X_pca.shape, <span class="st">"(6 students × 2 components)"</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the transformed data for our students</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>pca_df <span class="op">=</span> pd.DataFrame(X_pca,</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>                      columns<span class="op">=</span>[<span class="st">'PC1'</span>, <span class="st">'PC2'</span>],</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>                      index<span class="op">=</span>df[<span class="st">'Student'</span>])</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Student Data in PC Space:"</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pca_df.<span class="bu">round</span>(<span class="dv">2</span>))</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare: Alice vs Bob</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Alice's PC1 score: </span><span class="sc">{</span>X_pca[<span class="dv">0</span>, <span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss"> (overall academic performance)"</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bob's PC1 score: </span><span class="sc">{</span>X_pca[<span class="dv">1</span>, <span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss"> (overall academic performance)"</span>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"→ Alice scores much higher on PC1, reflecting stronger overall performance"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Original Data Shape:  (6, 4) (6 students × 4 features)
Transformed Data Shape: (6, 2) (6 students × 2 components)

Student Data in PC Space:
          PC1   PC2
Student            
Alice    1.65 -0.21
Bob     -2.90  0.07
Carol   -0.14 -0.04
David    2.79  0.24
Emma    -2.11  0.04
Frank    0.71 -0.10

Alice's PC1 score: 1.65 (overall academic performance)
Bob's PC1 score: -2.90 (overall academic performance)
→ Alice scores much higher on PC1, reflecting stronger overall performance</code></pre>
</div>
</div>
<p>The resulting <code>X_pca</code> contains your observations represented in the new coordinate system defined by the principal components. Each column is a principal component, and each row is a student. Notice how we’ve reduced from 4 features down to just 2 components, yet we still capture most of the variance in student performance.</p>
<p>You can now use <code>X_pca</code> instead of the original <code>X</code> for visualization, clustering, or as input to other machine learning models.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Fitting vs.&nbsp;Transforming
</div>
</div>
<div class="callout-body-container callout-body">
<p>Just like StandardScaler, PCA follows the fit-transform pattern:</p>
<ul>
<li><strong>fit()</strong>: Learns the principal components from the training data</li>
<li><strong>transform()</strong>: Projects data onto those components</li>
<li><strong>fit_transform()</strong>: Combines both steps for convenience on training data</li>
</ul>
<p>This pattern becomes important when you use train/test splits. You fit PCA on training data only, then transform both training and test data using the same components (just like you do with StandardScaler).</p>
</div>
</div>
</section>
</section>
<section id="case-study-pca-for-breast-cancer-classification" class="level2" data-number="32.5">
<h2 data-number="32.5" class="anchored" data-anchor-id="case-study-pca-for-breast-cancer-classification"><span class="header-section-number">32.5</span> Case Study: PCA for Breast Cancer Classification</h2>
<p>Now that you understand how PCA works, let’s apply it to a real-world problem: using PCA to improve breast cancer classification. The breast cancer dataset contains 30 features computed from cell nucleus measurements. We’ll use PCA to reduce these 30 features while retaining the information needed to distinguish malignant from benign tumors.</p>
<section id="step-1-load-and-standardize-the-data" class="level3">
<h3 class="anchored" data-anchor-id="step-1-load-and-standardize-the-data">Step 1: Load and Standardize the Data</h3>
<div id="4862f656" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the breast cancer dataset</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> load_breast_cancer()</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data.data</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data.target</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into train/test</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize the features</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> scaler.fit_transform(X_train)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>X_test_scaled <span class="op">=</span> scaler.transform(X_test)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original training data shape: </span><span class="sc">{</span>X_train_scaled<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of features: </span><span class="sc">{</span>X_train_scaled<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Original training data shape: (398, 30)
Number of features: 30</code></pre>
</div>
</div>
<p>We start with 30 features—too many to visualize easily and potentially redundant.</p>
</section>
<section id="step-2-fit-pca" class="level3">
<h3 class="anchored" data-anchor-id="step-2-fit-pca">Step 2: Fit PCA</h3>
<div id="233925b1" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit PCA with all components to see the full picture</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA()</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>pca.fit(X_train_scaled)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of components: </span><span class="sc">{</span>pca<span class="sc">.</span>n_components_<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of components: 30</code></pre>
</div>
</div>
<p>PCA has now computed all 30 principal components, ordered by how much variance they explain.</p>
</section>
<section id="step-3-choose-number-of-components-scree-plot-elbow-method" class="level3">
<h3 class="anchored" data-anchor-id="step-3-choose-number-of-components-scree-plot-elbow-method">Step 3: Choose Number of Components (Scree Plot &amp; Elbow Method)</h3>
<p>How many components should we keep? Let’s use a <strong>scree plot</strong> to visualize the explained variance:</p>
<div id="cell-fig-scree-plot" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create scree plot</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Left plot: Individual variance explained</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>ax1.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(pca.explained_variance_ratio_) <span class="op">+</span> <span class="dv">1</span>),</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>         pca.explained_variance_ratio_, <span class="st">'bo-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, markersize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Principal Component'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Variance Explained'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Scree Plot: Variance per Component'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Right plot: Cumulative variance explained</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>cumsum <span class="op">=</span> np.cumsum(pca.explained_variance_ratio_)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>ax2.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(cumsum) <span class="op">+</span> <span class="dv">1</span>), cumsum, <span class="st">'ro-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, markersize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>ax2.axhline(y<span class="op">=</span><span class="fl">0.95</span>, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'95% threshold'</span>)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>ax2.axhline(y<span class="op">=</span><span class="fl">0.90</span>, color<span class="op">=</span><span class="st">'orange'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'90% threshold'</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Number of Components'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Cumulative Variance Explained'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Cumulative Variance Explained'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Find number of components for 95% variance</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>n_components_95 <span class="op">=</span> np.argmax(cumsum <span class="op">&gt;=</span> <span class="fl">0.95</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Components needed for 95% variance: </span><span class="sc">{</span>n_components_95<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div id="fig-scree-plot" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scree-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="32-dimension-reduction_files/figure-html/fig-scree-plot-output-1.png" width="1142" height="374" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scree-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32.4: Scree plot showing explained variance by each principal component. The ‘elbow’ around PC5-PC7 suggests keeping 5-7 components captures most meaningful variance while discarding noise.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Components needed for 95% variance: 10</code></pre>
</div>
</div>
<p>The <strong>scree plot</strong> (left) shows individual variance explained by each PC. The <strong>elbow</strong> occurs around PC6-7, where the curve starts to flatten significantly. The <strong>cumulative plot</strong> (right) shows that we reach approximately 95% of variance (green line) with about 7 components instead of all 30.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>The Elbow Method
</div>
</div>
<div class="callout-body-container callout-body">
<p>Look for the “elbow” in the scree plot—the point where adding more components yields diminishing returns. Before the elbow, each component adds substantial information. After the elbow, components mostly capture noise.</p>
</div>
</div>
</section>
<section id="step-4-examine-principal-components" class="level3">
<h3 class="anchored" data-anchor-id="step-4-examine-principal-components">Step 4: Examine Principal Components</h3>
<p>Based on the scree plot, let’s keep 7 components (capturing ~95% variance) and examine what they represent:</p>
<div id="6baeb5c3" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Refit with 7 components</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>pca_7 <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>pca_7.fit(X_train_scaled)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Show variance explained by each component</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Variance explained by each component:"</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, var <span class="kw">in</span> <span class="bu">enumerate</span>(pca_7.explained_variance_ratio_, <span class="dv">1</span>):</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  PC</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>var<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%"</span>)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Show loadings for all 7 PCs</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>loadings_df <span class="op">=</span> pd.DataFrame(</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    pca_7.components_.T,</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>[<span class="ss">f'PC</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">8</span>)],</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span>data.feature_names</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Display top contributing features for each PC</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 3 features (by absolute loading) for each PC:"</span>)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pc <span class="kw">in</span> loadings_df.columns:</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    top_features <span class="op">=</span> loadings_df[pc].<span class="bu">abs</span>().nlargest(<span class="dv">3</span>)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>pc<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> feature, loading <span class="kw">in</span> top_features.items():</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>        actual_loading <span class="op">=</span> loadings_df.loc[feature, pc]</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>actual_loading<span class="sc">:+.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Variance explained by each component:
  PC1: 45.2%
  PC2: 19.6%
  PC3: 8.9%
  PC4: 6.6%
  PC5: 5.5%
  PC6: 4.0%
  PC7: 2.1%

Top 3 features (by absolute loading) for each PC:

PC1:
  mean concave points: +0.259
  mean concavity: +0.254
  worst concave points: +0.250

PC2:
  mean fractal dimension: +0.361
  fractal dimension error: +0.279
  worst fractal dimension: +0.269

PC3:
  texture error: +0.430
  worst smoothness: -0.293
  smoothness error: +0.258

PC4:
  worst texture: +0.635
  mean texture: +0.581
  texture error: +0.307

PC5:
  mean smoothness: +0.353
  concavity error: -0.329
  symmetry error: +0.314

PC6:
  worst symmetry: +0.491
  symmetry error: +0.436
  smoothness error: -0.418

PC7:
  worst fractal dimension: +0.376
  concave points error: -0.363
  mean fractal dimension: +0.333</code></pre>
</div>
</div>
<p>The loadings show which original features contribute most to each PC. High absolute values indicate strong influence. Notice how different PCs capture different combinations of features—PC1 might represent overall tumor characteristics, while PC2-7 capture more specific patterns.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Interpreting the Principal Components
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Based on the top feature loadings, here’s what each PC might represent:</p>
<p><strong>PC1: Overall Tumor Shape/Geometry</strong> (44.3% variance)</p>
<ul>
<li>Dominated by concave points, concavity—features related to tumor boundary irregularity</li>
<li>All positive loadings suggest PC1 represents overall severity of shape distortion</li>
<li>High PC1 = more irregular, complex tumor boundaries</li>
</ul>
<p><strong>PC2: Fractal Complexity</strong> (19.0% variance)</p>
<ul>
<li>Focused entirely on fractal dimension measures</li>
<li>Captures the complexity/roughness of the tumor boundary at different scales</li>
<li>High PC2 = higher fractal complexity (more irregular at fine scales)</li>
</ul>
<p><strong>PC3: Texture vs.&nbsp;Smoothness Contrast</strong> (9.4% variance)</p>
<ul>
<li>Texture error (+) contrasted with worst smoothness (−)</li>
<li>Captures tumors with variable texture but inconsistent smoothness</li>
<li>Shows the tradeoff between surface roughness variability and uniformity</li>
</ul>
<p><strong>PC4: Texture Characteristics</strong> (6.6% variance)</p>
<ul>
<li>All three top features relate to texture measurements</li>
<li>Represents the overall texture properties across different aggregations (worst, mean, error)</li>
<li>High PC4 = high texture values throughout</li>
</ul>
<p><strong>PC5: Smoothness &amp; Shape Balance</strong> (5.4% variance)</p>
<ul>
<li>Mean smoothness (+) vs.&nbsp;concavity error (−)</li>
<li>Captures the balance between surface smoothness and concavity consistency</li>
<li>Interesting contrast: smooth surfaces with variable concavity</li>
</ul>
<p><strong>PC6: Symmetry Features</strong> (5.1% variance)</p>
<ul>
<li>Dominated by symmetry measurements (worst, error)</li>
<li>Contrasts with smoothness error (−)</li>
<li>Represents asymmetry patterns in tumor shape</li>
</ul>
<p><strong>PC7: Fractal &amp; Concavity Mix</strong> (5.0% variance)</p>
<ul>
<li>Fractal dimension (+) vs.&nbsp;concave points error (−)</li>
<li>Captures complex relationship between fractal complexity and boundary irregularity</li>
<li>More nuanced pattern harder to interpret (typical for later PCs)</li>
</ul>
<p><strong>Key Insight:</strong> Notice how PC1 captures the most variance and has the clearest interpretation (overall tumor irregularity), while later PCs capture increasingly specific and nuanced patterns. This is typical in PCA—early components are often more interpretable and represent broad patterns, while later components capture subtle, specific variations.</p>
</div>
</div>
</div>
</section>
<section id="step-5-transform-the-data-1" class="level3">
<h3 class="anchored" data-anchor-id="step-5-transform-the-data-1">Step 5: Transform the Data</h3>
<p>Now that we’ve fit PCA and chosen 7 components, let’s transform our data:</p>
<div id="00cb9ca3" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform both training and test data</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>X_train_pca <span class="op">=</span> pca_7.transform(X_train_scaled)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>X_test_pca <span class="op">=</span> pca_7.transform(X_test_scaled)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original shape: </span><span class="sc">{</span>X_train_scaled<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Transformed shape: </span><span class="sc">{</span>X_train_pca<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Dimensionality reduced from 30 to 7 features (77% reduction)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Original shape: (398, 30)
Transformed shape: (398, 7)

Dimensionality reduced from 30 to 7 features (77% reduction)</code></pre>
</div>
</div>
<p>The transformed data now has only 7 features (the principal components) instead of 30, while retaining ~95% of the original information.</p>
</section>
<section id="step-6-using-pca-features-in-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="step-6-using-pca-features-in-machine-learning">Step 6: Using PCA Features in Machine Learning</h3>
<p>A major benefit of PCA is using the reduced features as input to machine learning models. Let’s compare a logistic regression model using all 30 original features versus one using just our 7 principal components:</p>
<div id="0e8c5131" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Model with all 30 original features</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>model_original <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">5000</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>model_original.fit(X_train_scaled, y_train)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>y_pred_original <span class="op">=</span> model_original.predict(X_test_scaled)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>acc_original <span class="op">=</span> accuracy_score(y_test, y_pred_original)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Model with 7 PCA components</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>model_pca <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">5000</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>model_pca.fit(X_train_pca, y_train)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>y_pred_pca <span class="op">=</span> model_pca.predict(X_test_pca)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>acc_pca <span class="op">=</span> accuracy_score(y_test, y_pred_pca)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy with 30 original features: </span><span class="sc">{</span>acc_original<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy with 7 PCA components:    </span><span class="sc">{</span>acc_pca<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy difference:                </span><span class="sc">{</span><span class="bu">abs</span>(acc_original <span class="op">-</span> acc_pca)<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy with 30 original features: 0.988
Accuracy with 7 PCA components:    0.959
Accuracy difference:                0.029</code></pre>
</div>
</div>
<p><strong>Why Accept Slightly Lower Accuracy?</strong></p>
<p>Even if the PCA model has slightly lower accuracy, using 7 components instead of 30 features offers several advantages:</p>
<ol type="1">
<li><strong>Faster training and prediction</strong> - Fewer features mean less computation, especially important for large datasets or complex models</li>
<li><strong>Reduced overfitting risk</strong> - Fewer features = simpler model = less likely to memorize noise in training data</li>
<li><strong>Eliminates multicollinearity</strong> - PCs are uncorrelated by design, avoiding problems when features are highly correlated</li>
<li><strong>Easier deployment</strong> - Smaller models are easier to deploy, maintain, and explain to stakeholders</li>
<li><strong>Better generalization</strong> - By removing noise (the 5% of variance we discarded), the model may perform better on new data</li>
</ol>
<p>In practice, reducing dimensionality from 30 to 7 while maintaining 95%+ of the predictive accuracy is an excellent tradeoff. The slight performance cost is often worth the gains in speed, simplicity, and robustness. And this tradeoff only becomes more obvious as our features grow to 100s if not 1000s of features!</p>
</section>
</section>
<section id="summary" class="level2" data-number="32.6">
<h2 data-number="32.6" class="anchored" data-anchor-id="summary"><span class="header-section-number">32.6</span> Summary</h2>
<p>Principal Component Analysis (PCA) is a powerful dimension reduction technique that transforms correlated features into a smaller set of uncorrelated principal components ordered by importance. PCA works by finding eigenvectors (directions of maximum variance) and eigenvalues (magnitude of variance along those directions) from the covariance matrix of standardized data. The key workflow involves: (1) standardizing features, (2) fitting PCA to compute components, (3) using scree plots and the elbow method to choose how many components to keep, (4) examining loadings to interpret what each component represents, (5) transforming data to the lower-dimensional space, and (6) using the transformed features in machine learning models. Through our breast cancer case study, we saw how PCA reduced 30 features to just 7 components while retaining ~95% of the variance.</p>
<p>While PCA offers substantial benefits—faster training, reduced overfitting, elimination of multicollinearity, and simpler models—it comes with important tradeoffs. The transformed features lose interpretability since principal components are mathematical combinations of original features rather than meaningful business concepts. PCA also assumes linear relationships, is sensitive to outliers, and requires careful standardization. Despite these limitations, PCA remains invaluable for high-dimensional data exploration, visualization, and as a preprocessing step for machine learning. The key is understanding when the benefits of dimensionality reduction outweigh the costs of reduced interpretability and potential information loss.</p>
</section>
<section id="end-of-chapter-exercises" class="level2" data-number="32.7">
<h2 data-number="32.7" class="anchored" data-anchor-id="end-of-chapter-exercises"><span class="header-section-number">32.7</span> End of Chapter Exercises</h2>
<p>These exercises give you hands-on practice with the complete PCA workflow using the Ames housing dataset. You’ll apply everything from this chapter—standardization, component selection, interpretation, and using PCA features in machine learning models.</p>
<p>The <code>ames_clean.csv</code> file can be imported from <a href="https://raw.githubusercontent.com/bradleyboehmke/uc-bana-4080/refs/heads/main/data/ames_clean.csv">here</a>. Key things to remember:</p>
<ul>
<li><code>SalePrice</code> is your target variable—don’t include it when fitting PCA</li>
<li>Some features have missing values—you may need to handle these before PCA</li>
<li>After one-hot encoding, you’ll have 200+ features—perfect for demonstrating PCA’s power!</li>
<li>Remember to fit PCA only on training data, then transform both train and test</li>
</ul>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Exercise 1: PCA with Numeric Features Only
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The Ames housing dataset contains many numeric features describing house characteristics. Let’s use PCA to reduce these features and predict sale prices.</p>
<p><strong>Your Tasks:</strong></p>
<ol type="1">
<li><strong>Load and Prepare the Data:</strong>
<ul>
<li>Load <code>data/ames_clean.csv</code></li>
<li>Select only numeric features (excluding <code>SalePrice</code>, which is your target)</li>
<li>Hint: Use <code>df.select_dtypes(include=[np.number])</code> to get numeric columns</li>
<li>Split into train/test sets (80/20 split)</li>
<li>Standardize the features using <code>StandardScaler</code></li>
</ul></li>
<li><strong>Fit PCA and Choose Components:</strong>
<ul>
<li>Fit PCA with all components on the training data</li>
<li>Create a scree plot showing variance explained by each component</li>
<li>Use the elbow method to determine how many components to keep</li>
<li>What percentage of variance do your chosen components capture?</li>
<li>Refit PCA with your chosen number of components</li>
</ul></li>
<li><strong>Interpret Principal Components:</strong>
<ul>
<li>Examine the loadings for your top 2-3 principal components</li>
<li>For each component, identify the top 5 features (by absolute loading value)</li>
<li>Based on these loadings, give each component a descriptive name</li>
<li>Example: If PC1 loads heavily on <code>GrLivArea</code>, <code>TotalBsmtSF</code>, <code>GarageArea</code>, you might call it “Overall Size”</li>
</ul></li>
<li><strong>Build and Compare Models:</strong>
<ul>
<li>Transform both train and test data using your fitted PCA</li>
<li>Train a linear regression model using the PCA-transformed features</li>
<li>Train another linear regression model using all original numeric features</li>
<li>Compare R² scores on the test set</li>
<li>How much did you reduce dimensionality? Was there a significant performance tradeoff?</li>
</ul></li>
<li><strong>Reflection:</strong>
<ul>
<li>Which approach would you recommend for this problem: original features or PCA features? Why?</li>
<li>Consider: interpretability, model complexity, performance, and ease of deployment</li>
</ul></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Exercise 2: PCA with Categorical Features Included
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Many important features in the Ames dataset are categorical (like <code>Neighborhood</code>, <code>HouseStyle</code>, etc.). Let’s incorporate these into our PCA analysis.</p>
<p><strong>Your Tasks:</strong></p>
<ol type="1">
<li><strong>Encode Categorical Variables:</strong>
<ul>
<li>Starting with the same Ames dataset, identify categorical columns</li>
<li>Use one-hot encoding to convert categorical features to numeric</li>
<li>Hint: <code>pd.get_dummies()</code> or <code>OneHotEncoder</code> from sklearn</li>
<li>Combine with your numeric features</li>
<li>How many total features do you now have after encoding?</li>
</ul></li>
<li><strong>Apply the Complete PCA Workflow:</strong>
<ul>
<li>Standardize all features (both original numeric and encoded categorical)</li>
<li>Fit PCA on the training data</li>
<li>Create a scree plot—how does it compare to Exercise 1?</li>
<li>Choose the number of components using the elbow method</li>
<li>What percentage of variance is captured?</li>
</ul></li>
<li><strong>Examine Component Loadings:</strong>
<ul>
<li>With more features, the loadings might be more complex</li>
<li>Look at the top 10 features (by absolute loading) for PC1 and PC2</li>
<li>Do categorical features appear in the top contributors?</li>
<li>Which neighborhoods or house styles load most heavily on PC1?</li>
</ul></li>
<li><strong>Model Comparison:</strong>
<ul>
<li>Transform train and test data using PCA</li>
<li>Train a linear regression model with PCA features</li>
<li>Compare to:
<ol type="a">
<li>Model with only numeric features (from Exercise 1)</li>
<li>Model with all features (numeric + one-hot encoded categorical)</li>
</ol></li>
<li>Which approach gives the best R²?</li>
<li>Which approach has the most features vs.&nbsp;fewest features?</li>
</ul></li>
<li><strong>Final Analysis:</strong>
<ul>
<li>Create a summary table comparing all three approaches:
<ul>
<li>Number of features</li>
<li>Test R² score</li>
<li>Training time (if noticeably different)</li>
</ul></li>
<li>Discuss the tradeoffs: When would PCA be most beneficial for the Ames dataset?</li>
<li>Would you recommend using PCA for this specific problem? Why or why not?</li>
</ul></li>
</ol>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./31-clustering.html" class="pagination-link" aria-label="Unsupervised Learning and Clustering">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Unsupervised Learning and Clustering</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./33-modern-ml-algorithms.html" class="pagination-link" aria-label="Beyond the Basics: Modern Machine Learning Algorithms">
        <span class="nav-page-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Beyond the Basics: Modern Machine Learning Algorithms</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/bradleyboehmke/uc-bana-4080/edit/main/32-dimension-reduction.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/bradleyboehmke/uc-bana-4080/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>