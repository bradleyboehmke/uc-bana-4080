# Welcome {.unnumbered}

Welcome to **BANA 4080: Introduction to Data Mining with Python**. This course provides an immersive, hands-on introduction to the tools and techniques used in modern data science. You’ll learn how to explore, analyze, and model data using Python and gain practical experience through labs, projects, and real-world datasets. 

Along the way you will develop core skills in data wrangling, exploratory data analysis, data visualization, and even key machine learning techniques such as supervised, unsupervised, and deep learning model.  We'll even take a quick detour into generative AI and large language models (LLMs).  Throughout this process we'll use real-world data and experiential learning to guide your learning.

By the end of the course, students will be able to:

- Use Python to read, clean, transform, and visualize data
- Apply exploratory and statistical techniques to understand datasets
- Build and evaluate basic machine learning models
- Gain exposure to cutting-edge techniques like deep learning and LLMs

## Who should read this? {-}

This book is designed for upper-level undergraduate students who may have little to no prior programming experience but are eager to explore the world of data science using Python. It’s also an ideal resource for early-career professionals or students in analytics, business, or quantitative fields who are looking to upskill—whether by learning Python for the first time or by building a deeper understanding of how to explore, visualize, and model data. The content is structured to be accessible and hands-on, guiding readers step-by-step through the core tools and techniques used in modern data-driven problem solving.

## How this book is structured {-}

This book is broken into **13 modules**, each aligned with a week of instruction in the BANA 4080 course. Every module introduces key concepts or techniques in data science, combining concise explanations with interactive, hands-on code examples. Whether you're reading independently or following along with the course, the modular structure makes it easy to work through the content at your own pace, week by week.


| **Module & Topics**                             | **Summary of Concepts Covered**                                            |
|-------------------------------------------------|-----------------------------------------------------------------------------|
| 1. Fundamentals I                               | Course overview, coding environment setup, Python basics                   |
| 2. Fundamentals II                              | Using Jupyter notebooks, data structures, Python libraries                 |
| 3. Pandas & Data Wrangling I                    | Importing data, DataFrame fundamentals, subsetting, cleaning, filtering     |
| 4. Pandas & Data Wrangling II                   | Working with datetime, text data, aggregating, merging, and joining data   |
| 5. Data Visualization                           | Creating plots using matplotlib and seaborn, exploratory data analysis     |
| 6. Writing Efficient Python Code                | Control flow, defining functions, loops, list comprehensions               |
| 7. Introduction to Machine Learning             | Overview of ML, features/labels, train/test split, key considerations      |
| 8. Linear Regression & Correlation             | Correlation analysis, simple & multiple linear regression, model evaluation |
| 9. Logistic Regression & Classification        | Binary classification, logistic regression, classification metrics         |
| 10. Tree-Based Models & Feature Importance      | Decision trees, random forests, ensemble methods, model interpretability   |
| 11. Model Optimization & Validation             | Feature engineering, cross-validation, hyperparameter tuning, best practices |
| 12. Unsupervised Learning                       | Clustering (K-Means), dimensionality reduction (PCA), pattern discovery     |
| 13. Modern ML & Your Learning Roadmap           | Gradient boosting, neural networks, deep learning, career paths, next steps |

## Conventions used in this book {-}

The following typographical conventions are used in this book:

* ___strong italic___: indicates new terms,
* __bold__: indicates package & file names,
* `inline code`: monospaced highlighted text indicates functions or other commands that could be typed literally by the user,
* code chunk: indicates commands or other text that could be typed literally by the user

```{python, first-code-chunk, collapse=TRUE}
1 + 2
```

In addition to the general text used throughout, you will notice the following code chunks with images:

::: {.callout-tip}
Signifies a tip or suggestion
:::

::: {.callout-note}
Signifies a general note
:::

::: {.callout-warning}
Signifies a warning or caution
:::

## Software used throughout this book {-}

This book is built around an open-source Python-based data science ecosystem. While the list of tools evolves with the field, the examples and exercises in this book are designed to work with **Python 3.x**, currently using...

```{python, python-version}
#| code-fold: true

# Display the Python version
import sys
print("Python version:", sys.version.split()[0])
```

...and are executed within *Jupyter Notebooks*, which provide an interactive, beginner-friendly environment for writing and running code.

Throughout the modules, we use foundational Python libraries such as:

* **pandas** and **numpy** for data wrangling and numerical computing,
* **matplotlib** and **seaborn** for data visualization,
* **scikit-learn** and **keras** for machine learning and deep learning, and
* **openai** and **transformers** for generative AI and large language model exploration.

Each module explicitly introduces the relevant software and libraries, explains how and why they are used, and provides reproducible code so that readers can follow along and generate similar results in their own environment.

### Running the Companion Notebooks {-}

Most chapters include companion Jupyter notebooks with hands-on examples and exercises. These notebooks can be executed in two ways:

1. **Google Colab** (Recommended): Click the "Open in Colab" badge at the top of any notebook to run it directly in your browser. All required dependencies are pre-installed in the Colab environment, so you can start coding immediately without any setup.

2. **Local Execution**: If you prefer to run notebooks on your local machine, you'll need Python 3.12 or greater. Install all dependencies used throughout this book by downloading this [`requirements.txt`](https://github.com/bradleyboehmke/uc-bana-4080/blob/main/requirements.txt) file and running:
   ```bash
   pip install -r requirements.txt
   ```
   This will include all necessary packages for data wrangling, visualization, machine learning, and deep learning examples covered in this book.

If you're new to Python or need help setting up your development environment, Chapter 2 walks you through getting started with Google Colab, Anaconda, or Visual Studio Code.
