{
  "hash": "d8fa7f9172d06e2c1a7dffb3a49e90c2",
  "result": {
    "engine": "jupyter",
    "markdown": "# Beyond the Basics: Modern Machine Learning Algorithms {#sec-modern-ml}\n\nYou've come a long way in this course. You started as brand new Python programmers, learning the fundamentals of variables, data structures, and control flow. You then mastered data wrangling with pandasâ€”importing, cleaning, transforming, and aggregating real-world datasets. From there, you progressed into machine learning: linear and logistic regression, powerful ensemble methods like random forests, cross-validation and hyperparameter tuning, and unsupervised learning with clustering and PCA. You've built a comprehensive data science toolkit that will serve you well throughout your career.\n\nBut if you've been following recent developments in AI (chatbots that write essays, models that generate realistic images, systems that beat world champions at complex games), you might be wondering: how do those fit into what we've learned? What's the relationship between the random forests you've been building and the neural networks powering modern AI systems?\n\nThis chapter bridges that gap. We'll explore **modern machine learning algorithms** that extend beyond the classical methods you've mastered. You'll see how gradient boosting machines push ensemble methods even further, how neural networks create the foundation for deep learning and generative AI, and how these advanced techniques build directly on concepts you already understand.\n\n::: {.callout-important}\n## A Note on This Chapter's Purpose\n\nThis chapter is about **exposure, not mastery**. We're not expecting you to become experts in gradient boosting or neural networks in one reading. Instead, we want you to:\n\n1. Understand how modern ML techniques connect to what you've learned\n2. Recognize when these advanced methods might be appropriate\n3. Know enough to have informed conversations about them\n4. Feel prepared to dive deeper when your career requires it\n\nThink of this as a guided tour of the machine learning landscape beyond our course boundaries, a preview of what awaits as you continue your data science journey.\n:::\n\n## Learning Objectives\n\nBy the end of this chapter, you will be able to:\n\n- Explain how gradient boosting machines (GBMs) differ from random forests and when each excels\n- Understand the basic architecture of neural networks and how they learn through backpropagation\n- Recognize the relationship between neural networks, deep learning, and generative AI\n- Identify appropriate use cases for advanced ML algorithms vs. classical methods\n- Implement basic gradient boosting models using XGBoost or LightGBM\n- Understand the computational and interpretability tradeoffs of complex models\n- Navigate the landscape of modern ML: when to use classical methods vs. advanced techniques\n- Know where to learn more about each algorithm family\n\n::: {.callout-note}\n## Follow along in Colab\n\nAs you read through this chapter, we encourage you to follow along using the [companion notebook](https://github.com/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/33_modern_ml_algorithms.ipynb) in Google Colab (or another editor of your choice). This interactive notebook lets you run all the code examples covered here and experiment with your own ideas.\n\nðŸ‘‰ Open the [Modern ML Algorithms Notebook in Colab](https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/33_modern_ml_algorithms.ipynb).\n:::\n\n## Recapping What You've Learned\n\nBefore we explore new territories, let's appreciate how far you've come. You've mastered a comprehensive toolkit of machine learning algorithms:\n\n### Supervised Learning for Regression\n\n- **Linear Regression**: Your first predictive model, which finds the best linear relationship between features and a continuous target. You learned about coefficients, R-squared, and how to interpret feature importance through regression weights.\n\n- **Decision Trees & Random Forests (Regression)**: Tree-based methods that partition the feature space into regions with similar outcomes. Random forests extend this by building many trees and averaging their predictions, dramatically improving performance and reducing overfitting.\n\n### Supervised Learning for Classification\n\n- **Logistic Regression**: Despite its name, this is a classification algorithm that predicts probabilities using the sigmoid function. You learned about log-odds, decision boundaries, and how to evaluate models using ROC curves and confusion matrices.\n\n- **Decision Trees & Random Forests (Classification)**: Tree-based classifiers that split data based on feature thresholds to create pure leaf nodes. Random forests again improve single trees by building diverse ensembles and voting on predictions.\n\n### Unsupervised Learning\n\n- **Clustering (K-Means)**: Automatically groups similar observations together without labeled data. You learned about centroids, the elbow method, and how to interpret cluster profiles for business applications.\n\n- **Dimension Reduction (PCA)**: Transforms many correlated features into fewer uncorrelated principal components, preserving the most important variation while reducing complexity.\n\n### Model Optimization & Evaluation\n\n- **Cross-Validation**: Robust methods for estimating model performance using k-fold and stratified splits, ensuring your models generalize to new data.\n\n- **Hyperparameter Tuning**: Systematic approaches (grid search, random search) to find optimal model configurations.\n\n- **Feature Engineering**: Creating new features, handling missing data, encoding categorical variables, and scaling features to improve model performance.\n\n### Why This Foundation Matters\n\nThese techniques aren't just academic exercises. They're the workhorse algorithms of professional data science:\n\n- **Linear and logistic regression** remain first choices when you need interpretability and fast predictions\n- **Random forests** consistently win Kaggle competitions and power production systems across industries\n- **Cross-validation and proper evaluation** separate amateur models from professional-grade solutions\n- **Feature engineering** often matters more than algorithm choice for real-world performance\n\nThe algorithms we'll explore in this chapter build directly on these foundations. Gradient boosting extends tree-based methods. Neural networks generalize logistic regression's architecture. Deep learning requires the same cross-validation and evaluation rigor you've already learned.\n\n::: {.callout-tip}\n## The 80/20 Rule of Machine Learning\n\nIn professional practice, classical methods (linear models, logistic regression, random forests, gradient boosting) solve 80% of business problems. The advanced techniques we'll discuss in this chapter are powerful, but they're not always necessary. As you learn about these modern methods, remember that simpler often wins when it comes to interpretability, maintenance, and deployment complexity.\n:::\n\nNow that we've recapped the solid foundation you've built, let's explore what comes next. We'll start with algorithms that extend what you already know (like taking random forests to the next level with gradient boosting), then venture into neural networks and deep learning, and finally connect these concepts to the generative AI systems making headlines today. Each section builds on your existing knowledge, showing you how modern ML techniques are evolutionary steps rather than completely foreign concepts.\n\n## Gradient Boosting Machines: The Next Level of Ensemble Learning\n\nYou've already mastered random forests, which build many trees in parallel and average their predictions. **Gradient boosting machines (GBMs)** take a fundamentally different approach: they build trees **sequentially**, where each new tree tries to correct the mistakes of the previous trees.\n\n::: {.callout-tip}\n## Think of it like this:\n\n**Random Forests**: Imagine asking 100 experts a question independently, then averaging their answers. Each expert thinks separately, and diversity comes from giving them slightly different information.\n\n**Gradient Boosting**: Imagine asking one expert, then asking a second expert to focus specifically on cases where the first expert was wrong, then asking a third expert to focus on remaining mistakes, and so on. Each expert specializes in fixing previous errors.\n:::\n\n### How Gradient Boosting Works\n\nThe algorithm follows this sequential process:\n\n1. **Start with a simple prediction** (often just the mean for regression, or majority class for classification)\n2. **Calculate the errors** (residuals) from this initial prediction\n3. **Build a tree** to predict those errors\n4. **Add this tree's predictions** to the existing predictions (with a small learning rate to prevent overfitting)\n5. **Repeat steps 2-4** for a specified number of trees (typically 100-1000)\n\nEach new tree \"boosts\" the model by focusing on the observations that previous trees struggled with. The \"gradient\" part comes from using gradient descent to minimize the loss function, similar to how neural networks learn (which we'll discuss shortly).\n\nLet's visualize this process with a simple example:\n\n::: {#12685931 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](33-modern-ml-algorithms_files/figure-html/cell-2-output-1.png){width=854 height=757}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nMean Absolute Error at each step:\n  Initial (mean):  5.09\n  After Tree 1:    3.56\n  After Tree 2:    2.49\n  After Tree 3:    1.75\n\nTotal error reduction: 3.34 (65.7% improvement)\n```\n:::\n:::\n\n\nThe visualization shows how each tree focuses on the remaining errors (shown as red lines between predictions and true values). Notice how the predictions get progressively closer to the true values with each additional tree. This is the essence of boosting: each tree learns from the mistakes of the ensemble so far.\n\n::: {.callout-tip}\n## Key Differences: Random Forests vs. Gradient Boosting\n\n| **Aspect** | **Random Forests** | **Gradient Boosting** |\n|---|---|---|\n| **Tree Building** | Parallel (all at once) | Sequential (one at a time) |\n| **Learning Focus** | Each tree learns from random subset | Each tree learns from previous mistakes |\n| **Tree Depth** | Typically deep trees (fully grown) | Typically shallow trees (weak learners) |\n| **Prediction** | Average/vote across all trees | Weighted sum of all trees |\n| **Training Speed** | Fast (parallelizable) | Slower (sequential) |\n| **Overfitting Risk** | Lower (averaging reduces variance) | Higher (needs careful tuning) |\n| **Performance** | Excellent | Often slightly better with tuning |\n| **Interpretability** | Moderate (feature importance) | Moderate (feature importance) |\n:::\n\n### Popular GBM Implementations\n\nWhile the core concept of gradient boosting has existed since the 1990s, recent implementations have made it incredibly powerful and efficient:\n\n**[XGBoost (Extreme Gradient Boosting)](https://xgboost.readthedocs.io/)**\n\n- Most popular implementation, winner of many Kaggle competitions\n- Highly optimized for speed and memory efficiency\n- Includes regularization to prevent overfitting\n- Handles missing values automatically\n- Great documentation and community support\n- [Original Paper (Chen & Guestrin, 2016)](https://arxiv.org/abs/1603.02754) | [GitHub](https://github.com/dmlc/xgboost)\n\n**[LightGBM (Light Gradient Boosting Machine)](https://lightgbm.readthedocs.io/)**\n\n- Developed by Microsoft, even faster than XGBoost for large datasets\n- Uses \"histogram-based\" algorithm for efficient splitting\n- Excellent for datasets with millions of observations\n- Lower memory usage than XGBoost\n- [Original Paper (Ke et al., 2017)](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree) | [GitHub](https://github.com/microsoft/LightGBM)\n\n**[CatBoost (Categorical Boosting)](https://catboost.ai/)**\n\n- Developed by Yandex, handles categorical features natively\n- Less hyperparameter tuning required (good defaults)\n- Particularly strong with text and categorical data\n- Built-in handling of categorical feature combinations\n- [Original Paper (Prokhorenkova et al., 2018)](https://arxiv.org/abs/1706.09516) | [GitHub](https://github.com/catboost/catboost)\n\n### A Quick XGBoost Example\n\nLet's see how gradient boosting compares to random forests using the California Housing dataset, a regression problem with 20,000+ observations where XGBoost's sequential learning really shines:\n\n::: {#18bb2c86 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load California housing data (20,640 observations)\nhousing = fetch_california_housing()\nX = pd.DataFrame(housing.data, columns=housing.feature_names)\ny = housing.target  # Median house value in $100,000s\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Random Forest (default parameters)\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nrf_model.fit(X_train, y_train)\nrf_pred = rf_model.predict(X_test)\n\n# XGBoost (default parameters)\nxgb_default = XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nxgb_default.fit(X_train, y_train)\nxgb_default_pred = xgb_default.predict(X_test)\n\n# XGBoost (tuned parameters - showing its potential)\nxgb_tuned = XGBRegressor(\n    n_estimators=300,      # More trees\n    learning_rate=0.05,    # Slower learning for better accuracy\n    max_depth=5,           # Moderate tree depth\n    subsample=0.8,         # Row sampling (like RF)\n    colsample_bytree=0.8,  # Column sampling (like RF)\n    random_state=42,\n    n_jobs=-1\n)\nxgb_tuned.fit(X_train, y_train)\nxgb_tuned_pred = xgb_tuned.predict(X_test)\n\n# Compare performance\nprint(\"Model Comparison (Lower RMSE is better):\")\nprint(\"=\" * 50)\nprint(f\"Random Forest RMSE:  {np.sqrt(mean_squared_error(y_test, rf_pred)):.4f}\")\nprint(f\"Random Forest RÂ²:    {r2_score(y_test, rf_pred):.4f}\")\nprint()\nprint(f\"XGBoost (default) RMSE: {np.sqrt(mean_squared_error(y_test, xgb_default_pred)):.4f}\")\nprint(f\"XGBoost (default) RÂ²:   {r2_score(y_test, xgb_default_pred):.4f}\")\nprint()\nprint(f\"XGBoost (tuned) RMSE:   {np.sqrt(mean_squared_error(y_test, xgb_tuned_pred)):.4f}\")\nprint(f\"XGBoost (tuned) RÂ²:     {r2_score(y_test, xgb_tuned_pred):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel Comparison (Lower RMSE is better):\n==================================================\nRandom Forest RMSE:  0.5057\nRandom Forest RÂ²:    0.8049\n\nXGBoost (default) RMSE: 0.4718\nXGBoost (default) RÂ²:   0.8301\n\nXGBoost (tuned) RMSE:   0.4685\nXGBoost (tuned) RÂ²:     0.8325\n```\n:::\n:::\n\n\nThis example demonstrates two key points:\n\n1. **Even with default parameters**, XGBoost often matches or slightly outperforms Random Forests\n2. **With proper tuning**, XGBoost can achieve significantly better performance, but this requires understanding its hyperparameters and investing time in optimization\n\nThe improvement might seem modest (a few percentage points in RÂ² or reduction in RMSE), but in real-world applications, these gains can translate to substantial business value. However, this comes at the cost of increased complexity and tuning effort.\n\n::: {.callout-important}\n## When to Use Gradient Boosting vs. Random Forests\n\n**Use Random Forests when:**\n\n- You want a reliable, easy-to-tune model (great defaults)\n- You need fast training time\n- You're working with a relatively small number of features\n- You want to minimize overfitting risk\n\n**Use Gradient Boosting when:**\n\n- You're willing to invest time in hyperparameter tuning\n- You need the absolute best predictive performance\n- You're entering a Kaggle competition or similar challenge\n- You have sufficient data to validate proper tuning (avoid overfitting)\n\n**Start with Random Forests**, get a baseline, then experiment with gradient boosting if you need incremental performance gains.\n:::\n\n### Key Hyperparameters for GBMs\n\nIf you decide to explore gradient boosting further, these are the most important hyperparameters to understand:\n\n- **`n_estimators`**: Number of trees to build (typical range: 100-1000)\n- **`learning_rate`** (or `eta`): How much each tree contributes (typical range: 0.01-0.3, lower is often better)\n- **`max_depth`**: Maximum depth of each tree (typical range: 3-10, shallower than random forests)\n- **`subsample`**: Fraction of observations to use for each tree (typical: 0.8, adds randomness like random forests)\n- **`colsample_bytree`**: Fraction of features to use for each tree (typical: 0.8, also adds randomness)\n\nThe combination of `n_estimators` and `learning_rate` is particularly important: more trees with a smaller learning rate often works best, but takes longer to train.\n\n## Neural Networks and Deep Learning: The Foundation of Modern AI\n\n### What Are Neural Networks?\n\nIf you've heard about AI breakthroughs (ChatGPT writing essays, DALL-E generating images, AlphaGo beating world champions), you've heard about **neural networks**. Despite the buzz, neural networks are conceptually related to something you already know well: logistic regression.\n\nRemember logistic regression? You learned that it:\n\n1. Takes input features (like age, income, credit score)\n2. Multiplies each feature by a weight (coefficient)\n3. Sums these weighted features\n4. Passes the sum through a sigmoid activation function (the S-shaped logistic function)\n5. Outputs a probability between 0 and 1\n\nA neural network does exactly the same thing, but **many times over**, in layers. Instead of one sigmoid transformation, you might have dozens or hundreds of interconnected transformations, allowing the model to learn complex, non-linear patterns.\n\n### Architecture Basics: Layers of Transformations\n\nA simple neural network has three types of layers:\n\n1. **Input Layer**: One node for each feature in your dataset (just like the features in logistic regression).\n\n2. **Hidden Layers**: One or more layers where the magic happens. Each node in a hidden layer:\n    - Receives inputs from the previous layer\n    - Multiplies each input by a weight\n    - Adds a bias term\n    - Passes the result through an activation function (like sigmoid, ReLU, or tanh)\n    - Sends the output to the next layer\n\n3. **Output Layer**: Produces the final prediction. For classification, this might use a sigmoid (binary) or softmax (multiclass) activation. For regression, it might use a linear activation.\n\nHere's a simple visualization:\n\n```{mermaid}\n%%{init: {'theme':'base', 'themeVariables': { 'fontSize':'16px'}}}%%\ngraph LR\n    subgraph Input[\"&nbsp;&nbsp;&nbsp;&nbsp;Input Layer&nbsp;&nbsp;&nbsp;&nbsp;<br/>(3 features)&nbsp;&nbsp;&nbsp;&nbsp;\"]\n        X1((X1))\n        X2((X2))\n        X3((X3))\n    end\n\n    subgraph Hidden1[\"&nbsp;&nbsp;&nbsp;&nbsp;Hidden Layer 1&nbsp;&nbsp;&nbsp;&nbsp;<br/>(4 nodes)&nbsp;&nbsp;&nbsp;&nbsp;\"]\n        H1((H1))\n        H2((H2))\n        H3((H3))\n        H4((H4))\n    end\n\n    subgraph Hidden2[\"&nbsp;&nbsp;&nbsp;&nbsp;Hidden Layer 2&nbsp;&nbsp;&nbsp;&nbsp;<br/>(3 nodes)&nbsp;&nbsp;&nbsp;&nbsp;\"]\n        H5((H5))\n        H6((H6))\n        H7((H7))\n    end\n\n    subgraph Hidden3[\"&nbsp;&nbsp;&nbsp;&nbsp;Hidden Layer 3&nbsp;&nbsp;&nbsp;&nbsp;<br/>(3 nodes)&nbsp;&nbsp;&nbsp;&nbsp;\"]\n        H8((H8))\n        H9((H9))\n        H10((H10))\n    end\n\n    subgraph Output[\"&nbsp;&nbsp;&nbsp;&nbsp;Output Layer&nbsp;&nbsp;&nbsp;&nbsp;<br/>(1 prediction)&nbsp;&nbsp;&nbsp;&nbsp;\"]\n        Y((Y))\n    end\n\n    X1 -.-> H1\n    X1 -.-> H2\n    X1 -.-> H3\n    X1 -.-> H4\n\n    X2 -.-> H1\n    X2 -.-> H2\n    X2 -.-> H3\n    X2 -.-> H4\n\n    X3 -.-> H1\n    X3 -.-> H2\n    X3 -.-> H3\n    X3 -.-> H4\n\n    H1 -.-> H5\n    H1 -.-> H6\n    H1 -.-> H7\n    H2 -.-> H5\n    H2 -.-> H6\n    H2 -.-> H7\n    H3 -.-> H5\n    H3 -.-> H6\n    H3 -.-> H7\n    H4 -.-> H5\n    H4 -.-> H6\n    H4 -.-> H7\n\n    H5 -.-> H8\n    H5 -.-> H9\n    H5 -.-> H10\n    H6 -.-> H8\n    H6 -.-> H9\n    H6 -.-> H10\n    H7 -.-> H8\n    H7 -.-> H9\n    H7 -.-> H10\n\n    H8 ==> Y\n    H9 ==> Y\n    H10 ==> Y\n\n    style Input fill:#e1f5ff\n    style Hidden1 fill:#fff4e1\n    style Hidden2 fill:#ffe4e1\n    style Hidden3 fill:#e8e1ff\n    style Output fill:#e8f5e9\n```\n\nEach arrow represents a weight (parameter) that the model learns during training. This network has:\n\n- 3 Ã— 4 = 12 weights from input to hidden layer 1\n- 4 Ã— 3 = 12 weights from hidden layer 1 to hidden layer 2\n- 3 Ã— 3 = 9 weights from hidden layer 2 to hidden layer 3\n- 3 Ã— 1 = 3 weights from hidden layer 3 to output\n- Plus bias terms for each of the 11 nodes (4 + 3 + 3 + 1)\n- Total: about 47 learnable parameters\n\nThis might seem like a lot compared to logistic regression's 4 parameters (3 coefficients + 1 intercept), but it's tiny compared to modern deep learning models that have millions or billions of parameters. The additional layers allow the network to learn increasingly complex, hierarchical representations of the data.\n\n::: {.callout-tip}\n## Why \"Deep\" Learning?\n\n**Deep learning** simply means using neural networks with many hidden layers (typically 10-1000+ layers). More layers allow the network to learn increasingly abstract representations:\n\n- **Early layers** might detect edges and textures in images\n- **Middle layers** might combine edges into shapes and patterns\n- **Late layers** might recognize high-level concepts like faces or objects\n\nThis hierarchical learning is why deep learning excels at complex tasks like image recognition, natural language processing, and speech recognition.\n:::\n\n### How Neural Networks Learn: Backpropagation\n\nNeural networks learn through a process called **backpropagation**, which works like this:\n\n1. **Forward Pass**: Input data flows through the network, producing predictions\n2. **Calculate Loss**: Compare predictions to actual values using a loss function (like mean squared error for regression or cross-entropy for classification)\n3. **Backward Pass**: Calculate how much each weight contributed to the error\n4. **Update Weights**: Adjust weights in the direction that reduces error (using gradient descent)\n5. **Repeat**: Iterate many times (epochs) over the training data\n\n::: {.callout-note}\n## What's an Epoch?\n\nAn **epoch** is one complete pass through the entire training dataset. If you have 1,000 training examples and train for 50 epochs, the network will see all 1,000 examples 50 times, updating weights after each small batch of data. More epochs generally lead to better learning (up to a point), but too many can cause overfitting.\n:::\n\nThis is conceptually similar to how we found the best coefficients for linear regression, but applied to potentially millions of parameters across many layers.\n\nHere's a visualization of the backpropagation process:\n\n```{mermaid}\n%%{init: {'theme':'base', 'themeVariables': { 'fontSize':'14px'}}}%%\ngraph LR\n    subgraph Step1[\" \"]\n        A[Input Data] -->|Forward Pass| B[Neural Network]\n        B --> C[Predictions]\n    end\n\n    subgraph Step2[\" \"]\n        C -->|Compare| D[Actual Values]\n        D --> E[Calculate Loss/Error]\n    end\n\n    subgraph Step3[\" \"]\n        E -->|Backward Pass<br/>Compute Gradients| F[How much did<br/>each weight<br/>contribute to error?]\n    end\n\n    subgraph Step4[\" \"]\n        F -->|Update Weights<br/>Gradient Descent| G[Adjust weights to<br/>reduce error]\n    end\n\n    G -->|Repeat for<br/>many epochs| A\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#ffe4e1\n    style D fill:#e8f5e9\n    style E fill:#ffebcc\n    style F fill:#e8e1ff\n    style G fill:#f0e1ff\n\n    style Step1 fill:#ffffff,stroke:#999,stroke-width:2px\n    style Step2 fill:#ffffff,stroke:#999,stroke-width:2px\n    style Step3 fill:#ffffff,stroke:#999,stroke-width:2px\n    style Step4 fill:#ffffff,stroke:#999,stroke-width:2px\n```\n\nLet's also visualize how the loss decreases during training:\n\n::: {#c1e81088 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](33-modern-ml-algorithms_files/figure-html/cell-4-output-1.png){width=855 height=470}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial Loss: 2.633\nFinal Loss: 0.258\nLoss Reduction: 2.375 (90.2% improvement)\n```\n:::\n:::\n\n\nThe first diagram shows the cycle of forward pass, error calculation, backward pass, and weight updates. The next plot demonstrates how backpropagation successfully reduces the loss over many training epochs (iterations through the data). Notice three distinct phases:\n\n1. **Rapid Learning**: Early epochs show dramatic loss reduction as the network discovers major patterns\n2. **Gradual Refinement**: Middle epochs show steady improvement with smaller weight adjustments\n3. **Convergence**: Later epochs show minimal changes as the network approaches optimal weights\n\n### A Simple Neural Network Example\n\nLet's build a simple neural network for the Iris classification problem (a classic dataset with 3 flower species):\n\n::: {.callout-note}\n## Running This Example\nDue to TensorFlow's initialization overhead, this code example is best run in the companion notebook rather than during book rendering. You can find the full working example in the [companion notebook](https://colab.research.google.com/github/bradleyboehmke/uc-bana-4080/blob/main/example-notebooks/33_modern_ml_algorithms.ipynb).\n:::\n\n::: {#42a559a8 .cell execution_count=4}\n``` {.python .cell-code}\n# Configure TensorFlow to suppress warnings and avoid GPU search\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Force CPU usage\n\nimport tensorflow as tf\ntf.get_logger().setLevel('ERROR')  # Only show errors\n\nprint(\"TF version:\", tf.__version__)\nprint(\"Devices:\", tf.config.list_physical_devices())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTF version: 2.20.0\nDevices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n```\n:::\n:::\n\n\nNow let's build a simple neural network for the Iris classification problem:\n\n::: {#c3313dc1 .cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Load data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split and scale (neural networks are sensitive to feature scale)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Build a simple neural network\nmodel = keras.Sequential([\n    layers.Dense(8, activation='relu', input_shape=(4,)),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(3, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\n# Train the model (suppress output)\nhistory = model.fit(\n    X_train_scaled, y_train,\n    epochs=50,\n    batch_size=16,\n    validation_split=0.2,\n    verbose=0\n)\n\n# Evaluate\ny_pred = model.predict(X_test_scaled, verbose=0).argmax(axis=1)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Neural Network Accuracy: {accuracy:.4f}\")\nprint(f\"Correct predictions: {(y_pred == y_test).sum()} out of {len(y_test)}\")\n```\n:::\n\n\nFor this simple dataset, the neural network will perform comparably to logistic regression or a random forest (often around 95-100% accuracy). Neural networks really shine when the relationships are more complex.\n\n::: {.callout-important}\n## When Neural Networks Excel vs. Classical ML\n\n**Neural Networks Excel When:**\n\n- You have **very large datasets** (100,000+ observations)\n- You're working with **unstructured data** (images, text, audio, video)\n- The **relationships are highly non-linear** and complex\n- You can afford the **computational cost** (GPUs, training time)\n- You don't need model **interpretability**\n\n**Classical ML (Random Forests, XGBoost) Excel When:**\n\n- You have **structured/tabular data** (typical business datasets)\n- You have **smaller to medium datasets** (< 100,000 observations)\n- You need **model interpretability** (feature importance, decision rules)\n- You want **faster training** and predictions\n- You have **limited computational resources**\n\nFor most business problems with structured data (customer records, transaction data, sensor data), classical ML methods often outperform neural networks while being easier to interpret and deploy.\n:::\n\n### The Deep Learning Revolution: What Makes It Possible\n\nDeep learning has exploded in the last decade due to three key factors:\n\n**1. Big Data**: Companies like Google, Facebook, and Amazon collect billions of labeled examples (images, text, clicks). Deep learning needs lots of data to learn effectively.\n\n**2. Computational Power**: GPUs (graphics processing units) can perform the massive matrix multiplications required by neural networks much faster than CPUs. Cloud computing makes this power accessible to everyone.\n\n**3. Algorithmic Innovations**: Techniques like dropout (prevents overfitting), batch normalization (stabilizes training), residual connections (enables very deep networks), and **attention mechanisms** (allows models to focus on relevant parts of the input) have made training large networks feasible. Attention, in particular, revolutionized how neural networks handle sequential data by allowing the model to dynamically weigh the importance of different input elementsâ€”a breakthrough that powers modern transformers.\n\n### From Neural Networks to Generative AI\n\nThe neural networks we've described so far are **discriminative models**. They learn to classify or predict based on input features. For example, given an image, a discriminative model might classify it as \"cat\" or \"dog.\" Given customer data, it might predict whether they'll make a purchase.\n\nBut recent breakthroughs involve a fundamentally different type of neural network: **generative models**. Instead of classifying or predicting, generative models create entirely new content. They can generate realistic images from text descriptions, write coherent essays, compose music, or even generate synthetic data for training other models. This shift from \"understanding and classifying\" to \"creating and generating\" represents one of the most exciting frontiers in AI.\n\nThe generative AI revolution has been driven by several innovative architectures, each with different strengths:\n\n**Generative Adversarial Networks (GANs)**: Two neural networks compete. One generates fake data (like images), while the other tries to detect fakes. This competition produces incredibly realistic generated images, videos, and more.\n\n**Transformers**: A neural network architecture (developed by Google in 2017) that revolutionized natural language processing. Transformers power:\n\n- **GPT-4** (powers ChatGPT for text generation and multimodal AI)\n- **Gemini 2.5** (powers Google's Gemini AI assistant)\n- **Sonnet 4.5** (powers Claude and Claude Code)\n- **BERT** (text understanding and search)\n- **DALL-E** (image generation from text)\n\n**Diffusion Models**: Generate images by gradually removing noise, producing stunning results (Stable Diffusion, Midjourney).\n\nThese models have billions of parameters and are trained on massive datasets (like the entire internet), requiring millions of dollars in computational resources. They represent the cutting edge of AI research and commercial applications.\n\n### Foundation Models: A Different Approach to Machine Learning\n\nThe generative AI models described above represent a fundamental shift in how we think about machine learning. These are often called **foundation models**, which are large-scale models pre-trained on vast amounts of data and can be adapted for many different tasks. They're a different beast compared to the models you've been building in this course.\n\n**The Traditional ML Workflow** you've learned involves:\n\n1. Collecting task-specific data\n2. Engineering features\n3. Training a model from scratch\n4. Evaluating and deploying that specific model\n\n**The Foundation Model Workflow** is fundamentally different:\n\n1. A large organization (OpenAI, Google, Anthropic) trains a massive model on enormous datasets (billions of dollars in compute)\n2. You access this pre-trained model through an API or download it\n3. You adapt it to your specific task without training from scratch\n4. You deploy solutions built on top of these models\n\nThis introduces the concept of **transfer learning**, which means leveraging knowledge from a model trained on one task to solve a different but related task. Instead of training GPT-4 yourself (which would cost millions), you use the already-trained model through ChatGPT's API. Instead of training an image recognition model from scratch, you might fine-tune a pre-trained vision model on your specific images.\n\n::: {.callout-tip}\n## How You'll Actually Use Foundation Models\n\nIn professional practice, you typically interact with foundation models in three ways:\n\n1. **Direct API Usage**: Call models like GPT-4, Gemini, or Claude through APIs for tasks like text generation, summarization, or question answering (no training required)\n\n2. **Fine-tuning**: Take a pre-trained model and adapt it to your specific domain by training it on your data (requires much less data and compute than training from scratch)\n\n3. **Prompt Engineering**: Carefully crafting the input (prompts) to guide the model's behavior without any training at all\n\nThe barrier to entry has shifted from \"Can I train a good model?\" to \"Can I effectively leverage these powerful pre-trained models?\"\n:::\n\nThis represents a democratization of AIâ€”you don't need millions of dollars in compute to build powerful AI applications. You just need to know how to effectively use and adapt these foundation models for your specific needs.\n\n::: {.callout-note}\n## Generative AI: Preview, Not Deep Dive\n\nWe won't dive deep into generative AI or foundation models in this course (that would require an entire course by itself). But understanding that these powerful systems are built on the same fundamental neural network principles we've discussed helps demystify them. They're not magic; they're just very large, very well-trained neural networks with clever architectures. And increasingly, they're tools you can access and use without training them yourself.\n:::\n\n## Other Advanced Methods: A Quick Tour\n\nBeyond gradient boosting and neural networks, several other algorithms are worth knowing about, even if we won't cover them in depth:\n\n### Support Vector Machines (SVMs)\n\n**What They Do**: Find the optimal boundary (hyperplane) that separates classes in high-dimensional space, maximizing the margin between classes.\n\n**When They Excel**:\n\n- High-dimensional data (many features relative to observations)\n- Text classification\n- When you have well-separated classes\n\n**Key Limitation**: Computationally expensive for large datasets; less popular than random forests and gradient boosting in modern practice.\n\n### k-Nearest Neighbors (k-NN)\n\n**What It Does**: Classifies observations based on the majority class of their k nearest neighbors in feature space. It doesn't build an explicit model; it just memorizes training data.\n\n**When It Excels**:\n\n- Simple baseline for classification\n- When decision boundaries are very irregular\n- Recommendation systems (collaborative filtering)\n\n**Key Limitations**:\n\n- Slow predictions (must compare to all training data)\n- Struggles with high-dimensional data (curse of dimensionality)\n- Very sensitive to feature scaling\n\n### Ensemble Stacking\n\n**What It Does**: Combines multiple different algorithms (e.g., logistic regression + random forest + XGBoost) by training a meta-model that learns how to weight each base model's predictions.\n\n**When It Excels**:\n\n- Kaggle competitions (often wins)\n- When you need absolute maximum performance\n- When different models capture different patterns\n\n**Key Limitations**:\n\n- Complex to implement and maintain\n- Risk of overfitting if not done carefully\n- Difficult to explain to stakeholders\n\n### Naive Bayes\n\n**What It Does**: Probabilistic classifier based on Bayes' theorem, assuming features are independent (which is often naive, hence the name).\n\n**When It Excels**:\n\n- Text classification (spam detection, sentiment analysis)\n- Very fast training and prediction\n- Works well with small datasets\n\n**Key Limitations**:\n\n- Independence assumption rarely holds\n- Outperformed by modern methods in most cases\n- Still useful as a fast baseline\n\n## Unifying Concepts: The Big Picture\n\nAs you've learned about these various algorithms, you might notice they share common themes:\n\n### All Models Balance Bias and Variance\n\n- **High bias** (underfitting): Model is too simple, misses patterns\n- **High variance** (overfitting): Model is too complex, memorizes noise\n\nDifferent algorithms address this tradeoff differently:\n\n- Linear models: typically high bias (unless many features)\n- Decision trees: high variance (unless pruned or ensembled)\n- Random forests: reduce variance through averaging\n- Gradient boosting: reduce bias by iteratively correcting mistakes\n- Neural networks: can have either problem; regularization is critical\n\n### All Models Need Proper Evaluation\n\nThe cross-validation principles you learned apply to every algorithm:\n\n- Always use holdout test sets\n- Use k-fold cross-validation for robust estimates\n- Choose metrics appropriate to your problem (accuracy, RMSE, ROC-AUC, etc.)\n- Watch for overfitting (training performance â‰« test performance)\n\n### Feature Engineering Often Matters More Than Algorithm Choice\n\nIn real-world applications, spending time on good feature engineering often improves performance more than switching from random forest to XGBoost or neural networks:\n\n- Domain-specific features capture expert knowledge\n- Proper handling of missing data prevents information loss\n- Feature scaling ensures algorithms work properly\n- Dimensionality reduction (PCA) can improve speed and performance\n\n### Interpretability vs. Performance Tradeoff\n\nThere's often a tradeoff between model interpretability and predictive performance:\n\n```\nMore Interpretable                          More Performant\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nLinear Regression  â†’  Decision Tree  â†’  Random Forest  â†’\nLogistic Regression  â†’  XGBoost  â†’  Neural Networks  â†’\n                                    Deep Learning Ensembles\n```\n\n**Choose interpretable models when**:\n- You need to explain decisions to regulators or customers\n- Domain experts need to validate the model logic\n- The cost of errors is very high (healthcare, finance)\n\n**Choose complex models when**:\n- Prediction accuracy is paramount\n- You can afford to treat the model as a black box\n- You have enough data to validate performance thoroughly\n\n::: {.callout-tip}\n## A Visual Framework: When to Use What\n\nHere's a practical decision tree for choosing algorithms:\n\n**Do you have structured/tabular data?**\n- **Yes** â†’ Random Forest or XGBoost (rarely neural networks)\n- **No** (images, text, audio) â†’ Neural Networks / Deep Learning\n\n**Within structured data:**\n- **Need interpretability?** â†’ Linear/Logistic Regression or single Decision Tree\n- **Small dataset (< 10,000 rows)?** â†’ Random Forest (more robust)\n- **Large dataset with tuning resources?** â†’ XGBoost (often best performance)\n- **Very large dataset (millions of rows)?** â†’ LightGBM (faster than XGBoost)\n\n**For unstructured data:**\n- **Images?** â†’ Convolutional Neural Networks (CNNs)\n- **Text?** â†’ Transformers (BERT, GPT) or simpler RNNs/LSTMs\n- **Time series?** â†’ LSTMs, GRUs, or classical methods (ARIMA)\n- **Audio/Video?** â†’ Specialized deep learning architectures\n\nThis isn't rigid. Always experiment with multiple approaches and let cross-validation guide your final choice.\n:::\n\n## Looking Forward: Continuing Your ML Journey\n\nYou've now been introduced to the major families of machine learning algorithms:\n\n- **Classical Supervised Learning**: Linear/logistic regression, decision trees, random forests\n- **Advanced Ensemble Methods**: Gradient boosting (XGBoost, LightGBM, CatBoost)\n- **Neural Networks and Deep Learning**: Feedforward networks, CNNs, RNNs, Transformers\n- **Unsupervised Learning**: Clustering, dimension reduction (PCA)\n\nThe next chapter will provide a roadmap for where to go from here, including specific learning paths, resources, and career directions based on your interests.\n\n::: {.callout-important}\n## Key Takeaways\n\n1. **Gradient boosting** (XGBoost, LightGBM) often provides the best performance on structured data, especially after careful tuning.\n\n2. **Neural networks and deep learning** excel with unstructured data (images, text, audio) and very large datasets, but often don't outperform gradient boosting on typical tabular business data.\n\n3. **Classical methods** (random forests, logistic regression) remain powerful and should always be your starting point. They're easier to interpret, faster to train, and often perform nearly as well as more complex methods.\n\n4. **Algorithm choice matters less than you think**. Proper feature engineering, cross-validation, and hyperparameter tuning matter more.\n\n5. **Generative AI** (ChatGPT, DALL-E) represents the frontier of deep learning, built on massive neural networks with billions of parameters.\n\n6. **Start simple, add complexity only when needed**. The best model is the simplest one that achieves your performance requirements.\n:::\n\n## Exercises\n\n::: {.callout-note}\n## Reflection Questions\n\n1. **Compare and Contrast**: How is gradient boosting fundamentally different from random forests in how it builds its ensemble? When might you choose one over the other?\n\n2. **Neural Network Intuition**: Explain how a neural network relates to logistic regression. What does adding hidden layers enable the model to do that logistic regression cannot?\n\n3. **Tradeoffs**: Why don't we use deep learning for every problem? What are the practical constraints that make classical ML methods often preferable for business applications?\n\n4. **Generative AI Connection**: How do the neural network principles discussed in this chapter relate to generative AI systems like ChatGPT? What additional components make generation possible?\n:::\n\n::: {.callout-tip}\n## For the Curious: Going Deeper\n\nWant to explore these advanced methods further? Here are recommended next steps:\n\n**Gradient Boosting**:\n- XGBoost documentation and tutorials: https://xgboost.readthedocs.io/\n- \"Hands-On Gradient Boosting with XGBoost and Scikit-learn\" by Corey Wade\n\n**Neural Networks and Deep Learning**:\n- Fast.ai's Practical Deep Learning for Coders (free): https://course.fast.ai/\n- Andrew Ng's Deep Learning Specialization (Coursera)\n- \"Deep Learning with Python\" by FranÃ§ois Chollet (creator of Keras)\n\n**Generative AI**:\n- OpenAI's GPT documentation and API: https://platform.openai.com/docs/\n- \"Generative Deep Learning\" by David Foster\n- Hugging Face Transformers library: https://huggingface.co/docs/transformers/\n\n**Broader ML Learning**:\n- Kaggle competitions (apply your skills, learn from others): https://www.kaggle.com/competitions\n- Google's Machine Learning Crash Course: https://developers.google.com/machine-learning/crash-course\n:::\n\nYou've built a strong foundation in machine learning fundamentals. The algorithms discussed in this chapter represent the next level, techniques that build directly on what you've learned and extend your capabilities to handle increasingly complex problems. In the next chapter, we'll chart a course for your continued learning journey beyond this course.\n\n",
    "supporting": [
      "33-modern-ml-algorithms_files"
    ],
    "filters": [],
    "includes": {}
  }
}